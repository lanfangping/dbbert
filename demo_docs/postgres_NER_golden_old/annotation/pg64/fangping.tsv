#FORMAT=WebAnno TSV 3.3


#Text=Apache Flink 1.12 Documentation: JDBC SQL Connector v1.12 Home Try Flink Local Installation Fraud Detection with the DataStream API Real Time Reporting with the Table API
1-1	0-6	Apache	
1-2	7-12	Flink	
1-3	13-17	1.12	
1-4	18-31	Documentation	
1-5	31-32	:	
1-6	33-37	JDBC	
1-7	38-41	SQL	
1-8	42-51	Connector	
1-9	52-57	v1.12	
1-10	58-62	Home	
1-11	63-66	Try	
1-12	67-72	Flink	
1-13	73-78	Local	
1-14	79-91	Installation	
1-15	92-97	Fraud	
1-16	98-107	Detection	
1-17	108-112	with	
1-18	113-116	the	
1-19	117-127	DataStream	
1-20	128-131	API	
1-21	132-136	Real	
1-22	137-141	Time	
1-23	142-151	Reporting	
1-24	152-156	with	
1-25	157-160	the	
1-26	161-166	Table	
1-27	167-170	API	

#Text=Flink Operations Playground Learn Flink Overview Intro to the DataStream API Data Pipelines & ETL Streaming Analytics Event-driven Applications Fault Tolerance Concepts Overview Stateful Stream Processing
2-1	171-176	Flink	
2-2	177-187	Operations	
2-3	188-198	Playground	
2-4	199-204	Learn	
2-5	205-210	Flink	
2-6	211-219	Overview	
2-7	220-225	Intro	
2-8	226-228	to	
2-9	229-232	the	
2-10	233-243	DataStream	
2-11	244-247	API	
2-12	248-252	Data	
2-13	253-262	Pipelines	
2-14	263-264	&	
2-15	265-268	ETL	
2-16	269-278	Streaming	
2-17	279-288	Analytics	
2-18	289-301	Event-driven	
2-19	302-314	Applications	
2-20	315-320	Fault	
2-21	321-330	Tolerance	
2-22	331-339	Concepts	
2-23	340-348	Overview	
2-24	349-357	Stateful	
2-25	358-364	Stream	
2-26	365-375	Processing	

#Text=Timely Stream Processing Flink Architecture Glossary Application Development DataStream API Overview Execution Mode (Batch/Streaming) Event Time Overview Generating Watermarks Builtin Watermark Generators
3-1	376-382	Timely	
3-2	383-389	Stream	
3-3	390-400	Processing	
3-4	401-406	Flink	
3-5	407-419	Architecture	
3-6	420-428	Glossary	
3-7	429-440	Application	
3-8	441-452	Development	
3-9	453-463	DataStream	
3-10	464-467	API	
3-11	468-476	Overview	
3-12	477-486	Execution	
3-13	487-491	Mode	
3-14	492-493	(	
3-15	493-498	Batch	
3-16	498-499	/	
3-17	499-508	Streaming	
3-18	508-509	)	
3-19	510-515	Event	
3-20	516-520	Time	
3-21	521-529	Overview	
3-22	530-540	Generating	
3-23	541-551	Watermarks	
3-24	552-559	Builtin	
3-25	560-569	Watermark	
3-26	570-580	Generators	

#Text=State & Fault Tolerance Overview Working with State The Broadcast State Pattern Checkpointing Queryable State State Backends State Schema Evolution Custom State Serialization User-Defined Functions Operators Overview
4-1	581-586	State	
4-2	587-588	&	
4-3	589-594	Fault	
4-4	595-604	Tolerance	
4-5	605-613	Overview	
4-6	614-621	Working	
4-7	622-626	with	
4-8	627-632	State	
4-9	633-636	The	
4-10	637-646	Broadcast	
4-11	647-652	State	
4-12	653-660	Pattern	
4-13	661-674	Checkpointing	
4-14	675-684	Queryable	
4-15	685-690	State	
4-16	691-696	State	
4-17	697-705	Backends	
4-18	706-711	State	
4-19	712-718	Schema	
4-20	719-728	Evolution	
4-21	729-735	Custom	
4-22	736-741	State	
4-23	742-755	Serialization	
4-24	756-768	User-Defined	
4-25	769-778	Functions	
4-26	779-788	Operators	
4-27	789-797	Overview	

#Text=Windows Joining Process Function Async I/O Data Sources Side Outputs Handling Application Parameters Testing Experimental Features Scala API Extensions Java Lambda Expressions Project Configuration
5-1	798-805	Windows	
5-2	806-813	Joining	
5-3	814-821	Process	
5-4	822-830	Function	
5-5	831-836	Async	
5-6	837-838	I	
5-7	838-839	/	
5-8	839-840	O	
5-9	841-845	Data	
5-10	846-853	Sources	
5-11	854-858	Side	
5-12	859-866	Outputs	
5-13	867-875	Handling	
5-14	876-887	Application	
5-15	888-898	Parameters	
5-16	899-906	Testing	
5-17	907-919	Experimental	
5-18	920-928	Features	
5-19	929-934	Scala	
5-20	935-938	API	
5-21	939-949	Extensions	
5-22	950-954	Java	
5-23	955-961	Lambda	
5-24	962-973	Expressions	
5-25	974-981	Project	
5-26	982-995	Configuration	

#Text=DataSet API Overview Transformations Iterations Zipping Elements Hadoop Compatibility Local Execution Cluster Execution Batch Examples Table API & SQL Overview Concepts & Common API
6-1	996-1003	DataSet	
6-2	1004-1007	API	
6-3	1008-1016	Overview	
6-4	1017-1032	Transformations	
6-5	1033-1043	Iterations	
6-6	1044-1051	Zipping	
6-7	1052-1060	Elements	
6-8	1061-1067	Hadoop	
6-9	1068-1081	Compatibility	
6-10	1082-1087	Local	
6-11	1088-1097	Execution	
6-12	1098-1105	Cluster	
6-13	1106-1115	Execution	
6-14	1116-1121	Batch	
6-15	1122-1130	Examples	
6-16	1131-1136	Table	
6-17	1137-1140	API	
6-18	1141-1142	&	
6-19	1143-1146	SQL	
6-20	1147-1155	Overview	
6-21	1156-1164	Concepts	
6-22	1165-1166	&	
6-23	1167-1173	Common	
6-24	1174-1177	API	

#Text=Streaming Concepts Overview Dynamic Tables Time Attributes Versioned Tables Joins in Continuous Queries Detecting Patterns Query Configuration Legacy Features Data Types Table API SQL
7-1	1178-1187	Streaming	
7-2	1188-1196	Concepts	
7-3	1197-1205	Overview	
7-4	1206-1213	Dynamic	
7-5	1214-1220	Tables	
7-6	1221-1225	Time	
7-7	1226-1236	Attributes	
7-8	1237-1246	Versioned	
7-9	1247-1253	Tables	
7-10	1254-1259	Joins	
7-11	1260-1262	in	
7-12	1263-1273	Continuous	
7-13	1274-1281	Queries	
7-14	1282-1291	Detecting	
7-15	1292-1300	Patterns	
7-16	1301-1306	Query	
7-17	1307-1320	Configuration	
7-18	1321-1327	Legacy	
7-19	1328-1336	Features	
7-20	1337-1341	Data	
7-21	1342-1347	Types	
7-22	1348-1353	Table	
7-23	1354-1357	API	
7-24	1358-1361	SQL	

#Text=Overview Queries CREATE Statements DROP Statements ALTER Statements INSERT Statement SQL Hints DESCRIBE Statements EXPLAIN Statements USE Statements SHOW Statements
8-1	1362-1370	Overview	
8-2	1371-1378	Queries	
8-3	1379-1385	CREATE	
8-4	1386-1396	Statements	
8-5	1397-1401	DROP	
8-6	1402-1412	Statements	
8-7	1413-1418	ALTER	
8-8	1419-1429	Statements	
8-9	1430-1436	INSERT	
8-10	1437-1446	Statement	
8-11	1447-1450	SQL	
8-12	1451-1456	Hints	
8-13	1457-1465	DESCRIBE	
8-14	1466-1476	Statements	
8-15	1477-1484	EXPLAIN	
8-16	1485-1495	Statements	
8-17	1496-1499	USE	
8-18	1500-1510	Statements	
8-19	1511-1515	SHOW	
8-20	1516-1526	Statements	

#Text=Functions Overview System (Built-in) Functions User-defined Functions Modules Catalogs SQL Client Configuration Performance Tuning Streaming Aggregation User-defined Sources & Sinks
9-1	1527-1536	Functions	
9-2	1537-1545	Overview	
9-3	1546-1552	System	
9-4	1553-1554	(	
9-5	1554-1562	Built-in	
9-6	1562-1563	)	
9-7	1564-1573	Functions	
9-8	1574-1586	User-defined	
9-9	1587-1596	Functions	
9-10	1597-1604	Modules	
9-11	1605-1613	Catalogs	
9-12	1614-1617	SQL	
9-13	1618-1624	Client	
9-14	1625-1638	Configuration	
9-15	1639-1650	Performance	
9-16	1651-1657	Tuning	
9-17	1658-1667	Streaming	
9-18	1668-1679	Aggregation	
9-19	1680-1692	User-defined	
9-20	1693-1700	Sources	
9-21	1701-1702	&	
9-22	1703-1708	Sinks	

#Text=Python API Overview Installation Table API Tutorial DataStream API Tutorial Table API User's Guide Intro to the Python Table API TableEnvironment Operations Data Types
10-1	1709-1715	Python	
10-2	1716-1719	API	
10-3	1720-1728	Overview	
10-4	1729-1741	Installation	
10-5	1742-1747	Table	
10-6	1748-1751	API	
10-7	1752-1760	Tutorial	
10-8	1761-1771	DataStream	
10-9	1772-1775	API	
10-10	1776-1784	Tutorial	
10-11	1785-1790	Table	
10-12	1791-1794	API	
10-13	1795-1801	User's	
10-14	1802-1807	Guide	
10-15	1808-1813	Intro	
10-16	1814-1816	to	
10-17	1817-1820	the	
10-18	1821-1827	Python	
10-19	1828-1833	Table	
10-20	1834-1837	API	
10-21	1838-1854	TableEnvironment	
10-22	1855-1865	Operations	
10-23	1866-1870	Data	
10-24	1871-1876	Types	

#Text=System (Built-in) Functions User Defined Functions General User-defined Functions Vectorized User-defined Functions Conversions between PyFlink Table and Pandas DataFrame Dependency Management SQL Catalogs
11-1	1877-1883	System	
11-2	1884-1885	(	
11-3	1885-1893	Built-in	
11-4	1893-1894	)	
11-5	1895-1904	Functions	
11-6	1905-1909	User	
11-7	1910-1917	Defined	
11-8	1918-1927	Functions	
11-9	1928-1935	General	
11-10	1936-1948	User-defined	
11-11	1949-1958	Functions	
11-12	1959-1969	Vectorized	
11-13	1970-1982	User-defined	
11-14	1983-1992	Functions	
11-15	1993-2004	Conversions	
11-16	2005-2012	between	
11-17	2013-2020	PyFlink	
11-18	2021-2026	Table	
11-19	2027-2030	and	
11-20	2031-2037	Pandas	
11-21	2038-2047	DataFrame	
11-22	2048-2058	Dependency	
11-23	2059-2069	Management	
11-24	2070-2073	SQL	
11-25	2074-2082	Catalogs	

#Text=Metrics Connectors DataStream API User's Guide Data Types Operators Dependency Management Configuration Environment Variables FAQ Data Types & Serialization Overview
12-1	2083-2090	Metrics	
12-2	2091-2101	Connectors	
12-3	2102-2112	DataStream	
12-4	2113-2116	API	
12-5	2117-2123	User's	
12-6	2124-2129	Guide	
12-7	2130-2134	Data	
12-8	2135-2140	Types	
12-9	2141-2150	Operators	
12-10	2151-2161	Dependency	
12-11	2162-2172	Management	
12-12	2173-2186	Configuration	
12-13	2187-2198	Environment	
12-14	2199-2208	Variables	
12-15	2209-2212	FAQ	
12-16	2213-2217	Data	
12-17	2218-2223	Types	
12-18	2224-2225	&	
12-19	2226-2239	Serialization	
12-20	2240-2248	Overview	

#Text=Custom Serializers Managing Execution Execution Configuration Program Packaging Parallel Execution Execution Plans Task Failure Recovery API Migration Guides Libraries Event Processing (CEP) State Processor API
13-1	2249-2255	Custom	
13-2	2256-2267	Serializers	
13-3	2268-2276	Managing	
13-4	2277-2286	Execution	
13-5	2287-2296	Execution	
13-6	2297-2310	Configuration	
13-7	2311-2318	Program	
13-8	2319-2328	Packaging	
13-9	2329-2337	Parallel	
13-10	2338-2347	Execution	
13-11	2348-2357	Execution	
13-12	2358-2363	Plans	
13-13	2364-2368	Task	
13-14	2369-2376	Failure	
13-15	2377-2385	Recovery	
13-16	2386-2389	API	
13-17	2390-2399	Migration	
13-18	2400-2406	Guides	
13-19	2407-2416	Libraries	
13-20	2417-2422	Event	
13-21	2423-2433	Processing	
13-22	2434-2435	(	
13-23	2435-2438	CEP	
13-24	2438-2439	)	
13-25	2440-2445	State	
13-26	2446-2455	Processor	
13-27	2456-2459	API	

#Text=Graphs: Gelly Overview Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators Bipartite Graph Connectors DataStream Connectors
14-1	2460-2466	Graphs	
14-2	2466-2467	:	
14-3	2468-2473	Gelly	
14-4	2474-2482	Overview	
14-5	2483-2488	Graph	
14-6	2489-2492	API	
14-7	2493-2502	Iterative	
14-8	2503-2508	Graph	
14-9	2509-2519	Processing	
14-10	2520-2527	Library	
14-11	2528-2535	Methods	
14-12	2536-2541	Graph	
14-13	2542-2552	Algorithms	
14-14	2553-2558	Graph	
14-15	2559-2569	Generators	
14-16	2570-2579	Bipartite	
14-17	2580-2585	Graph	
14-18	2586-2596	Connectors	
14-19	2597-2607	DataStream	
14-20	2608-2618	Connectors	

#Text=Overview Fault Tolerance Guarantees Kafka Cassandra Kinesis Elasticsearch File Sink Streaming File Sink RabbitMQ NiFi Google Cloud PubSub Twitter
15-1	2619-2627	Overview	
15-2	2628-2633	Fault	
15-3	2634-2643	Tolerance	
15-4	2644-2654	Guarantees	
15-5	2655-2660	Kafka	
15-6	2661-2670	Cassandra	
15-7	2671-2678	Kinesis	
15-8	2679-2692	Elasticsearch	
15-9	2693-2697	File	
15-10	2698-2702	Sink	
15-11	2703-2712	Streaming	
15-12	2713-2717	File	
15-13	2718-2722	Sink	
15-14	2723-2731	RabbitMQ	
15-15	2732-2736	NiFi	
15-16	2737-2743	Google	
15-17	2744-2749	Cloud	
15-18	2750-2756	PubSub	
15-19	2757-2764	Twitter	

#Text=JDBC Table & SQL Connectors Overview Formats Overview CSV JSON Confluent Avro Avro Debezium Canal Maxwell Parquet Orc
16-1	2765-2769	JDBC	
16-2	2770-2775	Table	
16-3	2776-2777	&	
16-4	2778-2781	SQL	
16-5	2782-2792	Connectors	
16-6	2793-2801	Overview	
16-7	2802-2809	Formats	
16-8	2810-2818	Overview	
16-9	2819-2822	CSV	
16-10	2823-2827	JSON	
16-11	2828-2837	Confluent	
16-12	2838-2842	Avro	
16-13	2843-2847	Avro	
16-14	2848-2856	Debezium	
16-15	2857-2862	Canal	
16-16	2863-2870	Maxwell	
16-17	2871-2878	Parquet	
16-18	2879-2882	Orc	

#Text=Raw Kafka Upsert Kafka Kinesis JDBC Elasticsearch FileSystem HBase DataGen Print BlackHole Hive Overview Hive Catalog
17-1	2883-2886	Raw	
17-2	2887-2892	Kafka	
17-3	2893-2899	Upsert	
17-4	2900-2905	Kafka	
17-5	2906-2913	Kinesis	
17-6	2914-2918	JDBC	
17-7	2919-2932	Elasticsearch	
17-8	2933-2943	FileSystem	
17-9	2944-2949	HBase	
17-10	2950-2957	DataGen	
17-11	2958-2963	Print	
17-12	2964-2973	BlackHole	
17-13	2974-2978	Hive	
17-14	2979-2987	Overview	
17-15	2988-2992	Hive	
17-16	2993-3000	Catalog	

#Text=Hive Dialect Hive Read & Write Hive Functions Download DataSet Connectors Deployment Overview Resource Providers Standalone Overview Docker Kubernetes
18-1	3001-3005	Hive	
18-2	3006-3013	Dialect	
18-3	3014-3018	Hive	
18-4	3019-3023	Read	
18-5	3024-3025	&	
18-6	3026-3031	Write	
18-7	3032-3036	Hive	
18-8	3037-3046	Functions	
18-9	3047-3055	Download	
18-10	3056-3063	DataSet	
18-11	3064-3074	Connectors	
18-12	3075-3085	Deployment	
18-13	3086-3094	Overview	
18-14	3095-3103	Resource	
18-15	3104-3113	Providers	
18-16	3114-3124	Standalone	
18-17	3125-3133	Overview	
18-18	3134-3140	Docker	
18-19	3141-3151	Kubernetes	

#Text=Native Kubernetes YARN Mesos Configuration Memory Configuration Set up Flink's Process Memory Set up TaskManager Memory Set up JobManager Memory Memory tuning guide Troubleshooting
19-1	3152-3158	Native	
19-2	3159-3169	Kubernetes	
19-3	3170-3174	YARN	
19-4	3175-3180	Mesos	
19-5	3181-3194	Configuration	
19-6	3195-3201	Memory	
19-7	3202-3215	Configuration	
19-8	3216-3219	Set	
19-9	3220-3222	up	
19-10	3223-3230	Flink's	
19-11	3231-3238	Process	
19-12	3239-3245	Memory	
19-13	3246-3249	Set	
19-14	3250-3252	up	
19-15	3253-3264	TaskManager	
19-16	3265-3271	Memory	
19-17	3272-3275	Set	
19-18	3276-3278	up	
19-19	3279-3289	JobManager	
19-20	3290-3296	Memory	
19-21	3297-3303	Memory	
19-22	3304-3310	tuning	
19-23	3311-3316	guide	
19-24	3317-3332	Troubleshooting	

#Text=Migration Guide Command-Line Interface File Systems Overview Common Configurations Amazon S3 Aliyun OSS Azure Blob Storage Plugins High Availability (HA) Overview
20-1	3333-3342	Migration	
20-2	3343-3348	Guide	
20-3	3349-3361	Command-Line	
20-4	3362-3371	Interface	
20-5	3372-3376	File	
20-6	3377-3384	Systems	
20-7	3385-3393	Overview	
20-8	3394-3400	Common	
20-9	3401-3415	Configurations	
20-10	3416-3422	Amazon	
20-11	3423-3425	S3	
20-12	3426-3432	Aliyun	
20-13	3433-3436	OSS	
20-14	3437-3442	Azure	
20-15	3443-3447	Blob	
20-16	3448-3455	Storage	
20-17	3456-3463	Plugins	
20-18	3464-3468	High	
20-19	3469-3481	Availability	
20-20	3482-3483	(	
20-21	3483-3485	HA	
20-22	3485-3486	)	
20-23	3487-3495	Overview	

#Text=ZooKeeper HA Services Kubernetes HA Services Metric Reporters Security SSL Setup Kerberos REPLs Python REPL Scala REPL Advanced External Resources History Server Logging
21-1	3496-3505	ZooKeeper	
21-2	3506-3508	HA	
21-3	3509-3517	Services	
21-4	3518-3528	Kubernetes	
21-5	3529-3531	HA	
21-6	3532-3540	Services	
21-7	3541-3547	Metric	
21-8	3548-3557	Reporters	
21-9	3558-3566	Security	
21-10	3567-3570	SSL	
21-11	3571-3576	Setup	
21-12	3577-3585	Kerberos	
21-13	3586-3591	REPLs	
21-14	3592-3598	Python	
21-15	3599-3603	REPL	
21-16	3604-3609	Scala	
21-17	3610-3614	REPL	
21-18	3615-3623	Advanced	
21-19	3624-3632	External	
21-20	3633-3642	Resources	
21-21	3643-3650	History	
21-22	3651-3657	Server	
21-23	3658-3665	Logging	

#Text=Operations State & Fault Tolerance Checkpoints Savepoints State Backends Tuning Checkpoints and Large State Metrics REST API Debugging Debugging Windows & Event Time Debugging Classloading
22-1	3666-3676	Operations	
22-2	3677-3682	State	
22-3	3683-3684	&	
22-4	3685-3690	Fault	
22-5	3691-3700	Tolerance	
22-6	3701-3712	Checkpoints	
22-7	3713-3723	Savepoints	
22-8	3724-3729	State	
22-9	3730-3738	Backends	
22-10	3739-3745	Tuning	
22-11	3746-3757	Checkpoints	
22-12	3758-3761	and	
22-13	3762-3767	Large	
22-14	3768-3773	State	
22-15	3774-3781	Metrics	
22-16	3782-3786	REST	
22-17	3787-3790	API	
22-18	3791-3800	Debugging	
22-19	3801-3810	Debugging	
22-20	3811-3818	Windows	
22-21	3819-3820	&	
22-22	3821-3826	Event	
22-23	3827-3831	Time	
22-24	3832-3841	Debugging	
22-25	3842-3854	Classloading	

#Text=Application Profiling & Debugging Monitoring Monitoring Checkpointing Monitoring Back Pressure Upgrading Applications and Flink Versions Production Readiness Checklist Flink Development Importing Flink into an IDE Building Flink from Source Internals
23-1	3855-3866	Application	
23-2	3867-3876	Profiling	
23-3	3877-3878	&	
23-4	3879-3888	Debugging	
23-5	3889-3899	Monitoring	
23-6	3900-3910	Monitoring	
23-7	3911-3924	Checkpointing	
23-8	3925-3935	Monitoring	
23-9	3936-3940	Back	
23-10	3941-3949	Pressure	
23-11	3950-3959	Upgrading	
23-12	3960-3972	Applications	
23-13	3973-3976	and	
23-14	3977-3982	Flink	
23-15	3983-3991	Versions	
23-16	3992-4002	Production	
23-17	4003-4012	Readiness	
23-18	4013-4022	Checklist	
23-19	4023-4028	Flink	
23-20	4029-4040	Development	
23-21	4041-4050	Importing	
23-22	4051-4056	Flink	
23-23	4057-4061	into	
23-24	4062-4064	an	
23-25	4065-4068	IDE	
23-26	4069-4077	Building	
23-27	4078-4083	Flink	
23-28	4084-4088	from	
23-29	4089-4095	Source	
23-30	4096-4105	Internals	

#Text=Jobs and Scheduling Task Lifecycle File Systems Javadocs Scaladocs Pythondocs Project Page Pick Docs Version v1.11 v1.10 v1.9 v1.8 v1.7
24-1	4106-4110	Jobs	
24-2	4111-4114	and	
24-3	4115-4125	Scheduling	
24-4	4126-4130	Task	
24-5	4131-4140	Lifecycle	
24-6	4141-4145	File	
24-7	4146-4153	Systems	
24-8	4154-4162	Javadocs	
24-9	4163-4172	Scaladocs	
24-10	4173-4183	Pythondocs	
24-11	4184-4191	Project	
24-12	4192-4196	Page	
24-13	4197-4201	Pick	
24-14	4202-4206	Docs	
24-15	4207-4214	Version	
24-16	4215-4220	v1.11	
24-17	4221-4226	v1.10	
24-18	4227-4231	v1.9	
24-19	4232-4236	v1.8	
24-20	4237-4241	v1.7	

#Text=v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 v1.0 中文版 Connectors Table & SQL Connectors JDBC
25-1	4242-4246	v1.6	
25-2	4247-4251	v1.5	
25-3	4252-4256	v1.4	
25-4	4257-4261	v1.3	
25-5	4262-4266	v1.2	
25-6	4267-4271	v1.1	
25-7	4272-4276	v1.0	
25-8	4277-4280	中文版	
25-9	4281-4291	Connectors	
25-10	4292-4297	Table	
25-11	4298-4299	&	
25-12	4300-4303	SQL	
25-13	4304-4314	Connectors	
25-14	4315-4319	JDBC	

#Text=JDBC SQL Connector Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Append & Upsert Mode Dependencies How to create a JDBC table Connector Options
26-1	4320-4324	JDBC	
26-2	4325-4328	SQL	
26-3	4329-4338	Connector	
26-4	4339-4343	Scan	
26-5	4344-4350	Source	
26-6	4350-4351	:	
26-7	4352-4359	Bounded	
26-8	4360-4366	Lookup	
26-9	4367-4373	Source	
26-10	4373-4374	:	
26-11	4375-4379	Sync	
26-12	4380-4384	Mode	
26-13	4385-4389	Sink	
26-14	4389-4390	:	
26-15	4391-4396	Batch	
26-16	4397-4401	Sink	
26-17	4401-4402	:	
26-18	4403-4412	Streaming	
26-19	4413-4419	Append	
26-20	4420-4421	&	
26-21	4422-4428	Upsert	
26-22	4429-4433	Mode	
26-23	4434-4446	Dependencies	
26-24	4447-4450	How	
26-25	4451-4453	to	
26-26	4454-4460	create	
26-27	4461-4462	a	
26-28	4463-4467	JDBC	
26-29	4468-4473	table	
26-30	4474-4483	Connector	
26-31	4484-4491	Options	

#Text=Features Key handling Partitioned Scan Lookup Cache Idempotent Writes Postgres Database as a Catalog Data Type Mapping
27-1	4492-4500	Features	
27-2	4501-4504	Key	
27-3	4505-4513	handling	
27-4	4514-4525	Partitioned	
27-5	4526-4530	Scan	
27-6	4531-4537	Lookup	
27-7	4538-4543	Cache	
27-8	4544-4554	Idempotent	
27-9	4555-4561	Writes	
27-10	4562-4570	Postgres	
27-11	4571-4579	Database	
27-12	4580-4582	as	
27-13	4583-4584	a	
27-14	4585-4592	Catalog	
27-15	4593-4597	Data	
27-16	4598-4602	Type	
27-17	4603-4610	Mapping	

#Text=The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver. This document describes how to setup the JDBC connector to run SQL queries against relational databases. The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn’t support to consume UPDATE/DELETE messages. Dependencies
28-1	4611-4614	The	
28-2	4615-4619	JDBC	
28-3	4620-4629	connector	
28-4	4630-4636	allows	
28-5	4637-4640	for	
28-6	4641-4648	reading	
28-7	4649-4653	data	
28-8	4654-4658	from	
28-9	4659-4662	and	
28-10	4663-4670	writing	
28-11	4671-4675	data	
28-12	4676-4680	into	
28-13	4681-4684	any	
28-14	4685-4695	relational	
28-15	4696-4705	databases	
28-16	4706-4710	with	
28-17	4711-4712	a	
28-18	4713-4717	JDBC	
28-19	4718-4724	driver	
28-20	4724-4725	.	
28-21	4726-4730	This	
28-22	4731-4739	document	
28-23	4740-4749	describes	
28-24	4750-4753	how	
28-25	4754-4756	to	
28-26	4757-4762	setup	
28-27	4763-4766	the	
28-28	4767-4771	JDBC	
28-29	4772-4781	connector	
28-30	4782-4784	to	
28-31	4785-4788	run	
28-32	4789-4792	SQL	
28-33	4793-4800	queries	
28-34	4801-4808	against	
28-35	4809-4819	relational	
28-36	4820-4829	databases	
28-37	4829-4830	.	
28-38	4831-4834	The	
28-39	4835-4839	JDBC	
28-40	4840-4844	sink	
28-41	4845-4852	operate	
28-42	4853-4855	in	
28-43	4856-4862	upsert	
28-44	4863-4867	mode	
28-45	4868-4871	for	
28-46	4872-4880	exchange	
28-47	4881-4887	UPDATE	
28-48	4887-4888	/	
28-49	4888-4894	DELETE	
28-50	4895-4903	messages	
28-51	4904-4908	with	
28-52	4909-4912	the	
28-53	4913-4921	external	
28-54	4922-4928	system	
28-55	4929-4931	if	
28-56	4932-4933	a	
28-57	4934-4941	primary	
28-58	4942-4945	key	
28-59	4946-4948	is	
28-60	4949-4956	defined	
28-61	4957-4959	on	
28-62	4960-4963	the	
28-63	4964-4967	DDL	
28-64	4967-4968	,	
28-65	4969-4978	otherwise	
28-66	4978-4979	,	
28-67	4980-4982	it	
28-68	4983-4991	operates	
28-69	4992-4994	in	
28-70	4995-5001	append	
28-71	5002-5006	mode	
28-72	5007-5010	and	
28-73	5011-5016	doesn	
28-74	5016-5017	’	
28-75	5017-5018	t	
28-76	5019-5026	support	
28-77	5027-5029	to	
28-78	5030-5037	consume	
28-79	5038-5044	UPDATE	
28-80	5044-5045	/	
28-81	5045-5051	DELETE	
28-82	5052-5060	messages	
28-83	5060-5061	.	
28-84	5062-5074	Dependencies	

#Text=In order to use the JDBC connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles. Maven dependency SQL Client JAR <dependency>
29-1	5075-5077	In	
29-2	5078-5083	order	
29-3	5084-5086	to	
29-4	5087-5090	use	
29-5	5091-5094	the	
29-6	5095-5099	JDBC	
29-7	5100-5109	connector	
29-8	5110-5113	the	
29-9	5114-5123	following	
29-10	5124-5136	dependencies	
29-11	5137-5140	are	
29-12	5141-5149	required	
29-13	5150-5153	for	
29-14	5154-5158	both	
29-15	5159-5167	projects	
29-16	5168-5173	using	
29-17	5174-5175	a	
29-18	5176-5181	build	
29-19	5182-5192	automation	
29-20	5193-5197	tool	
29-21	5198-5199	(	
29-22	5199-5203	such	
29-23	5204-5206	as	
29-24	5207-5212	Maven	
29-25	5213-5215	or	
29-26	5216-5219	SBT	
29-27	5219-5220	)	
29-28	5221-5224	and	
29-29	5225-5228	SQL	
29-30	5229-5235	Client	
29-31	5236-5240	with	
29-32	5241-5244	SQL	
29-33	5245-5248	JAR	
29-34	5249-5256	bundles	
29-35	5256-5257	.	
29-36	5258-5263	Maven	
29-37	5264-5274	dependency	
29-38	5275-5278	SQL	
29-39	5279-5285	Client	
29-40	5286-5289	JAR	
29-41	5290-5291	<	
29-42	5291-5301	dependency	
29-43	5301-5302	>	

#Text=<groupId>org.apache.flink</groupId> <artifactId>flink-connector-jdbc_2.11</artifactId> <version>1.12.0</version> </dependency> Download A driver dependency is also required to connect to a specified database. Here are drivers currently supported:
30-1	5303-5304	<	
30-2	5304-5311	groupId	
30-3	5311-5312	>	
30-4	5312-5328	org.apache.flink	
30-5	5328-5329	<	
30-6	5329-5330	/	
30-7	5330-5337	groupId	
30-8	5337-5338	>	
30-9	5339-5340	<	
30-10	5340-5350	artifactId	
30-11	5350-5351	>	
30-12	5351-5371	flink-connector-jdbc	
30-13	5371-5372	_	
30-14	5372-5376	2.11	
30-15	5376-5377	<	
30-16	5377-5378	/	
30-17	5378-5388	artifactId	
30-18	5388-5389	>	
30-19	5390-5391	<	
30-20	5391-5398	version	
30-21	5398-5399	>	
30-22	5399-5405	1.12.0	
30-23	5405-5406	<	
30-24	5406-5407	/	
30-25	5407-5414	version	
30-26	5414-5415	>	
30-27	5416-5417	<	
30-28	5417-5418	/	
30-29	5418-5428	dependency	
30-30	5428-5429	>	
30-31	5430-5438	Download	
30-32	5439-5440	A	
30-33	5441-5447	driver	
30-34	5448-5458	dependency	
30-35	5459-5461	is	
30-36	5462-5466	also	
30-37	5467-5475	required	
30-38	5476-5478	to	
30-39	5479-5486	connect	
30-40	5487-5489	to	
30-41	5490-5491	a	
30-42	5492-5501	specified	
30-43	5502-5510	database	
30-44	5510-5511	.	
30-45	5512-5516	Here	
30-46	5517-5520	are	
30-47	5521-5528	drivers	
30-48	5529-5538	currently	
30-49	5539-5548	supported	
30-50	5548-5549	:	

#Text=Driver Group Id Artifact Id JAR MySQL mysql mysql-connector-java Download PostgreSQL org.postgresql postgresql
31-1	5550-5556	Driver	
31-2	5557-5562	Group	
31-3	5563-5565	Id	
31-4	5566-5574	Artifact	
31-5	5575-5577	Id	
31-6	5578-5581	JAR	
31-7	5582-5587	MySQL	
31-8	5588-5593	mysql	
31-9	5594-5614	mysql-connector-java	
31-10	5615-5623	Download	
31-11	5624-5634	PostgreSQL	
31-12	5635-5649	org.postgresql	
31-13	5650-5660	postgresql	

#Text=Download Derby org.apache.derby derby Download JDBC connector and drivers are not currently part of Flink’s binary distribution. See how to link with them for cluster execution here. How to create a JDBC table
32-1	5661-5669	Download	
32-2	5670-5675	Derby	
32-3	5676-5692	org.apache.derby	
32-4	5693-5698	derby	
32-5	5699-5707	Download	
32-6	5708-5712	JDBC	
32-7	5713-5722	connector	
32-8	5723-5726	and	
32-9	5727-5734	drivers	
32-10	5735-5738	are	
32-11	5739-5742	not	
32-12	5743-5752	currently	
32-13	5753-5757	part	
32-14	5758-5760	of	
32-15	5761-5766	Flink	
32-16	5766-5767	’	
32-17	5767-5768	s	
32-18	5769-5775	binary	
32-19	5776-5788	distribution	
32-20	5788-5789	.	
32-21	5790-5793	See	
32-22	5794-5797	how	
32-23	5798-5800	to	
32-24	5801-5805	link	
32-25	5806-5810	with	
32-26	5811-5815	them	
32-27	5816-5819	for	
32-28	5820-5827	cluster	
32-29	5828-5837	execution	
32-30	5838-5842	here	
32-31	5842-5843	.	
32-32	5844-5847	How	
32-33	5848-5850	to	
32-34	5851-5857	create	
32-35	5858-5859	a	
32-36	5860-5864	JDBC	
32-37	5865-5870	table	

#Text=The JDBC table can be defined as following: -- register a MySQL table 'users' in Flink SQL CREATE TABLE MyUserTable ( id BIGINT, name STRING, age INT, status BOOLEAN,
33-1	5871-5874	The	
33-2	5875-5879	JDBC	
33-3	5880-5885	table	
33-4	5886-5889	can	
33-5	5890-5892	be	
33-6	5893-5900	defined	
33-7	5901-5903	as	
33-8	5904-5913	following	
33-9	5913-5914	:	
33-10	5915-5916	-	
33-11	5916-5917	-	
33-12	5918-5926	register	
33-13	5927-5928	a	
33-14	5929-5934	MySQL	
33-15	5935-5940	table	
33-16	5941-5942	'	
33-17	5942-5947	users	
33-18	5947-5948	'	
33-19	5949-5951	in	
33-20	5952-5957	Flink	
33-21	5958-5961	SQL	
33-22	5962-5968	CREATE	
33-23	5969-5974	TABLE	
33-24	5975-5986	MyUserTable	
33-25	5987-5988	(	
33-26	5989-5991	id	
33-27	5992-5998	BIGINT	
33-28	5998-5999	,	
33-29	6000-6004	name	
33-30	6005-6011	STRING	
33-31	6011-6012	,	
33-32	6013-6016	age	
33-33	6017-6020	INT	
33-34	6020-6021	,	
33-35	6022-6028	status	
33-36	6029-6036	BOOLEAN	
33-37	6036-6037	,	

#Text=PRIMARY KEY (id) NOT ENFORCED ) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://localhost:3306/mydatabase', 'table-name' = 'users'
34-1	6038-6045	PRIMARY	
34-2	6046-6049	KEY	
34-3	6050-6051	(	
34-4	6051-6053	id	
34-5	6053-6054	)	
34-6	6055-6058	NOT	
34-7	6059-6067	ENFORCED	
34-8	6068-6069	)	
34-9	6070-6074	WITH	
34-10	6075-6076	(	
34-11	6077-6078	'	
34-12	6078-6087	connector	
34-13	6087-6088	'	
34-14	6089-6090	=	
34-15	6091-6092	'	
34-16	6092-6096	jdbc	
34-17	6096-6097	'	
34-18	6097-6098	,	
34-19	6099-6100	'	
34-20	6100-6103	url	
34-21	6103-6104	'	
34-22	6105-6106	=	
34-23	6107-6108	'	
34-24	6108-6112	jdbc	
34-25	6112-6113	:	
34-26	6113-6118	mysql	
34-27	6118-6119	:	
34-28	6119-6120	/	
34-29	6120-6121	/	
34-30	6121-6130	localhost	
34-31	6130-6131	:	
34-32	6131-6135	3306	
34-33	6135-6136	/	
34-34	6136-6146	mydatabase	
34-35	6146-6147	'	
34-36	6147-6148	,	
34-37	6149-6150	'	
34-38	6150-6160	table-name	
34-39	6160-6161	'	
34-40	6162-6163	=	
34-41	6164-6165	'	
34-42	6165-6170	users	
34-43	6170-6171	'	

#Text=-- write data into the JDBC table from the other table "T" INSERT INTO MyUserTable SELECT id, name, age, status FROM T; -- scan data from the JDBC table SELECT id, name, age, status FROM MyUserTable;
35-1	6172-6173	-	
35-2	6173-6174	-	
35-3	6175-6180	write	
35-4	6181-6185	data	
35-5	6186-6190	into	
35-6	6191-6194	the	
35-7	6195-6199	JDBC	
35-8	6200-6205	table	
35-9	6206-6210	from	
35-10	6211-6214	the	
35-11	6215-6220	other	
35-12	6221-6226	table	
35-13	6227-6228	"	
35-14	6228-6229	T	
35-15	6229-6230	"	
35-16	6231-6237	INSERT	
35-17	6238-6242	INTO	
35-18	6243-6254	MyUserTable	
35-19	6255-6261	SELECT	
35-20	6262-6264	id	
35-21	6264-6265	,	
35-22	6266-6270	name	
35-23	6270-6271	,	
35-24	6272-6275	age	
35-25	6275-6276	,	
35-26	6277-6283	status	
35-27	6284-6288	FROM	
35-28	6289-6290	T	
35-29	6290-6291	;	
35-30	6292-6293	-	
35-31	6293-6294	-	
35-32	6295-6299	scan	
35-33	6300-6304	data	
35-34	6305-6309	from	
35-35	6310-6313	the	
35-36	6314-6318	JDBC	
35-37	6319-6324	table	
35-38	6325-6331	SELECT	
35-39	6332-6334	id	
35-40	6334-6335	,	
35-41	6336-6340	name	
35-42	6340-6341	,	
35-43	6342-6345	age	
35-44	6345-6346	,	
35-45	6347-6353	status	
35-46	6354-6358	FROM	
35-47	6359-6370	MyUserTable	
35-48	6370-6371	;	

#Text=-- temporal join the JDBC table as a dimension table SELECT * FROM myTopic LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime ON myTopic.key = MyUserTable.id;
36-1	6372-6373	-	
36-2	6373-6374	-	
36-3	6375-6383	temporal	
36-4	6384-6388	join	
36-5	6389-6392	the	
36-6	6393-6397	JDBC	
36-7	6398-6403	table	
36-8	6404-6406	as	
36-9	6407-6408	a	
36-10	6409-6418	dimension	
36-11	6419-6424	table	
36-12	6425-6431	SELECT	
36-13	6432-6433	*	
36-14	6434-6438	FROM	
36-15	6439-6446	myTopic	
36-16	6447-6451	LEFT	
36-17	6452-6456	JOIN	
36-18	6457-6468	MyUserTable	
36-19	6469-6472	FOR	
36-20	6473-6484	SYSTEM_TIME	
36-21	6485-6487	AS	
36-22	6488-6490	OF	
36-23	6491-6507	myTopic.proctime	
36-24	6508-6510	ON	
36-25	6511-6522	myTopic.key	
36-26	6523-6524	=	
36-27	6525-6539	MyUserTable.id	
36-28	6539-6540	;	

#Text=Connector Options Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'jdbc'. url required
37-1	6541-6550	Connector	
37-2	6551-6558	Options	
37-3	6559-6565	Option	
37-4	6566-6574	Required	
37-5	6575-6582	Default	
37-6	6583-6587	Type	
37-7	6588-6599	Description	
37-8	6600-6609	connector	
37-9	6610-6618	required	
37-10	6619-6620	(	
37-11	6620-6624	none	
37-12	6624-6625	)	
37-13	6626-6632	String	
37-14	6633-6640	Specify	
37-15	6641-6645	what	
37-16	6646-6655	connector	
37-17	6656-6658	to	
37-18	6659-6662	use	
37-19	6662-6663	,	
37-20	6664-6668	here	
37-21	6669-6675	should	
37-22	6676-6678	be	
37-23	6679-6680	'	
37-24	6680-6684	jdbc	
37-25	6684-6685	'	
37-26	6685-6686	.	
37-27	6687-6690	url	
37-28	6691-6699	required	

#Text=(none) String The JDBC database url. table-name required (none) String The name of JDBC table to connect. driver optional (none) String
38-1	6700-6701	(	
38-2	6701-6705	none	
38-3	6705-6706	)	
38-4	6707-6713	String	
38-5	6714-6717	The	
38-6	6718-6722	JDBC	
38-7	6723-6731	database	
38-8	6732-6735	url	
38-9	6735-6736	.	
38-10	6737-6747	table-name	
38-11	6748-6756	required	
38-12	6757-6758	(	
38-13	6758-6762	none	
38-14	6762-6763	)	
38-15	6764-6770	String	
38-16	6771-6774	The	
38-17	6775-6779	name	
38-18	6780-6782	of	
38-19	6783-6787	JDBC	
38-20	6788-6793	table	
38-21	6794-6796	to	
38-22	6797-6804	connect	
38-23	6804-6805	.	
38-24	6806-6812	driver	
38-25	6813-6821	optional	
38-26	6822-6823	(	
38-27	6823-6827	none	
38-28	6827-6828	)	
38-29	6829-6835	String	

#Text=The class name of the JDBC driver to use to connect to this URL, if not set, it will automatically be derived from the URL. username optional (none) String The JDBC user name. 'username' and 'password' must both be specified if any of them is specified. password optional (none) String The JDBC password.
39-1	6836-6839	The	
39-2	6840-6845	class	
39-3	6846-6850	name	
39-4	6851-6853	of	
39-5	6854-6857	the	
39-6	6858-6862	JDBC	
39-7	6863-6869	driver	
39-8	6870-6872	to	
39-9	6873-6876	use	
39-10	6877-6879	to	
39-11	6880-6887	connect	
39-12	6888-6890	to	
39-13	6891-6895	this	
39-14	6896-6899	URL	
39-15	6899-6900	,	
39-16	6901-6903	if	
39-17	6904-6907	not	
39-18	6908-6911	set	
39-19	6911-6912	,	
39-20	6913-6915	it	
39-21	6916-6920	will	
39-22	6921-6934	automatically	
39-23	6935-6937	be	
39-24	6938-6945	derived	
39-25	6946-6950	from	
39-26	6951-6954	the	
39-27	6955-6958	URL	
39-28	6958-6959	.	
39-29	6960-6968	username	
39-30	6969-6977	optional	
39-31	6978-6979	(	
39-32	6979-6983	none	
39-33	6983-6984	)	
39-34	6985-6991	String	
39-35	6992-6995	The	
39-36	6996-7000	JDBC	
39-37	7001-7005	user	
39-38	7006-7010	name	
39-39	7010-7011	.	
39-40	7012-7013	'	
39-41	7013-7021	username	
39-42	7021-7022	'	
39-43	7023-7026	and	
39-44	7027-7028	'	
39-45	7028-7036	password	
39-46	7036-7037	'	
39-47	7038-7042	must	
39-48	7043-7047	both	
39-49	7048-7050	be	
39-50	7051-7060	specified	
39-51	7061-7063	if	
39-52	7064-7067	any	
39-53	7068-7070	of	
39-54	7071-7075	them	
39-55	7076-7078	is	
39-56	7079-7088	specified	
39-57	7088-7089	.	
39-58	7090-7098	password	
39-59	7099-7107	optional	
39-60	7108-7109	(	
39-61	7109-7113	none	
39-62	7113-7114	)	
39-63	7115-7121	String	
39-64	7122-7125	The	
39-65	7126-7130	JDBC	
39-66	7131-7139	password	
39-67	7139-7140	.	

#Text=scan.partition.column optional (none) String The column name used for partitioning the input. See the following Partitioned Scan section for more details. scan.partition.num optional (none) Integer The number of partitions.
40-1	7141-7162	scan.partition.column	
40-2	7163-7171	optional	
40-3	7172-7173	(	
40-4	7173-7177	none	
40-5	7177-7178	)	
40-6	7179-7185	String	
40-7	7186-7189	The	
40-8	7190-7196	column	
40-9	7197-7201	name	
40-10	7202-7206	used	
40-11	7207-7210	for	
40-12	7211-7223	partitioning	
40-13	7224-7227	the	
40-14	7228-7233	input	
40-15	7233-7234	.	
40-16	7235-7238	See	
40-17	7239-7242	the	
40-18	7243-7252	following	
40-19	7253-7264	Partitioned	
40-20	7265-7269	Scan	
40-21	7270-7277	section	
40-22	7278-7281	for	
40-23	7282-7286	more	
40-24	7287-7294	details	
40-25	7294-7295	.	
40-26	7296-7314	scan.partition.num	
40-27	7315-7323	optional	
40-28	7324-7325	(	
40-29	7325-7329	none	
40-30	7329-7330	)	
40-31	7331-7338	Integer	
40-32	7339-7342	The	
40-33	7343-7349	number	
40-34	7350-7352	of	
40-35	7353-7363	partitions	
40-36	7363-7364	.	

#Text=scan.partition.lower-bound optional (none) Integer The smallest value of the first partition. scan.partition.upper-bound optional (none) Integer The largest value of the last partition. scan.fetch-size optional
41-1	7365-7391	scan.partition.lower-bound	
41-2	7392-7400	optional	
41-3	7401-7402	(	
41-4	7402-7406	none	
41-5	7406-7407	)	
41-6	7408-7415	Integer	
41-7	7416-7419	The	
41-8	7420-7428	smallest	
41-9	7429-7434	value	
41-10	7435-7437	of	
41-11	7438-7441	the	
41-12	7442-7447	first	
41-13	7448-7457	partition	
41-14	7457-7458	.	
41-15	7459-7485	scan.partition.upper-bound	
41-16	7486-7494	optional	
41-17	7495-7496	(	
41-18	7496-7500	none	
41-19	7500-7501	)	
41-20	7502-7509	Integer	
41-21	7510-7513	The	
41-22	7514-7521	largest	
41-23	7522-7527	value	
41-24	7528-7530	of	
41-25	7531-7534	the	
41-26	7535-7539	last	
41-27	7540-7549	partition	
41-28	7549-7550	.	
41-29	7551-7566	scan.fetch-size	
41-30	7567-7575	optional	

#Text=Integer The number of rows that should be fetched from the database when reading per round trip. If the value specified is zero, then the hint is ignored. scan.auto-commit optional true Boolean Sets the auto-commit flag on the JDBC driver,
42-1	7576-7583	Integer	
42-2	7584-7587	The	
42-3	7588-7594	number	
42-4	7595-7597	of	
42-5	7598-7602	rows	
42-6	7603-7607	that	
42-7	7608-7614	should	
42-8	7615-7617	be	
42-9	7618-7625	fetched	
42-10	7626-7630	from	
42-11	7631-7634	the	
42-12	7635-7643	database	
42-13	7644-7648	when	
42-14	7649-7656	reading	
42-15	7657-7660	per	
42-16	7661-7666	round	
42-17	7667-7671	trip	
42-18	7671-7672	.	
42-19	7673-7675	If	
42-20	7676-7679	the	
42-21	7680-7685	value	
42-22	7686-7695	specified	
42-23	7696-7698	is	
42-24	7699-7703	zero	
42-25	7703-7704	,	
42-26	7705-7709	then	
42-27	7710-7713	the	
42-28	7714-7718	hint	
42-29	7719-7721	is	
42-30	7722-7729	ignored	
42-31	7729-7730	.	
42-32	7731-7747	scan.auto-commit	
42-33	7748-7756	optional	
42-34	7757-7761	true	
42-35	7762-7769	Boolean	
42-36	7770-7774	Sets	
42-37	7775-7778	the	
42-38	7779-7790	auto-commit	
42-39	7791-7795	flag	
42-40	7796-7798	on	
42-41	7799-7802	the	
42-42	7803-7807	JDBC	
42-43	7808-7814	driver	
42-44	7814-7815	,	

#Text=which determines whether each statement is committed in a transaction automatically. Some JDBC drivers, specifically Postgres, may require this to be set to false in order to stream results. lookup.cache.max-rows optional (none) Integer The max number of rows of lookup cache, over this value, the oldest rows will be expired.
43-1	7816-7821	which	
43-2	7822-7832	determines	
43-3	7833-7840	whether	
43-4	7841-7845	each	
43-5	7846-7855	statement	
43-6	7856-7858	is	
43-7	7859-7868	committed	
43-8	7869-7871	in	
43-9	7872-7873	a	
43-10	7874-7885	transaction	
43-11	7886-7899	automatically	
43-12	7899-7900	.	
43-13	7901-7905	Some	
43-14	7906-7910	JDBC	
43-15	7911-7918	drivers	
43-16	7918-7919	,	
43-17	7920-7932	specifically	
43-18	7933-7941	Postgres	
43-19	7941-7942	,	
43-20	7943-7946	may	
43-21	7947-7954	require	
43-22	7955-7959	this	
43-23	7960-7962	to	
43-24	7963-7965	be	
43-25	7966-7969	set	
43-26	7970-7972	to	
43-27	7973-7978	false	
43-28	7979-7981	in	
43-29	7982-7987	order	
43-30	7988-7990	to	
43-31	7991-7997	stream	
43-32	7998-8005	results	
43-33	8005-8006	.	
43-34	8007-8028	lookup.cache.max-rows	
43-35	8029-8037	optional	
43-36	8038-8039	(	
43-37	8039-8043	none	
43-38	8043-8044	)	
43-39	8045-8052	Integer	
43-40	8053-8056	The	
43-41	8057-8060	max	
43-42	8061-8067	number	
43-43	8068-8070	of	
43-44	8071-8075	rows	
43-45	8076-8078	of	
43-46	8079-8085	lookup	
43-47	8086-8091	cache	
43-48	8091-8092	,	
43-49	8093-8097	over	
43-50	8098-8102	this	
43-51	8103-8108	value	
43-52	8108-8109	,	
43-53	8110-8113	the	
43-54	8114-8120	oldest	
43-55	8121-8125	rows	
43-56	8126-8130	will	
43-57	8131-8133	be	
43-58	8134-8141	expired	
43-59	8141-8142	.	

#Text=Lookup cache is disabled by default. See the following Lookup Cache section for more details. lookup.cache.ttl optional (none) Duration The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired. Lookup cache is disabled by default. See the following Lookup Cache section for more details.
44-1	8143-8149	Lookup	
44-2	8150-8155	cache	
44-3	8156-8158	is	
44-4	8159-8167	disabled	
44-5	8168-8170	by	
44-6	8171-8178	default	
44-7	8178-8179	.	
44-8	8180-8183	See	
44-9	8184-8187	the	
44-10	8188-8197	following	
44-11	8198-8204	Lookup	
44-12	8205-8210	Cache	
44-13	8211-8218	section	
44-14	8219-8222	for	
44-15	8223-8227	more	
44-16	8228-8235	details	
44-17	8235-8236	.	
44-18	8237-8253	lookup.cache.ttl	
44-19	8254-8262	optional	
44-20	8263-8264	(	
44-21	8264-8268	none	
44-22	8268-8269	)	
44-23	8270-8278	Duration	
44-24	8279-8282	The	
44-25	8283-8286	max	
44-26	8287-8291	time	
44-27	8292-8294	to	
44-28	8295-8299	live	
44-29	8300-8303	for	
44-30	8304-8308	each	
44-31	8309-8313	rows	
44-32	8314-8316	in	
44-33	8317-8323	lookup	
44-34	8324-8329	cache	
44-35	8329-8330	,	
44-36	8331-8335	over	
44-37	8336-8340	this	
44-38	8341-8345	time	
44-39	8345-8346	,	
44-40	8347-8350	the	
44-41	8351-8357	oldest	
44-42	8358-8362	rows	
44-43	8363-8367	will	
44-44	8368-8370	be	
44-45	8371-8378	expired	
44-46	8378-8379	.	
44-47	8380-8386	Lookup	
44-48	8387-8392	cache	
44-49	8393-8395	is	
44-50	8396-8404	disabled	
44-51	8405-8407	by	
44-52	8408-8415	default	
44-53	8415-8416	.	
44-54	8417-8420	See	
44-55	8421-8424	the	
44-56	8425-8434	following	
44-57	8435-8441	Lookup	
44-58	8442-8447	Cache	
44-59	8448-8455	section	
44-60	8456-8459	for	
44-61	8460-8464	more	
44-62	8465-8472	details	
44-63	8472-8473	.	

#Text=lookup.max-retries optional Integer The max retry times if lookup database failed. sink.buffer-flush.max-rows optional 100 Integer The max size of buffered records before flush. Can be set to zero to disable it.
45-1	8474-8492	lookup.max-retries	
45-2	8493-8501	optional	
45-3	8502-8509	Integer	
45-4	8510-8513	The	
45-5	8514-8517	max	
45-6	8518-8523	retry	
45-7	8524-8529	times	
45-8	8530-8532	if	
45-9	8533-8539	lookup	
45-10	8540-8548	database	
45-11	8549-8555	failed	
45-12	8555-8556	.	
45-13	8557-8583	sink.buffer-flush.max-rows	
45-14	8584-8592	optional	
45-15	8593-8596	100	
45-16	8597-8604	Integer	
45-17	8605-8608	The	
45-18	8609-8612	max	
45-19	8613-8617	size	
45-20	8618-8620	of	
45-21	8621-8629	buffered	
45-22	8630-8637	records	
45-23	8638-8644	before	
45-24	8645-8650	flush	
45-25	8650-8651	.	
45-26	8652-8655	Can	
45-27	8656-8658	be	
45-28	8659-8662	set	
45-29	8663-8665	to	
45-30	8666-8670	zero	
45-31	8671-8673	to	
45-32	8674-8681	disable	
45-33	8682-8684	it	
45-34	8684-8685	.	

#Text=sink.buffer-flush.interval optional Duration The flush interval mills, over this time, asynchronous threads will flush data. Can be set to '0' to disable it. Note, 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions.
46-1	8686-8712	sink.buffer-flush.interval	
46-2	8713-8721	optional	
46-3	8722-8730	Duration	
46-4	8731-8734	The	
46-5	8735-8740	flush	
46-6	8741-8749	interval	
46-7	8750-8755	mills	
46-8	8755-8756	,	
46-9	8757-8761	over	
46-10	8762-8766	this	
46-11	8767-8771	time	
46-12	8771-8772	,	
46-13	8773-8785	asynchronous	
46-14	8786-8793	threads	
46-15	8794-8798	will	
46-16	8799-8804	flush	
46-17	8805-8809	data	
46-18	8809-8810	.	
46-19	8811-8814	Can	
46-20	8815-8817	be	
46-21	8818-8821	set	
46-22	8822-8824	to	
46-23	8825-8826	'	
46-24	8826-8827	0	
46-25	8827-8828	'	
46-26	8829-8831	to	
46-27	8832-8839	disable	
46-28	8840-8842	it	
46-29	8842-8843	.	
46-30	8844-8848	Note	
46-31	8848-8849	,	
46-32	8850-8851	'	
46-33	8851-8877	sink.buffer-flush.max-rows	
46-34	8877-8878	'	
46-35	8879-8882	can	
46-36	8883-8885	be	
46-37	8886-8889	set	
46-38	8890-8892	to	
46-39	8893-8894	'	
46-40	8894-8895	0	
46-41	8895-8896	'	
46-42	8897-8901	with	
46-43	8902-8905	the	
46-44	8906-8911	flush	
46-45	8912-8920	interval	
46-46	8921-8924	set	
46-47	8925-8933	allowing	
46-48	8934-8937	for	
46-49	8938-8946	complete	
46-50	8947-8952	async	
46-51	8953-8963	processing	
46-52	8964-8966	of	
46-53	8967-8975	buffered	
46-54	8976-8983	actions	
46-55	8983-8984	.	

#Text=sink.max-retries optional Integer The max retry times if writing records to database failed. Features Key handling Flink uses the primary key that defined in DDL when writing data to external databases. The connector operate in upsert mode if the primary key was defined, otherwise, the connector operate in append mode.
47-1	8985-9001	sink.max-retries	
47-2	9002-9010	optional	
47-3	9011-9018	Integer	
47-4	9019-9022	The	
47-5	9023-9026	max	
47-6	9027-9032	retry	
47-7	9033-9038	times	
47-8	9039-9041	if	
47-9	9042-9049	writing	
47-10	9050-9057	records	
47-11	9058-9060	to	
47-12	9061-9069	database	
47-13	9070-9076	failed	
47-14	9076-9077	.	
47-15	9078-9086	Features	
47-16	9087-9090	Key	
47-17	9091-9099	handling	
47-18	9100-9105	Flink	
47-19	9106-9110	uses	
47-20	9111-9114	the	
47-21	9115-9122	primary	
47-22	9123-9126	key	
47-23	9127-9131	that	
47-24	9132-9139	defined	
47-25	9140-9142	in	
47-26	9143-9146	DDL	
47-27	9147-9151	when	
47-28	9152-9159	writing	
47-29	9160-9164	data	
47-30	9165-9167	to	
47-31	9168-9176	external	
47-32	9177-9186	databases	
47-33	9186-9187	.	
47-34	9188-9191	The	
47-35	9192-9201	connector	
47-36	9202-9209	operate	
47-37	9210-9212	in	
47-38	9213-9219	upsert	
47-39	9220-9224	mode	
47-40	9225-9227	if	
47-41	9228-9231	the	
47-42	9232-9239	primary	
47-43	9240-9243	key	
47-44	9244-9247	was	
47-45	9248-9255	defined	
47-46	9255-9256	,	
47-47	9257-9266	otherwise	
47-48	9266-9267	,	
47-49	9268-9271	the	
47-50	9272-9281	connector	
47-51	9282-9289	operate	
47-52	9290-9292	in	
47-53	9293-9299	append	
47-54	9300-9304	mode	
47-55	9304-9305	.	

#Text=In upsert mode, Flink will insert a new row or update the existing row according to the primary key, Flink can ensure the idempotence in this way. To guarantee the output result is as expected, it’s recommended to define primary key for the table and make sure the primary key is one of the unique key sets or primary key of the underlying database table. In append mode, Flink will interpret all records as INSERT messages, the INSERT operation may fail if a primary key or unique constraint violation happens in the underlying database. See CREATE TABLE DDL for more details about PRIMARY KEY syntax. Partitioned Scan To accelerate reading data in parallel Source task instances, Flink provides partitioned scan feature for JDBC table.
48-1	9306-9308	In	
48-2	9309-9315	upsert	
48-3	9316-9320	mode	
48-4	9320-9321	,	
48-5	9322-9327	Flink	
48-6	9328-9332	will	
48-7	9333-9339	insert	
48-8	9340-9341	a	
48-9	9342-9345	new	
48-10	9346-9349	row	
48-11	9350-9352	or	
48-12	9353-9359	update	
48-13	9360-9363	the	
48-14	9364-9372	existing	
48-15	9373-9376	row	
48-16	9377-9386	according	
48-17	9387-9389	to	
48-18	9390-9393	the	
48-19	9394-9401	primary	
48-20	9402-9405	key	
48-21	9405-9406	,	
48-22	9407-9412	Flink	
48-23	9413-9416	can	
48-24	9417-9423	ensure	
48-25	9424-9427	the	
48-26	9428-9439	idempotence	
48-27	9440-9442	in	
48-28	9443-9447	this	
48-29	9448-9451	way	
48-30	9451-9452	.	
48-31	9453-9455	To	
48-32	9456-9465	guarantee	
48-33	9466-9469	the	
48-34	9470-9476	output	
48-35	9477-9483	result	
48-36	9484-9486	is	
48-37	9487-9489	as	
48-38	9490-9498	expected	
48-39	9498-9499	,	
48-40	9500-9502	it	
48-41	9502-9503	’	
48-42	9503-9504	s	
48-43	9505-9516	recommended	
48-44	9517-9519	to	
48-45	9520-9526	define	
48-46	9527-9534	primary	
48-47	9535-9538	key	
48-48	9539-9542	for	
48-49	9543-9546	the	
48-50	9547-9552	table	
48-51	9553-9556	and	
48-52	9557-9561	make	
48-53	9562-9566	sure	
48-54	9567-9570	the	
48-55	9571-9578	primary	
48-56	9579-9582	key	
48-57	9583-9585	is	
48-58	9586-9589	one	
48-59	9590-9592	of	
48-60	9593-9596	the	
48-61	9597-9603	unique	
48-62	9604-9607	key	
48-63	9608-9612	sets	
48-64	9613-9615	or	
48-65	9616-9623	primary	
48-66	9624-9627	key	
48-67	9628-9630	of	
48-68	9631-9634	the	
48-69	9635-9645	underlying	
48-70	9646-9654	database	
48-71	9655-9660	table	
48-72	9660-9661	.	
48-73	9662-9664	In	
48-74	9665-9671	append	
48-75	9672-9676	mode	
48-76	9676-9677	,	
48-77	9678-9683	Flink	
48-78	9684-9688	will	
48-79	9689-9698	interpret	
48-80	9699-9702	all	
48-81	9703-9710	records	
48-82	9711-9713	as	
48-83	9714-9720	INSERT	
48-84	9721-9729	messages	
48-85	9729-9730	,	
48-86	9731-9734	the	
48-87	9735-9741	INSERT	
48-88	9742-9751	operation	
48-89	9752-9755	may	
48-90	9756-9760	fail	
48-91	9761-9763	if	
48-92	9764-9765	a	
48-93	9766-9773	primary	
48-94	9774-9777	key	
48-95	9778-9780	or	
48-96	9781-9787	unique	
48-97	9788-9798	constraint	
48-98	9799-9808	violation	
48-99	9809-9816	happens	
48-100	9817-9819	in	
48-101	9820-9823	the	
48-102	9824-9834	underlying	
48-103	9835-9843	database	
48-104	9843-9844	.	
48-105	9845-9848	See	
48-106	9849-9855	CREATE	
48-107	9856-9861	TABLE	
48-108	9862-9865	DDL	
48-109	9866-9869	for	
48-110	9870-9874	more	
48-111	9875-9882	details	
48-112	9883-9888	about	
48-113	9889-9896	PRIMARY	
48-114	9897-9900	KEY	
48-115	9901-9907	syntax	
48-116	9907-9908	.	
48-117	9909-9920	Partitioned	
48-118	9921-9925	Scan	
48-119	9926-9928	To	
48-120	9929-9939	accelerate	
48-121	9940-9947	reading	
48-122	9948-9952	data	
48-123	9953-9955	in	
48-124	9956-9964	parallel	
48-125	9965-9971	Source	
48-126	9972-9976	task	
48-127	9977-9986	instances	
48-128	9986-9987	,	
48-129	9988-9993	Flink	
48-130	9994-10002	provides	
48-131	10003-10014	partitioned	
48-132	10015-10019	scan	
48-133	10020-10027	feature	
48-134	10028-10031	for	
48-135	10032-10036	JDBC	
48-136	10037-10042	table	
48-137	10042-10043	.	

#Text=All the following scan partition options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple tasks. The scan.partition.column must be a numeric, date, or timestamp column from the table in question. Notice that scan.partition.lower-bound and scan.partition.upper-bound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.
49-1	10044-10047	All	
49-2	10048-10051	the	
49-3	10052-10061	following	
49-4	10062-10066	scan	
49-5	10067-10076	partition	
49-6	10077-10084	options	
49-7	10085-10089	must	
49-8	10090-10093	all	
49-9	10094-10096	be	
49-10	10097-10106	specified	
49-11	10107-10109	if	
49-12	10110-10113	any	
49-13	10114-10116	of	
49-14	10117-10121	them	
49-15	10122-10124	is	
49-16	10125-10134	specified	
49-17	10134-10135	.	
49-18	10136-10140	They	
49-19	10141-10149	describe	
49-20	10150-10153	how	
49-21	10154-10156	to	
49-22	10157-10166	partition	
49-23	10167-10170	the	
49-24	10171-10176	table	
49-25	10177-10181	when	
49-26	10182-10189	reading	
49-27	10190-10192	in	
49-28	10193-10201	parallel	
49-29	10202-10206	from	
49-30	10207-10215	multiple	
49-31	10216-10221	tasks	
49-32	10221-10222	.	
49-33	10223-10226	The	
49-34	10227-10248	scan.partition.column	
49-35	10249-10253	must	
49-36	10254-10256	be	
49-37	10257-10258	a	
49-38	10259-10266	numeric	
49-39	10266-10267	,	
49-40	10268-10272	date	
49-41	10272-10273	,	
49-42	10274-10276	or	
49-43	10277-10286	timestamp	
49-44	10287-10293	column	
49-45	10294-10298	from	
49-46	10299-10302	the	
49-47	10303-10308	table	
49-48	10309-10311	in	
49-49	10312-10320	question	
49-50	10320-10321	.	
49-51	10322-10328	Notice	
49-52	10329-10333	that	
49-53	10334-10360	scan.partition.lower-bound	
49-54	10361-10364	and	
49-55	10365-10391	scan.partition.upper-bound	
49-56	10392-10395	are	
49-57	10396-10400	just	
49-58	10401-10405	used	
49-59	10406-10408	to	
49-60	10409-10415	decide	
49-61	10416-10419	the	
49-62	10420-10429	partition	
49-63	10430-10436	stride	
49-64	10436-10437	,	
49-65	10438-10441	not	
49-66	10442-10445	for	
49-67	10446-10455	filtering	
49-68	10456-10459	the	
49-69	10460-10464	rows	
49-70	10465-10467	in	
49-71	10468-10473	table	
49-72	10473-10474	.	
49-73	10475-10477	So	
49-74	10478-10481	all	
49-75	10482-10486	rows	
49-76	10487-10489	in	
49-77	10490-10493	the	
49-78	10494-10499	table	
49-79	10500-10504	will	
49-80	10505-10507	be	
49-81	10508-10519	partitioned	
49-82	10520-10523	and	
49-83	10524-10532	returned	
49-84	10532-10533	.	

#Text=scan.partition.column: The column name used for partitioning the input. scan.partition.num: The number of partitions. scan.partition.lower-bound: The smallest value of the first partition. scan.partition.upper-bound: The largest value of the last partition. Lookup Cache
50-1	10534-10555	scan.partition.column	
50-2	10555-10556	:	
50-3	10557-10560	The	
50-4	10561-10567	column	
50-5	10568-10572	name	
50-6	10573-10577	used	
50-7	10578-10581	for	
50-8	10582-10594	partitioning	
50-9	10595-10598	the	
50-10	10599-10604	input	
50-11	10604-10605	.	
50-12	10606-10624	scan.partition.num	
50-13	10624-10625	:	
50-14	10626-10629	The	
50-15	10630-10636	number	
50-16	10637-10639	of	
50-17	10640-10650	partitions	
50-18	10650-10651	.	
50-19	10652-10678	scan.partition.lower-bound	
50-20	10678-10679	:	
50-21	10680-10683	The	
50-22	10684-10692	smallest	
50-23	10693-10698	value	
50-24	10699-10701	of	
50-25	10702-10705	the	
50-26	10706-10711	first	
50-27	10712-10721	partition	
50-28	10721-10722	.	
50-29	10723-10749	scan.partition.upper-bound	
50-30	10749-10750	:	
50-31	10751-10754	The	
50-32	10755-10762	largest	
50-33	10763-10768	value	
50-34	10769-10771	of	
50-35	10772-10775	the	
50-36	10776-10780	last	
50-37	10781-10790	partition	
50-38	10790-10791	.	
50-39	10792-10798	Lookup	
50-40	10799-10804	Cache	

#Text=JDBC connector can be used in temporal join as a lookup source (aka. dimension table). Currently, only sync lookup mode is supported. By default, lookup cache is not enabled. You can enable it by setting both lookup.cache.max-rows and lookup.cache.ttl. The lookup cache is used to improve performance of temporal join the JDBC connector. By default, lookup cache is not enabled, so all the requests are sent to external database.
51-1	10805-10809	JDBC	
51-2	10810-10819	connector	
51-3	10820-10823	can	
51-4	10824-10826	be	
51-5	10827-10831	used	
51-6	10832-10834	in	
51-7	10835-10843	temporal	
51-8	10844-10848	join	
51-9	10849-10851	as	
51-10	10852-10853	a	
51-11	10854-10860	lookup	
51-12	10861-10867	source	
51-13	10868-10869	(	
51-14	10869-10872	aka	
51-15	10872-10873	.	
51-16	10874-10883	dimension	
51-17	10884-10889	table	
51-18	10889-10890	)	
51-19	10890-10891	.	
51-20	10892-10901	Currently	
51-21	10901-10902	,	
51-22	10903-10907	only	
51-23	10908-10912	sync	
51-24	10913-10919	lookup	
51-25	10920-10924	mode	
51-26	10925-10927	is	
51-27	10928-10937	supported	
51-28	10937-10938	.	
51-29	10939-10941	By	
51-30	10942-10949	default	
51-31	10949-10950	,	
51-32	10951-10957	lookup	
51-33	10958-10963	cache	
51-34	10964-10966	is	
51-35	10967-10970	not	
51-36	10971-10978	enabled	
51-37	10978-10979	.	
51-38	10980-10983	You	
51-39	10984-10987	can	
51-40	10988-10994	enable	
51-41	10995-10997	it	
51-42	10998-11000	by	
51-43	11001-11008	setting	
51-44	11009-11013	both	
51-45	11014-11035	lookup.cache.max-rows	
51-46	11036-11039	and	
51-47	11040-11056	lookup.cache.ttl	
51-48	11056-11057	.	
51-49	11058-11061	The	
51-50	11062-11068	lookup	
51-51	11069-11074	cache	
51-52	11075-11077	is	
51-53	11078-11082	used	
51-54	11083-11085	to	
51-55	11086-11093	improve	
51-56	11094-11105	performance	
51-57	11106-11108	of	
51-58	11109-11117	temporal	
51-59	11118-11122	join	
51-60	11123-11126	the	
51-61	11127-11131	JDBC	
51-62	11132-11141	connector	
51-63	11141-11142	.	
51-64	11143-11145	By	
51-65	11146-11153	default	
51-66	11153-11154	,	
51-67	11155-11161	lookup	
51-68	11162-11167	cache	
51-69	11168-11170	is	
51-70	11171-11174	not	
51-71	11175-11182	enabled	
51-72	11182-11183	,	
51-73	11184-11186	so	
51-74	11187-11190	all	
51-75	11191-11194	the	
51-76	11195-11203	requests	
51-77	11204-11207	are	
51-78	11208-11212	sent	
51-79	11213-11215	to	
51-80	11216-11224	external	
51-81	11225-11233	database	
51-82	11233-11234	.	

#Text=When lookup cache is enabled, each process (i.e. TaskManager) will hold a cache. Flink will lookup the cache first, and only send requests to external database when cache missing, and update cache with the rows returned. The oldest rows in cache will be expired when the cache hit to the max cached rows lookup.cache.max-rows or when the row exceeds the max time to live lookup.cache.ttl.
52-1	11235-11239	When	
52-2	11240-11246	lookup	
52-3	11247-11252	cache	
52-4	11253-11255	is	
52-5	11256-11263	enabled	
52-6	11263-11264	,	
52-7	11265-11269	each	
52-8	11270-11277	process	
52-9	11278-11279	(	
52-10	11279-11282	i.e	
52-11	11282-11283	.	
52-12	11284-11295	TaskManager	
52-13	11295-11296	)	
52-14	11297-11301	will	
52-15	11302-11306	hold	
52-16	11307-11308	a	
52-17	11309-11314	cache	
52-18	11314-11315	.	
52-19	11316-11321	Flink	
52-20	11322-11326	will	
52-21	11327-11333	lookup	
52-22	11334-11337	the	
52-23	11338-11343	cache	
52-24	11344-11349	first	
52-25	11349-11350	,	
52-26	11351-11354	and	
52-27	11355-11359	only	
52-28	11360-11364	send	
52-29	11365-11373	requests	
52-30	11374-11376	to	
52-31	11377-11385	external	
52-32	11386-11394	database	
52-33	11395-11399	when	
52-34	11400-11405	cache	
52-35	11406-11413	missing	
52-36	11413-11414	,	
52-37	11415-11418	and	
52-38	11419-11425	update	
52-39	11426-11431	cache	
52-40	11432-11436	with	
52-41	11437-11440	the	
52-42	11441-11445	rows	
52-43	11446-11454	returned	
52-44	11454-11455	.	
52-45	11456-11459	The	
52-46	11460-11466	oldest	
52-47	11467-11471	rows	
52-48	11472-11474	in	
52-49	11475-11480	cache	
52-50	11481-11485	will	
52-51	11486-11488	be	
52-52	11489-11496	expired	
52-53	11497-11501	when	
52-54	11502-11505	the	
52-55	11506-11511	cache	
52-56	11512-11515	hit	
52-57	11516-11518	to	
52-58	11519-11522	the	
52-59	11523-11526	max	
52-60	11527-11533	cached	
52-61	11534-11538	rows	
52-62	11539-11560	lookup.cache.max-rows	
52-63	11561-11563	or	
52-64	11564-11568	when	
52-65	11569-11572	the	
52-66	11573-11576	row	
52-67	11577-11584	exceeds	
52-68	11585-11588	the	
52-69	11589-11592	max	
52-70	11593-11597	time	
52-71	11598-11600	to	
52-72	11601-11605	live	
52-73	11606-11622	lookup.cache.ttl	
52-74	11622-11623	.	

#Text=The cached rows might not be the latest, users can tune lookup.cache.ttl to a smaller value to have a better fresh data, but this may increase the number of requests send to database. So this is a balance between throughput and correctness. Idempotent Writes JDBC sink will use upsert semantics rather than plain INSERT statements if primary key is defined in DDL. Upsert semantics refer to atomically adding a new row or updating the existing row if there is a unique constraint violation in the underlying database, which provides idempotence.
53-1	11624-11627	The	
53-2	11628-11634	cached	
53-3	11635-11639	rows	
53-4	11640-11645	might	
53-5	11646-11649	not	
53-6	11650-11652	be	
53-7	11653-11656	the	
53-8	11657-11663	latest	
53-9	11663-11664	,	
53-10	11665-11670	users	
53-11	11671-11674	can	
53-12	11675-11679	tune	
53-13	11680-11696	lookup.cache.ttl	
53-14	11697-11699	to	
53-15	11700-11701	a	
53-16	11702-11709	smaller	
53-17	11710-11715	value	
53-18	11716-11718	to	
53-19	11719-11723	have	
53-20	11724-11725	a	
53-21	11726-11732	better	
53-22	11733-11738	fresh	
53-23	11739-11743	data	
53-24	11743-11744	,	
53-25	11745-11748	but	
53-26	11749-11753	this	
53-27	11754-11757	may	
53-28	11758-11766	increase	
53-29	11767-11770	the	
53-30	11771-11777	number	
53-31	11778-11780	of	
53-32	11781-11789	requests	
53-33	11790-11794	send	
53-34	11795-11797	to	
53-35	11798-11806	database	
53-36	11806-11807	.	
53-37	11808-11810	So	
53-38	11811-11815	this	
53-39	11816-11818	is	
53-40	11819-11820	a	
53-41	11821-11828	balance	
53-42	11829-11836	between	
53-43	11837-11847	throughput	
53-44	11848-11851	and	
53-45	11852-11863	correctness	
53-46	11863-11864	.	
53-47	11865-11875	Idempotent	
53-48	11876-11882	Writes	
53-49	11883-11887	JDBC	
53-50	11888-11892	sink	
53-51	11893-11897	will	
53-52	11898-11901	use	
53-53	11902-11908	upsert	
53-54	11909-11918	semantics	
53-55	11919-11925	rather	
53-56	11926-11930	than	
53-57	11931-11936	plain	
53-58	11937-11943	INSERT	
53-59	11944-11954	statements	
53-60	11955-11957	if	
53-61	11958-11965	primary	
53-62	11966-11969	key	
53-63	11970-11972	is	
53-64	11973-11980	defined	
53-65	11981-11983	in	
53-66	11984-11987	DDL	
53-67	11987-11988	.	
53-68	11989-11995	Upsert	
53-69	11996-12005	semantics	
53-70	12006-12011	refer	
53-71	12012-12014	to	
53-72	12015-12025	atomically	
53-73	12026-12032	adding	
53-74	12033-12034	a	
53-75	12035-12038	new	
53-76	12039-12042	row	
53-77	12043-12045	or	
53-78	12046-12054	updating	
53-79	12055-12058	the	
53-80	12059-12067	existing	
53-81	12068-12071	row	
53-82	12072-12074	if	
53-83	12075-12080	there	
53-84	12081-12083	is	
53-85	12084-12085	a	
53-86	12086-12092	unique	
53-87	12093-12103	constraint	
53-88	12104-12113	violation	
53-89	12114-12116	in	
53-90	12117-12120	the	
53-91	12121-12131	underlying	
53-92	12132-12140	database	
53-93	12140-12141	,	
53-94	12142-12147	which	
53-95	12148-12156	provides	
53-96	12157-12168	idempotence	
53-97	12168-12169	.	

#Text=If there are failures, the Flink job will recover and re-process from last successful checkpoint, which can lead to re-processing messages during recovery. The upsert mode is highly recommended as it helps avoid constraint violations or duplicate data if records need to be re-processed. Aside from failure recovery, the source topic may also naturally contain multiple records over time with the same primary key, making upserts desirable. As there is no standard syntax for upsert, the following table describes the database-specific DML that is used. Database Upsert Grammar
54-1	12170-12172	If	
54-2	12173-12178	there	
54-3	12179-12182	are	
54-4	12183-12191	failures	
54-5	12191-12192	,	
54-6	12193-12196	the	
54-7	12197-12202	Flink	
54-8	12203-12206	job	
54-9	12207-12211	will	
54-10	12212-12219	recover	
54-11	12220-12223	and	
54-12	12224-12234	re-process	
54-13	12235-12239	from	
54-14	12240-12244	last	
54-15	12245-12255	successful	
54-16	12256-12266	checkpoint	
54-17	12266-12267	,	
54-18	12268-12273	which	
54-19	12274-12277	can	
54-20	12278-12282	lead	
54-21	12283-12285	to	
54-22	12286-12299	re-processing	
54-23	12300-12308	messages	
54-24	12309-12315	during	
54-25	12316-12324	recovery	
54-26	12324-12325	.	
54-27	12326-12329	The	
54-28	12330-12336	upsert	
54-29	12337-12341	mode	
54-30	12342-12344	is	
54-31	12345-12351	highly	
54-32	12352-12363	recommended	
54-33	12364-12366	as	
54-34	12367-12369	it	
54-35	12370-12375	helps	
54-36	12376-12381	avoid	
54-37	12382-12392	constraint	
54-38	12393-12403	violations	
54-39	12404-12406	or	
54-40	12407-12416	duplicate	
54-41	12417-12421	data	
54-42	12422-12424	if	
54-43	12425-12432	records	
54-44	12433-12437	need	
54-45	12438-12440	to	
54-46	12441-12443	be	
54-47	12444-12456	re-processed	
54-48	12456-12457	.	
54-49	12458-12463	Aside	
54-50	12464-12468	from	
54-51	12469-12476	failure	
54-52	12477-12485	recovery	
54-53	12485-12486	,	
54-54	12487-12490	the	
54-55	12491-12497	source	
54-56	12498-12503	topic	
54-57	12504-12507	may	
54-58	12508-12512	also	
54-59	12513-12522	naturally	
54-60	12523-12530	contain	
54-61	12531-12539	multiple	
54-62	12540-12547	records	
54-63	12548-12552	over	
54-64	12553-12557	time	
54-65	12558-12562	with	
54-66	12563-12566	the	
54-67	12567-12571	same	
54-68	12572-12579	primary	
54-69	12580-12583	key	
54-70	12583-12584	,	
54-71	12585-12591	making	
54-72	12592-12599	upserts	
54-73	12600-12609	desirable	
54-74	12609-12610	.	
54-75	12611-12613	As	
54-76	12614-12619	there	
54-77	12620-12622	is	
54-78	12623-12625	no	
54-79	12626-12634	standard	
54-80	12635-12641	syntax	
54-81	12642-12645	for	
54-82	12646-12652	upsert	
54-83	12652-12653	,	
54-84	12654-12657	the	
54-85	12658-12667	following	
54-86	12668-12673	table	
54-87	12674-12683	describes	
54-88	12684-12687	the	
54-89	12688-12705	database-specific	
54-90	12706-12709	DML	
54-91	12710-12714	that	
54-92	12715-12717	is	
54-93	12718-12722	used	
54-94	12722-12723	.	
54-95	12724-12732	Database	
54-96	12733-12739	Upsert	
54-97	12740-12747	Grammar	

#Text=MySQL INSERT .. ON DUPLICATE KEY UPDATE .. PostgreSQL INSERT .. ON CONFLICT .. DO UPDATE SET .. Postgres Database as a Catalog
55-1	12748-12753	MySQL	
55-2	12754-12760	INSERT	
55-3	12761-12762	.	
55-4	12762-12763	.	
55-5	12764-12766	ON	
55-6	12767-12776	DUPLICATE	
55-7	12777-12780	KEY	
55-8	12781-12787	UPDATE	
55-9	12788-12789	.	
55-10	12789-12790	.	
55-11	12791-12801	PostgreSQL	
55-12	12802-12808	INSERT	
55-13	12809-12810	.	
55-14	12810-12811	.	
55-15	12812-12814	ON	
55-16	12815-12823	CONFLICT	
55-17	12824-12825	.	
55-18	12825-12826	.	
55-19	12827-12829	DO	
55-20	12830-12836	UPDATE	
55-21	12837-12840	SET	
55-22	12841-12842	.	
55-23	12842-12843	.	
55-24	12844-12852	Postgres	
55-25	12853-12861	Database	
55-26	12862-12864	as	
55-27	12865-12866	a	
55-28	12867-12874	Catalog	

#Text=The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol. Currently, PostgresCatalog is the only implementation of JDBC Catalog at the moment, PostgresCatalog only supports limited Catalog methods include: // The supported methods by Postgres Catalog. PostgresCatalog.databaseExists(String databaseName)
56-1	12875-12878	The	
56-2	12879-12890	JdbcCatalog	
56-3	12891-12898	enables	
56-4	12899-12904	users	
56-5	12905-12907	to	
56-6	12908-12915	connect	
56-7	12916-12921	Flink	
56-8	12922-12924	to	
56-9	12925-12935	relational	
56-10	12936-12945	databases	
56-11	12946-12950	over	
56-12	12951-12955	JDBC	
56-13	12956-12964	protocol	
56-14	12964-12965	.	
56-15	12966-12975	Currently	
56-16	12975-12976	,	
56-17	12977-12992	PostgresCatalog	
56-18	12993-12995	is	
56-19	12996-12999	the	
56-20	13000-13004	only	
56-21	13005-13019	implementation	
56-22	13020-13022	of	
56-23	13023-13027	JDBC	
56-24	13028-13035	Catalog	
56-25	13036-13038	at	
56-26	13039-13042	the	
56-27	13043-13049	moment	
56-28	13049-13050	,	
56-29	13051-13066	PostgresCatalog	
56-30	13067-13071	only	
56-31	13072-13080	supports	
56-32	13081-13088	limited	
56-33	13089-13096	Catalog	
56-34	13097-13104	methods	
56-35	13105-13112	include	
56-36	13112-13113	:	
56-37	13114-13115	/	
56-38	13115-13116	/	
56-39	13117-13120	The	
56-40	13121-13130	supported	
56-41	13131-13138	methods	
56-42	13139-13141	by	
56-43	13142-13150	Postgres	
56-44	13151-13158	Catalog	
56-45	13158-13159	.	
56-46	13160-13190	PostgresCatalog.databaseExists	
56-47	13190-13191	(	
56-48	13191-13197	String	
56-49	13198-13210	databaseName	
56-50	13210-13211	)	

#Text=PostgresCatalog.listDatabases() PostgresCatalog.getDatabase(String databaseName) PostgresCatalog.listTables(String databaseName) PostgresCatalog.getTable(ObjectPath tablePath)
57-1	13212-13241	PostgresCatalog.listDatabases	
57-2	13241-13242	(	
57-3	13242-13243	)	
57-4	13244-13271	PostgresCatalog.getDatabase	
57-5	13271-13272	(	
57-6	13272-13278	String	
57-7	13279-13291	databaseName	
57-8	13291-13292	)	
57-9	13293-13319	PostgresCatalog.listTables	
57-10	13319-13320	(	
57-11	13320-13326	String	
57-12	13327-13339	databaseName	
57-13	13339-13340	)	
57-14	13341-13365	PostgresCatalog.getTable	
57-15	13365-13366	(	
57-16	13366-13376	ObjectPath	
57-17	13377-13386	tablePath	
57-18	13386-13387	)	

#Text=PostgresCatalog.tableExists(ObjectPath tablePath) Other Catalog methods is unsupported now. Usage of PostgresCatalog Please refer to Dependencies section for how to setup a JDBC connector and Postgres driver. Postgres catalog supports the following options:
58-1	13388-13415	PostgresCatalog.tableExists	
58-2	13415-13416	(	
58-3	13416-13426	ObjectPath	
58-4	13427-13436	tablePath	
58-5	13436-13437	)	
58-6	13438-13443	Other	
58-7	13444-13451	Catalog	
58-8	13452-13459	methods	
58-9	13460-13462	is	
58-10	13463-13474	unsupported	
58-11	13475-13478	now	
58-12	13478-13479	.	
58-13	13480-13485	Usage	
58-14	13486-13488	of	
58-15	13489-13504	PostgresCatalog	
58-16	13505-13511	Please	
58-17	13512-13517	refer	
58-18	13518-13520	to	
58-19	13521-13533	Dependencies	
58-20	13534-13541	section	
58-21	13542-13545	for	
58-22	13546-13549	how	
58-23	13550-13552	to	
58-24	13553-13558	setup	
58-25	13559-13560	a	
58-26	13561-13565	JDBC	
58-27	13566-13575	connector	
58-28	13576-13579	and	
58-29	13580-13588	Postgres	
58-30	13589-13595	driver	
58-31	13595-13596	.	
58-32	13597-13605	Postgres	
58-33	13606-13613	catalog	
58-34	13614-13622	supports	
58-35	13623-13626	the	
58-36	13627-13636	following	
58-37	13637-13644	options	
58-38	13644-13645	:	

#Text=name: required, name of the catalog. default-database: required, default database to connect to. username: required, username of Postgres account. password: required, password of the account. base-url: required, should be of format "jdbc:postgresql://<ip>:<port>", and should not contain database name here.
59-1	13646-13650	name	
59-2	13650-13651	:	
59-3	13652-13660	required	
59-4	13660-13661	,	
59-5	13662-13666	name	
59-6	13667-13669	of	
59-7	13670-13673	the	
59-8	13674-13681	catalog	
59-9	13681-13682	.	
59-10	13683-13699	default-database	
59-11	13699-13700	:	
59-12	13701-13709	required	
59-13	13709-13710	,	
59-14	13711-13718	default	
59-15	13719-13727	database	
59-16	13728-13730	to	
59-17	13731-13738	connect	
59-18	13739-13741	to	
59-19	13741-13742	.	
59-20	13743-13751	username	
59-21	13751-13752	:	
59-22	13753-13761	required	
59-23	13761-13762	,	
59-24	13763-13771	username	
59-25	13772-13774	of	
59-26	13775-13783	Postgres	
59-27	13784-13791	account	
59-28	13791-13792	.	
59-29	13793-13801	password	
59-30	13801-13802	:	
59-31	13803-13811	required	
59-32	13811-13812	,	
59-33	13813-13821	password	
59-34	13822-13824	of	
59-35	13825-13828	the	
59-36	13829-13836	account	
59-37	13836-13837	.	
59-38	13838-13846	base-url	
59-39	13846-13847	:	
59-40	13848-13856	required	
59-41	13856-13857	,	
59-42	13858-13864	should	
59-43	13865-13867	be	
59-44	13868-13870	of	
59-45	13871-13877	format	
59-46	13878-13879	"	
59-47	13879-13883	jdbc	
59-48	13883-13884	:	
59-49	13884-13894	postgresql	
59-50	13894-13895	:	
59-51	13895-13896	/	
59-52	13896-13897	/	
59-53	13897-13898	<	
59-54	13898-13900	ip	
59-55	13900-13901	>	
59-56	13901-13902	:	
59-57	13902-13903	<	
59-58	13903-13907	port	
59-59	13907-13908	>	
59-60	13908-13909	"	
59-61	13909-13910	,	
59-62	13911-13914	and	
59-63	13915-13921	should	
59-64	13922-13925	not	
59-65	13926-13933	contain	
59-66	13934-13942	database	
59-67	13943-13947	name	
59-68	13948-13952	here	
59-69	13952-13953	.	

#Text=CREATE CATALOG mypg WITH( 'type' = 'jdbc', 'default-database' = '...', 'username' = '...', 'password' = '...', 'base-url' = '...'
60-1	13954-13960	CREATE	
60-2	13961-13968	CATALOG	
60-3	13969-13973	mypg	
60-4	13974-13978	WITH	
60-5	13978-13979	(	
60-6	13980-13981	'	
60-7	13981-13985	type	
60-8	13985-13986	'	
60-9	13987-13988	=	
60-10	13989-13990	'	
60-11	13990-13994	jdbc	
60-12	13994-13995	'	
60-13	13995-13996	,	
60-14	13997-13998	'	
60-15	13998-14014	default-database	
60-16	14014-14015	'	
60-17	14016-14017	=	
60-18	14018-14019	'	
60-19	14019-14020	.	
60-20	14020-14021	.	
60-21	14021-14022	.	
60-22	14022-14023	'	
60-23	14023-14024	,	
60-24	14025-14026	'	
60-25	14026-14034	username	
60-26	14034-14035	'	
60-27	14036-14037	=	
60-28	14038-14039	'	
60-29	14039-14040	.	
60-30	14040-14041	.	
60-31	14041-14042	.	
60-32	14042-14043	'	
60-33	14043-14044	,	
60-34	14045-14046	'	
60-35	14046-14054	password	
60-36	14054-14055	'	
60-37	14056-14057	=	
60-38	14058-14059	'	
60-39	14059-14060	.	
60-40	14060-14061	.	
60-41	14061-14062	.	
60-42	14062-14063	'	
60-43	14063-14064	,	
60-44	14065-14066	'	
60-45	14066-14074	base-url	
60-46	14074-14075	'	
60-47	14076-14077	=	
60-48	14078-14079	'	
60-49	14079-14080	.	
60-50	14080-14081	.	
60-51	14081-14082	.	
60-52	14082-14083	'	

#Text=USE CATALOG mypg; EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build(); TableEnvironment tableEnv = TableEnvironment.create(settings); String name
61-1	14084-14087	USE	
61-2	14088-14095	CATALOG	
61-3	14096-14100	mypg	
61-4	14100-14101	;	
61-5	14102-14121	EnvironmentSettings	
61-6	14122-14130	settings	
61-7	14131-14132	=	
61-8	14133-14164	EnvironmentSettings.newInstance	
61-9	14164-14165	(	
61-10	14165-14166	)	
61-11	14166-14167	.	
61-12	14167-14182	inStreamingMode	
61-13	14182-14183	(	
61-14	14183-14184	)	
61-15	14184-14185	.	
61-16	14185-14190	build	
61-17	14190-14191	(	
61-18	14191-14192	)	
61-19	14192-14193	;	
61-20	14194-14210	TableEnvironment	
61-21	14211-14219	tableEnv	
61-22	14220-14221	=	
61-23	14222-14245	TableEnvironment.create	
61-24	14245-14246	(	
61-25	14246-14254	settings	
61-26	14254-14255	)	
61-27	14255-14256	;	
61-28	14257-14263	String	
61-29	14264-14268	name	

#Text== "mypg"; String defaultDatabase = "mydb"; String username = "..."; String password = "..."; String baseUrl = "..."
62-1	14269-14270	=	
62-2	14271-14272	"	
62-3	14272-14276	mypg	
62-4	14276-14277	"	
62-5	14277-14278	;	
62-6	14279-14285	String	
62-7	14286-14301	defaultDatabase	
62-8	14302-14303	=	
62-9	14304-14305	"	
62-10	14305-14309	mydb	
62-11	14309-14310	"	
62-12	14310-14311	;	
62-13	14312-14318	String	
62-14	14319-14327	username	
62-15	14328-14329	=	
62-16	14330-14331	"	
62-17	14331-14332	.	
62-18	14332-14333	.	
62-19	14333-14334	.	
62-20	14334-14335	"	
62-21	14335-14336	;	
62-22	14337-14343	String	
62-23	14344-14352	password	
62-24	14353-14354	=	
62-25	14355-14356	"	
62-26	14356-14357	.	
62-27	14357-14358	.	
62-28	14358-14359	.	
62-29	14359-14360	"	
62-30	14360-14361	;	
62-31	14362-14368	String	
62-32	14369-14376	baseUrl	
62-33	14377-14378	=	
62-34	14379-14380	"	
62-35	14380-14381	.	
62-36	14381-14382	.	
62-37	14382-14383	.	
62-38	14383-14384	"	

#Text=JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl); tableEnv.registerCatalog("mypg", catalog); // set the JdbcCatalog as the current catalog of the session tableEnv.useCatalog("mypg");
63-1	14385-14396	JdbcCatalog	
63-2	14397-14404	catalog	
63-3	14405-14406	=	
63-4	14407-14410	new	
63-5	14411-14422	JdbcCatalog	
63-6	14422-14423	(	
63-7	14423-14427	name	
63-8	14427-14428	,	
63-9	14429-14444	defaultDatabase	
63-10	14444-14445	,	
63-11	14446-14454	username	
63-12	14454-14455	,	
63-13	14456-14464	password	
63-14	14464-14465	,	
63-15	14466-14473	baseUrl	
63-16	14473-14474	)	
63-17	14474-14475	;	
63-18	14476-14500	tableEnv.registerCatalog	
63-19	14500-14501	(	
63-20	14501-14502	"	
63-21	14502-14506	mypg	
63-22	14506-14507	"	
63-23	14507-14508	,	
63-24	14509-14516	catalog	
63-25	14516-14517	)	
63-26	14517-14518	;	
63-27	14519-14520	/	
63-28	14520-14521	/	
63-29	14522-14525	set	
63-30	14526-14529	the	
63-31	14530-14541	JdbcCatalog	
63-32	14542-14544	as	
63-33	14545-14548	the	
63-34	14549-14556	current	
63-35	14557-14564	catalog	
63-36	14565-14567	of	
63-37	14568-14571	the	
63-38	14572-14579	session	
63-39	14580-14599	tableEnv.useCatalog	
63-40	14599-14600	(	
63-41	14600-14601	"	
63-42	14601-14605	mypg	
63-43	14605-14606	"	
63-44	14606-14607	)	
63-45	14607-14608	;	

#Text=val settings = EnvironmentSettings.newInstance().inStreamingMode().build() val tableEnv = TableEnvironment.create(settings) val name = "mypg" val defaultDatabase = "mydb" val username = "..." val password
64-1	14609-14612	val	
64-2	14613-14621	settings	
64-3	14622-14623	=	
64-4	14624-14655	EnvironmentSettings.newInstance	
64-5	14655-14656	(	
64-6	14656-14657	)	
64-7	14657-14658	.	
64-8	14658-14673	inStreamingMode	
64-9	14673-14674	(	
64-10	14674-14675	)	
64-11	14675-14676	.	
64-12	14676-14681	build	
64-13	14681-14682	(	
64-14	14682-14683	)	
64-15	14684-14687	val	
64-16	14688-14696	tableEnv	
64-17	14697-14698	=	
64-18	14699-14722	TableEnvironment.create	
64-19	14722-14723	(	
64-20	14723-14731	settings	
64-21	14731-14732	)	
64-22	14733-14736	val	
64-23	14737-14741	name	
64-24	14742-14743	=	
64-25	14744-14745	"	
64-26	14745-14749	mypg	
64-27	14749-14750	"	
64-28	14751-14754	val	
64-29	14755-14770	defaultDatabase	
64-30	14771-14772	=	
64-31	14773-14774	"	
64-32	14774-14778	mydb	
64-33	14778-14779	"	
64-34	14780-14783	val	
64-35	14784-14792	username	
64-36	14793-14794	=	
64-37	14795-14796	"	
64-38	14796-14797	.	
64-39	14797-14798	.	
64-40	14798-14799	.	
64-41	14799-14800	"	
64-42	14801-14804	val	
64-43	14805-14813	password	

#Text== "..." val baseUrl = "..." val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl) tableEnv.registerCatalog("mypg", catalog)
65-1	14814-14815	=	
65-2	14816-14817	"	
65-3	14817-14818	.	
65-4	14818-14819	.	
65-5	14819-14820	.	
65-6	14820-14821	"	
65-7	14822-14825	val	
65-8	14826-14833	baseUrl	
65-9	14834-14835	=	
65-10	14836-14837	"	
65-11	14837-14838	.	
65-12	14838-14839	.	
65-13	14839-14840	.	
65-14	14840-14841	"	
65-15	14842-14845	val	
65-16	14846-14853	catalog	
65-17	14854-14855	=	
65-18	14856-14859	new	
65-19	14860-14871	JdbcCatalog	
65-20	14871-14872	(	
65-21	14872-14876	name	
65-22	14876-14877	,	
65-23	14878-14893	defaultDatabase	
65-24	14893-14894	,	
65-25	14895-14903	username	
65-26	14903-14904	,	
65-27	14905-14913	password	
65-28	14913-14914	,	
65-29	14915-14922	baseUrl	
65-30	14922-14923	)	
65-31	14924-14948	tableEnv.registerCatalog	
65-32	14948-14949	(	
65-33	14949-14950	"	
65-34	14950-14954	mypg	
65-35	14954-14955	"	
65-36	14955-14956	,	
65-37	14957-14964	catalog	
65-38	14964-14965	)	

#Text=// set the JdbcCatalog as the current catalog of the session tableEnv.useCatalog("mypg") from pyflink.table.catalog import JdbcCatalog environment_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
66-1	14966-14967	/	
66-2	14967-14968	/	
66-3	14969-14972	set	
66-4	14973-14976	the	
66-5	14977-14988	JdbcCatalog	
66-6	14989-14991	as	
66-7	14992-14995	the	
66-8	14996-15003	current	
66-9	15004-15011	catalog	
66-10	15012-15014	of	
66-11	15015-15018	the	
66-12	15019-15026	session	
66-13	15027-15046	tableEnv.useCatalog	
66-14	15046-15047	(	
66-15	15047-15048	"	
66-16	15048-15052	mypg	
66-17	15052-15053	"	
66-18	15053-15054	)	
66-19	15055-15059	from	
66-20	15060-15081	pyflink.table.catalog	
66-21	15082-15088	import	
66-22	15089-15100	JdbcCatalog	
66-23	15101-15121	environment_settings	
66-24	15122-15123	=	
66-25	15124-15156	EnvironmentSettings.new_instance	
66-26	15156-15157	(	
66-27	15157-15158	)	
66-28	15158-15159	.	
66-29	15159-15176	in_streaming_mode	
66-30	15176-15177	(	
66-31	15177-15178	)	
66-32	15178-15179	.	
66-33	15179-15196	use_blink_planner	
66-34	15196-15197	(	
66-35	15197-15198	)	
66-36	15198-15199	.	
66-37	15199-15204	build	
66-38	15204-15205	(	
66-39	15205-15206	)	

#Text=t_env = StreamTableEnvironment.create(environment_settings=environment_settings) name = "mypg" default_database = "mydb" username = "..." password = "..." base_url = "..."
67-1	15207-15212	t_env	
67-2	15213-15214	=	
67-3	15215-15244	StreamTableEnvironment.create	
67-4	15244-15245	(	
67-5	15245-15265	environment_settings	
67-6	15265-15266	=	
67-7	15266-15286	environment_settings	
67-8	15286-15287	)	
67-9	15288-15292	name	
67-10	15293-15294	=	
67-11	15295-15296	"	
67-12	15296-15300	mypg	
67-13	15300-15301	"	
67-14	15302-15318	default_database	
67-15	15319-15320	=	
67-16	15321-15322	"	
67-17	15322-15326	mydb	
67-18	15326-15327	"	
67-19	15328-15336	username	
67-20	15337-15338	=	
67-21	15339-15340	"	
67-22	15340-15341	.	
67-23	15341-15342	.	
67-24	15342-15343	.	
67-25	15343-15344	"	
67-26	15345-15353	password	
67-27	15354-15355	=	
67-28	15356-15357	"	
67-29	15357-15358	.	
67-30	15358-15359	.	
67-31	15359-15360	.	
67-32	15360-15361	"	
67-33	15362-15370	base_url	
67-34	15371-15372	=	
67-35	15373-15374	"	
67-36	15374-15375	.	
67-37	15375-15376	.	
67-38	15376-15377	.	
67-39	15377-15378	"	

#Text=catalog = JdbcCatalog(name, default_database, username, password, base_url) t_env.register_catalog("mypg", catalog) # set the JdbcCatalog as the current catalog of the session t_env.use_catalog("mypg") execution: planner: blink
68-1	15379-15386	catalog	
68-2	15387-15388	=	
68-3	15389-15400	JdbcCatalog	
68-4	15400-15401	(	
68-5	15401-15405	name	
68-6	15405-15406	,	
68-7	15407-15423	default_database	
68-8	15423-15424	,	
68-9	15425-15433	username	
68-10	15433-15434	,	
68-11	15435-15443	password	
68-12	15443-15444	,	
68-13	15445-15453	base_url	
68-14	15453-15454	)	
68-15	15455-15477	t_env.register_catalog	
68-16	15477-15478	(	
68-17	15478-15479	"	
68-18	15479-15483	mypg	
68-19	15483-15484	"	
68-20	15484-15485	,	
68-21	15486-15493	catalog	
68-22	15493-15494	)	
68-23	15495-15496	#	
68-24	15497-15500	set	
68-25	15501-15504	the	
68-26	15505-15516	JdbcCatalog	
68-27	15517-15519	as	
68-28	15520-15523	the	
68-29	15524-15531	current	
68-30	15532-15539	catalog	
68-31	15540-15542	of	
68-32	15543-15546	the	
68-33	15547-15554	session	
68-34	15555-15572	t_env.use_catalog	
68-35	15572-15573	(	
68-36	15573-15574	"	
68-37	15574-15578	mypg	
68-38	15578-15579	"	
68-39	15579-15580	)	
68-40	15581-15590	execution	
68-41	15590-15591	:	
68-42	15592-15599	planner	
68-43	15599-15600	:	
68-44	15601-15606	blink	

#Text=... current-catalog: mypg # set the JdbcCatalog as the current catalog of the session current-database: mydb catalogs: - name: mypg type: jdbc default-database: mydb
69-1	15607-15608	.	
69-2	15608-15609	.	
69-3	15609-15610	.	
69-4	15611-15626	current-catalog	
69-5	15626-15627	:	
69-6	15628-15632	mypg	
69-7	15633-15634	#	
69-8	15635-15638	set	
69-9	15639-15642	the	
69-10	15643-15654	JdbcCatalog	
69-11	15655-15657	as	
69-12	15658-15661	the	
69-13	15662-15669	current	
69-14	15670-15677	catalog	
69-15	15678-15680	of	
69-16	15681-15684	the	
69-17	15685-15692	session	
69-18	15693-15709	current-database	
69-19	15709-15710	:	
69-20	15711-15715	mydb	
69-21	15716-15724	catalogs	
69-22	15724-15725	:	
69-23	15726-15727	-	
69-24	15728-15732	name	
69-25	15732-15733	:	
69-26	15734-15738	mypg	
69-27	15739-15743	type	
69-28	15743-15744	:	
69-29	15745-15749	jdbc	
69-30	15750-15766	default-database	
69-31	15766-15767	:	
69-32	15768-15772	mydb	

#Text=username: ... password: ... base-url: ... PostgresSQL Metaspace Mapping PostgresSQL has an additional namespace as schema besides database. A Postgres instance can have multiple databases, each database can have multiple schemas with a default one named “public”, each schema can have multiple tables.
70-1	15773-15781	username	
70-2	15781-15782	:	
70-3	15783-15784	.	
70-4	15784-15785	.	
70-5	15785-15786	.	
70-6	15787-15795	password	
70-7	15795-15796	:	
70-8	15797-15798	.	
70-9	15798-15799	.	
70-10	15799-15800	.	
70-11	15801-15809	base-url	
70-12	15809-15810	:	
70-13	15811-15812	.	
70-14	15812-15813	.	
70-15	15813-15814	.	
70-16	15815-15826	PostgresSQL	
70-17	15827-15836	Metaspace	
70-18	15837-15844	Mapping	
70-19	15845-15856	PostgresSQL	
70-20	15857-15860	has	
70-21	15861-15863	an	
70-22	15864-15874	additional	
70-23	15875-15884	namespace	
70-24	15885-15887	as	
70-25	15888-15894	schema	
70-26	15895-15902	besides	
70-27	15903-15911	database	
70-28	15911-15912	.	
70-29	15913-15914	A	
70-30	15915-15923	Postgres	
70-31	15924-15932	instance	
70-32	15933-15936	can	
70-33	15937-15941	have	
70-34	15942-15950	multiple	
70-35	15951-15960	databases	
70-36	15960-15961	,	
70-37	15962-15966	each	
70-38	15967-15975	database	
70-39	15976-15979	can	
70-40	15980-15984	have	
70-41	15985-15993	multiple	
70-42	15994-16001	schemas	
70-43	16002-16006	with	
70-44	16007-16008	a	
70-45	16009-16016	default	
70-46	16017-16020	one	
70-47	16021-16026	named	
70-48	16027-16028	“	
70-49	16028-16034	public	
70-50	16034-16035	”	
70-51	16035-16036	,	
70-52	16037-16041	each	
70-53	16042-16048	schema	
70-54	16049-16052	can	
70-55	16053-16057	have	
70-56	16058-16066	multiple	
70-57	16067-16073	tables	
70-58	16073-16074	.	

#Text=In Flink, when querying tables registered by Postgres catalog, users can use either schema_name.table_name or just table_name. The schema_name is optional and defaults to “public”. Therefor the metaspace mapping between Flink Catalog and Postgres is as following: Flink Catalog Metaspace Structure Postgres Metaspace Structure catalog name (defined in Flink only)  database name database name table name
71-1	16075-16077	In	
71-2	16078-16083	Flink	
71-3	16083-16084	,	
71-4	16085-16089	when	
71-5	16090-16098	querying	
71-6	16099-16105	tables	
71-7	16106-16116	registered	
71-8	16117-16119	by	
71-9	16120-16128	Postgres	
71-10	16129-16136	catalog	
71-11	16136-16137	,	
71-12	16138-16143	users	
71-13	16144-16147	can	
71-14	16148-16151	use	
71-15	16152-16158	either	
71-16	16159-16181	schema_name.table_name	
71-17	16182-16184	or	
71-18	16185-16189	just	
71-19	16190-16200	table_name	
71-20	16200-16201	.	
71-21	16202-16205	The	
71-22	16206-16217	schema_name	
71-23	16218-16220	is	
71-24	16221-16229	optional	
71-25	16230-16233	and	
71-26	16234-16242	defaults	
71-27	16243-16245	to	
71-28	16246-16247	“	
71-29	16247-16253	public	
71-30	16253-16254	”	
71-31	16254-16255	.	
71-32	16256-16264	Therefor	
71-33	16265-16268	the	
71-34	16269-16278	metaspace	
71-35	16279-16286	mapping	
71-36	16287-16294	between	
71-37	16295-16300	Flink	
71-38	16301-16308	Catalog	
71-39	16309-16312	and	
71-40	16313-16321	Postgres	
71-41	16322-16324	is	
71-42	16325-16327	as	
71-43	16328-16337	following	
71-44	16337-16338	:	
71-45	16339-16344	Flink	
71-46	16345-16352	Catalog	
71-47	16353-16362	Metaspace	
71-48	16363-16372	Structure	
71-49	16373-16381	Postgres	
71-50	16382-16391	Metaspace	
71-51	16392-16401	Structure	
71-52	16402-16409	catalog	
71-53	16410-16414	name	
71-54	16415-16416	(	
71-55	16416-16423	defined	
71-56	16424-16426	in	
71-57	16427-16432	Flink	
71-58	16433-16437	only	
71-59	16437-16438	)	
71-60	16440-16448	database	
71-61	16449-16453	name	
71-62	16454-16462	database	
71-63	16463-16467	name	
71-64	16468-16473	table	
71-65	16474-16478	name	

#Text=[schema_name.]table_name The full path of Postgres table in Flink should be "<catalog>.<db>.`<schema.table>`" if schema is specified, note the <schema.table> should be escaped. Here are some examples to access Postgres tables:
72-1	16479-16480	[	
72-2	16480-16491	schema_name	
72-3	16491-16492	.	
72-4	16492-16493	]	
72-5	16493-16503	table_name	
72-6	16504-16507	The	
72-7	16508-16512	full	
72-8	16513-16517	path	
72-9	16518-16520	of	
72-10	16521-16529	Postgres	
72-11	16530-16535	table	
72-12	16536-16538	in	
72-13	16539-16544	Flink	
72-14	16545-16551	should	
72-15	16552-16554	be	
72-16	16555-16556	"	
72-17	16556-16557	<	
72-18	16557-16564	catalog	
72-19	16564-16565	>	
72-20	16565-16566	.	
72-21	16566-16567	<	
72-22	16567-16569	db	
72-23	16569-16570	>	
72-24	16570-16571	.	
72-25	16571-16572	`	
72-26	16572-16573	<	
72-27	16573-16585	schema.table	
72-28	16585-16586	>	
72-29	16586-16587	`	
72-30	16587-16588	"	
72-31	16589-16591	if	
72-32	16592-16598	schema	
72-33	16599-16601	is	
72-34	16602-16611	specified	
72-35	16611-16612	,	
72-36	16613-16617	note	
72-37	16618-16621	the	
72-38	16622-16623	<	
72-39	16623-16635	schema.table	
72-40	16635-16636	>	
72-41	16637-16643	should	
72-42	16644-16646	be	
72-43	16647-16654	escaped	
72-44	16654-16655	.	
72-45	16656-16660	Here	
72-46	16661-16664	are	
72-47	16665-16669	some	
72-48	16670-16678	examples	
72-49	16679-16681	to	
72-50	16682-16688	access	
72-51	16689-16697	Postgres	
72-52	16698-16704	tables	
72-53	16704-16705	:	

#Text=-- scan table 'test_table' of 'public' schema (i.e. the default schema), the schema name can be omitted SELECT * FROM mypg.mydb.test_table; SELECT * FROM mydb.test_table; SELECT * FROM test_table; -- scan table 'test_table2' of 'custom_schema' schema,
73-1	16706-16707	-	
73-2	16707-16708	-	
73-3	16709-16713	scan	
73-4	16714-16719	table	
73-5	16720-16721	'	
73-6	16721-16731	test_table	
73-7	16731-16732	'	
73-8	16733-16735	of	
73-9	16736-16737	'	
73-10	16737-16743	public	
73-11	16743-16744	'	
73-12	16745-16751	schema	
73-13	16752-16753	(	
73-14	16753-16756	i.e	
73-15	16756-16757	.	
73-16	16758-16761	the	
73-17	16762-16769	default	
73-18	16770-16776	schema	
73-19	16776-16777	)	
73-20	16777-16778	,	
73-21	16779-16782	the	
73-22	16783-16789	schema	
73-23	16790-16794	name	
73-24	16795-16798	can	
73-25	16799-16801	be	
73-26	16802-16809	omitted	
73-27	16810-16816	SELECT	
73-28	16817-16818	*	
73-29	16819-16823	FROM	
73-30	16824-16844	mypg.mydb.test_table	
73-31	16844-16845	;	
73-32	16846-16852	SELECT	
73-33	16853-16854	*	
73-34	16855-16859	FROM	
73-35	16860-16875	mydb.test_table	
73-36	16875-16876	;	
73-37	16877-16883	SELECT	
73-38	16884-16885	*	
73-39	16886-16890	FROM	
73-40	16891-16901	test_table	
73-41	16901-16902	;	
73-42	16903-16904	-	
73-43	16904-16905	-	
73-44	16906-16910	scan	
73-45	16911-16916	table	
73-46	16917-16918	'	
73-47	16918-16929	test_table2	
73-48	16929-16930	'	
73-49	16931-16933	of	
73-50	16934-16935	'	
73-51	16935-16948	custom_schema	
73-52	16948-16949	'	
73-53	16950-16956	schema	
73-54	16956-16957	,	

#Text=-- the custom schema can not be omitted and must be escaped with table. SELECT * FROM mypg.mydb.`custom_schema.test_table2` SELECT * FROM mydb.`custom_schema.test_table2`;
74-1	16958-16959	-	
74-2	16959-16960	-	
74-3	16961-16964	the	
74-4	16965-16971	custom	
74-5	16972-16978	schema	
74-6	16979-16982	can	
74-7	16983-16986	not	
74-8	16987-16989	be	
74-9	16990-16997	omitted	
74-10	16998-17001	and	
74-11	17002-17006	must	
74-12	17007-17009	be	
74-13	17010-17017	escaped	
74-14	17018-17022	with	
74-15	17023-17028	table	
74-16	17028-17029	.	
74-17	17030-17036	SELECT	
74-18	17037-17038	*	
74-19	17039-17043	FROM	
74-20	17044-17053	mypg.mydb	
74-21	17053-17054	.	
74-22	17054-17055	`	
74-23	17055-17080	custom_schema.test_table2	
74-24	17080-17081	`	
74-25	17082-17088	SELECT	
74-26	17089-17090	*	
74-27	17091-17095	FROM	
74-28	17096-17100	mydb	
74-29	17100-17101	.	
74-30	17101-17102	`	
74-31	17102-17127	custom_schema.test_table2	
74-32	17127-17128	`	
74-33	17128-17129	;	

#Text=SELECT * FROM `custom_schema.test_table2`; Data Type Mapping Flink supports connect to several databases which uses dialect like MySQL, PostgresSQL, Derby. The Derby dialect usually used for testing purpose. The field data type mappings from relational databases data types to Flink SQL data types are listed in the following table, the mapping table can help define JDBC table in Flink easily.
75-1	17130-17136	SELECT	
75-2	17137-17138	*	
75-3	17139-17143	FROM	
75-4	17144-17145	`	
75-5	17145-17170	custom_schema.test_table2	
75-6	17170-17171	`	
75-7	17171-17172	;	
75-8	17173-17177	Data	
75-9	17178-17182	Type	
75-10	17183-17190	Mapping	
75-11	17191-17196	Flink	
75-12	17197-17205	supports	
75-13	17206-17213	connect	
75-14	17214-17216	to	
75-15	17217-17224	several	
75-16	17225-17234	databases	
75-17	17235-17240	which	
75-18	17241-17245	uses	
75-19	17246-17253	dialect	
75-20	17254-17258	like	
75-21	17259-17264	MySQL	
75-22	17264-17265	,	
75-23	17266-17277	PostgresSQL	
75-24	17277-17278	,	
75-25	17279-17284	Derby	
75-26	17284-17285	.	
75-27	17286-17289	The	
75-28	17290-17295	Derby	
75-29	17296-17303	dialect	
75-30	17304-17311	usually	
75-31	17312-17316	used	
75-32	17317-17320	for	
75-33	17321-17328	testing	
75-34	17329-17336	purpose	
75-35	17336-17337	.	
75-36	17338-17341	The	
75-37	17342-17347	field	
75-38	17348-17352	data	
75-39	17353-17357	type	
75-40	17358-17366	mappings	
75-41	17367-17371	from	
75-42	17372-17382	relational	
75-43	17383-17392	databases	
75-44	17393-17397	data	
75-45	17398-17403	types	
75-46	17404-17406	to	
75-47	17407-17412	Flink	
75-48	17413-17416	SQL	
75-49	17417-17421	data	
75-50	17422-17427	types	
75-51	17428-17431	are	
75-52	17432-17438	listed	
75-53	17439-17441	in	
75-54	17442-17445	the	
75-55	17446-17455	following	
75-56	17456-17461	table	
75-57	17461-17462	,	
75-58	17463-17466	the	
75-59	17467-17474	mapping	
75-60	17475-17480	table	
75-61	17481-17484	can	
75-62	17485-17489	help	
75-63	17490-17496	define	
75-64	17497-17501	JDBC	
75-65	17502-17507	table	
75-66	17508-17510	in	
75-67	17511-17516	Flink	
75-68	17517-17523	easily	
75-69	17523-17524	.	

#Text=MySQL type PostgreSQL type Flink SQL type TINYINT TINYINT SMALLINT TINYINT UNSIGNED SMALLINT INT2 SMALLSERIAL
76-1	17525-17530	MySQL	
76-2	17531-17535	type	
76-3	17536-17546	PostgreSQL	
76-4	17547-17551	type	
76-5	17552-17557	Flink	
76-6	17558-17561	SQL	
76-7	17562-17566	type	
76-8	17567-17574	TINYINT	
76-9	17575-17582	TINYINT	
76-10	17583-17591	SMALLINT	
76-11	17592-17599	TINYINT	
76-12	17600-17608	UNSIGNED	
76-13	17609-17617	SMALLINT	
76-14	17618-17622	INT2	
76-15	17623-17634	SMALLSERIAL	

#Text=SERIAL2 SMALLINT INT MEDIUMINT SMALLINT UNSIGNED INTEGER SERIAL INT BIGINT INT UNSIGNED BIGINT
77-1	17635-17642	SERIAL2	
77-2	17643-17651	SMALLINT	
77-3	17652-17655	INT	
77-4	17656-17665	MEDIUMINT	
77-5	17666-17674	SMALLINT	
77-6	17675-17683	UNSIGNED	
77-7	17684-17691	INTEGER	
77-8	17692-17698	SERIAL	
77-9	17699-17702	INT	
77-10	17703-17709	BIGINT	
77-11	17710-17713	INT	
77-12	17714-17722	UNSIGNED	
77-13	17723-17729	BIGINT	

#Text=BIGSERIAL BIGINT BIGINT UNSIGNED DECIMAL(20, 0) BIGINT BIGINT BIGINT FLOAT REAL FLOAT4
78-1	17730-17739	BIGSERIAL	
78-2	17740-17746	BIGINT	
78-3	17747-17753	BIGINT	
78-4	17754-17762	UNSIGNED	
78-5	17763-17770	DECIMAL	
78-6	17770-17771	(	
78-7	17771-17773	20	
78-8	17773-17774	,	
78-9	17775-17776	0	
78-10	17776-17777	)	
78-11	17778-17784	BIGINT	
78-12	17785-17791	BIGINT	
78-13	17792-17798	BIGINT	
78-14	17799-17804	FLOAT	
78-15	17805-17809	REAL	
78-16	17810-17816	FLOAT4	

#Text=FLOAT DOUBLE DOUBLE PRECISION FLOAT8 DOUBLE PRECISION DOUBLE NUMERIC(p, s) DECIMAL(p, s)
79-1	17817-17822	FLOAT	
79-2	17823-17829	DOUBLE	
79-3	17830-17836	DOUBLE	
79-4	17837-17846	PRECISION	
79-5	17847-17853	FLOAT8	
79-6	17854-17860	DOUBLE	
79-7	17861-17870	PRECISION	
79-8	17871-17877	DOUBLE	
79-9	17878-17885	NUMERIC	
79-10	17885-17886	(	
79-11	17886-17887	p	
79-12	17887-17888	,	
79-13	17889-17890	s	
79-14	17890-17891	)	
79-15	17892-17899	DECIMAL	
79-16	17899-17900	(	
79-17	17900-17901	p	
79-18	17901-17902	,	
79-19	17903-17904	s	
79-20	17904-17905	)	

#Text=NUMERIC(p, s) DECIMAL(p, s) DECIMAL(p, s) BOOLEAN TINYINT(1) BOOLEAN BOOLEAN DATE DATE
80-1	17906-17913	NUMERIC	
80-2	17913-17914	(	
80-3	17914-17915	p	
80-4	17915-17916	,	
80-5	17917-17918	s	
80-6	17918-17919	)	
80-7	17920-17927	DECIMAL	
80-8	17927-17928	(	
80-9	17928-17929	p	
80-10	17929-17930	,	
80-11	17931-17932	s	
80-12	17932-17933	)	
80-13	17934-17941	DECIMAL	
80-14	17941-17942	(	
80-15	17942-17943	p	
80-16	17943-17944	,	
80-17	17945-17946	s	
80-18	17946-17947	)	
80-19	17948-17955	BOOLEAN	
80-20	17956-17963	TINYINT	
80-21	17963-17964	(	
80-22	17964-17965	1	
80-23	17965-17966	)	
80-24	17967-17974	BOOLEAN	
80-25	17975-17982	BOOLEAN	
80-26	17983-17987	DATE	
80-27	17988-17992	DATE	

#Text=DATE TIME [(p)] TIME [(p)] [WITHOUT TIMEZONE] TIME [(p)] [WITHOUT TIMEZONE] DATETIME [(p)]
81-1	17993-17997	DATE	
81-2	17998-18002	TIME	
81-3	18003-18004	[	
81-4	18004-18005	(	
81-5	18005-18006	p	
81-6	18006-18007	)	
81-7	18007-18008	]	
81-8	18009-18013	TIME	
81-9	18014-18015	[	
81-10	18015-18016	(	
81-11	18016-18017	p	
81-12	18017-18018	)	
81-13	18018-18019	]	
81-14	18020-18021	[	
81-15	18021-18028	WITHOUT	
81-16	18029-18037	TIMEZONE	
81-17	18037-18038	]	
81-18	18039-18043	TIME	
81-19	18044-18045	[	
81-20	18045-18046	(	
81-21	18046-18047	p	
81-22	18047-18048	)	
81-23	18048-18049	]	
81-24	18050-18051	[	
81-25	18051-18058	WITHOUT	
81-26	18059-18067	TIMEZONE	
81-27	18067-18068	]	
81-28	18069-18077	DATETIME	
81-29	18078-18079	[	
81-30	18079-18080	(	
81-31	18080-18081	p	
81-32	18081-18082	)	
81-33	18082-18083	]	

#Text=TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] CHAR(n) VARCHAR(n) TEXT CHAR(n) CHARACTER(n)
82-1	18084-18093	TIMESTAMP	
82-2	18094-18095	[	
82-3	18095-18096	(	
82-4	18096-18097	p	
82-5	18097-18098	)	
82-6	18098-18099	]	
82-7	18100-18101	[	
82-8	18101-18108	WITHOUT	
82-9	18109-18117	TIMEZONE	
82-10	18117-18118	]	
82-11	18119-18128	TIMESTAMP	
82-12	18129-18130	[	
82-13	18130-18131	(	
82-14	18131-18132	p	
82-15	18132-18133	)	
82-16	18133-18134	]	
82-17	18135-18136	[	
82-18	18136-18143	WITHOUT	
82-19	18144-18152	TIMEZONE	
82-20	18152-18153	]	
82-21	18154-18158	CHAR	
82-22	18158-18159	(	
82-23	18159-18160	n	
82-24	18160-18161	)	
82-25	18162-18169	VARCHAR	
82-26	18169-18170	(	
82-27	18170-18171	n	
82-28	18171-18172	)	
82-29	18173-18177	TEXT	
82-30	18178-18182	CHAR	
82-31	18182-18183	(	
82-32	18183-18184	n	
82-33	18184-18185	)	
82-34	18186-18195	CHARACTER	
82-35	18195-18196	(	
82-36	18196-18197	n	
82-37	18197-18198	)	

#Text=VARCHAR(n) CHARACTER VARYING(n) TEXT STRING BINARY VARBINARY BLOB BYTEA BYTES ARRAY ARRAY
83-1	18199-18206	VARCHAR	
83-2	18206-18207	(	
83-3	18207-18208	n	
83-4	18208-18209	)	
83-5	18210-18219	CHARACTER	
83-6	18220-18227	VARYING	
83-7	18227-18228	(	
83-8	18228-18229	n	
83-9	18229-18230	)	
83-10	18231-18235	TEXT	
83-11	18236-18242	STRING	
83-12	18243-18249	BINARY	
83-13	18250-18259	VARBINARY	
83-14	18260-18264	BLOB	
83-15	18265-18270	BYTEA	
83-16	18271-18276	BYTES	
83-17	18277-18282	ARRAY	
83-18	18283-18288	ARRAY	
