#FORMAT=WebAnno TSV 3.3
#T_SP=custom.Span|label
#T_RL=custom.Relation|label|BT_custom.Span


#Text=ZFSTuningGuide - FreeBSD Wiki
#Text=Search:
#Text=Login
#Text=ZFSTuningGuide
#Text=RecentChangesFindPageHelpContentsZFSTuningGuide
#Text=Immutable PageCommentsInfoAttachments
#Text=More Actions:
#Text=Raw Text
#Text=Print View
#Text=Render as Docbook
#Text=Delete Cache
#Text=------------------------
#Text=Check Spelling
#Text=Like Pages
#Text=Local Site Map
#Text=------------------------
#Text=Rename Page
#Text=Delete Page
#Text=------------------------
#Text=Subscribe User
#Text=------------------------
#Text=Remove Spam
#Text=Revert to this revision
#Text=Package Pages
#Text=Sync Pages
#Text=------------------------
#Text=Load
#Text=Save
#Text=SlideShow
#Text=Contents
#Text=ZFS Tuning Guide
#Text=i386
#Text=amd64
#Text=Generic ARC discussion
#Text=L2ARC discussion
#Text=Application Issues
#Text=General Tuning
#Text=Deduplication
#Text=Suggestions
#Text=References
#Text=NFS tuning
#Text=MySQL
#Text=Scrub and Resilver Performance
#Text=See also: Solaris: ZFS Evil Tuning Guide, loader.conf(5), sysctl(8).
1-1	0-14	ZFSTuningGuide	_	_	_	
1-2	15-16	-	_	_	_	
1-3	17-24	FreeBSD	_	_	_	
1-4	25-29	Wiki	_	_	_	
1-5	30-36	Search	_	_	_	
1-6	36-37	:	_	_	_	
1-7	38-43	Login	_	_	_	
1-8	44-58	ZFSTuningGuide	_	_	_	
1-9	59-106	RecentChangesFindPageHelpContentsZFSTuningGuide	_	_	_	
1-10	107-116	Immutable	_	_	_	
1-11	117-144	PageCommentsInfoAttachments	_	_	_	
1-12	145-149	More	_	_	_	
1-13	150-157	Actions	_	_	_	
1-14	157-158	:	_	_	_	
1-15	159-162	Raw	_	_	_	
1-16	163-167	Text	_	_	_	
1-17	168-173	Print	_	_	_	
1-18	174-178	View	_	_	_	
1-19	179-185	Render	_	_	_	
1-20	186-188	as	_	_	_	
1-21	189-196	Docbook	_	_	_	
1-22	197-203	Delete	_	_	_	
1-23	204-209	Cache	_	_	_	
1-24	210-211	-	_	_	_	
1-25	211-212	-	_	_	_	
1-26	212-213	-	_	_	_	
1-27	213-214	-	_	_	_	
1-28	214-215	-	_	_	_	
1-29	215-216	-	_	_	_	
1-30	216-217	-	_	_	_	
1-31	217-218	-	_	_	_	
1-32	218-219	-	_	_	_	
1-33	219-220	-	_	_	_	
1-34	220-221	-	_	_	_	
1-35	221-222	-	_	_	_	
1-36	222-223	-	_	_	_	
1-37	223-224	-	_	_	_	
1-38	224-225	-	_	_	_	
1-39	225-226	-	_	_	_	
1-40	226-227	-	_	_	_	
1-41	227-228	-	_	_	_	
1-42	228-229	-	_	_	_	
1-43	229-230	-	_	_	_	
1-44	230-231	-	_	_	_	
1-45	231-232	-	_	_	_	
1-46	232-233	-	_	_	_	
1-47	233-234	-	_	_	_	
1-48	235-240	Check	_	_	_	
1-49	241-249	Spelling	_	_	_	
1-50	250-254	Like	_	_	_	
1-51	255-260	Pages	_	_	_	
1-52	261-266	Local	_	_	_	
1-53	267-271	Site	_	_	_	
1-54	272-275	Map	_	_	_	
1-55	276-277	-	_	_	_	
1-56	277-278	-	_	_	_	
1-57	278-279	-	_	_	_	
1-58	279-280	-	_	_	_	
1-59	280-281	-	_	_	_	
1-60	281-282	-	_	_	_	
1-61	282-283	-	_	_	_	
1-62	283-284	-	_	_	_	
1-63	284-285	-	_	_	_	
1-64	285-286	-	_	_	_	
1-65	286-287	-	_	_	_	
1-66	287-288	-	_	_	_	
1-67	288-289	-	_	_	_	
1-68	289-290	-	_	_	_	
1-69	290-291	-	_	_	_	
1-70	291-292	-	_	_	_	
1-71	292-293	-	_	_	_	
1-72	293-294	-	_	_	_	
1-73	294-295	-	_	_	_	
1-74	295-296	-	_	_	_	
1-75	296-297	-	_	_	_	
1-76	297-298	-	_	_	_	
1-77	298-299	-	_	_	_	
1-78	299-300	-	_	_	_	
1-79	301-307	Rename	_	_	_	
1-80	308-312	Page	_	_	_	
1-81	313-319	Delete	_	_	_	
1-82	320-324	Page	_	_	_	
1-83	325-326	-	_	_	_	
1-84	326-327	-	_	_	_	
1-85	327-328	-	_	_	_	
1-86	328-329	-	_	_	_	
1-87	329-330	-	_	_	_	
1-88	330-331	-	_	_	_	
1-89	331-332	-	_	_	_	
1-90	332-333	-	_	_	_	
1-91	333-334	-	_	_	_	
1-92	334-335	-	_	_	_	
1-93	335-336	-	_	_	_	
1-94	336-337	-	_	_	_	
1-95	337-338	-	_	_	_	
1-96	338-339	-	_	_	_	
1-97	339-340	-	_	_	_	
1-98	340-341	-	_	_	_	
1-99	341-342	-	_	_	_	
1-100	342-343	-	_	_	_	
1-101	343-344	-	_	_	_	
1-102	344-345	-	_	_	_	
1-103	345-346	-	_	_	_	
1-104	346-347	-	_	_	_	
1-105	347-348	-	_	_	_	
1-106	348-349	-	_	_	_	
1-107	350-359	Subscribe	_	_	_	
1-108	360-364	User	_	_	_	
1-109	365-366	-	_	_	_	
1-110	366-367	-	_	_	_	
1-111	367-368	-	_	_	_	
1-112	368-369	-	_	_	_	
1-113	369-370	-	_	_	_	
1-114	370-371	-	_	_	_	
1-115	371-372	-	_	_	_	
1-116	372-373	-	_	_	_	
1-117	373-374	-	_	_	_	
1-118	374-375	-	_	_	_	
1-119	375-376	-	_	_	_	
1-120	376-377	-	_	_	_	
1-121	377-378	-	_	_	_	
1-122	378-379	-	_	_	_	
1-123	379-380	-	_	_	_	
1-124	380-381	-	_	_	_	
1-125	381-382	-	_	_	_	
1-126	382-383	-	_	_	_	
1-127	383-384	-	_	_	_	
1-128	384-385	-	_	_	_	
1-129	385-386	-	_	_	_	
1-130	386-387	-	_	_	_	
1-131	387-388	-	_	_	_	
1-132	388-389	-	_	_	_	
1-133	390-396	Remove	_	_	_	
1-134	397-401	Spam	_	_	_	
1-135	402-408	Revert	_	_	_	
1-136	409-411	to	_	_	_	
1-137	412-416	this	_	_	_	
1-138	417-425	revision	_	_	_	
1-139	426-433	Package	_	_	_	
1-140	434-439	Pages	_	_	_	
1-141	440-444	Sync	_	_	_	
1-142	445-450	Pages	_	_	_	
1-143	451-452	-	_	_	_	
1-144	452-453	-	_	_	_	
1-145	453-454	-	_	_	_	
1-146	454-455	-	_	_	_	
1-147	455-456	-	_	_	_	
1-148	456-457	-	_	_	_	
1-149	457-458	-	_	_	_	
1-150	458-459	-	_	_	_	
1-151	459-460	-	_	_	_	
1-152	460-461	-	_	_	_	
1-153	461-462	-	_	_	_	
1-154	462-463	-	_	_	_	
1-155	463-464	-	_	_	_	
1-156	464-465	-	_	_	_	
1-157	465-466	-	_	_	_	
1-158	466-467	-	_	_	_	
1-159	467-468	-	_	_	_	
1-160	468-469	-	_	_	_	
1-161	469-470	-	_	_	_	
1-162	470-471	-	_	_	_	
1-163	471-472	-	_	_	_	
1-164	472-473	-	_	_	_	
1-165	473-474	-	_	_	_	
1-166	474-475	-	_	_	_	
1-167	476-480	Load	_	_	_	
1-168	481-485	Save	_	_	_	
1-169	486-495	SlideShow	_	_	_	
1-170	496-504	Contents	_	_	_	
1-171	505-508	ZFS	_	_	_	
1-172	509-515	Tuning	_	_	_	
1-173	516-521	Guide	_	_	_	
1-174	522-526	i386	_	_	_	
1-175	527-532	amd64	_	_	_	
1-176	533-540	Generic	_	_	_	
1-177	541-544	ARC	_	_	_	
1-178	545-555	discussion	_	_	_	
1-179	556-561	L2ARC	_	_	_	
1-180	562-572	discussion	_	_	_	
1-181	573-584	Application	_	_	_	
1-182	585-591	Issues	_	_	_	
1-183	592-599	General	_	_	_	
1-184	600-606	Tuning	_	_	_	
1-185	607-620	Deduplication	_	_	_	
1-186	621-632	Suggestions	_	_	_	
1-187	633-643	References	_	_	_	
1-188	644-647	NFS	_	_	_	
1-189	648-654	tuning	_	_	_	
1-190	655-660	MySQL	_	_	_	
1-191	661-666	Scrub	_	_	_	
1-192	667-670	and	_	_	_	
1-193	671-679	Resilver	_	_	_	
1-194	680-691	Performance	_	_	_	
1-195	692-695	See	_	_	_	
1-196	696-700	also	_	_	_	
1-197	700-701	:	_	_	_	
1-198	702-709	Solaris	_	_	_	
1-199	709-710	:	_	_	_	
1-200	711-714	ZFS	_	_	_	
1-201	715-719	Evil	_	_	_	
1-202	720-726	Tuning	_	_	_	
1-203	727-732	Guide	_	_	_	
1-204	732-733	,	_	_	_	
1-205	734-745	loader.conf	_	_	_	
1-206	745-746	(	_	_	_	
1-207	746-747	5	_	_	_	
1-208	747-748	)	_	_	_	
1-209	748-749	,	_	_	_	
1-210	750-756	sysctl	_	_	_	
1-211	756-757	(	_	_	_	
1-212	757-758	8	_	_	_	
1-213	758-759	)	_	_	_	
1-214	759-760	.	_	_	_	

#Text=ZFS Tuning Guide
#Text=(Work in Progress) To use ZFS, at least 1 GB of memory is recommended (for all architectures) but more is helpful as ZFS needs *lots* of memory.
2-1	761-764	ZFS	_	_	_	
2-2	765-771	Tuning	_	_	_	
2-3	772-777	Guide	_	_	_	
2-4	778-779	(	_	_	_	
2-5	779-783	Work	_	_	_	
2-6	784-786	in	_	_	_	
2-7	787-795	Progress	_	_	_	
2-8	795-796	)	_	_	_	
2-9	797-799	To	_	_	_	
2-10	800-803	use	_	_	_	
2-11	804-807	ZFS	_	_	_	
2-12	807-808	,	_	_	_	
2-13	809-811	at	_	_	_	
2-14	812-817	least	_	_	_	
2-15	818-819	1	_	_	_	
2-16	819-820	 	_	_	_	
2-17	820-822	GB	_	_	_	
2-18	823-825	of	_	_	_	
2-19	826-832	memory	_	_	_	
2-20	833-835	is	_	_	_	
2-21	836-847	recommended	_	_	_	
2-22	848-849	(	_	_	_	
2-23	849-852	for	_	_	_	
2-24	853-856	all	_	_	_	
2-25	857-870	architectures	_	_	_	
2-26	870-871	)	_	_	_	
2-27	872-875	but	_	_	_	
2-28	876-880	more	_	_	_	
2-29	881-883	is	_	_	_	
2-30	884-891	helpful	_	_	_	
2-31	892-894	as	_	_	_	
2-32	895-898	ZFS	_	_	_	
2-33	899-904	needs	_	_	_	
2-34	905-906	*	_	_	_	
2-35	906-910	lots	_	_	_	
2-36	910-911	*	_	_	_	
2-37	912-914	of	_	_	_	
2-38	915-921	memory	_	_	_	
2-39	921-922	.	_	_	_	

#Text=Depending on your workload, it may be possible to use ZFS on systems with less memory, but it requires careful tuning to avoid panics from memory exhaustion in the kernel.
3-1	923-932	Depending	_	_	_	
3-2	933-935	on	_	_	_	
3-3	936-940	your	_	_	_	
3-4	941-949	workload	_	_	_	
3-5	949-950	,	_	_	_	
3-6	951-953	it	_	_	_	
3-7	954-957	may	_	_	_	
3-8	958-960	be	_	_	_	
3-9	961-969	possible	_	_	_	
3-10	970-972	to	_	_	_	
3-11	973-976	use	_	_	_	
3-12	977-980	ZFS	_	_	_	
3-13	981-983	on	_	_	_	
3-14	984-991	systems	_	_	_	
3-15	992-996	with	_	_	_	
3-16	997-1001	less	_	_	_	
3-17	1002-1008	memory	_	_	_	
3-18	1008-1009	,	_	_	_	
3-19	1010-1013	but	_	_	_	
3-20	1014-1016	it	_	_	_	
3-21	1017-1025	requires	_	_	_	
3-22	1026-1033	careful	_	_	_	
3-23	1034-1040	tuning	_	_	_	
3-24	1041-1043	to	_	_	_	
3-25	1044-1049	avoid	_	_	_	
3-26	1050-1056	panics	_	_	_	
3-27	1057-1061	from	_	_	_	
3-28	1062-1068	memory	_	_	_	
3-29	1069-1079	exhaustion	_	_	_	
3-30	1080-1082	in	_	_	_	
3-31	1083-1086	the	_	_	_	
3-32	1087-1093	kernel	_	_	_	
3-33	1093-1094	.	_	_	_	

#Text=A 64-bit system is preferred due to its larger address space and better performance on 64-bit variables, which are used extensively by ZFS. 32-bit systems are supported though, with sufficient tuning.
4-1	1095-1096	A	_	_	_	
4-2	1097-1099	64	_	_	_	
4-3	1099-1100	-	_	_	_	
4-4	1100-1103	bit	_	_	_	
4-5	1104-1110	system	_	_	_	
4-6	1111-1113	is	_	_	_	
4-7	1114-1123	preferred	_	_	_	
4-8	1124-1127	due	_	_	_	
4-9	1128-1130	to	_	_	_	
4-10	1131-1134	its	_	_	_	
4-11	1135-1141	larger	_	_	_	
4-12	1142-1149	address	_	_	_	
4-13	1150-1155	space	_	_	_	
4-14	1156-1159	and	_	_	_	
4-15	1160-1166	better	_	_	_	
4-16	1167-1178	performance	_	_	_	
4-17	1179-1181	on	_	_	_	
4-18	1182-1184	64	_	_	_	
4-19	1184-1185	-	_	_	_	
4-20	1185-1188	bit	_	_	_	
4-21	1189-1198	variables	_	_	_	
4-22	1198-1199	,	_	_	_	
4-23	1200-1205	which	_	_	_	
4-24	1206-1209	are	_	_	_	
4-25	1210-1214	used	_	_	_	
4-26	1215-1226	extensively	_	_	_	
4-27	1227-1229	by	_	_	_	
4-28	1230-1233	ZFS	_	_	_	
4-29	1233-1234	.	_	_	_	
4-30	1235-1237	32	_	_	_	
4-31	1237-1238	-	_	_	_	
4-32	1238-1241	bit	_	_	_	
4-33	1242-1249	systems	_	_	_	
4-34	1250-1253	are	_	_	_	
4-35	1254-1263	supported	_	_	_	
4-36	1264-1270	though	_	_	_	
4-37	1270-1271	,	_	_	_	
4-38	1272-1276	with	_	_	_	
4-39	1277-1287	sufficient	_	_	_	
4-40	1288-1294	tuning	_	_	_	
4-41	1294-1295	.	_	_	_	

#Text=History of FreeBSD releases with ZFS is as follows: 7.0+ - original ZFS import, ZFS v6; requires significant tuning for stable operation (no longer supported) 7.2 - still ZFS v6, improved memory handling, amd64 may need no memory tuning (no longer supported) 7.3+ - backport of new ZFS v13 code, similar to the 8.0 code 8.0 - new ZFS v13 code, lots of bug fixes - recommended over all past versions.
5-1	1296-1303	History	_	_	_	
5-2	1304-1306	of	_	_	_	
5-3	1307-1314	FreeBSD	_	_	_	
5-4	1315-1323	releases	_	_	_	
5-5	1324-1328	with	_	_	_	
5-6	1329-1332	ZFS	_	_	_	
5-7	1333-1335	is	_	_	_	
5-8	1336-1338	as	_	_	_	
5-9	1339-1346	follows	_	_	_	
5-10	1346-1347	:	_	_	_	
5-11	1348-1351	7.0	_	_	_	
5-12	1351-1352	+	_	_	_	
5-13	1353-1354	-	_	_	_	
5-14	1355-1363	original	_	_	_	
5-15	1364-1367	ZFS	_	_	_	
5-16	1368-1374	import	_	_	_	
5-17	1374-1375	,	_	_	_	
5-18	1376-1379	ZFS	_	_	_	
5-19	1380-1382	v6	_	_	_	
5-20	1382-1383	;	_	_	_	
5-21	1384-1392	requires	_	_	_	
5-22	1393-1404	significant	_	_	_	
5-23	1405-1411	tuning	_	_	_	
5-24	1412-1415	for	_	_	_	
5-25	1416-1422	stable	_	_	_	
5-26	1423-1432	operation	_	_	_	
5-27	1433-1434	(	_	_	_	
5-28	1434-1436	no	_	_	_	
5-29	1437-1443	longer	_	_	_	
5-30	1444-1453	supported	_	_	_	
5-31	1453-1454	)	_	_	_	
5-32	1455-1458	7.2	_	_	_	
5-33	1459-1460	-	_	_	_	
5-34	1461-1466	still	_	_	_	
5-35	1467-1470	ZFS	_	_	_	
5-36	1471-1473	v6	_	_	_	
5-37	1473-1474	,	_	_	_	
5-38	1475-1483	improved	_	_	_	
5-39	1484-1490	memory	_	_	_	
5-40	1491-1499	handling	_	_	_	
5-41	1499-1500	,	_	_	_	
5-42	1501-1506	amd64	_	_	_	
5-43	1507-1510	may	_	_	_	
5-44	1511-1515	need	_	_	_	
5-45	1516-1518	no	_	_	_	
5-46	1519-1525	memory	_	_	_	
5-47	1526-1532	tuning	_	_	_	
5-48	1533-1534	(	_	_	_	
5-49	1534-1536	no	_	_	_	
5-50	1537-1543	longer	_	_	_	
5-51	1544-1553	supported	_	_	_	
5-52	1553-1554	)	_	_	_	
5-53	1555-1558	7.3	_	_	_	
5-54	1558-1559	+	_	_	_	
5-55	1560-1561	-	_	_	_	
5-56	1562-1570	backport	_	_	_	
5-57	1571-1573	of	_	_	_	
5-58	1574-1577	new	_	_	_	
5-59	1578-1581	ZFS	_	_	_	
5-60	1582-1585	v13	_	_	_	
5-61	1586-1590	code	_	_	_	
5-62	1590-1591	,	_	_	_	
5-63	1592-1599	similar	_	_	_	
5-64	1600-1602	to	_	_	_	
5-65	1603-1606	the	_	_	_	
5-66	1607-1610	8.0	_	_	_	
5-67	1611-1615	code	_	_	_	
5-68	1616-1619	8.0	_	_	_	
5-69	1620-1621	-	_	_	_	
5-70	1622-1625	new	_	_	_	
5-71	1626-1629	ZFS	_	_	_	
5-72	1630-1633	v13	_	_	_	
5-73	1634-1638	code	_	_	_	
5-74	1638-1639	,	_	_	_	
5-75	1640-1644	lots	_	_	_	
5-76	1645-1647	of	_	_	_	
5-77	1648-1651	bug	_	_	_	
5-78	1652-1657	fixes	_	_	_	
5-79	1658-1659	-	_	_	_	
5-80	1660-1671	recommended	_	_	_	
5-81	1672-1676	over	_	_	_	
5-82	1677-1680	all	_	_	_	
5-83	1681-1685	past	_	_	_	
5-84	1686-1694	versions	_	_	_	
5-85	1694-1695	.	_	_	_	

#Text=(no longer supported) 8.1+ - ZFS v14 8.2+ - ZFS v15 8.3+ - ZFS v28 9.0+ - ZFS v28
#Text=i386
#Text=Typically you need to increase vm.kmem_size_max and vm.kmem_size (with vm.kmem_size_max >= vm.kmem_size) to not get kernel panics (kmem too small).
6-1	1696-1697	(	_	_	_	
6-2	1697-1699	no	_	_	_	
6-3	1700-1706	longer	_	_	_	
6-4	1707-1716	supported	_	_	_	
6-5	1716-1717	)	_	_	_	
6-6	1718-1721	8.1	_	_	_	
6-7	1721-1722	+	_	_	_	
6-8	1723-1724	-	_	_	_	
6-9	1725-1728	ZFS	_	_	_	
6-10	1729-1732	v14	_	_	_	
6-11	1733-1736	8.2	_	_	_	
6-12	1736-1737	+	_	_	_	
6-13	1738-1739	-	_	_	_	
6-14	1740-1743	ZFS	_	_	_	
6-15	1744-1747	v15	_	_	_	
6-16	1748-1751	8.3	_	_	_	
6-17	1751-1752	+	_	_	_	
6-18	1753-1754	-	_	_	_	
6-19	1755-1758	ZFS	_	_	_	
6-20	1759-1762	v28	_	_	_	
6-21	1763-1766	9.0	_	_	_	
6-22	1766-1767	+	_	_	_	
6-23	1768-1769	-	_	_	_	
6-24	1770-1773	ZFS	_	_	_	
6-25	1774-1777	v28	_	_	_	
6-26	1778-1782	i386	_	_	_	
6-27	1783-1792	Typically	_	_	_	
6-28	1793-1796	you	_	_	_	
6-29	1797-1801	need	_	_	_	
6-30	1802-1804	to	_	_	_	
6-31	1805-1813	increase	_	_	_	
6-32	1814-1830	vm.kmem_size_max	_	_	_	
6-33	1831-1834	and	_	_	_	
6-34	1835-1847	vm.kmem_size	_	_	_	
6-35	1848-1849	(	_	_	_	
6-36	1849-1853	with	_	_	_	
6-37	1854-1870	vm.kmem_size_max	_	_	_	
6-38	1871-1872	>	_	_	_	
6-39	1872-1873	=	_	_	_	
6-40	1874-1886	vm.kmem_size	_	_	_	
6-41	1886-1887	)	_	_	_	
6-42	1888-1890	to	_	_	_	
6-43	1891-1894	not	_	_	_	
6-44	1895-1898	get	_	_	_	
6-45	1899-1905	kernel	_	_	_	
6-46	1906-1912	panics	_	_	_	
6-47	1913-1914	(	_	_	_	
6-48	1914-1918	kmem	_	_	_	
6-49	1919-1922	too	_	_	_	
6-50	1923-1928	small	_	_	_	
6-51	1928-1929	)	_	_	_	
6-52	1929-1930	.	_	_	_	

#Text=The value depends upon the workload.
7-1	1931-1934	The	_	_	_	
7-2	1935-1940	value	_	_	_	
7-3	1941-1948	depends	_	_	_	
7-4	1949-1953	upon	_	_	_	
7-5	1954-1957	the	_	_	_	
7-6	1958-1966	workload	_	_	_	
7-7	1966-1967	.	_	_	_	

#Text=If you need to extend them beyond 512M, you need to recompile your kernel with increased KVA_PAGES option, e.g. add the following line to your kernel configuration file to increase available space for vm.kmem_size beyond 1 GB: options KVA_PAGES=512 To chose a good value for KVA_PAGES read the explanation in the sys/i386/conf/NOTES file.
8-1	1968-1970	If	_	_	_	
8-2	1971-1974	you	_	_	_	
8-3	1975-1979	need	_	_	_	
8-4	1980-1982	to	_	_	_	
8-5	1983-1989	extend	_	_	_	
8-6	1990-1994	them	_	_	_	
8-7	1995-2001	beyond	_	_	_	
8-8	2002-2006	512M	_	_	_	
8-9	2006-2007	,	_	_	_	
8-10	2008-2011	you	_	_	_	
8-11	2012-2016	need	_	_	_	
8-12	2017-2019	to	_	_	_	
8-13	2020-2029	recompile	_	_	_	
8-14	2030-2034	your	_	_	_	
8-15	2035-2041	kernel	_	_	_	
8-16	2042-2046	with	_	_	_	
8-17	2047-2056	increased	_	_	_	
8-18	2057-2066	KVA_PAGES	_	_	_	
8-19	2067-2073	option	_	_	_	
8-20	2073-2074	,	_	_	_	
8-21	2075-2078	e.g	_	_	_	
8-22	2078-2079	.	_	_	_	
8-23	2080-2083	add	_	_	_	
8-24	2084-2087	the	_	_	_	
8-25	2088-2097	following	_	_	_	
8-26	2098-2102	line	_	_	_	
8-27	2103-2105	to	_	_	_	
8-28	2106-2110	your	_	_	_	
8-29	2111-2117	kernel	_	_	_	
8-30	2118-2131	configuration	_	_	_	
8-31	2132-2136	file	_	_	_	
8-32	2137-2139	to	_	_	_	
8-33	2140-2148	increase	_	_	_	
8-34	2149-2158	available	_	_	_	
8-35	2159-2164	space	_	_	_	
8-36	2165-2168	for	_	_	_	
8-37	2169-2181	vm.kmem_size	_	_	_	
8-38	2182-2188	beyond	_	_	_	
8-39	2189-2190	1	_	_	_	
8-40	2190-2191	 	_	_	_	
8-41	2191-2193	GB	_	_	_	
8-42	2193-2194	:	_	_	_	
8-43	2195-2202	options	_	_	_	
8-44	2202-2203	 	_	_	_	
8-45	2203-2212	KVA_PAGES	_	_	_	
8-46	2212-2213	=	_	_	_	
8-47	2213-2216	512	_	_	_	
8-48	2217-2219	To	_	_	_	
8-49	2220-2225	chose	_	_	_	
8-50	2226-2227	a	_	_	_	
8-51	2228-2232	good	_	_	_	
8-52	2233-2238	value	_	_	_	
8-53	2239-2242	for	_	_	_	
8-54	2243-2252	KVA_PAGES	_	_	_	
8-55	2253-2257	read	_	_	_	
8-56	2258-2261	the	_	_	_	
8-57	2262-2273	explanation	_	_	_	
8-58	2274-2276	in	_	_	_	
8-59	2277-2280	the	_	_	_	
8-60	2281-2284	sys	_	_	_	
8-61	2284-2285	/	_	_	_	
8-62	2285-2289	i386	_	_	_	
8-63	2289-2290	/	_	_	_	
8-64	2290-2294	conf	_	_	_	
8-65	2294-2295	/	_	_	_	
8-66	2295-2300	NOTES	_	_	_	
8-67	2301-2305	file	_	_	_	
8-68	2305-2306	.	_	_	_	

#Text=By default the kernel receives 1 GB of the 4 GB of address space available on the i386 architecture, and this is used for all of the kernel address space needs, not just the kmem map.
9-1	2307-2309	By	_	_	_	
9-2	2310-2317	default	_	_	_	
9-3	2318-2321	the	_	_	_	
9-4	2322-2328	kernel	_	_	_	
9-5	2329-2337	receives	_	_	_	
9-6	2338-2339	1	_	_	_	
9-7	2339-2340	 	_	_	_	
9-8	2340-2342	GB	_	_	_	
9-9	2343-2345	of	_	_	_	
9-10	2346-2349	the	_	_	_	
9-11	2350-2351	4	_	_	_	
9-12	2351-2352	 	_	_	_	
9-13	2352-2354	GB	_	_	_	
9-14	2355-2357	of	_	_	_	
9-15	2358-2365	address	_	_	_	
9-16	2366-2371	space	_	_	_	
9-17	2372-2381	available	_	_	_	
9-18	2382-2384	on	_	_	_	
9-19	2385-2388	the	_	_	_	
9-20	2389-2393	i386	_	_	_	
9-21	2394-2406	architecture	_	_	_	
9-22	2406-2407	,	_	_	_	
9-23	2408-2411	and	_	_	_	
9-24	2412-2416	this	_	_	_	
9-25	2417-2419	is	_	_	_	
9-26	2420-2424	used	_	_	_	
9-27	2425-2428	for	_	_	_	
9-28	2429-2432	all	_	_	_	
9-29	2433-2435	of	_	_	_	
9-30	2436-2439	the	_	_	_	
9-31	2440-2446	kernel	_	_	_	
9-32	2447-2454	address	_	_	_	
9-33	2455-2460	space	_	_	_	
9-34	2461-2466	needs	_	_	_	
9-35	2466-2467	,	_	_	_	
9-36	2468-2471	not	_	_	_	
9-37	2472-2476	just	_	_	_	
9-38	2477-2480	the	_	_	_	
9-39	2481-2485	kmem	_	_	_	
9-40	2486-2489	map	_	_	_	
9-41	2489-2490	.	_	_	_	

#Text=By increasing KVA_PAGES you can allocate a larger proportion of the 4 GB address space to the kernel (2 GB in the above example), allowing more room to increase vm.kmem_size.
10-1	2491-2493	By	_	_	_	
10-2	2494-2504	increasing	_	_	_	
10-3	2505-2514	KVA_PAGES	_	_	_	
10-4	2515-2518	you	_	_	_	
10-5	2519-2522	can	_	_	_	
10-6	2523-2531	allocate	_	_	_	
10-7	2532-2533	a	_	_	_	
10-8	2534-2540	larger	_	_	_	
10-9	2541-2551	proportion	_	_	_	
10-10	2552-2554	of	_	_	_	
10-11	2555-2558	the	_	_	_	
10-12	2559-2560	4	_	_	_	
10-13	2560-2561	 	_	_	_	
10-14	2561-2563	GB	_	_	_	
10-15	2564-2571	address	_	_	_	
10-16	2572-2577	space	_	_	_	
10-17	2578-2580	to	_	_	_	
10-18	2581-2584	the	_	_	_	
10-19	2585-2591	kernel	_	_	_	
10-20	2592-2593	(	_	_	_	
10-21	2593-2594	2	_	_	_	
10-22	2594-2595	 	_	_	_	
10-23	2595-2597	GB	_	_	_	
10-24	2598-2600	in	_	_	_	
10-25	2601-2604	the	_	_	_	
10-26	2605-2610	above	_	_	_	
10-27	2611-2618	example	_	_	_	
10-28	2618-2619	)	_	_	_	
10-29	2619-2620	,	_	_	_	
10-30	2621-2629	allowing	_	_	_	
10-31	2630-2634	more	_	_	_	
10-32	2635-2639	room	_	_	_	
10-33	2640-2642	to	_	_	_	
10-34	2643-2651	increase	_	_	_	
10-35	2652-2664	vm.kmem_size	_	_	_	
10-36	2664-2665	.	_	_	_	

#Text=The trade-off is that user applications have less address space available, and some programs (e.g. those that rely on mapping data at a fixed address that is now in the kernel address space, or which require close to the full 3 GB of address space themselves) may no longer run.
11-1	2666-2669	The	_	_	_	
11-2	2670-2679	trade-off	_	_	_	
11-3	2680-2682	is	_	_	_	
11-4	2683-2687	that	_	_	_	
11-5	2688-2692	user	_	_	_	
11-6	2693-2705	applications	_	_	_	
11-7	2706-2710	have	_	_	_	
11-8	2711-2715	less	_	_	_	
11-9	2716-2723	address	_	_	_	
11-10	2724-2729	space	_	_	_	
11-11	2730-2739	available	_	_	_	
11-12	2739-2740	,	_	_	_	
11-13	2741-2744	and	_	_	_	
11-14	2745-2749	some	_	_	_	
11-15	2750-2758	programs	_	_	_	
11-16	2759-2760	(	_	_	_	
11-17	2760-2763	e.g	_	_	_	
11-18	2763-2764	.	_	_	_	
11-19	2765-2770	those	_	_	_	
11-20	2771-2775	that	_	_	_	
11-21	2776-2780	rely	_	_	_	
11-22	2781-2783	on	_	_	_	
11-23	2784-2791	mapping	_	_	_	
11-24	2792-2796	data	_	_	_	
11-25	2797-2799	at	_	_	_	
11-26	2800-2801	a	_	_	_	
11-27	2802-2807	fixed	_	_	_	
11-28	2808-2815	address	_	_	_	
11-29	2816-2820	that	_	_	_	
11-30	2821-2823	is	_	_	_	
11-31	2824-2827	now	_	_	_	
11-32	2828-2830	in	_	_	_	
11-33	2831-2834	the	_	_	_	
11-34	2835-2841	kernel	_	_	_	
11-35	2842-2849	address	_	_	_	
11-36	2850-2855	space	_	_	_	
11-37	2855-2856	,	_	_	_	
11-38	2857-2859	or	_	_	_	
11-39	2860-2865	which	_	_	_	
11-40	2866-2873	require	_	_	_	
11-41	2874-2879	close	_	_	_	
11-42	2880-2882	to	_	_	_	
11-43	2883-2886	the	_	_	_	
11-44	2887-2891	full	_	_	_	
11-45	2892-2893	3	_	_	_	
11-46	2893-2894	 	_	_	_	
11-47	2894-2896	GB	_	_	_	
11-48	2897-2899	of	_	_	_	
11-49	2900-2907	address	_	_	_	
11-50	2908-2913	space	_	_	_	
11-51	2914-2924	themselves	_	_	_	
11-52	2924-2925	)	_	_	_	
11-53	2926-2929	may	_	_	_	
11-54	2930-2932	no	_	_	_	
11-55	2933-2939	longer	_	_	_	
11-56	2940-2943	run	_	_	_	
11-57	2943-2944	.	_	_	_	

#Text=If you change KVA_PAGES and the system reboots (no panic) after running a while this may be because the address space for userland applications is too small now.
12-1	2945-2947	If	_	_	_	
12-2	2948-2951	you	_	_	_	
12-3	2952-2958	change	_	_	_	
12-4	2959-2968	KVA_PAGES	_	_	_	
12-5	2969-2972	and	_	_	_	
12-6	2973-2976	the	_	_	_	
12-7	2977-2983	system	_	_	_	
12-8	2984-2991	reboots	_	_	_	
12-9	2992-2993	(	_	_	_	
12-10	2993-2995	no	_	_	_	
12-11	2996-3001	panic	_	_	_	
12-12	3001-3002	)	_	_	_	
12-13	3003-3008	after	_	_	_	
12-14	3009-3016	running	_	_	_	
12-15	3017-3018	a	_	_	_	
12-16	3019-3024	while	_	_	_	
12-17	3025-3029	this	_	_	_	
12-18	3030-3033	may	_	_	_	
12-19	3034-3036	be	_	_	_	
12-20	3037-3044	because	_	_	_	
12-21	3045-3048	the	_	_	_	
12-22	3049-3056	address	_	_	_	
12-23	3057-3062	space	_	_	_	
12-24	3063-3066	for	_	_	_	
12-25	3067-3075	userland	_	_	_	
12-26	3076-3088	applications	_	_	_	
12-27	3089-3091	is	_	_	_	
12-28	3092-3095	too	_	_	_	
12-29	3096-3101	small	_	_	_	
12-30	3102-3105	now	_	_	_	
12-31	3105-3106	.	_	_	_	

#Text=For *really* memory constrained systems it is also recommended to strip out as many unused drivers and options from the kernel (which will free a couple of MB of memory).
13-1	3107-3110	For	_	_	_	
13-2	3111-3112	*	_	_	_	
13-3	3112-3118	really	_	_	_	
13-4	3118-3119	*	_	_	_	
13-5	3120-3126	memory	_	_	_	
13-6	3127-3138	constrained	_	_	_	
13-7	3139-3146	systems	_	_	_	
13-8	3147-3149	it	_	_	_	
13-9	3150-3152	is	_	_	_	
13-10	3153-3157	also	_	_	_	
13-11	3158-3169	recommended	_	_	_	
13-12	3170-3172	to	_	_	_	
13-13	3173-3178	strip	_	_	_	
13-14	3179-3182	out	_	_	_	
13-15	3183-3185	as	_	_	_	
13-16	3186-3190	many	_	_	_	
13-17	3191-3197	unused	_	_	_	
13-18	3198-3205	drivers	_	_	_	
13-19	3206-3209	and	_	_	_	
13-20	3210-3217	options	_	_	_	
13-21	3218-3222	from	_	_	_	
13-22	3223-3226	the	_	_	_	
13-23	3227-3233	kernel	_	_	_	
13-24	3234-3235	(	_	_	_	
13-25	3235-3240	which	_	_	_	
13-26	3241-3245	will	_	_	_	
13-27	3246-3250	free	_	_	_	
13-28	3251-3252	a	_	_	_	
13-29	3253-3259	couple	_	_	_	
13-30	3260-3262	of	_	_	_	
13-31	3263-3265	MB	_	_	_	
13-32	3266-3268	of	_	_	_	
13-33	3269-3275	memory	_	_	_	
13-34	3275-3276	)	_	_	_	
13-35	3276-3277	.	_	_	_	

#Text=A stable configuration with vm.kmem_size="1536M" has been reported using an unmodified 7.0-RELEASE kernel, relatively sparse drivers as required for the hardware and options KVA_PAGES=512.
14-1	3278-3279	A	_	_	_	
14-2	3280-3286	stable	_	_	_	
14-3	3287-3300	configuration	_	_	_	
14-4	3301-3305	with	_	_	_	
14-5	3306-3318	vm.kmem_size	_	_	_	
14-6	3318-3319	=	_	_	_	
14-7	3319-3320	"	_	_	_	
14-8	3320-3325	1536M	_	_	_	
14-9	3325-3326	"	_	_	_	
14-10	3327-3330	has	_	_	_	
14-11	3331-3335	been	_	_	_	
14-12	3336-3344	reported	_	_	_	
14-13	3345-3350	using	_	_	_	
14-14	3351-3353	an	_	_	_	
14-15	3354-3364	unmodified	_	_	_	
14-16	3365-3368	7.0	_	_	_	
14-17	3368-3369	-	_	_	_	
14-18	3369-3376	RELEASE	_	_	_	
14-19	3377-3383	kernel	_	_	_	
14-20	3383-3384	,	_	_	_	
14-21	3385-3395	relatively	_	_	_	
14-22	3396-3402	sparse	_	_	_	
14-23	3403-3410	drivers	_	_	_	
14-24	3411-3413	as	_	_	_	
14-25	3414-3422	required	_	_	_	
14-26	3423-3426	for	_	_	_	
14-27	3427-3430	the	_	_	_	
14-28	3431-3439	hardware	_	_	_	
14-29	3440-3443	and	_	_	_	
14-30	3444-3451	options	_	_	_	
14-31	3451-3452	 	_	_	_	
14-32	3452-3461	KVA_PAGES	_	_	_	
14-33	3461-3462	=	_	_	_	
14-34	3462-3465	512	_	_	_	
14-35	3465-3466	.	_	_	_	

#Text=Some workloads need greatly reduced ARC size and the size of VDEV cache.
15-1	3467-3471	Some	_	_	_	
15-2	3472-3481	workloads	_	_	_	
15-3	3482-3486	need	_	_	_	
15-4	3487-3494	greatly	_	_	_	
15-5	3495-3502	reduced	_	_	_	
15-6	3503-3506	ARC	_	_	_	
15-7	3507-3511	size	_	_	_	
15-8	3512-3515	and	_	_	_	
15-9	3516-3519	the	_	_	_	
15-10	3520-3524	size	_	_	_	
15-11	3525-3527	of	_	_	_	
15-12	3528-3532	VDEV	_	_	_	
15-13	3533-3538	cache	_	_	_	
15-14	3538-3539	.	_	_	_	

#Text=ZFS manages the ARC through a multi-threaded process.
16-1	3540-3543	ZFS	_	_	_	
16-2	3544-3551	manages	_	_	_	
16-3	3552-3555	the	_	_	_	
16-4	3556-3559	ARC	_	_	_	
16-5	3560-3567	through	_	_	_	
16-6	3568-3569	a	_	_	_	
16-7	3570-3584	multi-threaded	_	_	_	
16-8	3585-3592	process	_	_	_	
16-9	3592-3593	.	_	_	_	

#Text=If it requires more memory for ARC ZFS will allocate it.
17-1	3594-3596	If	_	_	_	
17-2	3597-3599	it	_	_	_	
17-3	3600-3608	requires	_	_	_	
17-4	3609-3613	more	_	_	_	
17-5	3614-3620	memory	_	_	_	
17-6	3621-3624	for	_	_	_	
17-7	3625-3628	ARC	_	_	_	
17-8	3629-3632	ZFS	_	_	_	
17-9	3633-3637	will	_	_	_	
17-10	3638-3646	allocate	_	_	_	
17-11	3647-3649	it	_	_	_	
17-12	3649-3650	.	_	_	_	

#Text=Previously it exceeded arc_max (vfs.zfs.arc_max) from time to time, but with 7.3 and 8-stable as of mid-January 2010 this is not the case anymore.
18-1	3651-3661	Previously	_	_	_	
18-2	3662-3664	it	_	_	_	
18-3	3665-3673	exceeded	_	_	_	
18-4	3674-3681	arc_max	_	_	_	
18-5	3682-3683	(	_	_	_	
18-6	3683-3698	vfs.zfs.arc_max	_	_	_	
18-7	3698-3699	)	_	_	_	
18-8	3700-3704	from	_	_	_	
18-9	3705-3709	time	_	_	_	
18-10	3710-3712	to	_	_	_	
18-11	3713-3717	time	_	_	_	
18-12	3717-3718	,	_	_	_	
18-13	3719-3722	but	_	_	_	
18-14	3723-3727	with	_	_	_	
18-15	3728-3731	7.3	_	_	_	
18-16	3732-3735	and	_	_	_	
18-17	3736-3737	8	_	_	_	
18-18	3737-3738	-	_	_	_	
18-19	3738-3744	stable	_	_	_	
18-20	3745-3747	as	_	_	_	
18-21	3748-3750	of	_	_	_	
18-22	3751-3762	mid-January	_	_	_	
18-23	3763-3767	2010	_	_	_	
18-24	3768-3772	this	_	_	_	
18-25	3773-3775	is	_	_	_	
18-26	3776-3779	not	_	_	_	
18-27	3780-3783	the	_	_	_	
18-28	3784-3788	case	_	_	_	
18-29	3789-3796	anymore	_	_	_	
18-30	3796-3797	.	_	_	_	

#Text=On memory constrained systems it is safer to use an arbitrarily low arc_max.
19-1	3798-3800	On	_	_	_	
19-2	3801-3807	memory	_	_	_	
19-3	3808-3819	constrained	_	_	_	
19-4	3820-3827	systems	_	_	_	
19-5	3828-3830	it	_	_	_	
19-6	3831-3833	is	_	_	_	
19-7	3834-3839	safer	_	_	_	
19-8	3840-3842	to	_	_	_	
19-9	3843-3846	use	_	_	_	
19-10	3847-3849	an	_	_	_	
19-11	3850-3861	arbitrarily	_	_	_	
19-12	3862-3865	low	_	_	_	
19-13	3866-3873	arc_max	_	_	_	
19-14	3873-3874	.	_	_	_	

#Text=For example it is possible to set vm.kmem_size and vm.kmem_size_max to 512M, vfs.zfs.arc_max to 160M, keeping vfs.zfs.vdev.cache.size to half its default size of 10 Megs (setting it to 5 Megs can even achieve better stability, but this depends upon your workload).
20-1	3875-3878	For	_	_	_	
20-2	3879-3886	example	_	_	_	
20-3	3887-3889	it	_	_	_	
20-4	3890-3892	is	_	_	_	
20-5	3893-3901	possible	_	_	_	
20-6	3902-3904	to	_	_	_	
20-7	3905-3908	set	_	_	_	
20-8	3909-3921	vm.kmem_size	_	_	_	
20-9	3922-3925	and	_	_	_	
20-10	3926-3942	vm.kmem_size_max	_	_	_	
20-11	3942-3943	 	_	_	_	
20-12	3943-3945	to	_	_	_	
20-13	3945-3946	 	_	_	_	
20-14	3946-3950	512M	_	_	_	
20-15	3950-3951	,	_	_	_	
20-16	3952-3967	vfs.zfs.arc_max	_	_	_	
20-17	3968-3970	to	_	_	_	
20-18	3971-3975	160M	_	_	_	
20-19	3975-3976	,	_	_	_	
20-20	3977-3984	keeping	_	_	_	
20-21	3985-4008	vfs.zfs.vdev.cache.size	_	_	_	
20-22	4009-4011	to	_	_	_	
20-23	4012-4016	half	_	_	_	
20-24	4017-4020	its	_	_	_	
20-25	4021-4028	default	_	_	_	
20-26	4029-4033	size	_	_	_	
20-27	4034-4036	of	_	_	_	
20-28	4037-4039	10	_	_	_	
20-29	4040-4044	Megs	_	_	_	
20-30	4045-4046	(	_	_	_	
20-31	4046-4053	setting	_	_	_	
20-32	4054-4056	it	_	_	_	
20-33	4057-4059	to	_	_	_	
20-34	4060-4061	5	_	_	_	
20-35	4062-4066	Megs	_	_	_	
20-36	4067-4070	can	_	_	_	
20-37	4071-4075	even	_	_	_	
20-38	4076-4083	achieve	_	_	_	
20-39	4084-4090	better	_	_	_	
20-40	4091-4100	stability	_	_	_	
20-41	4100-4101	,	_	_	_	
20-42	4102-4105	but	_	_	_	
20-43	4106-4110	this	_	_	_	
20-44	4111-4118	depends	_	_	_	
20-45	4119-4123	upon	_	_	_	
20-46	4124-4128	your	_	_	_	
20-47	4129-4137	workload	_	_	_	
20-48	4137-4138	)	_	_	_	
20-49	4138-4139	.	_	_	_	

#Text=There is one example (CySchubert) of ZFS running nicely on a laptop with 768 Megs of physical RAM with the following settings in /boot/loader.conf: vm.kmem_size="330M" vm.kmem_size_max="330M" vfs.zfs.arc_max="40M" vfs.zfs.vdev.cache.size="5M" Kernel memory should be monitored while tuning to ensure a comfortable amount of free kernel address space.
21-1	4140-4145	There	_	_	_	
21-2	4146-4148	is	_	_	_	
21-3	4149-4152	one	_	_	_	
21-4	4153-4160	example	_	_	_	
21-5	4161-4162	(	_	_	_	
21-6	4162-4172	CySchubert	_	_	_	
21-7	4172-4173	)	_	_	_	
21-8	4174-4176	of	_	_	_	
21-9	4177-4180	ZFS	_	_	_	
21-10	4181-4188	running	_	_	_	
21-11	4189-4195	nicely	_	_	_	
21-12	4196-4198	on	_	_	_	
21-13	4199-4200	a	_	_	_	
21-14	4201-4207	laptop	_	_	_	
21-15	4208-4212	with	_	_	_	
21-16	4213-4216	768	_	_	_	
21-17	4217-4221	Megs	_	_	_	
21-18	4222-4224	of	_	_	_	
21-19	4225-4233	physical	_	_	_	
21-20	4234-4237	RAM	_	_	_	
21-21	4238-4242	with	_	_	_	
21-22	4243-4246	the	_	_	_	
21-23	4247-4256	following	_	_	_	
21-24	4257-4265	settings	_	_	_	
21-25	4266-4268	in	_	_	_	
21-26	4269-4270	/	_	_	_	
21-27	4270-4274	boot	_	_	_	
21-28	4274-4275	/	_	_	_	
21-29	4275-4286	loader.conf	_	_	_	
21-30	4286-4287	:	_	_	_	
21-31	4288-4300	vm.kmem_size	_	_	_	
21-32	4300-4301	=	_	_	_	
21-33	4301-4302	"	_	_	_	
21-34	4302-4306	330M	_	_	_	
21-35	4306-4307	"	_	_	_	
21-36	4308-4324	vm.kmem_size_max	_	_	_	
21-37	4324-4325	=	_	_	_	
21-38	4325-4326	"	_	_	_	
21-39	4326-4330	330M	_	_	_	
21-40	4330-4331	"	_	_	_	
21-41	4332-4347	vfs.zfs.arc_max	_	_	_	
21-42	4347-4348	=	_	_	_	
21-43	4348-4349	"	_	_	_	
21-44	4349-4352	40M	_	_	_	
21-45	4352-4353	"	_	_	_	
21-46	4354-4377	vfs.zfs.vdev.cache.size	_	_	_	
21-47	4377-4378	=	_	_	_	
21-48	4378-4379	"	_	_	_	
21-49	4379-4381	5M	_	_	_	
21-50	4381-4382	"	_	_	_	
21-51	4383-4389	Kernel	_	_	_	
21-52	4390-4396	memory	_	_	_	
21-53	4397-4403	should	_	_	_	
21-54	4404-4406	be	_	_	_	
21-55	4407-4416	monitored	_	_	_	
21-56	4417-4422	while	_	_	_	
21-57	4423-4429	tuning	_	_	_	
21-58	4430-4432	to	_	_	_	
21-59	4433-4439	ensure	_	_	_	
21-60	4440-4441	a	_	_	_	
21-61	4442-4453	comfortable	_	_	_	
21-62	4454-4460	amount	_	_	_	
21-63	4461-4463	of	_	_	_	
21-64	4464-4468	free	_	_	_	
21-65	4469-4475	kernel	_	_	_	
21-66	4476-4483	address	_	_	_	
21-67	4484-4489	space	_	_	_	
21-68	4489-4490	.	_	_	_	

#Text=The following script will summarize kernel memory utilization and assist in tuning arc_max and VDEV cache size. #!
22-1	4491-4494	The	_	_	_	
22-2	4495-4504	following	_	_	_	
22-3	4505-4511	script	_	_	_	
22-4	4512-4516	will	_	_	_	
22-5	4517-4526	summarize	_	_	_	
22-6	4527-4533	kernel	_	_	_	
22-7	4534-4540	memory	_	_	_	
22-8	4541-4552	utilization	_	_	_	
22-9	4553-4556	and	_	_	_	
22-10	4557-4563	assist	_	_	_	
22-11	4564-4566	in	_	_	_	
22-12	4567-4573	tuning	_	_	_	
22-13	4574-4581	arc_max	_	_	_	
22-14	4582-4585	and	_	_	_	
22-15	4586-4590	VDEV	_	_	_	
22-16	4591-4596	cache	_	_	_	
22-17	4597-4601	size	_	_	_	
22-18	4601-4602	.	_	_	_	
22-19	4603-4604	#	_	_	_	
22-20	4604-4605	!	_	_	_	

#Text=/bin/sh -
#Text=TEXT=`kldstat | awk 'BEGIN {print "16i 0";} NR>1 {print toupper($4) "+"} END {print "p"}' | dc`
#Text=DATA=`vmstat -m | sed -Ee '1s/.*/0/;s/.* ([0-9]+)K.*/\\1+/;$s/$/1024*p/' | dc`
#Text=TOTAL=$((DATA + TEXT))
#Text=echo TEXT=$TEXT, `echo $TEXT | awk '{print $1/1048576 " MB"}'`
#Text=echo DATA=$DATA, `echo $DATA | awk '{print $1/1048576 " MB"}'`
#Text=echo TOTAL=$TOTAL, `echo $TOTAL | awk '{print $1/1048576 " MB"}'`
#Text=Note: Perhaps there is a more precise way to calculate / measure how large of a vm.kmem_size setting can be used with a particular kernel, but the authors of this wiki do not know it.
23-1	4605-4606	/	_	_	_	
23-2	4606-4609	bin	_	_	_	
23-3	4609-4610	/	_	_	_	
23-4	4610-4612	sh	_	_	_	
23-5	4612-4613	 	_	_	_	
23-6	4613-4614	-	_	_	_	
23-7	4615-4619	TEXT	_	_	_	
23-8	4619-4620	=	_	_	_	
23-9	4620-4621	`	_	_	_	
23-10	4621-4628	kldstat	_	_	_	
23-11	4628-4629	 	_	_	_	
23-12	4629-4630	|	_	_	_	
23-13	4630-4631	 	_	_	_	
23-14	4631-4634	awk	_	_	_	
23-15	4634-4635	 	_	_	_	
23-16	4635-4636	'	_	_	_	
23-17	4636-4641	BEGIN	_	_	_	
23-18	4641-4642	 	_	_	_	
23-19	4642-4643	{	_	_	_	
23-20	4643-4648	print	_	_	_	
23-21	4648-4649	 	_	_	_	
23-22	4649-4650	"	_	_	_	
23-23	4650-4653	16i	_	_	_	
23-24	4653-4654	 	_	_	_	
23-25	4654-4655	0	_	_	_	
23-26	4655-4656	"	_	_	_	
23-27	4656-4657	;	_	_	_	
23-28	4657-4658	}	_	_	_	
23-29	4658-4659	 	_	_	_	
23-30	4659-4661	NR	_	_	_	
23-31	4661-4662	>	_	_	_	
23-32	4662-4663	1	_	_	_	
23-33	4663-4664	 	_	_	_	
23-34	4664-4665	{	_	_	_	
23-35	4665-4670	print	_	_	_	
23-36	4670-4671	 	_	_	_	
23-37	4671-4678	toupper	_	_	_	
23-38	4678-4679	(	_	_	_	
23-39	4679-4681	$4	_	_	_	
23-40	4681-4682	)	_	_	_	
23-41	4682-4683	 	_	_	_	
23-42	4683-4684	"	_	_	_	
23-43	4684-4685	+	_	_	_	
23-44	4685-4686	"	_	_	_	
23-45	4686-4687	}	_	_	_	
23-46	4687-4688	 	_	_	_	
23-47	4688-4691	END	_	_	_	
23-48	4691-4692	 	_	_	_	
23-49	4692-4693	{	_	_	_	
23-50	4693-4698	print	_	_	_	
23-51	4698-4699	 	_	_	_	
23-52	4699-4700	"	_	_	_	
23-53	4700-4701	p	_	_	_	
23-54	4701-4702	"	_	_	_	
23-55	4702-4703	}	_	_	_	
23-56	4703-4704	'	_	_	_	
23-57	4704-4705	 	_	_	_	
23-58	4705-4706	|	_	_	_	
23-59	4706-4707	 	_	_	_	
23-60	4707-4709	dc	_	_	_	
23-61	4709-4710	`	_	_	_	
23-62	4711-4715	DATA	_	_	_	
23-63	4715-4716	=	_	_	_	
23-64	4716-4717	`	_	_	_	
23-65	4717-4723	vmstat	_	_	_	
23-66	4723-4724	 	_	_	_	
23-67	4724-4725	-	_	_	_	
23-68	4725-4726	m	_	_	_	
23-69	4726-4727	 	_	_	_	
23-70	4727-4728	|	_	_	_	
23-71	4728-4729	 	_	_	_	
23-72	4729-4732	sed	_	_	_	
23-73	4732-4733	 	_	_	_	
23-74	4733-4734	-	_	_	_	
23-75	4734-4736	Ee	_	_	_	
23-76	4736-4737	 	_	_	_	
23-77	4737-4738	'	_	_	_	
23-78	4738-4740	1s	_	_	_	
23-79	4740-4741	/	_	_	_	
23-80	4741-4742	.	_	_	_	
23-81	4742-4743	*	_	_	_	
23-82	4743-4744	/	_	_	_	
23-83	4744-4745	0	_	_	_	
23-84	4745-4746	/	_	_	_	
23-85	4746-4747	;	_	_	_	
23-86	4747-4748	s	_	_	_	
23-87	4748-4749	/	_	_	_	
23-88	4749-4750	.	_	_	_	
23-89	4750-4751	*	_	_	_	
23-90	4751-4752	 	_	_	_	
23-91	4752-4753	(	_	_	_	
23-92	4753-4754	[	_	_	_	
23-93	4754-4755	0	_	_	_	
23-94	4755-4756	-	_	_	_	
23-95	4756-4757	9	_	_	_	
23-96	4757-4758	]	_	_	_	
23-97	4758-4759	+	_	_	_	
23-98	4759-4760	)	_	_	_	
23-99	4760-4761	K	_	_	_	
23-100	4761-4762	.	_	_	_	
23-101	4762-4763	*	_	_	_	
23-102	4763-4764	/	_	_	_	
23-103	4764-4765	\	_	_	_	
23-104	4765-4766	1	_	_	_	
23-105	4766-4767	+	_	_	_	
23-106	4767-4768	/	_	_	_	
23-107	4768-4769	;	_	_	_	
23-108	4769-4770	$	_	_	_	
23-109	4770-4771	s	_	_	_	
23-110	4771-4772	/	_	_	_	
23-111	4772-4773	$	_	_	_	
23-112	4773-4774	/	_	_	_	
23-113	4774-4778	1024	_	_	_	
23-114	4778-4779	*	_	_	_	
23-115	4779-4780	p	_	_	_	
23-116	4780-4781	/	_	_	_	
23-117	4781-4782	'	_	_	_	
23-118	4782-4783	 	_	_	_	
23-119	4783-4784	|	_	_	_	
23-120	4784-4785	 	_	_	_	
23-121	4785-4787	dc	_	_	_	
23-122	4787-4788	`	_	_	_	
23-123	4789-4794	TOTAL	_	_	_	
23-124	4794-4795	=	_	_	_	
23-125	4795-4796	$	_	_	_	
23-126	4796-4797	(	_	_	_	
23-127	4797-4798	(	_	_	_	
23-128	4798-4802	DATA	_	_	_	
23-129	4802-4803	 	_	_	_	
23-130	4803-4804	+	_	_	_	
23-131	4804-4805	 	_	_	_	
23-132	4805-4809	TEXT	_	_	_	
23-133	4809-4810	)	_	_	_	
23-134	4810-4811	)	_	_	_	
23-135	4812-4816	echo	_	_	_	
23-136	4816-4817	 	_	_	_	
23-137	4817-4821	TEXT	_	_	_	
23-138	4821-4822	=	_	_	_	
23-139	4822-4823	$	_	_	_	
23-140	4823-4827	TEXT	_	_	_	
23-141	4827-4828	,	_	_	_	
23-142	4828-4829	 	_	_	_	
23-143	4829-4830	`	_	_	_	
23-144	4830-4834	echo	_	_	_	
23-145	4834-4835	 	_	_	_	
23-146	4835-4836	$	_	_	_	
23-147	4836-4840	TEXT	_	_	_	
23-148	4840-4841	 	_	_	_	
23-149	4841-4842	|	_	_	_	
23-150	4842-4843	 	_	_	_	
23-151	4843-4846	awk	_	_	_	
23-152	4846-4847	 	_	_	_	
23-153	4847-4848	'	_	_	_	
23-154	4848-4849	{	_	_	_	
23-155	4849-4854	print	_	_	_	
23-156	4854-4855	 	_	_	_	
23-157	4855-4857	$1	_	_	_	
23-158	4857-4858	/	_	_	_	
23-159	4858-4865	1048576	_	_	_	
23-160	4865-4866	 	_	_	_	
23-161	4866-4867	"	_	_	_	
23-162	4867-4868	 	_	_	_	
23-163	4868-4870	MB	_	_	_	
23-164	4870-4871	"	_	_	_	
23-165	4871-4872	}	_	_	_	
23-166	4872-4873	'	_	_	_	
23-167	4873-4874	`	_	_	_	
23-168	4875-4879	echo	_	_	_	
23-169	4879-4880	 	_	_	_	
23-170	4880-4884	DATA	_	_	_	
23-171	4884-4885	=	_	_	_	
23-172	4885-4886	$	_	_	_	
23-173	4886-4890	DATA	_	_	_	
23-174	4890-4891	,	_	_	_	
23-175	4891-4892	 	_	_	_	
23-176	4892-4893	`	_	_	_	
23-177	4893-4897	echo	_	_	_	
23-178	4897-4898	 	_	_	_	
23-179	4898-4899	$	_	_	_	
23-180	4899-4903	DATA	_	_	_	
23-181	4903-4904	 	_	_	_	
23-182	4904-4905	|	_	_	_	
23-183	4905-4906	 	_	_	_	
23-184	4906-4909	awk	_	_	_	
23-185	4909-4910	 	_	_	_	
23-186	4910-4911	'	_	_	_	
23-187	4911-4912	{	_	_	_	
23-188	4912-4917	print	_	_	_	
23-189	4917-4918	 	_	_	_	
23-190	4918-4920	$1	_	_	_	
23-191	4920-4921	/	_	_	_	
23-192	4921-4928	1048576	_	_	_	
23-193	4928-4929	 	_	_	_	
23-194	4929-4930	"	_	_	_	
23-195	4930-4931	 	_	_	_	
23-196	4931-4933	MB	_	_	_	
23-197	4933-4934	"	_	_	_	
23-198	4934-4935	}	_	_	_	
23-199	4935-4936	'	_	_	_	
23-200	4936-4937	`	_	_	_	
23-201	4938-4942	echo	_	_	_	
23-202	4942-4943	 	_	_	_	
23-203	4943-4948	TOTAL	_	_	_	
23-204	4948-4949	=	_	_	_	
23-205	4949-4950	$	_	_	_	
23-206	4950-4955	TOTAL	_	_	_	
23-207	4955-4956	,	_	_	_	
23-208	4956-4957	 	_	_	_	
23-209	4957-4958	`	_	_	_	
23-210	4958-4962	echo	_	_	_	
23-211	4962-4963	 	_	_	_	
23-212	4963-4964	$	_	_	_	
23-213	4964-4969	TOTAL	_	_	_	
23-214	4969-4970	 	_	_	_	
23-215	4970-4971	|	_	_	_	
23-216	4971-4972	 	_	_	_	
23-217	4972-4975	awk	_	_	_	
23-218	4975-4976	 	_	_	_	
23-219	4976-4977	'	_	_	_	
23-220	4977-4978	{	_	_	_	
23-221	4978-4983	print	_	_	_	
23-222	4983-4984	 	_	_	_	
23-223	4984-4986	$1	_	_	_	
23-224	4986-4987	/	_	_	_	
23-225	4987-4994	1048576	_	_	_	
23-226	4994-4995	 	_	_	_	
23-227	4995-4996	"	_	_	_	
23-228	4996-4997	 	_	_	_	
23-229	4997-4999	MB	_	_	_	
23-230	4999-5000	"	_	_	_	
23-231	5000-5001	}	_	_	_	
23-232	5001-5002	'	_	_	_	
23-233	5002-5003	`	_	_	_	
23-234	5004-5008	Note	_	_	_	
23-235	5008-5009	:	_	_	_	
23-236	5010-5017	Perhaps	_	_	_	
23-237	5018-5023	there	_	_	_	
23-238	5024-5026	is	_	_	_	
23-239	5027-5028	a	_	_	_	
23-240	5029-5033	more	_	_	_	
23-241	5034-5041	precise	_	_	_	
23-242	5042-5045	way	_	_	_	
23-243	5046-5048	to	_	_	_	
23-244	5049-5058	calculate	_	_	_	
23-245	5059-5060	/	_	_	_	
23-246	5061-5068	measure	_	_	_	
23-247	5069-5072	how	_	_	_	
23-248	5073-5078	large	_	_	_	
23-249	5079-5081	of	_	_	_	
23-250	5082-5083	a	_	_	_	
23-251	5084-5096	vm.kmem_size	_	_	_	
23-252	5097-5104	setting	_	_	_	
23-253	5105-5108	can	_	_	_	
23-254	5109-5111	be	_	_	_	
23-255	5112-5116	used	_	_	_	
23-256	5117-5121	with	_	_	_	
23-257	5122-5123	a	_	_	_	
23-258	5124-5134	particular	_	_	_	
23-259	5135-5141	kernel	_	_	_	
23-260	5141-5142	,	_	_	_	
23-261	5143-5146	but	_	_	_	
23-262	5147-5150	the	_	_	_	
23-263	5151-5158	authors	_	_	_	
23-264	5159-5161	of	_	_	_	
23-265	5162-5166	this	_	_	_	
23-266	5167-5171	wiki	_	_	_	
23-267	5172-5174	do	_	_	_	
23-268	5175-5178	not	_	_	_	
23-269	5179-5183	know	_	_	_	
23-270	5184-5186	it	_	_	_	
23-271	5186-5187	.	_	_	_	

#Text=Experimentation does work.
24-1	5188-5203	Experimentation	_	_	_	
24-2	5204-5208	does	_	_	_	
24-3	5209-5213	work	_	_	_	
24-4	5213-5214	.	_	_	_	

#Text=However, if you set vm.kmem_size too high in loader.conf, the kernel will panic on boot.
25-1	5215-5222	However	_	_	_	
25-2	5222-5223	,	_	_	_	
25-3	5224-5226	if	_	_	_	
25-4	5227-5230	you	_	_	_	
25-5	5231-5234	set	_	_	_	
25-6	5235-5247	vm.kmem_size	_	_	_	
25-7	5248-5251	too	_	_	_	
25-8	5252-5256	high	_	_	_	
25-9	5257-5259	in	_	_	_	
25-10	5260-5271	loader.conf	_	_	_	
25-11	5271-5272	,	_	_	_	
25-12	5273-5276	the	_	_	_	
25-13	5277-5283	kernel	_	_	_	
25-14	5284-5288	will	_	_	_	
25-15	5289-5294	panic	_	_	_	
25-16	5295-5297	on	_	_	_	
25-17	5298-5302	boot	_	_	_	
25-18	5302-5303	.	_	_	_	

#Text=You can fix this by dropping to the boot loader prompt and typing set vm.kmem_size="512M" (or a similar smaller number known to work.)
26-1	5304-5307	You	_	_	_	
26-2	5308-5311	can	_	_	_	
26-3	5312-5315	fix	_	_	_	
26-4	5316-5320	this	_	_	_	
26-5	5321-5323	by	_	_	_	
26-6	5324-5332	dropping	_	_	_	
26-7	5333-5335	to	_	_	_	
26-8	5336-5339	the	_	_	_	
26-9	5340-5344	boot	_	_	_	
26-10	5345-5351	loader	_	_	_	
26-11	5352-5358	prompt	_	_	_	
26-12	5359-5362	and	_	_	_	
26-13	5363-5369	typing	_	_	_	
26-14	5370-5373	set	_	_	_	
26-15	5373-5374	 	_	_	_	
26-16	5374-5386	vm.kmem_size	_	_	_	
26-17	5386-5387	=	_	_	_	
26-18	5387-5388	"	_	_	_	
26-19	5388-5392	512M	_	_	_	
26-20	5392-5393	"	_	_	_	
26-21	5394-5395	(	_	_	_	
26-22	5395-5397	or	_	_	_	
26-23	5398-5399	a	_	_	_	
26-24	5400-5407	similar	_	_	_	
26-25	5408-5415	smaller	_	_	_	
26-26	5416-5422	number	_	_	_	
26-27	5423-5428	known	_	_	_	
26-28	5429-5431	to	_	_	_	
26-29	5432-5436	work	_	_	_	
26-30	5436-5437	.	_	_	_	
26-31	5437-5438	)	_	_	_	

#Text=The vm.kmem_size_max setting is not used directly during the system operation (i.e. it is not a limit which kmem can "grow" into) but for initial autoconfiguration of various system settings, the most important of which for this discussion is the ARC size.
27-1	5439-5442	The	_	_	_	
27-2	5443-5459	vm.kmem_size_max	_	_	_	
27-3	5460-5467	setting	_	_	_	
27-4	5468-5470	is	_	_	_	
27-5	5471-5474	not	_	_	_	
27-6	5475-5479	used	_	_	_	
27-7	5480-5488	directly	_	_	_	
27-8	5489-5495	during	_	_	_	
27-9	5496-5499	the	_	_	_	
27-10	5500-5506	system	_	_	_	
27-11	5507-5516	operation	_	_	_	
27-12	5517-5518	(	_	_	_	
27-13	5518-5521	i.e	_	_	_	
27-14	5521-5522	.	_	_	_	
27-15	5523-5525	it	_	_	_	
27-16	5526-5528	is	_	_	_	
27-17	5529-5532	not	_	_	_	
27-18	5533-5534	a	_	_	_	
27-19	5535-5540	limit	_	_	_	
27-20	5541-5546	which	_	_	_	
27-21	5547-5551	kmem	_	_	_	
27-22	5552-5555	can	_	_	_	
27-23	5556-5557	"	_	_	_	
27-24	5557-5561	grow	_	_	_	
27-25	5561-5562	"	_	_	_	
27-26	5563-5567	into	_	_	_	
27-27	5567-5568	)	_	_	_	
27-28	5569-5572	but	_	_	_	
27-29	5573-5576	for	_	_	_	
27-30	5577-5584	initial	_	_	_	
27-31	5585-5602	autoconfiguration	_	_	_	
27-32	5603-5605	of	_	_	_	
27-33	5606-5613	various	_	_	_	
27-34	5614-5620	system	_	_	_	
27-35	5621-5629	settings	_	_	_	
27-36	5629-5630	,	_	_	_	
27-37	5631-5634	the	_	_	_	
27-38	5635-5639	most	_	_	_	
27-39	5640-5649	important	_	_	_	
27-40	5650-5652	of	_	_	_	
27-41	5653-5658	which	_	_	_	
27-42	5659-5662	for	_	_	_	
27-43	5663-5667	this	_	_	_	
27-44	5668-5678	discussion	_	_	_	
27-45	5679-5681	is	_	_	_	
27-46	5682-5685	the	_	_	_	
27-47	5686-5689	ARC	_	_	_	
27-48	5690-5694	size	_	_	_	
27-49	5694-5695	.	_	_	_	

#Text=If kmem_size and arc_max are tuned manually, kmem_size_max will be ignored, but it is still required to be set.
28-1	5696-5698	If	_	_	_	
28-2	5699-5708	kmem_size	_	_	_	
28-3	5709-5712	and	_	_	_	
28-4	5713-5720	arc_max	_	_	_	
28-5	5721-5724	are	_	_	_	
28-6	5725-5730	tuned	_	_	_	
28-7	5731-5739	manually	_	_	_	
28-8	5739-5740	,	_	_	_	
28-9	5741-5754	kmem_size_max	_	_	_	
28-10	5755-5759	will	_	_	_	
28-11	5760-5762	be	_	_	_	
28-12	5763-5770	ignored	_	_	_	
28-13	5770-5771	,	_	_	_	
28-14	5772-5775	but	_	_	_	
28-15	5776-5778	it	_	_	_	
28-16	5779-5781	is	_	_	_	
28-17	5782-5787	still	_	_	_	
28-18	5788-5796	required	_	_	_	
28-19	5797-5799	to	_	_	_	
28-20	5800-5802	be	_	_	_	
28-21	5803-5806	set	_	_	_	
28-22	5806-5807	.	_	_	_	

#Text=The issue of kernel memory exhaustion is a complex one, involving the interaction between disk speeds, application loads and the special caching ZFS does.
29-1	5808-5811	The	_	_	_	
29-2	5812-5817	issue	_	_	_	
29-3	5818-5820	of	_	_	_	
29-4	5821-5827	kernel	_	_	_	
29-5	5828-5834	memory	_	_	_	
29-6	5835-5845	exhaustion	_	_	_	
29-7	5846-5848	is	_	_	_	
29-8	5849-5850	a	_	_	_	
29-9	5851-5858	complex	_	_	_	
29-10	5859-5862	one	_	_	_	
29-11	5862-5863	,	_	_	_	
29-12	5864-5873	involving	_	_	_	
29-13	5874-5877	the	_	_	_	
29-14	5878-5889	interaction	_	_	_	
29-15	5890-5897	between	_	_	_	
29-16	5898-5902	disk	_	_	_	
29-17	5903-5909	speeds	_	_	_	
29-18	5909-5910	,	_	_	_	
29-19	5911-5922	application	_	_	_	
29-20	5923-5928	loads	_	_	_	
29-21	5929-5932	and	_	_	_	
29-22	5933-5936	the	_	_	_	
29-23	5937-5944	special	_	_	_	
29-24	5945-5952	caching	_	_	_	
29-25	5953-5956	ZFS	_	_	_	
29-26	5957-5961	does	_	_	_	
29-27	5961-5962	.	_	_	_	

#Text=Faster drives will write the cached data faster but will also fill the caches up faster.
30-1	5963-5969	Faster	_	_	_	
30-2	5970-5976	drives	_	_	_	
30-3	5977-5981	will	_	_	_	
30-4	5982-5987	write	_	_	_	
30-5	5988-5991	the	_	_	_	
30-6	5992-5998	cached	_	_	_	
30-7	5999-6003	data	_	_	_	
30-8	6004-6010	faster	_	_	_	
30-9	6011-6014	but	_	_	_	
30-10	6015-6019	will	_	_	_	
30-11	6020-6024	also	_	_	_	
30-12	6025-6029	fill	_	_	_	
30-13	6030-6033	the	_	_	_	
30-14	6034-6040	caches	_	_	_	
30-15	6041-6043	up	_	_	_	
30-16	6044-6050	faster	_	_	_	
30-17	6050-6051	.	_	_	_	

#Text=Generally, larger and faster drives will need more memory for ZFS.
31-1	6052-6061	Generally	_	_	_	
31-2	6061-6062	,	_	_	_	
31-3	6063-6069	larger	_	_	_	
31-4	6070-6073	and	_	_	_	
31-5	6074-6080	faster	_	_	_	
31-6	6081-6087	drives	_	_	_	
31-7	6088-6092	will	_	_	_	
31-8	6093-6097	need	_	_	_	
31-9	6098-6102	more	_	_	_	
31-10	6103-6109	memory	_	_	_	
31-11	6110-6113	for	_	_	_	
31-12	6114-6117	ZFS	_	_	_	
31-13	6117-6118	.	_	_	_	

#Text=To increase performance, you may increase kern.maxvnodes (in /etc/sysctl.conf) way up if you have the RAM for it (e.g. 400000 for a 2GB system).
32-1	6119-6121	To	_	_	_	
32-2	6122-6130	increase	_	_	_	
32-3	6131-6142	performance	_	_	_	
32-4	6142-6143	,	_	_	_	
32-5	6144-6147	you	_	_	_	
32-6	6148-6151	may	_	_	_	
32-7	6152-6160	increase	_	_	_	
32-8	6161-6175	kern.maxvnodes	_	_	_	
32-9	6176-6177	(	_	_	_	
32-10	6177-6179	in	_	_	_	
32-11	6180-6181	/	_	_	_	
32-12	6181-6184	etc	_	_	_	
32-13	6184-6185	/	_	_	_	
32-14	6185-6196	sysctl.conf	_	_	_	
32-15	6196-6197	)	_	_	_	
32-16	6198-6201	way	_	_	_	
32-17	6202-6204	up	_	_	_	
32-18	6205-6207	if	_	_	_	
32-19	6208-6211	you	_	_	_	
32-20	6212-6216	have	_	_	_	
32-21	6217-6220	the	_	_	_	
32-22	6221-6224	RAM	_	_	_	
32-23	6225-6228	for	_	_	_	
32-24	6229-6231	it	_	_	_	
32-25	6232-6233	(	_	_	_	
32-26	6233-6236	e.g	_	_	_	
32-27	6236-6237	.	_	_	_	
32-28	6238-6244	400000	_	_	_	
32-29	6245-6248	for	_	_	_	
32-30	6249-6250	a	_	_	_	
32-31	6251-6254	2GB	_	_	_	
32-32	6255-6261	system	_	_	_	
32-33	6261-6262	)	_	_	_	
32-34	6262-6263	.	_	_	_	

#Text=On i386, keep an eye on vfs.numvnodes during production to see where it stabilizes.
33-1	6264-6266	On	_	_	_	
33-2	6267-6271	i386	_	_	_	
33-3	6271-6272	,	_	_	_	
33-4	6273-6277	keep	_	_	_	
33-5	6278-6280	an	_	_	_	
33-6	6281-6284	eye	_	_	_	
33-7	6285-6287	on	_	_	_	
33-8	6288-6301	vfs.numvnodes	_	_	_	
33-9	6302-6308	during	_	_	_	
33-10	6309-6319	production	_	_	_	
33-11	6320-6322	to	_	_	_	
33-12	6323-6326	see	_	_	_	
33-13	6327-6332	where	_	_	_	
33-14	6333-6335	it	_	_	_	
33-15	6336-6346	stabilizes	_	_	_	
33-16	6346-6347	.	_	_	_	

#Text=(AMD64 uses direct mapping for vnodes, so you don't have to worry about address space for vnodes on this architecture).
#Text=amd64
#Text=NOTE (gcooper): this blanket statement is far from true 100% of the time, depending on how the system with ZFS is being used.
34-1	6348-6349	(	_	_	_	
34-2	6349-6354	AMD64	_	_	_	
34-3	6355-6359	uses	_	_	_	
34-4	6360-6366	direct	_	_	_	
34-5	6367-6374	mapping	_	_	_	
34-6	6375-6378	for	_	_	_	
34-7	6379-6385	vnodes	_	_	_	
34-8	6385-6386	,	_	_	_	
34-9	6387-6389	so	_	_	_	
34-10	6390-6393	you	_	_	_	
34-11	6394-6399	don't	_	_	_	
34-12	6400-6404	have	_	_	_	
34-13	6405-6407	to	_	_	_	
34-14	6408-6413	worry	_	_	_	
34-15	6414-6419	about	_	_	_	
34-16	6420-6427	address	_	_	_	
34-17	6428-6433	space	_	_	_	
34-18	6434-6437	for	_	_	_	
34-19	6438-6444	vnodes	_	_	_	
34-20	6445-6447	on	_	_	_	
34-21	6448-6452	this	_	_	_	
34-22	6453-6465	architecture	_	_	_	
34-23	6465-6466	)	_	_	_	
34-24	6466-6467	.	_	_	_	
34-25	6468-6473	amd64	_	_	_	
34-26	6474-6478	NOTE	_	_	_	
34-27	6479-6480	(	_	_	_	
34-28	6480-6487	gcooper	_	_	_	
34-29	6487-6488	)	_	_	_	
34-30	6488-6489	:	_	_	_	
34-31	6490-6494	this	_	_	_	
34-32	6495-6502	blanket	_	_	_	
34-33	6503-6512	statement	_	_	_	
34-34	6513-6515	is	_	_	_	
34-35	6516-6519	far	_	_	_	
34-36	6520-6524	from	_	_	_	
34-37	6525-6529	true	_	_	_	
34-38	6530-6534	100%	_	_	_	
34-39	6535-6537	of	_	_	_	
34-40	6538-6541	the	_	_	_	
34-41	6542-6546	time	_	_	_	
34-42	6546-6547	,	_	_	_	
34-43	6548-6557	depending	_	_	_	
34-44	6558-6560	on	_	_	_	
34-45	6561-6564	how	_	_	_	
34-46	6565-6568	the	_	_	_	
34-47	6569-6575	system	_	_	_	
34-48	6576-6580	with	_	_	_	
34-49	6581-6584	ZFS	_	_	_	
34-50	6585-6587	is	_	_	_	
34-51	6588-6593	being	_	_	_	
34-52	6594-6598	used	_	_	_	
34-53	6598-6599	.	_	_	_	

#Text=FreeBSD 7.2+ has improved kernel memory allocation strategy and no tuning may be necessary on systems with more than 2 GB of RAM.
35-1	6600-6607	FreeBSD	_	_	_	
35-2	6608-6611	7.2	_	_	_	
35-3	6611-6612	+	_	_	_	
35-4	6613-6616	has	_	_	_	
35-5	6617-6625	improved	_	_	_	
35-6	6626-6632	kernel	_	_	_	
35-7	6633-6639	memory	_	_	_	
35-8	6640-6650	allocation	_	_	_	
35-9	6651-6659	strategy	_	_	_	
35-10	6660-6663	and	_	_	_	
35-11	6664-6666	no	_	_	_	
35-12	6667-6673	tuning	_	_	_	
35-13	6674-6677	may	_	_	_	
35-14	6678-6680	be	_	_	_	
35-15	6681-6690	necessary	_	_	_	
35-16	6691-6693	on	_	_	_	
35-17	6694-6701	systems	_	_	_	
35-18	6702-6706	with	_	_	_	
35-19	6707-6711	more	_	_	_	
35-20	6712-6716	than	_	_	_	
35-21	6717-6718	2	_	_	_	
35-22	6718-6719	 	_	_	_	
35-23	6719-6721	GB	_	_	_	
35-24	6722-6724	of	_	_	_	
35-25	6725-6728	RAM	_	_	_	
35-26	6728-6729	.	_	_	_	

#Text=Generic ARC discussion
#Text=The value for vfs.zfs.arc_max needs to be smaller than the value for vm.kmem_size (not only ZFS is using the kmem).
36-1	6730-6737	Generic	_	_	_	
36-2	6738-6741	ARC	_	_	_	
36-3	6742-6752	discussion	_	_	_	
36-4	6753-6756	The	_	_	_	
36-5	6757-6762	value	_	_	_	
36-6	6763-6766	for	_	_	_	
36-7	6767-6782	vfs.zfs.arc_max	_	_	_	
36-8	6783-6788	needs	_	_	_	
36-9	6789-6791	to	_	_	_	
36-10	6792-6794	be	_	_	_	
36-11	6795-6802	smaller	_	_	_	
36-12	6803-6807	than	_	_	_	
36-13	6808-6811	the	_	_	_	
36-14	6812-6817	value	_	_	_	
36-15	6818-6821	for	_	_	_	
36-16	6822-6834	vm.kmem_size	_	_	_	
36-17	6835-6836	(	_	_	_	
36-18	6836-6839	not	_	_	_	
36-19	6840-6844	only	_	_	_	
36-20	6845-6848	ZFS	_	_	_	
36-21	6849-6851	is	_	_	_	
36-22	6852-6857	using	_	_	_	
36-23	6858-6861	the	_	_	_	
36-24	6862-6866	kmem	_	_	_	
36-25	6866-6867	)	_	_	_	
36-26	6867-6868	.	_	_	_	

#Text=To monitor the ARC, you should install the sysutils/zfs-stats port;
#Text=the port is an evolution of the arc_stat.pl script available in Solaris that was ported to FreeBSD by FreeBSD contributor, jhell.
37-1	6869-6871	To	_	_	_	
37-2	6872-6879	monitor	_	_	_	
37-3	6880-6883	the	_	_	_	
37-4	6884-6887	ARC	_	_	_	
37-5	6887-6888	,	_	_	_	
37-6	6889-6892	you	_	_	_	
37-7	6893-6899	should	_	_	_	
37-8	6900-6907	install	_	_	_	
37-9	6908-6911	the	_	_	_	
37-10	6912-6920	sysutils	_	_	_	
37-11	6920-6921	/	_	_	_	
37-12	6921-6930	zfs-stats	_	_	_	
37-13	6931-6935	port	_	_	_	
37-14	6935-6936	;	_	_	_	
37-15	6937-6940	the	_	_	_	
37-16	6941-6945	port	_	_	_	
37-17	6946-6948	is	_	_	_	
37-18	6949-6951	an	_	_	_	
37-19	6952-6961	evolution	_	_	_	
37-20	6962-6964	of	_	_	_	
37-21	6965-6968	the	_	_	_	
37-22	6969-6980	arc_stat.pl	_	_	_	
37-23	6981-6987	script	_	_	_	
37-24	6988-6997	available	_	_	_	
37-25	6998-7000	in	_	_	_	
37-26	7001-7008	Solaris	_	_	_	
37-27	7009-7013	that	_	_	_	
37-28	7014-7017	was	_	_	_	
37-29	7018-7024	ported	_	_	_	
37-30	7025-7027	to	_	_	_	
37-31	7028-7035	FreeBSD	_	_	_	
37-32	7036-7038	by	_	_	_	
37-33	7039-7046	FreeBSD	_	_	_	
37-34	7047-7058	contributor	_	_	_	
37-35	7058-7059	,	_	_	_	
37-36	7060-7065	jhell	_	_	_	
37-37	7065-7066	.	_	_	_	

#Text=To improve the random read performance, a separate L2ARC device can be used (zpool add <pool> cache <device>).
38-1	7067-7069	To	_	_	_	
38-2	7070-7077	improve	_	_	_	
38-3	7078-7081	the	_	_	_	
38-4	7082-7088	random	_	_	_	
38-5	7089-7093	read	_	_	_	
38-6	7094-7105	performance	_	_	_	
38-7	7105-7106	,	_	_	_	
38-8	7107-7108	a	_	_	_	
38-9	7109-7117	separate	_	_	_	
38-10	7118-7123	L2ARC	_	_	_	
38-11	7124-7130	device	_	_	_	
38-12	7131-7134	can	_	_	_	
38-13	7135-7137	be	_	_	_	
38-14	7138-7142	used	_	_	_	
38-15	7143-7144	(	_	_	_	
38-16	7144-7149	zpool	_	_	_	
38-17	7150-7153	add	_	_	_	
38-18	7154-7155	<	_	_	_	
38-19	7155-7159	pool	_	_	_	
38-20	7159-7160	>	_	_	_	
38-21	7161-7166	cache	_	_	_	
38-22	7167-7168	<	_	_	_	
38-23	7168-7174	device	_	_	_	
38-24	7174-7175	>	_	_	_	
38-25	7175-7176	)	_	_	_	
38-26	7176-7177	.	_	_	_	

#Text=A cheap solution is to add an USB memory stick (see http://www.leidinger.net/blog/2010/02/10/making-zfs-faster/).
39-1	7178-7179	A	_	_	_	
39-2	7180-7185	cheap	_	_	_	
39-3	7186-7194	solution	_	_	_	
39-4	7195-7197	is	_	_	_	
39-5	7198-7200	to	_	_	_	
39-6	7201-7204	add	_	_	_	
39-7	7205-7207	an	_	_	_	
39-8	7208-7211	USB	_	_	_	
39-9	7212-7218	memory	_	_	_	
39-10	7219-7224	stick	_	_	_	
39-11	7225-7226	(	_	_	_	
39-12	7226-7229	see	_	_	_	
39-13	7230-7234	http	_	_	_	
39-14	7234-7235	:	_	_	_	
39-15	7235-7236	/	_	_	_	
39-16	7236-7237	/	_	_	_	
39-17	7237-7254	www.leidinger.net	_	_	_	
39-18	7254-7255	/	_	_	_	
39-19	7255-7259	blog	_	_	_	
39-20	7259-7260	/	_	_	_	
39-21	7260-7264	2010	_	_	_	
39-22	7264-7265	/	_	_	_	
39-23	7265-7267	02	_	_	_	
39-24	7267-7268	/	_	_	_	
39-25	7268-7270	10	_	_	_	
39-26	7270-7271	/	_	_	_	
39-27	7271-7288	making-zfs-faster	_	_	_	
39-28	7288-7289	/	_	_	_	
39-29	7289-7290	)	_	_	_	
39-30	7290-7291	.	_	_	_	

#Text=The high performance solution is to add a SSD.
40-1	7292-7295	The	_	_	_	
40-2	7296-7300	high	_	_	_	
40-3	7301-7312	performance	_	_	_	
40-4	7313-7321	solution	_	_	_	
40-5	7322-7324	is	_	_	_	
40-6	7325-7327	to	_	_	_	
40-7	7328-7331	add	_	_	_	
40-8	7332-7333	a	_	_	_	
40-9	7334-7337	SSD	_	_	_	
40-10	7337-7338	.	_	_	_	

#Text=Using a L2ARC device will increase the amount of memory ZFS needs to allocate, see http://www.mail-archive.com/zfs-discuss@opensolaris.org/msg34674.html for more info.
41-1	7339-7344	Using	_	_	_	
41-2	7345-7346	a	_	_	_	
41-3	7347-7352	L2ARC	_	_	_	
41-4	7353-7359	device	_	_	_	
41-5	7360-7364	will	_	_	_	
41-6	7365-7373	increase	_	_	_	
41-7	7374-7377	the	_	_	_	
41-8	7378-7384	amount	_	_	_	
41-9	7385-7387	of	_	_	_	
41-10	7388-7394	memory	_	_	_	
41-11	7395-7398	ZFS	_	_	_	
41-12	7399-7404	needs	_	_	_	
41-13	7405-7407	to	_	_	_	
41-14	7408-7416	allocate	_	_	_	
41-15	7416-7417	,	_	_	_	
41-16	7418-7421	see	_	_	_	
41-17	7422-7426	http	_	_	_	
41-18	7426-7427	:	_	_	_	
41-19	7427-7428	/	_	_	_	
41-20	7428-7429	/	_	_	_	
41-21	7429-7449	www.mail-archive.com	_	_	_	
41-22	7449-7450	/	_	_	_	
41-23	7450-7461	zfs-discuss	_	_	_	
41-24	7461-7462	@	_	_	_	
41-25	7462-7477	opensolaris.org	_	_	_	
41-26	7477-7478	/	_	_	_	
41-27	7478-7486	msg34674	_	_	_	
41-28	7486-7487	.	_	_	_	
41-29	7487-7491	html	_	_	_	
41-30	7492-7495	for	_	_	_	
41-31	7496-7500	more	_	_	_	
41-32	7501-7505	info	_	_	_	
41-33	7505-7506	.	_	_	_	

#Text=L2ARC discussion
#Text=ZFS has the ability to extend the ARC with one or more L2ARC devices, which provides the best benefit for random read workloads.
42-1	7507-7512	L2ARC	_	_	_	
42-2	7513-7523	discussion	_	_	_	
42-3	7524-7527	ZFS	_	_	_	
42-4	7528-7531	has	_	_	_	
42-5	7532-7535	the	_	_	_	
42-6	7536-7543	ability	_	_	_	
42-7	7544-7546	to	_	_	_	
42-8	7547-7553	extend	_	_	_	
42-9	7554-7557	the	_	_	_	
42-10	7558-7561	ARC	_	_	_	
42-11	7562-7566	with	_	_	_	
42-12	7567-7570	one	_	_	_	
42-13	7571-7573	or	_	_	_	
42-14	7574-7578	more	_	_	_	
42-15	7579-7584	L2ARC	_	_	_	
42-16	7585-7592	devices	_	_	_	
42-17	7592-7593	,	_	_	_	
42-18	7594-7599	which	_	_	_	
42-19	7600-7608	provides	_	_	_	
42-20	7609-7612	the	_	_	_	
42-21	7613-7617	best	_	_	_	
42-22	7618-7625	benefit	_	_	_	
42-23	7626-7629	for	_	_	_	
42-24	7630-7636	random	_	_	_	
42-25	7637-7641	read	_	_	_	
42-26	7642-7651	workloads	_	_	_	
42-27	7651-7652	.	_	_	_	

#Text=These L2ARC devices should be faster and/or lower latency than the storage pool.
43-1	7653-7658	These	_	_	_	
43-2	7659-7664	L2ARC	_	_	_	
43-3	7665-7672	devices	_	_	_	
43-4	7673-7679	should	_	_	_	
43-5	7680-7682	be	_	_	_	
43-6	7683-7689	faster	_	_	_	
43-7	7690-7693	and	_	_	_	
43-8	7693-7694	/	_	_	_	
43-9	7694-7696	or	_	_	_	
43-10	7697-7702	lower	_	_	_	
43-11	7703-7710	latency	_	_	_	
43-12	7711-7715	than	_	_	_	
43-13	7716-7719	the	_	_	_	
43-14	7720-7727	storage	_	_	_	
43-15	7728-7732	pool	_	_	_	
43-16	7732-7733	.	_	_	_	

#Text=Generally speaking this limits the useful choices to flash based devices.
44-1	7734-7743	Generally	_	_	_	
44-2	7744-7752	speaking	_	_	_	
44-3	7753-7757	this	_	_	_	
44-4	7758-7764	limits	_	_	_	
44-5	7765-7768	the	_	_	_	
44-6	7769-7775	useful	_	_	_	
44-7	7776-7783	choices	_	_	_	
44-8	7784-7786	to	_	_	_	
44-9	7787-7792	flash	_	_	_	
44-10	7793-7798	based	_	_	_	
44-11	7799-7806	devices	_	_	_	
44-12	7806-7807	.	_	_	_	

#Text=In very large pools the ability to have devices faster than the pool may be problematic.
45-1	7808-7810	In	_	_	_	
45-2	7811-7815	very	_	_	_	
45-3	7816-7821	large	_	_	_	
45-4	7822-7827	pools	_	_	_	
45-5	7828-7831	the	_	_	_	
45-6	7832-7839	ability	_	_	_	
45-7	7840-7842	to	_	_	_	
45-8	7843-7847	have	_	_	_	
45-9	7848-7855	devices	_	_	_	
45-10	7856-7862	faster	_	_	_	
45-11	7863-7867	than	_	_	_	
45-12	7868-7871	the	_	_	_	
45-13	7872-7876	pool	_	_	_	
45-14	7877-7880	may	_	_	_	
45-15	7881-7883	be	_	_	_	
45-16	7884-7895	problematic	_	_	_	
45-17	7895-7896	.	_	_	_	

#Text=In smaller pools it may be tempting to use a spinning disk as a dedicated L2ARC device.
46-1	7897-7899	In	_	_	_	
46-2	7900-7907	smaller	_	_	_	
46-3	7908-7913	pools	_	_	_	
46-4	7914-7916	it	_	_	_	
46-5	7917-7920	may	_	_	_	
46-6	7921-7923	be	_	_	_	
46-7	7924-7932	tempting	_	_	_	
46-8	7933-7935	to	_	_	_	
46-9	7936-7939	use	_	_	_	
46-10	7940-7941	a	_	_	_	
46-11	7942-7950	spinning	_	_	_	
46-12	7951-7955	disk	_	_	_	
46-13	7956-7958	as	_	_	_	
46-14	7959-7960	a	_	_	_	
46-15	7961-7970	dedicated	_	_	_	
46-16	7971-7976	L2ARC	_	_	_	
46-17	7977-7983	device	_	_	_	
46-18	7983-7984	.	_	_	_	

#Text=Generally this will result in lower pool performance (and definitely capacity) than if it was just placed in the pool.
47-1	7985-7994	Generally	_	_	_	
47-2	7995-7999	this	_	_	_	
47-3	8000-8004	will	_	_	_	
47-4	8005-8011	result	_	_	_	
47-5	8012-8014	in	_	_	_	
47-6	8015-8020	lower	_	_	_	
47-7	8021-8025	pool	_	_	_	
47-8	8026-8037	performance	_	_	_	
47-9	8038-8039	(	_	_	_	
47-10	8039-8042	and	_	_	_	
47-11	8043-8053	definitely	_	_	_	
47-12	8054-8062	capacity	_	_	_	
47-13	8062-8063	)	_	_	_	
47-14	8064-8068	than	_	_	_	
47-15	8069-8071	if	_	_	_	
47-16	8072-8074	it	_	_	_	
47-17	8075-8078	was	_	_	_	
47-18	8079-8083	just	_	_	_	
47-19	8084-8090	placed	_	_	_	
47-20	8091-8093	in	_	_	_	
47-21	8094-8097	the	_	_	_	
47-22	8098-8102	pool	_	_	_	
47-23	8102-8103	.	_	_	_	

#Text=There may be scenarios in lower memory systems where a single 15K SAS disk can improve the performance of a small pool of 5.4k or 7.2 drives, but this is not a typical case.
48-1	8104-8109	There	_	_	_	
48-2	8110-8113	may	_	_	_	
48-3	8114-8116	be	_	_	_	
48-4	8117-8126	scenarios	_	_	_	
48-5	8127-8129	in	_	_	_	
48-6	8130-8135	lower	_	_	_	
48-7	8136-8142	memory	_	_	_	
48-8	8143-8150	systems	_	_	_	
48-9	8151-8156	where	_	_	_	
48-10	8157-8158	a	_	_	_	
48-11	8159-8165	single	_	_	_	
48-12	8166-8169	15K	_	_	_	
48-13	8170-8173	SAS	_	_	_	
48-14	8174-8178	disk	_	_	_	
48-15	8179-8182	can	_	_	_	
48-16	8183-8190	improve	_	_	_	
48-17	8191-8194	the	_	_	_	
48-18	8195-8206	performance	_	_	_	
48-19	8207-8209	of	_	_	_	
48-20	8210-8211	a	_	_	_	
48-21	8212-8217	small	_	_	_	
48-22	8218-8222	pool	_	_	_	
48-23	8223-8225	of	_	_	_	
48-24	8226-8230	5.4k	_	_	_	
48-25	8231-8233	or	_	_	_	
48-26	8234-8237	7.2	_	_	_	
48-27	8238-8244	drives	_	_	_	
48-28	8244-8245	,	_	_	_	
48-29	8246-8249	but	_	_	_	
48-30	8250-8254	this	_	_	_	
48-31	8255-8257	is	_	_	_	
48-32	8258-8261	not	_	_	_	
48-33	8262-8263	a	_	_	_	
48-34	8264-8271	typical	_	_	_	
48-35	8272-8276	case	_	_	_	
48-36	8276-8277	.	_	_	_	

#Text=By default the L2ARC does not attempt to cache prefetched/streaming workloads, on the assumption that most data of this type is sequential and the combined throughput of your pool disks exceeds the throughput of the L2ARC devices, and therefore, this workload is best left for the pool disks to serve.
49-1	8278-8280	By	_	_	_	
49-2	8281-8288	default	_	_	_	
49-3	8289-8292	the	_	_	_	
49-4	8293-8298	L2ARC	_	_	_	
49-5	8299-8303	does	_	_	_	
49-6	8304-8307	not	_	_	_	
49-7	8308-8315	attempt	_	_	_	
49-8	8316-8318	to	_	_	_	
49-9	8319-8324	cache	_	_	_	
49-10	8325-8335	prefetched	_	_	_	
49-11	8335-8336	/	_	_	_	
49-12	8336-8345	streaming	_	_	_	
49-13	8346-8355	workloads	_	_	_	
49-14	8355-8356	,	_	_	_	
49-15	8357-8359	on	_	_	_	
49-16	8360-8363	the	_	_	_	
49-17	8364-8374	assumption	_	_	_	
49-18	8375-8379	that	_	_	_	
49-19	8380-8384	most	_	_	_	
49-20	8385-8389	data	_	_	_	
49-21	8390-8392	of	_	_	_	
49-22	8393-8397	this	_	_	_	
49-23	8398-8402	type	_	_	_	
49-24	8403-8405	is	_	_	_	
49-25	8406-8416	sequential	_	_	_	
49-26	8417-8420	and	_	_	_	
49-27	8421-8424	the	_	_	_	
49-28	8425-8433	combined	_	_	_	
49-29	8434-8444	throughput	_	_	_	
49-30	8445-8447	of	_	_	_	
49-31	8448-8452	your	_	_	_	
49-32	8453-8457	pool	_	_	_	
49-33	8458-8463	disks	_	_	_	
49-34	8464-8471	exceeds	_	_	_	
49-35	8472-8475	the	_	_	_	
49-36	8476-8486	throughput	_	_	_	
49-37	8487-8489	of	_	_	_	
49-38	8490-8493	the	_	_	_	
49-39	8494-8499	L2ARC	_	_	_	
49-40	8500-8507	devices	_	_	_	
49-41	8507-8508	,	_	_	_	
49-42	8509-8512	and	_	_	_	
49-43	8513-8522	therefore	_	_	_	
49-44	8522-8523	,	_	_	_	
49-45	8524-8528	this	_	_	_	
49-46	8529-8537	workload	_	_	_	
49-47	8538-8540	is	_	_	_	
49-48	8541-8545	best	_	_	_	
49-49	8546-8550	left	_	_	_	
49-50	8551-8554	for	_	_	_	
49-51	8555-8558	the	_	_	_	
49-52	8559-8563	pool	_	_	_	
49-53	8564-8569	disks	_	_	_	
49-54	8570-8572	to	_	_	_	
49-55	8573-8578	serve	_	_	_	
49-56	8578-8579	.	_	_	_	

#Text=This is usually the case.
50-1	8580-8584	This	_	_	_	
50-2	8585-8587	is	_	_	_	
50-3	8588-8595	usually	_	_	_	
50-4	8596-8599	the	_	_	_	
50-5	8600-8604	case	_	_	_	
50-6	8604-8605	.	_	_	_	

#Text=If you believe otherwise (number of L2ARC devices X their max throughput > number of pool disks X their max throughput, or you are not doing large amounts of sequential access), then this can be toggled with the following sysctl: vfs.zfs.l2arc_noprefetchThe default value of 1 does not allow caching of streaming and/or sequential workloads, and will not read from L2ARC when prefetching blocks.
51-1	8606-8608	If	_	_	_	
51-2	8609-8612	you	_	_	_	
51-3	8613-8620	believe	_	_	_	
51-4	8621-8630	otherwise	_	_	_	
51-5	8631-8632	(	_	_	_	
51-6	8632-8638	number	_	_	_	
51-7	8639-8641	of	_	_	_	
51-8	8642-8647	L2ARC	_	_	_	
51-9	8648-8655	devices	_	_	_	
51-10	8656-8657	X	_	_	_	
51-11	8658-8663	their	_	_	_	
51-12	8664-8667	max	_	_	_	
51-13	8668-8678	throughput	_	_	_	
51-14	8679-8680	>	_	_	_	
51-15	8681-8687	number	_	_	_	
51-16	8688-8690	of	_	_	_	
51-17	8691-8695	pool	_	_	_	
51-18	8696-8701	disks	_	_	_	
51-19	8702-8703	X	_	_	_	
51-20	8704-8709	their	_	_	_	
51-21	8710-8713	max	_	_	_	
51-22	8714-8724	throughput	_	_	_	
51-23	8724-8725	,	_	_	_	
51-24	8726-8728	or	_	_	_	
51-25	8729-8732	you	_	_	_	
51-26	8733-8736	are	_	_	_	
51-27	8737-8740	not	_	_	_	
51-28	8741-8746	doing	_	_	_	
51-29	8747-8752	large	_	_	_	
51-30	8753-8760	amounts	_	_	_	
51-31	8761-8763	of	_	_	_	
51-32	8764-8774	sequential	_	_	_	
51-33	8775-8781	access	_	_	_	
51-34	8781-8782	)	_	_	_	
51-35	8782-8783	,	_	_	_	
51-36	8784-8788	then	_	_	_	
51-37	8789-8793	this	_	_	_	
51-38	8794-8797	can	_	_	_	
51-39	8798-8800	be	_	_	_	
51-40	8801-8808	toggled	_	_	_	
51-41	8809-8813	with	_	_	_	
51-42	8814-8817	the	_	_	_	
51-43	8818-8827	following	_	_	_	
51-44	8828-8834	sysctl	_	_	_	
51-45	8834-8835	:	_	_	_	
51-46	8836-8863	vfs.zfs.l2arc_noprefetchThe	_	_	_	
51-47	8864-8871	default	_	_	_	
51-48	8872-8877	value	_	_	_	
51-49	8878-8880	of	_	_	_	
51-50	8881-8882	1	_	_	_	
51-51	8883-8887	does	_	_	_	
51-52	8888-8891	not	_	_	_	
51-53	8892-8897	allow	_	_	_	
51-54	8898-8905	caching	_	_	_	
51-55	8906-8908	of	_	_	_	
51-56	8909-8918	streaming	_	_	_	
51-57	8919-8922	and	_	_	_	
51-58	8922-8923	/	_	_	_	
51-59	8923-8925	or	_	_	_	
51-60	8926-8936	sequential	_	_	_	
51-61	8937-8946	workloads	_	_	_	
51-62	8946-8947	,	_	_	_	
51-63	8948-8951	and	_	_	_	
51-64	8952-8956	will	_	_	_	
51-65	8957-8960	not	_	_	_	
51-66	8961-8965	read	_	_	_	
51-67	8966-8970	from	_	_	_	
51-68	8971-8976	L2ARC	_	_	_	
51-69	8977-8981	when	_	_	_	
51-70	8982-8993	prefetching	_	_	_	
51-71	8994-9000	blocks	_	_	_	
51-72	9000-9001	.	_	_	_	

#Text=Switching it to 0 will allow prefetched/streaming reads to be cached, and may significantly improve performance if you are storing many small files in a large directory hierarchy (since many metadata blocks are read via the prefetcher and would ordinarily always be read from pool disks).
52-1	9002-9011	Switching	_	_	_	
52-2	9012-9014	it	_	_	_	
52-3	9015-9017	to	_	_	_	
52-4	9018-9019	0	_	_	_	
52-5	9020-9024	will	_	_	_	
52-6	9025-9030	allow	_	_	_	
52-7	9031-9041	prefetched	_	_	_	
52-8	9041-9042	/	_	_	_	
52-9	9042-9051	streaming	_	_	_	
52-10	9052-9057	reads	_	_	_	
52-11	9058-9060	to	_	_	_	
52-12	9061-9063	be	_	_	_	
52-13	9064-9070	cached	_	_	_	
52-14	9070-9071	,	_	_	_	
52-15	9072-9075	and	_	_	_	
52-16	9076-9079	may	_	_	_	
52-17	9080-9093	significantly	_	_	_	
52-18	9094-9101	improve	_	_	_	
52-19	9102-9113	performance	_	_	_	
52-20	9114-9116	if	_	_	_	
52-21	9117-9120	you	_	_	_	
52-22	9121-9124	are	_	_	_	
52-23	9125-9132	storing	_	_	_	
52-24	9133-9137	many	_	_	_	
52-25	9138-9143	small	_	_	_	
52-26	9144-9149	files	_	_	_	
52-27	9150-9152	in	_	_	_	
52-28	9153-9154	a	_	_	_	
52-29	9155-9160	large	_	_	_	
52-30	9161-9170	directory	_	_	_	
52-31	9171-9180	hierarchy	_	_	_	
52-32	9181-9182	(	_	_	_	
52-33	9182-9187	since	_	_	_	
52-34	9188-9192	many	_	_	_	
52-35	9193-9201	metadata	_	_	_	
52-36	9202-9208	blocks	_	_	_	
52-37	9209-9212	are	_	_	_	
52-38	9213-9217	read	_	_	_	
52-39	9218-9221	via	_	_	_	
52-40	9222-9225	the	_	_	_	
52-41	9226-9236	prefetcher	_	_	_	
52-42	9237-9240	and	_	_	_	
52-43	9241-9246	would	_	_	_	
52-44	9247-9257	ordinarily	_	_	_	
52-45	9258-9264	always	_	_	_	
52-46	9265-9267	be	_	_	_	
52-47	9268-9272	read	_	_	_	
52-48	9273-9277	from	_	_	_	
52-49	9278-9282	pool	_	_	_	
52-50	9283-9288	disks	_	_	_	
52-51	9288-9289	)	_	_	_	
52-52	9289-9290	.	_	_	_	

#Text=The default throttling of loading the L2ARC device is 8 Mbytes/sec, on the assumption that the L2ARC is warming up from a random read workload from spinning disks, for which 8 Mbytes/sec is usually more than the spinning disks can provide.
53-1	9291-9294	The	_	_	_	
53-2	9295-9302	default	_	_	_	
53-3	9303-9313	throttling	_	_	_	
53-4	9314-9316	of	_	_	_	
53-5	9317-9324	loading	_	_	_	
53-6	9325-9328	the	_	_	_	
53-7	9329-9334	L2ARC	_	_	_	
53-8	9335-9341	device	_	_	_	
53-9	9342-9344	is	_	_	_	
53-10	9345-9346	8	_	_	_	
53-11	9347-9353	Mbytes	_	_	_	
53-12	9353-9354	/	_	_	_	
53-13	9354-9357	sec	_	_	_	
53-14	9357-9358	,	_	_	_	
53-15	9359-9361	on	_	_	_	
53-16	9362-9365	the	_	_	_	
53-17	9366-9376	assumption	_	_	_	
53-18	9377-9381	that	_	_	_	
53-19	9382-9385	the	_	_	_	
53-20	9386-9391	L2ARC	_	_	_	
53-21	9392-9394	is	_	_	_	
53-22	9395-9402	warming	_	_	_	
53-23	9403-9405	up	_	_	_	
53-24	9406-9410	from	_	_	_	
53-25	9411-9412	a	_	_	_	
53-26	9413-9419	random	_	_	_	
53-27	9420-9424	read	_	_	_	
53-28	9425-9433	workload	_	_	_	
53-29	9434-9438	from	_	_	_	
53-30	9439-9447	spinning	_	_	_	
53-31	9448-9453	disks	_	_	_	
53-32	9453-9454	,	_	_	_	
53-33	9455-9458	for	_	_	_	
53-34	9459-9464	which	_	_	_	
53-35	9465-9466	8	_	_	_	
53-36	9467-9473	Mbytes	_	_	_	
53-37	9473-9474	/	_	_	_	
53-38	9474-9477	sec	_	_	_	
53-39	9478-9480	is	_	_	_	
53-40	9481-9488	usually	_	_	_	
53-41	9489-9493	more	_	_	_	
53-42	9494-9498	than	_	_	_	
53-43	9499-9502	the	_	_	_	
53-44	9503-9511	spinning	_	_	_	
53-45	9512-9517	disks	_	_	_	
53-46	9518-9521	can	_	_	_	
53-47	9522-9529	provide	_	_	_	
53-48	9529-9530	.	_	_	_	

#Text=For example, at a 4 Kbyte I/O size, this is 2048 random disk IOPS, which may take at least 20 pool disks to drive.
54-1	9531-9534	For	_	_	_	
54-2	9535-9542	example	_	_	_	
54-3	9542-9543	,	_	_	_	
54-4	9544-9546	at	_	_	_	
54-5	9547-9548	a	_	_	_	
54-6	9549-9550	4	_	_	_	
54-7	9551-9556	Kbyte	_	_	_	
54-8	9557-9558	I	_	_	_	
54-9	9558-9559	/	_	_	_	
54-10	9559-9560	O	_	_	_	
54-11	9561-9565	size	_	_	_	
54-12	9565-9566	,	_	_	_	
54-13	9567-9571	this	_	_	_	
54-14	9572-9574	is	_	_	_	
54-15	9575-9579	2048	_	_	_	
54-16	9580-9586	random	_	_	_	
54-17	9587-9591	disk	_	_	_	
54-18	9592-9596	IOPS	_	_	_	
54-19	9596-9597	,	_	_	_	
54-20	9598-9603	which	_	_	_	
54-21	9604-9607	may	_	_	_	
54-22	9608-9612	take	_	_	_	
54-23	9613-9615	at	_	_	_	
54-24	9616-9621	least	_	_	_	
54-25	9622-9624	20	_	_	_	
54-26	9625-9629	pool	_	_	_	
54-27	9630-9635	disks	_	_	_	
54-28	9636-9638	to	_	_	_	
54-29	9639-9644	drive	_	_	_	
54-30	9644-9645	.	_	_	_	

#Text=Should the L2ARC throttling be increased from 8 Mbytes, it would make no difference in many configurations, which cannot provide more random IOPS.
55-1	9646-9652	Should	_	_	_	
55-2	9653-9656	the	_	_	_	
55-3	9657-9662	L2ARC	_	_	_	
55-4	9663-9673	throttling	_	_	_	
55-5	9674-9676	be	_	_	_	
55-6	9677-9686	increased	_	_	_	
55-7	9687-9691	from	_	_	_	
55-8	9692-9693	8	_	_	_	
55-9	9694-9700	Mbytes	_	_	_	
55-10	9700-9701	,	_	_	_	
55-11	9702-9704	it	_	_	_	
55-12	9705-9710	would	_	_	_	
55-13	9711-9715	make	_	_	_	
55-14	9716-9718	no	_	_	_	
55-15	9719-9729	difference	_	_	_	
55-16	9730-9732	in	_	_	_	
55-17	9733-9737	many	_	_	_	
55-18	9738-9752	configurations	_	_	_	
55-19	9752-9753	,	_	_	_	
55-20	9754-9759	which	_	_	_	
55-21	9760-9766	cannot	_	_	_	
55-22	9767-9774	provide	_	_	_	
55-23	9775-9779	more	_	_	_	
55-24	9780-9786	random	_	_	_	
55-25	9787-9791	IOPS	_	_	_	
55-26	9791-9792	.	_	_	_	

#Text=The downside of increasing the throttling is CPU consumption: the L2ARC periodically scans the ARC to find buffers to cache, based on the throttling size.
56-1	9793-9796	The	_	_	_	
56-2	9797-9805	downside	_	_	_	
56-3	9806-9808	of	_	_	_	
56-4	9809-9819	increasing	_	_	_	
56-5	9820-9823	the	_	_	_	
56-6	9824-9834	throttling	_	_	_	
56-7	9835-9837	is	_	_	_	
56-8	9838-9841	CPU	_	_	_	
56-9	9842-9853	consumption	_	_	_	
56-10	9853-9854	:	_	_	_	
56-11	9855-9858	the	_	_	_	
56-12	9859-9864	L2ARC	_	_	_	
56-13	9865-9877	periodically	_	_	_	
56-14	9878-9883	scans	_	_	_	
56-15	9884-9887	the	_	_	_	
56-16	9888-9891	ARC	_	_	_	
56-17	9892-9894	to	_	_	_	
56-18	9895-9899	find	_	_	_	
56-19	9900-9907	buffers	_	_	_	
56-20	9908-9910	to	_	_	_	
56-21	9911-9916	cache	_	_	_	
56-22	9916-9917	,	_	_	_	
56-23	9918-9923	based	_	_	_	
56-24	9924-9926	on	_	_	_	
56-25	9927-9930	the	_	_	_	
56-26	9931-9941	throttling	_	_	_	
56-27	9942-9946	size	_	_	_	
56-28	9946-9947	.	_	_	_	

#Text=If you increase the throttling but the pool disks cannot keep up, you burn CPU needlessly.
57-1	9948-9950	If	_	_	_	
57-2	9951-9954	you	_	_	_	
57-3	9955-9963	increase	_	_	_	
57-4	9964-9967	the	_	_	_	
57-5	9968-9978	throttling	_	_	_	
57-6	9979-9982	but	_	_	_	
57-7	9983-9986	the	_	_	_	
57-8	9987-9991	pool	_	_	_	
57-9	9992-9997	disks	_	_	_	
57-10	9998-10004	cannot	_	_	_	
57-11	10005-10009	keep	_	_	_	
57-12	10010-10012	up	_	_	_	
57-13	10012-10013	,	_	_	_	
57-14	10014-10017	you	_	_	_	
57-15	10018-10022	burn	_	_	_	
57-16	10023-10026	CPU	_	_	_	
57-17	10027-10037	needlessly	_	_	_	
57-18	10037-10038	.	_	_	_	

#Text=In extreme cases of tuning, this can consume an entire CPU for the ARC scan.
58-1	10039-10041	In	_	_	_	
58-2	10042-10049	extreme	_	_	_	
58-3	10050-10055	cases	_	_	_	
58-4	10056-10058	of	_	_	_	
58-5	10059-10065	tuning	_	_	_	
58-6	10065-10066	,	_	_	_	
58-7	10067-10071	this	_	_	_	
58-8	10072-10075	can	_	_	_	
58-9	10076-10083	consume	_	_	_	
58-10	10084-10086	an	_	_	_	
58-11	10087-10093	entire	_	_	_	
58-12	10094-10097	CPU	_	_	_	
58-13	10098-10101	for	_	_	_	
58-14	10102-10105	the	_	_	_	
58-15	10106-10109	ARC	_	_	_	
58-16	10110-10114	scan	_	_	_	
58-17	10114-10115	.	_	_	_	

#Text=If you are using the L2ARC in its typical use case: say, fewer than 30 pool disks, and caching a random read workload for ~4 Kbyte I/O which is mostly being pulled from the pool disks, then 8 Mbytes is usually sufficient.
59-1	10116-10118	If	_	_	_	
59-2	10119-10122	you	_	_	_	
59-3	10123-10126	are	_	_	_	
59-4	10127-10132	using	_	_	_	
59-5	10133-10136	the	_	_	_	
59-6	10137-10142	L2ARC	_	_	_	
59-7	10143-10145	in	_	_	_	
59-8	10146-10149	its	_	_	_	
59-9	10150-10157	typical	_	_	_	
59-10	10158-10161	use	_	_	_	
59-11	10162-10166	case	_	_	_	
59-12	10166-10167	:	_	_	_	
59-13	10168-10171	say	_	_	_	
59-14	10171-10172	,	_	_	_	
59-15	10173-10178	fewer	_	_	_	
59-16	10179-10183	than	_	_	_	
59-17	10184-10186	30	_	_	_	
59-18	10187-10191	pool	_	_	_	
59-19	10192-10197	disks	_	_	_	
59-20	10197-10198	,	_	_	_	
59-21	10199-10202	and	_	_	_	
59-22	10203-10210	caching	_	_	_	
59-23	10211-10212	a	_	_	_	
59-24	10213-10219	random	_	_	_	
59-25	10220-10224	read	_	_	_	
59-26	10225-10233	workload	_	_	_	
59-27	10234-10237	for	_	_	_	
59-28	10238-10239	~	_	_	_	
59-29	10239-10240	4	_	_	_	
59-30	10241-10246	Kbyte	_	_	_	
59-31	10247-10248	I	_	_	_	
59-32	10248-10249	/	_	_	_	
59-33	10249-10250	O	_	_	_	
59-34	10251-10256	which	_	_	_	
59-35	10257-10259	is	_	_	_	
59-36	10260-10266	mostly	_	_	_	
59-37	10267-10272	being	_	_	_	
59-38	10273-10279	pulled	_	_	_	
59-39	10280-10284	from	_	_	_	
59-40	10285-10288	the	_	_	_	
59-41	10289-10293	pool	_	_	_	
59-42	10294-10299	disks	_	_	_	
59-43	10299-10300	,	_	_	_	
59-44	10301-10305	then	_	_	_	
59-45	10306-10307	8	_	_	_	
59-46	10308-10314	Mbytes	_	_	_	
59-47	10315-10317	is	_	_	_	
59-48	10318-10325	usually	_	_	_	
59-49	10326-10336	sufficient	_	_	_	
59-50	10336-10337	.	_	_	_	

#Text=If you are not this typical use case: say, you are caching streaming workloads, or have several dozens of disks, then you may want to consider tuning the rate.
60-1	10338-10340	If	_	_	_	
60-2	10341-10344	you	_	_	_	
60-3	10345-10348	are	_	_	_	
60-4	10349-10352	not	_	_	_	
60-5	10353-10357	this	_	_	_	
60-6	10358-10365	typical	_	_	_	
60-7	10366-10369	use	_	_	_	
60-8	10370-10374	case	_	_	_	
60-9	10374-10375	:	_	_	_	
60-10	10376-10379	say	_	_	_	
60-11	10379-10380	,	_	_	_	
60-12	10381-10384	you	_	_	_	
60-13	10385-10388	are	_	_	_	
60-14	10389-10396	caching	_	_	_	
60-15	10397-10406	streaming	_	_	_	
60-16	10407-10416	workloads	_	_	_	
60-17	10416-10417	,	_	_	_	
60-18	10418-10420	or	_	_	_	
60-19	10421-10425	have	_	_	_	
60-20	10426-10433	several	_	_	_	
60-21	10434-10440	dozens	_	_	_	
60-22	10441-10443	of	_	_	_	
60-23	10444-10449	disks	_	_	_	
60-24	10449-10450	,	_	_	_	
60-25	10451-10455	then	_	_	_	
60-26	10456-10459	you	_	_	_	
60-27	10460-10463	may	_	_	_	
60-28	10464-10468	want	_	_	_	
60-29	10469-10471	to	_	_	_	
60-30	10472-10480	consider	_	_	_	
60-31	10481-10487	tuning	_	_	_	
60-32	10488-10491	the	_	_	_	
60-33	10492-10496	rate	_	_	_	
60-34	10496-10497	.	_	_	_	

#Text=Modern L2ARC devices (SSDs) can handle an order of magnitude higher than the default.
61-1	10498-10504	Modern	_	_	_	
61-2	10505-10510	L2ARC	_	_	_	
61-3	10511-10518	devices	_	_	_	
61-4	10519-10520	(	_	_	_	
61-5	10520-10524	SSDs	_	_	_	
61-6	10524-10525	)	_	_	_	
61-7	10526-10529	can	_	_	_	
61-8	10530-10536	handle	_	_	_	
61-9	10537-10539	an	_	_	_	
61-10	10540-10545	order	_	_	_	
61-11	10546-10548	of	_	_	_	
61-12	10549-10558	magnitude	_	_	_	
61-13	10559-10565	higher	_	_	_	
61-14	10566-10570	than	_	_	_	
61-15	10571-10574	the	_	_	_	
61-16	10575-10582	default	_	_	_	
61-17	10582-10583	.	_	_	_	

#Text=It can be tuned by setting the following sysctls: vfs.zfs.l2arc_write_max
#Text=vfs.zfs.l2arc_write_boostThe former value sets the runtime max that data will be loaded into L2ARC.
62-1	10584-10586	It	_	_	_	
62-2	10587-10590	can	_	_	_	
62-3	10591-10593	be	_	_	_	
62-4	10594-10599	tuned	_	_	_	
62-5	10600-10602	by	_	_	_	
62-6	10603-10610	setting	_	_	_	
62-7	10611-10614	the	_	_	_	
62-8	10615-10624	following	_	_	_	
62-9	10625-10632	sysctls	_	_	_	
62-10	10632-10633	:	_	_	_	
62-11	10634-10657	vfs.zfs.l2arc_write_max	_	_	_	
62-12	10658-10686	vfs.zfs.l2arc_write_boostThe	_	_	_	
62-13	10687-10693	former	_	_	_	
62-14	10694-10699	value	_	_	_	
62-15	10700-10704	sets	_	_	_	
62-16	10705-10708	the	_	_	_	
62-17	10709-10716	runtime	_	_	_	
62-18	10717-10720	max	_	_	_	
62-19	10721-10725	that	_	_	_	
62-20	10726-10730	data	_	_	_	
62-21	10731-10735	will	_	_	_	
62-22	10736-10738	be	_	_	_	
62-23	10739-10745	loaded	_	_	_	
62-24	10746-10750	into	_	_	_	
62-25	10751-10756	L2ARC	_	_	_	
62-26	10756-10757	.	_	_	_	

#Text=The latter can be used to accelerate the loading of a freshly booted system.
63-1	10758-10761	The	_	_	_	
63-2	10762-10768	latter	_	_	_	
63-3	10769-10772	can	_	_	_	
63-4	10773-10775	be	_	_	_	
63-5	10776-10780	used	_	_	_	
63-6	10781-10783	to	_	_	_	
63-7	10784-10794	accelerate	_	_	_	
63-8	10795-10798	the	_	_	_	
63-9	10799-10806	loading	_	_	_	
63-10	10807-10809	of	_	_	_	
63-11	10810-10811	a	_	_	_	
63-12	10812-10819	freshly	_	_	_	
63-13	10820-10826	booted	_	_	_	
63-14	10827-10833	system	_	_	_	
63-15	10833-10834	.	_	_	_	

#Text=Note that the same caveats apply about these sysctls and pool imports as the previous one.
64-1	10835-10839	Note	_	_	_	
64-2	10840-10844	that	_	_	_	
64-3	10845-10848	the	_	_	_	
64-4	10849-10853	same	_	_	_	
64-5	10854-10861	caveats	_	_	_	
64-6	10862-10867	apply	_	_	_	
64-7	10868-10873	about	_	_	_	
64-8	10874-10879	these	_	_	_	
64-9	10880-10887	sysctls	_	_	_	
64-10	10888-10891	and	_	_	_	
64-11	10892-10896	pool	_	_	_	
64-12	10897-10904	imports	_	_	_	
64-13	10905-10907	as	_	_	_	
64-14	10908-10911	the	_	_	_	
64-15	10912-10920	previous	_	_	_	
64-16	10921-10924	one	_	_	_	
64-17	10924-10925	.	_	_	_	

#Text=While you can improve the L2ARC warmup rate, keep an eye on increased CPU consumption due to scanning by the l2arc_feed_thread().
65-1	10926-10931	While	_	_	_	
65-2	10932-10935	you	_	_	_	
65-3	10936-10939	can	_	_	_	
65-4	10940-10947	improve	_	_	_	
65-5	10948-10951	the	_	_	_	
65-6	10952-10957	L2ARC	_	_	_	
65-7	10958-10964	warmup	_	_	_	
65-8	10965-10969	rate	_	_	_	
65-9	10969-10970	,	_	_	_	
65-10	10971-10975	keep	_	_	_	
65-11	10976-10978	an	_	_	_	
65-12	10979-10982	eye	_	_	_	
65-13	10983-10985	on	_	_	_	
65-14	10986-10995	increased	_	_	_	
65-15	10996-10999	CPU	_	_	_	
65-16	11000-11011	consumption	_	_	_	
65-17	11012-11015	due	_	_	_	
65-18	11016-11018	to	_	_	_	
65-19	11019-11027	scanning	_	_	_	
65-20	11028-11030	by	_	_	_	
65-21	11031-11034	the	_	_	_	
65-22	11035-11052	l2arc_feed_thread	_	_	_	
65-23	11052-11053	(	_	_	_	
65-24	11053-11054	)	_	_	_	
65-25	11054-11055	.	_	_	_	

#Text=Eg, use DTrace to profile on-CPU thread names (see DTrace One-Liners).
66-1	11056-11058	Eg	_	_	_	
66-2	11058-11059	,	_	_	_	
66-3	11060-11063	use	_	_	_	
66-4	11064-11070	DTrace	_	_	_	
66-5	11071-11073	to	_	_	_	
66-6	11074-11081	profile	_	_	_	
66-7	11082-11088	on-CPU	_	_	_	
66-8	11089-11095	thread	_	_	_	
66-9	11096-11101	names	_	_	_	
66-10	11102-11103	(	_	_	_	
66-11	11103-11106	see	_	_	_	
66-12	11107-11113	DTrace	_	_	_	
66-13	11114-11124	One-Liners	_	_	_	
66-14	11124-11125	)	_	_	_	
66-15	11125-11126	.	_	_	_	

#Text=The known caveats: There's no free lunch.
67-1	11127-11130	The	_	_	_	
67-2	11131-11136	known	_	_	_	
67-3	11137-11144	caveats	_	_	_	
67-4	11144-11145	:	_	_	_	
67-5	11146-11153	There's	_	_	_	
67-6	11154-11156	no	_	_	_	
67-7	11157-11161	free	_	_	_	
67-8	11162-11167	lunch	_	_	_	
67-9	11167-11168	.	_	_	_	

#Text=A properly tuned L2ARC will increase read performance, but it comes at the price of decreased write performance.
68-1	11169-11170	A	_	_	_	
68-2	11171-11179	properly	_	_	_	
68-3	11180-11185	tuned	_	_	_	
68-4	11186-11191	L2ARC	_	_	_	
68-5	11192-11196	will	_	_	_	
68-6	11197-11205	increase	_	_	_	
68-7	11206-11210	read	_	_	_	
68-8	11211-11222	performance	_	_	_	
68-9	11222-11223	,	_	_	_	
68-10	11224-11227	but	_	_	_	
68-11	11228-11230	it	_	_	_	
68-12	11231-11236	comes	_	_	_	
68-13	11237-11239	at	_	_	_	
68-14	11240-11243	the	_	_	_	
68-15	11244-11249	price	_	_	_	
68-16	11250-11252	of	_	_	_	
68-17	11253-11262	decreased	_	_	_	
68-18	11263-11268	write	_	_	_	
68-19	11269-11280	performance	_	_	_	
68-20	11280-11281	.	_	_	_	

#Text=The pool essentially magnifies writes by writing them to the pool as well as the L2ARC device.
69-1	11282-11285	The	_	_	_	
69-2	11286-11290	pool	_	_	_	
69-3	11291-11302	essentially	_	_	_	
69-4	11303-11312	magnifies	_	_	_	
69-5	11313-11319	writes	_	_	_	
69-6	11320-11322	by	_	_	_	
69-7	11323-11330	writing	_	_	_	
69-8	11331-11335	them	_	_	_	
69-9	11336-11338	to	_	_	_	
69-10	11339-11342	the	_	_	_	
69-11	11343-11347	pool	_	_	_	
69-12	11348-11350	as	_	_	_	
69-13	11351-11355	well	_	_	_	
69-14	11356-11358	as	_	_	_	
69-15	11359-11362	the	_	_	_	
69-16	11363-11368	L2ARC	_	_	_	
69-17	11369-11375	device	_	_	_	
69-18	11375-11376	.	_	_	_	

#Text=Another interesting effect that's been observed is a falloff in L2ARC performance when doing a streaming read from L2ARC while simultaneously doing a heavy write workload.
70-1	11377-11384	Another	_	_	_	
70-2	11385-11396	interesting	_	_	_	
70-3	11397-11403	effect	_	_	_	
70-4	11404-11410	that's	_	_	_	
70-5	11411-11415	been	_	_	_	
70-6	11416-11424	observed	_	_	_	
70-7	11425-11427	is	_	_	_	
70-8	11428-11429	a	_	_	_	
70-9	11430-11437	falloff	_	_	_	
70-10	11438-11440	in	_	_	_	
70-11	11441-11446	L2ARC	_	_	_	
70-12	11447-11458	performance	_	_	_	
70-13	11459-11463	when	_	_	_	
70-14	11464-11469	doing	_	_	_	
70-15	11470-11471	a	_	_	_	
70-16	11472-11481	streaming	_	_	_	
70-17	11482-11486	read	_	_	_	
70-18	11487-11491	from	_	_	_	
70-19	11492-11497	L2ARC	_	_	_	
70-20	11498-11503	while	_	_	_	
70-21	11504-11518	simultaneously	_	_	_	
70-22	11519-11524	doing	_	_	_	
70-23	11525-11526	a	_	_	_	
70-24	11527-11532	heavy	_	_	_	
70-25	11533-11538	write	_	_	_	
70-26	11539-11547	workload	_	_	_	
70-27	11547-11548	.	_	_	_	

#Text=My conjecture is that the write can cause cache thrashing but this hasn't been confirmed at this time.
71-1	11549-11551	My	_	_	_	
71-2	11552-11562	conjecture	_	_	_	
71-3	11563-11565	is	_	_	_	
71-4	11566-11570	that	_	_	_	
71-5	11571-11574	the	_	_	_	
71-6	11575-11580	write	_	_	_	
71-7	11581-11584	can	_	_	_	
71-8	11585-11590	cause	_	_	_	
71-9	11591-11596	cache	_	_	_	
71-10	11597-11606	thrashing	_	_	_	
71-11	11607-11610	but	_	_	_	
71-12	11611-11615	this	_	_	_	
71-13	11616-11622	hasn't	_	_	_	
71-14	11623-11627	been	_	_	_	
71-15	11628-11637	confirmed	_	_	_	
71-16	11638-11640	at	_	_	_	
71-17	11641-11645	this	_	_	_	
71-18	11646-11650	time	_	_	_	
71-19	11650-11651	.	_	_	_	

#Text=Given a working set close to ARC size an L2ARC can actually hurt performance.
72-1	11652-11657	Given	_	_	_	
72-2	11658-11659	a	_	_	_	
72-3	11660-11667	working	_	_	_	
72-4	11668-11671	set	_	_	_	
72-5	11672-11677	close	_	_	_	
72-6	11678-11680	to	_	_	_	
72-7	11681-11684	ARC	_	_	_	
72-8	11685-11689	size	_	_	_	
72-9	11690-11692	an	_	_	_	
72-10	11693-11698	L2ARC	_	_	_	
72-11	11699-11702	can	_	_	_	
72-12	11703-11711	actually	_	_	_	
72-13	11712-11716	hurt	_	_	_	
72-14	11717-11728	performance	_	_	_	
72-15	11728-11729	.	_	_	_	

#Text=If a system has a 14GB ARC and a 13GB working set, adding an L2ARC device will rob ARC space to map the L2ARC.
73-1	11730-11732	If	_	_	_	
73-2	11733-11734	a	_	_	_	
73-3	11735-11741	system	_	_	_	
73-4	11742-11745	has	_	_	_	
73-5	11746-11747	a	_	_	_	
73-6	11748-11752	14GB	_	_	_	
73-7	11753-11756	ARC	_	_	_	
73-8	11757-11760	and	_	_	_	
73-9	11761-11762	a	_	_	_	
73-10	11763-11767	13GB	_	_	_	
73-11	11768-11775	working	_	_	_	
73-12	11776-11779	set	_	_	_	
73-13	11779-11780	,	_	_	_	
73-14	11781-11787	adding	_	_	_	
73-15	11788-11790	an	_	_	_	
73-16	11791-11796	L2ARC	_	_	_	
73-17	11797-11803	device	_	_	_	
73-18	11804-11808	will	_	_	_	
73-19	11809-11812	rob	_	_	_	
73-20	11813-11816	ARC	_	_	_	
73-21	11817-11822	space	_	_	_	
73-22	11823-11825	to	_	_	_	
73-23	11826-11829	map	_	_	_	
73-24	11830-11833	the	_	_	_	
73-25	11834-11839	L2ARC	_	_	_	
73-26	11839-11840	.	_	_	_	

#Text=If the reduced ARC size is smaller than the working set reads will be evicted from the ARC into the (ostensibly slower) L2ARC.
74-1	11841-11843	If	_	_	_	
74-2	11844-11847	the	_	_	_	
74-3	11848-11855	reduced	_	_	_	
74-4	11856-11859	ARC	_	_	_	
74-5	11860-11864	size	_	_	_	
74-6	11865-11867	is	_	_	_	
74-7	11868-11875	smaller	_	_	_	
74-8	11876-11880	than	_	_	_	
74-9	11881-11884	the	_	_	_	
74-10	11885-11892	working	_	_	_	
74-11	11893-11896	set	_	_	_	
74-12	11897-11902	reads	_	_	_	
74-13	11903-11907	will	_	_	_	
74-14	11908-11910	be	_	_	_	
74-15	11911-11918	evicted	_	_	_	
74-16	11919-11923	from	_	_	_	
74-17	11924-11927	the	_	_	_	
74-18	11928-11931	ARC	_	_	_	
74-19	11932-11936	into	_	_	_	
74-20	11937-11940	the	_	_	_	
74-21	11941-11942	(	_	_	_	
74-22	11942-11952	ostensibly	_	_	_	
74-23	11953-11959	slower	_	_	_	
74-24	11959-11960	)	_	_	_	
74-25	11961-11966	L2ARC	_	_	_	
74-26	11966-11967	.	_	_	_	

#Text=Multiple L2ARC devices are concatenated, there's no provision for mirroring them.
75-1	11968-11976	Multiple	_	_	_	
75-2	11977-11982	L2ARC	_	_	_	
75-3	11983-11990	devices	_	_	_	
75-4	11991-11994	are	_	_	_	
75-5	11995-12007	concatenated	_	_	_	
75-6	12007-12008	,	_	_	_	
75-7	12009-12016	there's	_	_	_	
75-8	12017-12019	no	_	_	_	
75-9	12020-12029	provision	_	_	_	
75-10	12030-12033	for	_	_	_	
75-11	12034-12043	mirroring	_	_	_	
75-12	12044-12048	them	_	_	_	
75-13	12048-12049	.	_	_	_	

#Text=If a heavily used L2ARC device fails the pool will continue to operate with reduced performance.
76-1	12050-12052	If	_	_	_	
76-2	12053-12054	a	_	_	_	
76-3	12055-12062	heavily	_	_	_	
76-4	12063-12067	used	_	_	_	
76-5	12068-12073	L2ARC	_	_	_	
76-6	12074-12080	device	_	_	_	
76-7	12081-12086	fails	_	_	_	
76-8	12087-12090	the	_	_	_	
76-9	12091-12095	pool	_	_	_	
76-10	12096-12100	will	_	_	_	
76-11	12101-12109	continue	_	_	_	
76-12	12110-12112	to	_	_	_	
76-13	12113-12120	operate	_	_	_	
76-14	12121-12125	with	_	_	_	
76-15	12126-12133	reduced	_	_	_	
76-16	12134-12145	performance	_	_	_	
76-17	12145-12146	.	_	_	_	

#Text=There's also no provision for striping reads across multiple devices.
77-1	12147-12154	There's	_	_	_	
77-2	12155-12159	also	_	_	_	
77-3	12160-12162	no	_	_	_	
77-4	12163-12172	provision	_	_	_	
77-5	12173-12176	for	_	_	_	
77-6	12177-12185	striping	_	_	_	
77-7	12186-12191	reads	_	_	_	
77-8	12192-12198	across	_	_	_	
77-9	12199-12207	multiple	_	_	_	
77-10	12208-12215	devices	_	_	_	
77-11	12215-12216	.	_	_	_	

#Text=If the blocks for a file end up in multiple devices you'll see striping but there's no way to force this behavior.
78-1	12217-12219	If	_	_	_	
78-2	12220-12223	the	_	_	_	
78-3	12224-12230	blocks	_	_	_	
78-4	12231-12234	for	_	_	_	
78-5	12235-12236	a	_	_	_	
78-6	12237-12241	file	_	_	_	
78-7	12242-12245	end	_	_	_	
78-8	12246-12248	up	_	_	_	
78-9	12249-12251	in	_	_	_	
78-10	12252-12260	multiple	_	_	_	
78-11	12261-12268	devices	_	_	_	
78-12	12269-12275	you'll	_	_	_	
78-13	12276-12279	see	_	_	_	
78-14	12280-12288	striping	_	_	_	
78-15	12289-12292	but	_	_	_	
78-16	12293-12300	there's	_	_	_	
78-17	12301-12303	no	_	_	_	
78-18	12304-12307	way	_	_	_	
78-19	12308-12310	to	_	_	_	
78-20	12311-12316	force	_	_	_	
78-21	12317-12321	this	_	_	_	
78-22	12322-12330	behavior	_	_	_	
78-23	12330-12331	.	_	_	_	

#Text=Be very careful when adding devices to a production pool.
79-1	12332-12334	Be	_	_	_	
79-2	12335-12339	very	_	_	_	
79-3	12340-12347	careful	_	_	_	
79-4	12348-12352	when	_	_	_	
79-5	12353-12359	adding	_	_	_	
79-6	12360-12367	devices	_	_	_	
79-7	12368-12370	to	_	_	_	
79-8	12371-12372	a	_	_	_	
79-9	12373-12383	production	_	_	_	
79-10	12384-12388	pool	_	_	_	
79-11	12388-12389	.	_	_	_	

#Text=By default zpool add stripes vdevs to the pool.
80-1	12390-12392	By	_	_	_	
80-2	12393-12400	default	_	_	_	
80-3	12401-12406	zpool	_	_	_	
80-4	12407-12410	add	_	_	_	
80-5	12411-12418	stripes	_	_	_	
80-6	12419-12424	vdevs	_	_	_	
80-7	12425-12427	to	_	_	_	
80-8	12428-12431	the	_	_	_	
80-9	12432-12436	pool	_	_	_	
80-10	12436-12437	.	_	_	_	

#Text=If you do this you'll end up striping the device you intended to add as an L2ARC to the pool, and the only way to remove it will be backing up the pool, destroying it, and recreating it.
81-1	12438-12440	If	_	_	_	
81-2	12441-12444	you	_	_	_	
81-3	12445-12447	do	_	_	_	
81-4	12448-12452	this	_	_	_	
81-5	12453-12459	you'll	_	_	_	
81-6	12460-12463	end	_	_	_	
81-7	12464-12466	up	_	_	_	
81-8	12467-12475	striping	_	_	_	
81-9	12476-12479	the	_	_	_	
81-10	12480-12486	device	_	_	_	
81-11	12487-12490	you	_	_	_	
81-12	12491-12499	intended	_	_	_	
81-13	12500-12502	to	_	_	_	
81-14	12503-12506	add	_	_	_	
81-15	12507-12509	as	_	_	_	
81-16	12510-12512	an	_	_	_	
81-17	12513-12518	L2ARC	_	_	_	
81-18	12519-12521	to	_	_	_	
81-19	12522-12525	the	_	_	_	
81-20	12526-12530	pool	_	_	_	
81-21	12530-12531	,	_	_	_	
81-22	12532-12535	and	_	_	_	
81-23	12536-12539	the	_	_	_	
81-24	12540-12544	only	_	_	_	
81-25	12545-12548	way	_	_	_	
81-26	12549-12551	to	_	_	_	
81-27	12552-12558	remove	_	_	_	
81-28	12559-12561	it	_	_	_	
81-29	12562-12566	will	_	_	_	
81-30	12567-12569	be	_	_	_	
81-31	12570-12577	backing	_	_	_	
81-32	12578-12580	up	_	_	_	
81-33	12581-12584	the	_	_	_	
81-34	12585-12589	pool	_	_	_	
81-35	12589-12590	,	_	_	_	
81-36	12591-12601	destroying	_	_	_	
81-37	12602-12604	it	_	_	_	
81-38	12604-12605	,	_	_	_	
81-39	12606-12609	and	_	_	_	
81-40	12610-12620	recreating	_	_	_	
81-41	12621-12623	it	_	_	_	
81-42	12623-12624	.	_	_	_	

#Text=Many SSDs benefit from 4K alignment.
82-1	12625-12629	Many	_	_	_	
82-2	12630-12634	SSDs	_	_	_	
82-3	12635-12642	benefit	_	_	_	
82-4	12643-12647	from	_	_	_	
82-5	12648-12650	4K	_	_	_	
82-6	12651-12660	alignment	_	_	_	
82-7	12660-12661	.	_	_	_	

#Text=Using gpart and gnop on L2ARC devices can help with accomplishing this.
83-1	12662-12667	Using	_	_	_	
83-2	12668-12673	gpart	_	_	_	
83-3	12674-12677	and	_	_	_	
83-4	12678-12682	gnop	_	_	_	
83-5	12683-12685	on	_	_	_	
83-6	12686-12691	L2ARC	_	_	_	
83-7	12692-12699	devices	_	_	_	
83-8	12700-12703	can	_	_	_	
83-9	12704-12708	help	_	_	_	
83-10	12709-12713	with	_	_	_	
83-11	12714-12727	accomplishing	_	_	_	
83-12	12728-12732	this	_	_	_	
83-13	12732-12733	.	_	_	_	

#Text=Because the pool ID isn't stored on hot spare or L2ARC devices they can get lost if the system changes device names.
84-1	12734-12741	Because	_	_	_	
84-2	12742-12745	the	_	_	_	
84-3	12746-12750	pool	_	_	_	
84-4	12751-12753	ID	_	_	_	
84-5	12754-12759	isn't	_	_	_	
84-6	12760-12766	stored	_	_	_	
84-7	12767-12769	on	_	_	_	
84-8	12770-12773	hot	_	_	_	
84-9	12774-12779	spare	_	_	_	
84-10	12780-12782	or	_	_	_	
84-11	12783-12788	L2ARC	_	_	_	
84-12	12789-12796	devices	_	_	_	
84-13	12797-12801	they	_	_	_	
84-14	12802-12805	can	_	_	_	
84-15	12806-12809	get	_	_	_	
84-16	12810-12814	lost	_	_	_	
84-17	12815-12817	if	_	_	_	
84-18	12818-12821	the	_	_	_	
84-19	12822-12828	system	_	_	_	
84-20	12829-12836	changes	_	_	_	
84-21	12837-12843	device	_	_	_	
84-22	12844-12849	names	_	_	_	
84-23	12849-12850	.	_	_	_	

#Text=The caveat about only giving ZFS full devices is a solarism that doesn't apply to FreeBSD.
85-1	12851-12854	The	_	_	_	
85-2	12855-12861	caveat	_	_	_	
85-3	12862-12867	about	_	_	_	
85-4	12868-12872	only	_	_	_	
85-5	12873-12879	giving	_	_	_	
85-6	12880-12883	ZFS	_	_	_	
85-7	12884-12888	full	_	_	_	
85-8	12889-12896	devices	_	_	_	
85-9	12897-12899	is	_	_	_	
85-10	12900-12901	a	_	_	_	
85-11	12902-12910	solarism	_	_	_	
85-12	12911-12915	that	_	_	_	
85-13	12916-12923	doesn't	_	_	_	
85-14	12924-12929	apply	_	_	_	
85-15	12930-12932	to	_	_	_	
85-16	12933-12940	FreeBSD	_	_	_	
85-17	12940-12941	.	_	_	_	

#Text=On Solaris write caches are disabled on drives if partitions are handed to ZFS.
86-1	12942-12944	On	_	_	_	
86-2	12945-12952	Solaris	_	_	_	
86-3	12953-12958	write	_	_	_	
86-4	12959-12965	caches	_	_	_	
86-5	12966-12969	are	_	_	_	
86-6	12970-12978	disabled	_	_	_	
86-7	12979-12981	on	_	_	_	
86-8	12982-12988	drives	_	_	_	
86-9	12989-12991	if	_	_	_	
86-10	12992-13002	partitions	_	_	_	
86-11	13003-13006	are	_	_	_	
86-12	13007-13013	handed	_	_	_	
86-13	13014-13016	to	_	_	_	
86-14	13017-13020	ZFS	_	_	_	
86-15	13020-13021	.	_	_	_	

#Text=On FreeBSD this isn't the case.
87-1	13022-13024	On	_	_	_	
87-2	13025-13032	FreeBSD	_	_	_	
87-3	13033-13037	this	_	_	_	
87-4	13038-13043	isn't	_	_	_	
87-5	13044-13047	the	_	_	_	
87-6	13048-13052	case	_	_	_	
87-7	13052-13053	.	_	_	_	

#Text=Application Issues
#Text=ZFS is a copy-on-write filesystem.
88-1	13054-13065	Application	_	_	_	
88-2	13066-13072	Issues	_	_	_	
88-3	13073-13076	ZFS	_	_	_	
88-4	13077-13079	is	_	_	_	
88-5	13080-13081	a	_	_	_	
88-6	13082-13095	copy-on-write	_	_	_	
88-7	13096-13106	filesystem	_	_	_	
88-8	13106-13107	.	_	_	_	

#Text=As such metadata from the top of the hierarchy is copied in order to maintain consistency in case of sudden failure, i.e. loss of power during a write operation.
89-1	13108-13110	As	_	_	_	
89-2	13111-13115	such	_	_	_	
89-3	13116-13124	metadata	_	_	_	
89-4	13125-13129	from	_	_	_	
89-5	13130-13133	the	_	_	_	
89-6	13134-13137	top	_	_	_	
89-7	13138-13140	of	_	_	_	
89-8	13141-13144	the	_	_	_	
89-9	13145-13154	hierarchy	_	_	_	
89-10	13155-13157	is	_	_	_	
89-11	13158-13164	copied	_	_	_	
89-12	13165-13167	in	_	_	_	
89-13	13168-13173	order	_	_	_	
89-14	13174-13176	to	_	_	_	
89-15	13177-13185	maintain	_	_	_	
89-16	13186-13197	consistency	_	_	_	
89-17	13198-13200	in	_	_	_	
89-18	13201-13205	case	_	_	_	
89-19	13206-13208	of	_	_	_	
89-20	13209-13215	sudden	_	_	_	
89-21	13216-13223	failure	_	_	_	
89-22	13223-13224	,	_	_	_	
89-23	13225-13228	i.e	_	_	_	
89-24	13228-13229	.	_	_	_	
89-25	13230-13234	loss	_	_	_	
89-26	13235-13237	of	_	_	_	
89-27	13238-13243	power	_	_	_	
89-28	13244-13250	during	_	_	_	
89-29	13251-13252	a	_	_	_	
89-30	13253-13258	write	_	_	_	
89-31	13259-13268	operation	_	_	_	
89-32	13268-13269	.	_	_	_	

#Text=This obviates the need for an fsck-like requirement of ZFS filesystems at boot.
90-1	13270-13274	This	_	_	_	
90-2	13275-13283	obviates	_	_	_	
90-3	13284-13287	the	_	_	_	
90-4	13288-13292	need	_	_	_	
90-5	13293-13296	for	_	_	_	
90-6	13297-13299	an	_	_	_	
90-7	13300-13309	fsck-like	_	_	_	
90-8	13310-13321	requirement	_	_	_	
90-9	13322-13324	of	_	_	_	
90-10	13325-13328	ZFS	_	_	_	
90-11	13329-13340	filesystems	_	_	_	
90-12	13341-13343	at	_	_	_	
90-13	13344-13348	boot	_	_	_	
90-14	13348-13349	.	_	_	_	

#Text=However the downside to this is that applications which perform updates in place to large files, e.g. databases, will likely perform poorly in this application of the filesystem due to excessive I/O from copy-on-write (a fast SLOG device -- e.g. a SSD -- can help regarding the write performance of databases or any application which is doing synchronous writes (e.g. open with O_FSYNC) to the FS to make sure the data is on non-volatile storage when the write-call returns).
91-1	13350-13357	However	_	_	_	
91-2	13358-13361	the	_	_	_	
91-3	13362-13370	downside	_	_	_	
91-4	13371-13373	to	_	_	_	
91-5	13374-13378	this	_	_	_	
91-6	13379-13381	is	_	_	_	
91-7	13382-13386	that	_	_	_	
91-8	13387-13399	applications	_	_	_	
91-9	13400-13405	which	_	_	_	
91-10	13406-13413	perform	_	_	_	
91-11	13414-13421	updates	_	_	_	
91-12	13422-13424	in	_	_	_	
91-13	13425-13430	place	_	_	_	
91-14	13431-13433	to	_	_	_	
91-15	13434-13439	large	_	_	_	
91-16	13440-13445	files	_	_	_	
91-17	13445-13446	,	_	_	_	
91-18	13447-13450	e.g	_	_	_	
91-19	13450-13451	.	_	_	_	
91-20	13452-13461	databases	_	_	_	
91-21	13461-13462	,	_	_	_	
91-22	13463-13467	will	_	_	_	
91-23	13468-13474	likely	_	_	_	
91-24	13475-13482	perform	_	_	_	
91-25	13483-13489	poorly	_	_	_	
91-26	13490-13492	in	_	_	_	
91-27	13493-13497	this	_	_	_	
91-28	13498-13509	application	_	_	_	
91-29	13510-13512	of	_	_	_	
91-30	13513-13516	the	_	_	_	
91-31	13517-13527	filesystem	_	_	_	
91-32	13528-13531	due	_	_	_	
91-33	13532-13534	to	_	_	_	
91-34	13535-13544	excessive	_	_	_	
91-35	13545-13546	I	_	_	_	
91-36	13546-13547	/	_	_	_	
91-37	13547-13548	O	_	_	_	
91-38	13549-13553	from	_	_	_	
91-39	13554-13567	copy-on-write	_	_	_	
91-40	13568-13569	(	_	_	_	
91-41	13569-13570	a	_	_	_	
91-42	13571-13575	fast	_	_	_	
91-43	13576-13580	SLOG	_	_	_	
91-44	13581-13587	device	_	_	_	
91-45	13588-13589	-	_	_	_	
91-46	13589-13590	-	_	_	_	
91-47	13591-13594	e.g	_	_	_	
91-48	13594-13595	.	_	_	_	
91-49	13596-13597	a	_	_	_	
91-50	13598-13601	SSD	_	_	_	
91-51	13602-13603	-	_	_	_	
91-52	13603-13604	-	_	_	_	
91-53	13605-13608	can	_	_	_	
91-54	13609-13613	help	_	_	_	
91-55	13614-13623	regarding	_	_	_	
91-56	13624-13627	the	_	_	_	
91-57	13628-13633	write	_	_	_	
91-58	13634-13645	performance	_	_	_	
91-59	13646-13648	of	_	_	_	
91-60	13649-13658	databases	_	_	_	
91-61	13659-13661	or	_	_	_	
91-62	13662-13665	any	_	_	_	
91-63	13666-13677	application	_	_	_	
91-64	13678-13683	which	_	_	_	
91-65	13684-13686	is	_	_	_	
91-66	13687-13692	doing	_	_	_	
91-67	13693-13704	synchronous	_	_	_	
91-68	13705-13711	writes	_	_	_	
91-69	13712-13713	(	_	_	_	
91-70	13713-13716	e.g	_	_	_	
91-71	13716-13717	.	_	_	_	
91-72	13718-13722	open	_	_	_	
91-73	13723-13727	with	_	_	_	
91-74	13728-13735	O_FSYNC	_	_	_	
91-75	13735-13736	)	_	_	_	
91-76	13737-13739	to	_	_	_	
91-77	13740-13743	the	_	_	_	
91-78	13744-13746	FS	_	_	_	
91-79	13747-13749	to	_	_	_	
91-80	13750-13754	make	_	_	_	
91-81	13755-13759	sure	_	_	_	
91-82	13760-13763	the	_	_	_	
91-83	13764-13768	data	_	_	_	
91-84	13769-13771	is	_	_	_	
91-85	13772-13774	on	_	_	_	
91-86	13775-13787	non-volatile	_	_	_	
91-87	13788-13795	storage	_	_	_	
91-88	13796-13800	when	_	_	_	
91-89	13801-13804	the	_	_	_	
91-90	13805-13815	write-call	_	_	_	
91-91	13816-13823	returns	_	_	_	
91-92	13823-13824	)	_	_	_	
91-93	13824-13825	.	_	_	_	

#Text=Additionally, database applications, such as Oracle, maintain a large cache (called the SGA in Oracle) in memory will perform poorly due to double caching of data in the ARC and in the application's own cache.
92-1	13826-13838	Additionally	_	_	_	
92-2	13838-13839	,	_	_	_	
92-3	13840-13848	database	_	_	_	
92-4	13849-13861	applications	_	_	_	
92-5	13861-13862	,	_	_	_	
92-6	13863-13867	such	_	_	_	
92-7	13868-13870	as	_	_	_	
92-8	13871-13877	Oracle	_	_	_	
92-9	13877-13878	,	_	_	_	
92-10	13879-13887	maintain	_	_	_	
92-11	13888-13889	a	_	_	_	
92-12	13890-13895	large	_	_	_	
92-13	13896-13901	cache	_	_	_	
92-14	13902-13903	(	_	_	_	
92-15	13903-13909	called	_	_	_	
92-16	13910-13913	the	_	_	_	
92-17	13914-13917	SGA	_	_	_	
92-18	13918-13920	in	_	_	_	
92-19	13921-13927	Oracle	_	_	_	
92-20	13927-13928	)	_	_	_	
92-21	13929-13931	in	_	_	_	
92-22	13932-13938	memory	_	_	_	
92-23	13939-13943	will	_	_	_	
92-24	13944-13951	perform	_	_	_	
92-25	13952-13958	poorly	_	_	_	
92-26	13959-13962	due	_	_	_	
92-27	13963-13965	to	_	_	_	
92-28	13966-13972	double	_	_	_	
92-29	13973-13980	caching	_	_	_	
92-30	13981-13983	of	_	_	_	
92-31	13984-13988	data	_	_	_	
92-32	13989-13991	in	_	_	_	
92-33	13992-13995	the	_	_	_	
92-34	13996-13999	ARC	_	_	_	
92-35	14000-14003	and	_	_	_	
92-36	14004-14006	in	_	_	_	
92-37	14007-14010	the	_	_	_	
92-38	14011-14024	application's	_	_	_	
92-39	14025-14028	own	_	_	_	
92-40	14029-14034	cache	_	_	_	
92-41	14034-14035	.	_	_	_	

#Text=Reducing the ARC to a minimum can improve performance of applications which maintain their own cache.
93-1	14036-14044	Reducing	_	_	_	
93-2	14045-14048	the	_	_	_	
93-3	14049-14052	ARC	_	_	_	
93-4	14053-14055	to	_	_	_	
93-5	14056-14057	a	_	_	_	
93-6	14058-14065	minimum	_	_	_	
93-7	14066-14069	can	_	_	_	
93-8	14070-14077	improve	_	_	_	
93-9	14078-14089	performance	_	_	_	
93-10	14090-14092	of	_	_	_	
93-11	14093-14105	applications	_	_	_	
93-12	14106-14111	which	_	_	_	
93-13	14112-14120	maintain	_	_	_	
93-14	14121-14126	their	_	_	_	
93-15	14127-14130	own	_	_	_	
93-16	14131-14136	cache	_	_	_	
93-17	14136-14137	.	_	_	_	

#Text=At ZFS Best Practices Guide there are some generic recommendations for ZFS on Solaris which mostly apply to FreeBSD too.
94-1	14138-14140	At	_	_	_	
94-2	14141-14144	ZFS	_	_	_	
94-3	14145-14149	Best	_	_	_	
94-4	14150-14159	Practices	_	_	_	
94-5	14160-14165	Guide	_	_	_	
94-6	14166-14171	there	_	_	_	
94-7	14172-14175	are	_	_	_	
94-8	14176-14180	some	_	_	_	
94-9	14181-14188	generic	_	_	_	
94-10	14189-14204	recommendations	_	_	_	
94-11	14205-14208	for	_	_	_	
94-12	14209-14212	ZFS	_	_	_	
94-13	14213-14215	on	_	_	_	
94-14	14216-14223	Solaris	_	_	_	
94-15	14224-14229	which	_	_	_	
94-16	14230-14236	mostly	_	_	_	
94-17	14237-14242	apply	_	_	_	
94-18	14243-14245	to	_	_	_	
94-19	14246-14253	FreeBSD	_	_	_	
94-20	14254-14257	too	_	_	_	
94-21	14257-14258	.	_	_	_	

#Text=General Tuning
#Text=There are some changes that can be made to improve performance in certain situations and avoid the bursty IO that's often seen with ZFS.
95-1	14259-14266	General	_	_	_	
95-2	14267-14273	Tuning	_	_	_	
95-3	14274-14279	There	_	_	_	
95-4	14280-14283	are	_	_	_	
95-5	14284-14288	some	_	_	_	
95-6	14289-14296	changes	_	_	_	
95-7	14297-14301	that	_	_	_	
95-8	14302-14305	can	_	_	_	
95-9	14306-14308	be	_	_	_	
95-10	14309-14313	made	_	_	_	
95-11	14314-14316	to	_	_	_	
95-12	14317-14324	improve	_	_	_	
95-13	14325-14336	performance	_	_	_	
95-14	14337-14339	in	_	_	_	
95-15	14340-14347	certain	_	_	_	
95-16	14348-14358	situations	_	_	_	
95-17	14359-14362	and	_	_	_	
95-18	14363-14368	avoid	_	_	_	
95-19	14369-14372	the	_	_	_	
95-20	14373-14379	bursty	_	_	_	
95-21	14380-14382	IO	_	_	_	
95-22	14383-14389	that's	_	_	_	
95-23	14390-14395	often	_	_	_	
95-24	14396-14400	seen	_	_	_	
95-25	14401-14405	with	_	_	_	
95-26	14406-14409	ZFS	_	_	_	
95-27	14409-14410	.	_	_	_	

#Text=Loader tunables (in /boot/loader.conf): # Disable ZFS prefetching
#Text=# http://southbrain.com/south/2008/04/the-nightmare-comes-slowly-zfs.html
#Text=# Increases overall speed of ZFS, but when disk flushing/writes occur,
#Text=# system is less responsive (due to extreme disk I/O).
#Text=# NOTE: Systems with 4 GB of RAM or more have prefetch enabled by default.
#Text=vfs.zfs.prefetch_disable="1"
#Text=# Decrease ZFS txg timeout value from 30 (default) to 5 seconds.
96-1	14411-14417	Loader	_	_	_	
96-2	14418-14426	tunables	_	_	_	
96-3	14427-14428	(	_	_	_	
96-4	14428-14430	in	_	_	_	
96-5	14431-14432	/	_	_	_	
96-6	14432-14436	boot	_	_	_	
96-7	14436-14437	/	_	_	_	
96-8	14437-14448	loader.conf	_	_	_	
96-9	14448-14449	)	_	_	_	
96-10	14449-14450	:	_	_	_	
96-11	14451-14452	#	_	_	_	
96-12	14453-14460	Disable	_	_	_	
96-13	14461-14464	ZFS	_	_	_	
96-14	14465-14476	prefetching	_	_	_	
96-15	14477-14478	#	_	_	_	
96-16	14479-14483	http	_	_	_	
96-17	14483-14484	:	_	_	_	
96-18	14484-14485	/	_	_	_	
96-19	14485-14486	/	_	_	_	
96-20	14486-14500	southbrain.com	_	_	_	
96-21	14500-14501	/	_	_	_	
96-22	14501-14506	south	_	_	_	
96-23	14506-14507	/	_	_	_	
96-24	14507-14511	2008	_	_	_	
96-25	14511-14512	/	_	_	_	
96-26	14512-14514	04	_	_	_	
96-27	14514-14515	/	_	_	_	
96-28	14515-14550	the-nightmare-comes-slowly-zfs.html	_	_	_	
96-29	14551-14552	#	_	_	_	
96-30	14553-14562	Increases	_	_	_	
96-31	14563-14570	overall	_	_	_	
96-32	14571-14576	speed	_	_	_	
96-33	14577-14579	of	_	_	_	
96-34	14580-14583	ZFS	_	_	_	
96-35	14583-14584	,	_	_	_	
96-36	14585-14588	but	_	_	_	
96-37	14589-14593	when	_	_	_	
96-38	14594-14598	disk	_	_	_	
96-39	14599-14607	flushing	_	_	_	
96-40	14607-14608	/	_	_	_	
96-41	14608-14614	writes	_	_	_	
96-42	14615-14620	occur	_	_	_	
96-43	14620-14621	,	_	_	_	
96-44	14622-14623	#	_	_	_	
96-45	14624-14630	system	_	_	_	
96-46	14631-14633	is	_	_	_	
96-47	14634-14638	less	_	_	_	
96-48	14639-14649	responsive	_	_	_	
96-49	14650-14651	(	_	_	_	
96-50	14651-14654	due	_	_	_	
96-51	14655-14657	to	_	_	_	
96-52	14658-14665	extreme	_	_	_	
96-53	14666-14670	disk	_	_	_	
96-54	14671-14672	I	_	_	_	
96-55	14672-14673	/	_	_	_	
96-56	14673-14674	O	_	_	_	
96-57	14674-14675	)	_	_	_	
96-58	14675-14676	.	_	_	_	
96-59	14677-14678	#	_	_	_	
96-60	14679-14683	NOTE	_	_	_	
96-61	14683-14684	:	_	_	_	
96-62	14685-14692	Systems	_	_	_	
96-63	14693-14697	with	_	_	_	
96-64	14698-14699	4	_	_	_	
96-65	14700-14702	GB	_	_	_	
96-66	14703-14705	of	_	_	_	
96-67	14706-14709	RAM	_	_	_	
96-68	14710-14712	or	_	_	_	
96-69	14713-14717	more	_	_	_	
96-70	14718-14722	have	_	_	_	
96-71	14723-14731	prefetch	_	_	_	
96-72	14732-14739	enabled	_	_	_	
96-73	14740-14742	by	_	_	_	
96-74	14743-14750	default	_	_	_	
96-75	14750-14751	.	_	_	_	
96-76	14752-14776	vfs.zfs.prefetch_disable	_	_	_	
96-77	14776-14777	=	_	_	_	
96-78	14777-14778	"	_	_	_	
96-79	14778-14779	1	_	_	_	
96-80	14779-14780	"	_	_	_	
96-81	14781-14782	#	_	_	_	
96-82	14783-14791	Decrease	_	_	_	
96-83	14792-14795	ZFS	_	_	_	
96-84	14796-14799	txg	_	_	_	
96-85	14800-14807	timeout	_	_	_	
96-86	14808-14813	value	_	_	_	
96-87	14814-14818	from	_	_	_	
96-88	14819-14821	30	_	_	_	
96-89	14822-14823	(	_	_	_	
96-90	14823-14830	default	_	_	_	
96-91	14830-14831	)	_	_	_	
96-92	14832-14834	to	_	_	_	
96-93	14835-14836	5	_	_	_	
96-94	14837-14844	seconds	_	_	_	
96-95	14844-14845	.	_	_	_	

#Text=This
#Text=# should increase throughput and decrease the "bursty" stalls that
#Text=# happen during immense I/O with ZFS.
#Text=# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007343.html
#Text=# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007355.html
#Text=# default in FreeBSD since ZFS v28
#Text=vfs.zfs.txg.timeout="5"
#Text=Sysctl variables (/etc/sysctl.conf):
#Text=# Increase number of vnodes; we've seen vfs.numvnodes reach 115,000
#Text=# at times.
97-1	14846-14850	This	_	_	_	
97-2	14851-14852	#	_	_	_	
97-3	14853-14859	should	_	_	_	
97-4	14860-14868	increase	_	_	_	
97-5	14869-14879	throughput	_	_	_	
97-6	14880-14883	and	_	_	_	
97-7	14884-14892	decrease	_	_	_	
97-8	14893-14896	the	_	_	_	
97-9	14897-14898	"	_	_	_	
97-10	14898-14904	bursty	_	_	_	
97-11	14904-14905	"	_	_	_	
97-12	14906-14912	stalls	_	_	_	
97-13	14913-14917	that	_	_	_	
97-14	14918-14919	#	_	_	_	
97-15	14920-14926	happen	_	_	_	
97-16	14927-14933	during	_	_	_	
97-17	14934-14941	immense	_	_	_	
97-18	14942-14943	I	_	_	_	
97-19	14943-14944	/	_	_	_	
97-20	14944-14945	O	_	_	_	
97-21	14946-14950	with	_	_	_	
97-22	14951-14954	ZFS	_	_	_	
97-23	14954-14955	.	_	_	_	
97-24	14956-14957	#	_	_	_	
97-25	14958-14962	http	_	_	_	
97-26	14962-14963	:	_	_	_	
97-27	14963-14964	/	_	_	_	
97-28	14964-14965	/	_	_	_	
97-29	14965-14982	lists.freebsd.org	_	_	_	
97-30	14982-14983	/	_	_	_	
97-31	14983-14992	pipermail	_	_	_	
97-32	14992-14993	/	_	_	_	
97-33	14993-15003	freebsd-fs	_	_	_	
97-34	15003-15004	/	_	_	_	
97-35	15004-15008	2009	_	_	_	
97-36	15008-15009	-	_	_	_	
97-37	15009-15017	December	_	_	_	
97-38	15017-15018	/	_	_	_	
97-39	15018-15024	007343	_	_	_	
97-40	15024-15025	.	_	_	_	
97-41	15025-15029	html	_	_	_	
97-42	15030-15031	#	_	_	_	
97-43	15032-15036	http	_	_	_	
97-44	15036-15037	:	_	_	_	
97-45	15037-15038	/	_	_	_	
97-46	15038-15039	/	_	_	_	
97-47	15039-15056	lists.freebsd.org	_	_	_	
97-48	15056-15057	/	_	_	_	
97-49	15057-15066	pipermail	_	_	_	
97-50	15066-15067	/	_	_	_	
97-51	15067-15077	freebsd-fs	_	_	_	
97-52	15077-15078	/	_	_	_	
97-53	15078-15082	2009	_	_	_	
97-54	15082-15083	-	_	_	_	
97-55	15083-15091	December	_	_	_	
97-56	15091-15092	/	_	_	_	
97-57	15092-15098	007355	_	_	_	
97-58	15098-15099	.	_	_	_	
97-59	15099-15103	html	_	_	_	
97-60	15104-15105	#	_	_	_	
97-61	15106-15113	default	_	_	_	
97-62	15114-15116	in	_	_	_	
97-63	15117-15124	FreeBSD	_	_	_	
97-64	15125-15130	since	_	_	_	
97-65	15131-15134	ZFS	_	_	_	
97-66	15135-15138	v28	_	_	_	
97-67	15139-15158	vfs.zfs.txg.timeout	_	_	_	
97-68	15158-15159	=	_	_	_	
97-69	15159-15160	"	_	_	_	
97-70	15160-15161	5	_	_	_	
97-71	15161-15162	"	_	_	_	
97-72	15163-15169	Sysctl	_	_	_	
97-73	15170-15179	variables	_	_	_	
97-74	15180-15181	(	_	_	_	
97-75	15181-15182	/	_	_	_	
97-76	15182-15185	etc	_	_	_	
97-77	15185-15186	/	_	_	_	
97-78	15186-15197	sysctl.conf	_	_	_	
97-79	15197-15198	)	_	_	_	
97-80	15198-15199	:	_	_	_	
97-81	15200-15201	#	_	_	_	
97-82	15202-15210	Increase	_	_	_	
97-83	15211-15217	number	_	_	_	
97-84	15218-15220	of	_	_	_	
97-85	15221-15227	vnodes	_	_	_	
97-86	15227-15228	;	_	_	_	
97-87	15229-15234	we've	_	_	_	
97-88	15235-15239	seen	_	_	_	
97-89	15240-15253	vfs.numvnodes	_	_	_	
97-90	15254-15259	reach	_	_	_	
97-91	15260-15267	115,000	_	_	_	
97-92	15268-15269	#	_	_	_	
97-93	15270-15272	at	_	_	_	
97-94	15273-15278	times	_	_	_	
97-95	15278-15279	.	_	_	_	

#Text=Default max is a little over 200,000.
98-1	15280-15287	Default	_	_	_	
98-2	15288-15291	max	_	_	_	
98-3	15292-15294	is	_	_	_	
98-4	15295-15296	a	_	_	_	
98-5	15297-15303	little	_	_	_	
98-6	15304-15308	over	_	_	_	
98-7	15309-15316	200,000	_	_	_	
98-8	15316-15317	.	_	_	_	

#Text=Playing it safe...
#Text=# If numvnodes reaches maxvnode performance substantially decreases.
#Text=kern.maxvnodes=250000
#Text=# Set TXG write limit to a lower threshold.
99-1	15318-15325	Playing	_	_	_	
99-2	15326-15328	it	_	_	_	
99-3	15329-15333	safe	_	_	_	
99-4	15333-15334	.	_	_	_	
99-5	15334-15335	.	_	_	_	
99-6	15335-15336	.	_	_	_	
99-7	15337-15338	#	_	_	_	
99-8	15339-15341	If	_	_	_	
99-9	15342-15351	numvnodes	_	_	_	
99-10	15352-15359	reaches	_	_	_	
99-11	15360-15368	maxvnode	_	_	_	
99-12	15369-15380	performance	_	_	_	
99-13	15381-15394	substantially	_	_	_	
99-14	15395-15404	decreases	_	_	_	
99-15	15404-15405	.	_	_	_	
99-16	15406-15420	kern.maxvnodes	_	_	_	
99-17	15420-15421	=	_	_	_	
99-18	15421-15427	250000	_	_	_	
99-19	15428-15429	#	_	_	_	
99-20	15430-15433	Set	_	_	_	
99-21	15434-15437	TXG	_	_	_	
99-22	15438-15443	write	_	_	_	
99-23	15444-15449	limit	_	_	_	
99-24	15450-15452	to	_	_	_	
99-25	15453-15454	a	_	_	_	
99-26	15455-15460	lower	_	_	_	
99-27	15461-15470	threshold	_	_	_	
99-28	15470-15471	.	_	_	_	

#Text=This helps "level out"
#Text=# the throughput rate (see "zpool iostat").
100-1	15472-15476	This	_	_	_	
100-2	15477-15482	helps	_	_	_	
100-3	15483-15484	"	_	_	_	
100-4	15484-15489	level	_	_	_	
100-5	15490-15493	out	_	_	_	
100-6	15493-15494	"	_	_	_	
100-7	15495-15496	#	_	_	_	
100-8	15497-15500	the	_	_	_	
100-9	15501-15511	throughput	_	_	_	
100-10	15512-15516	rate	_	_	_	
100-11	15517-15518	(	_	_	_	
100-12	15518-15521	see	_	_	_	
100-13	15522-15523	"	_	_	_	
100-14	15523-15528	zpool	_	_	_	
100-15	15529-15535	iostat	_	_	_	
100-16	15535-15536	"	_	_	_	
100-17	15536-15537	)	_	_	_	
100-18	15537-15538	.	_	_	_	

#Text=A value of 256MB works well
#Text=# for systems with 4 GB of RAM, while 1 GB works well for us w/ 8 GB on
#Text=# disks which have 64 MB cache.
101-1	15539-15540	A	_	_	_	
101-2	15541-15546	value	_	_	_	
101-3	15547-15549	of	_	_	_	
101-4	15550-15555	256MB	_	_	_	
101-5	15556-15561	works	_	_	_	
101-6	15562-15566	well	_	_	_	
101-7	15567-15568	#	_	_	_	
101-8	15569-15572	for	_	_	_	
101-9	15573-15580	systems	_	_	_	
101-10	15581-15585	with	_	_	_	
101-11	15586-15587	4	_	_	_	
101-12	15588-15590	GB	_	_	_	
101-13	15591-15593	of	_	_	_	
101-14	15594-15597	RAM	_	_	_	
101-15	15597-15598	,	_	_	_	
101-16	15599-15604	while	_	_	_	
101-17	15605-15606	1	_	_	_	
101-18	15607-15609	GB	_	_	_	
101-19	15610-15615	works	_	_	_	
101-20	15616-15620	well	_	_	_	
101-21	15621-15624	for	_	_	_	
101-22	15625-15627	us	_	_	_	
101-23	15628-15629	w	_	_	_	
101-24	15629-15630	/	_	_	_	
101-25	15631-15632	8	_	_	_	
101-26	15633-15635	GB	_	_	_	
101-27	15636-15638	on	_	_	_	
101-28	15639-15640	#	_	_	_	
101-29	15641-15646	disks	_	_	_	
101-30	15647-15652	which	_	_	_	
101-31	15653-15657	have	_	_	_	
101-32	15658-15660	64	_	_	_	
101-33	15661-15663	MB	_	_	_	
101-34	15664-15669	cache	_	_	_	
101-35	15669-15670	.	_	_	_	

#Text=<<BR>>
#Text=# NOTE: in <v28, this tunable is called 'vfs.zfs.txg.write_limit_override'.
#Text=vfs.zfs.write_limit_override=1073741824
#Text=Be aware that the vfs.zfs.write_limit_override tuning you see above
#Text=may need to be adjusted for your system.
102-1	15671-15672	<	_	_	_	
102-2	15672-15673	<	_	_	_	
102-3	15673-15675	BR	_	_	_	
102-4	15675-15676	>	_	_	_	
102-5	15676-15677	>	_	_	_	
102-6	15678-15679	#	_	_	_	
102-7	15680-15684	NOTE	_	_	_	
102-8	15684-15685	:	_	_	_	
102-9	15686-15688	in	_	_	_	
102-10	15689-15690	<	_	_	_	
102-11	15690-15693	v28	_	_	_	
102-12	15693-15694	,	_	_	_	
102-13	15695-15699	this	_	_	_	
102-14	15700-15707	tunable	_	_	_	
102-15	15708-15710	is	_	_	_	
102-16	15711-15717	called	_	_	_	
102-17	15718-15719	'	_	_	_	
102-18	15719-15751	vfs.zfs.txg.write_limit_override	_	_	_	
102-19	15751-15752	'	_	_	_	
102-20	15752-15753	.	_	_	_	
102-21	15754-15782	vfs.zfs.write_limit_override	_	_	_	
102-22	15782-15783	=	_	_	_	
102-23	15783-15793	1073741824	_	_	_	
102-24	15794-15796	Be	_	_	_	
102-25	15797-15802	aware	_	_	_	
102-26	15803-15807	that	_	_	_	
102-27	15808-15811	the	_	_	_	
102-28	15812-15840	vfs.zfs.write_limit_override	_	_	_	
102-29	15841-15847	tuning	_	_	_	
102-30	15848-15851	you	_	_	_	
102-31	15852-15855	see	_	_	_	
102-32	15856-15861	above	_	_	_	
102-33	15862-15865	may	_	_	_	
102-34	15866-15870	need	_	_	_	
102-35	15871-15873	to	_	_	_	
102-36	15874-15876	be	_	_	_	
102-37	15877-15885	adjusted	_	_	_	
102-38	15886-15889	for	_	_	_	
102-39	15890-15894	your	_	_	_	
102-40	15895-15901	system	_	_	_	
102-41	15901-15902	.	_	_	_	

#Text=It's up to you to figure out
#Text=what works best in your environment.
103-1	15903-15907	It's	_	_	_	
103-2	15908-15910	up	_	_	_	
103-3	15911-15913	to	_	_	_	
103-4	15914-15917	you	_	_	_	
103-5	15918-15920	to	_	_	_	
103-6	15921-15927	figure	_	_	_	
103-7	15928-15931	out	_	_	_	
103-8	15932-15936	what	_	_	_	
103-9	15937-15942	works	_	_	_	
103-10	15943-15947	best	_	_	_	
103-11	15948-15950	in	_	_	_	
103-12	15951-15955	your	_	_	_	
103-13	15956-15967	environment	_	_	_	
103-14	15967-15968	.	_	_	_	

#Text=Deduplication
#Text=Deduplication is a misunderstood feature in ZFS v21+; some users see it as a silver bullet for increasing capacity by reducing redundancies in data.
104-1	15969-15982	Deduplication	_	_	_	
104-2	15983-15996	Deduplication	_	_	_	
104-3	15997-15999	is	_	_	_	
104-4	16000-16001	a	_	_	_	
104-5	16002-16015	misunderstood	_	_	_	
104-6	16016-16023	feature	_	_	_	
104-7	16024-16026	in	_	_	_	
104-8	16027-16030	ZFS	_	_	_	
104-9	16031-16034	v21	_	_	_	
104-10	16034-16035	+	_	_	_	
104-11	16035-16036	;	_	_	_	
104-12	16037-16041	some	_	_	_	
104-13	16042-16047	users	_	_	_	
104-14	16048-16051	see	_	_	_	
104-15	16052-16054	it	_	_	_	
104-16	16055-16057	as	_	_	_	
104-17	16058-16059	a	_	_	_	
104-18	16060-16066	silver	_	_	_	
104-19	16067-16073	bullet	_	_	_	
104-20	16074-16077	for	_	_	_	
104-21	16078-16088	increasing	_	_	_	
104-22	16089-16097	capacity	_	_	_	
104-23	16098-16100	by	_	_	_	
104-24	16101-16109	reducing	_	_	_	
104-25	16110-16122	redundancies	_	_	_	
104-26	16123-16125	in	_	_	_	
104-27	16126-16130	data	_	_	_	
104-28	16130-16131	.	_	_	_	

#Text=Here are the author's (gcooper's) observations: There are some resources that suggest that one needs 2GB per TB of storage with deduplication [i] (in fact this is a misinterpretation of the text).
105-1	16132-16136	Here	_	_	_	
105-2	16137-16140	are	_	_	_	
105-3	16141-16144	the	_	_	_	
105-4	16145-16153	author's	_	_	_	
105-5	16154-16155	(	_	_	_	
105-6	16155-16164	gcooper's	_	_	_	
105-7	16164-16165	)	_	_	_	
105-8	16166-16178	observations	_	_	_	
105-9	16178-16179	:	_	_	_	
105-10	16180-16185	There	_	_	_	
105-11	16186-16189	are	_	_	_	
105-12	16190-16194	some	_	_	_	
105-13	16195-16204	resources	_	_	_	
105-14	16205-16209	that	_	_	_	
105-15	16210-16217	suggest	_	_	_	
105-16	16218-16222	that	_	_	_	
105-17	16223-16226	one	_	_	_	
105-18	16227-16232	needs	_	_	_	
105-19	16233-16236	2GB	_	_	_	
105-20	16237-16240	per	_	_	_	
105-21	16241-16243	TB	_	_	_	
105-22	16244-16246	of	_	_	_	
105-23	16247-16254	storage	_	_	_	
105-24	16255-16259	with	_	_	_	
105-25	16260-16273	deduplication	_	_	_	
105-26	16274-16275	[	_	_	_	
105-27	16275-16276	i	_	_	_	
105-28	16276-16277	]	_	_	_	
105-29	16278-16279	(	_	_	_	
105-30	16279-16281	in	_	_	_	
105-31	16282-16286	fact	_	_	_	
105-32	16287-16291	this	_	_	_	
105-33	16292-16294	is	_	_	_	
105-34	16295-16296	a	_	_	_	
105-35	16297-16314	misinterpretation	_	_	_	
105-36	16315-16317	of	_	_	_	
105-37	16318-16321	the	_	_	_	
105-38	16322-16326	text	_	_	_	
105-39	16326-16327	)	_	_	_	
105-40	16327-16328	.	_	_	_	

#Text=In practice with FreeBSD, based on empirical testing and additional reading, it's closer to 5GB per TB.
106-1	16329-16331	In	_	_	_	
106-2	16332-16340	practice	_	_	_	
106-3	16341-16345	with	_	_	_	
106-4	16346-16353	FreeBSD	_	_	_	
106-5	16353-16354	,	_	_	_	
106-6	16355-16360	based	_	_	_	
106-7	16361-16363	on	_	_	_	
106-8	16364-16373	empirical	_	_	_	
106-9	16374-16381	testing	_	_	_	
106-10	16382-16385	and	_	_	_	
106-11	16386-16396	additional	_	_	_	
106-12	16397-16404	reading	_	_	_	
106-13	16404-16405	,	_	_	_	
106-14	16406-16410	it's	_	_	_	
106-15	16411-16417	closer	_	_	_	
106-16	16418-16420	to	_	_	_	
106-17	16421-16424	5GB	_	_	_	
106-18	16425-16428	per	_	_	_	
106-19	16429-16431	TB	_	_	_	
106-20	16431-16432	.	_	_	_	

#Text=Using deduplication is slower than not running it.
107-1	16433-16438	Using	_	_	_	
107-2	16439-16452	deduplication	_	_	_	
107-3	16453-16455	is	_	_	_	
107-4	16456-16462	slower	_	_	_	
107-5	16463-16467	than	_	_	_	
107-6	16468-16471	not	_	_	_	
107-7	16472-16479	running	_	_	_	
107-8	16480-16482	it	_	_	_	
107-9	16482-16483	.	_	_	_	

#Text=Deduplication [on 8.x/9.x at least] lies via stat(2) / statvfs(2); it reports the theoretical used space -- not the actual used space -- which can confuse scripts that look at df output, etc (TODO: find PR that mentions this).
108-1	16484-16497	Deduplication	_	_	_	
108-2	16498-16499	[	_	_	_	
108-3	16499-16501	on	_	_	_	
108-4	16502-16503	8	_	_	_	
108-5	16503-16504	.	_	_	_	
108-6	16504-16505	x	_	_	_	
108-7	16505-16506	/	_	_	_	
108-8	16506-16507	9	_	_	_	
108-9	16507-16508	.	_	_	_	
108-10	16508-16509	x	_	_	_	
108-11	16510-16512	at	_	_	_	
108-12	16513-16518	least	_	_	_	
108-13	16518-16519	]	_	_	_	
108-14	16520-16524	lies	_	_	_	
108-15	16525-16528	via	_	_	_	
108-16	16529-16533	stat	_	_	_	
108-17	16533-16534	(	_	_	_	
108-18	16534-16535	2	_	_	_	
108-19	16535-16536	)	_	_	_	
108-20	16537-16538	/	_	_	_	
108-21	16539-16546	statvfs	_	_	_	
108-22	16546-16547	(	_	_	_	
108-23	16547-16548	2	_	_	_	
108-24	16548-16549	)	_	_	_	
108-25	16549-16550	;	_	_	_	
108-26	16551-16553	it	_	_	_	
108-27	16554-16561	reports	_	_	_	
108-28	16562-16565	the	_	_	_	
108-29	16566-16577	theoretical	_	_	_	
108-30	16578-16582	used	_	_	_	
108-31	16583-16588	space	_	_	_	
108-32	16589-16590	-	_	_	_	
108-33	16590-16591	-	_	_	_	
108-34	16592-16595	not	_	_	_	
108-35	16596-16599	the	_	_	_	
108-36	16600-16606	actual	_	_	_	
108-37	16607-16611	used	_	_	_	
108-38	16612-16617	space	_	_	_	
108-39	16618-16619	-	_	_	_	
108-40	16619-16620	-	_	_	_	
108-41	16621-16626	which	_	_	_	
108-42	16627-16630	can	_	_	_	
108-43	16631-16638	confuse	_	_	_	
108-44	16639-16646	scripts	_	_	_	
108-45	16647-16651	that	_	_	_	
108-46	16652-16656	look	_	_	_	
108-47	16657-16659	at	_	_	_	
108-48	16660-16662	df	_	_	_	
108-49	16663-16669	output	_	_	_	
108-50	16669-16670	,	_	_	_	
108-51	16671-16674	etc	_	_	_	
108-52	16675-16676	(	_	_	_	
108-53	16676-16680	TODO	_	_	_	
108-54	16680-16681	:	_	_	_	
108-55	16682-16686	find	_	_	_	
108-56	16687-16689	PR	_	_	_	
108-57	16690-16694	that	_	_	_	
108-58	16695-16703	mentions	_	_	_	
108-59	16704-16708	this	_	_	_	
108-60	16708-16709	)	_	_	_	
108-61	16709-16710	.	_	_	_	

#Text=Suggestions
#Text=If you are going to use deduplication and your machine is underspec'ed, you must set vfs.zfs.arc_max to a sane value or ZFS will wire down as much available memory as possible, which can create memory starvation scenarios.
109-1	16711-16722	Suggestions	_	_	_	
109-2	16723-16725	If	_	_	_	
109-3	16726-16729	you	_	_	_	
109-4	16730-16733	are	_	_	_	
109-5	16734-16739	going	_	_	_	
109-6	16740-16742	to	_	_	_	
109-7	16743-16746	use	_	_	_	
109-8	16747-16760	deduplication	_	_	_	
109-9	16761-16764	and	_	_	_	
109-10	16765-16769	your	_	_	_	
109-11	16770-16777	machine	_	_	_	
109-12	16778-16780	is	_	_	_	
109-13	16781-16793	underspec'ed	_	_	_	
109-14	16793-16794	,	_	_	_	
109-15	16795-16798	you	_	_	_	
109-16	16799-16803	must	_	_	_	
109-17	16804-16807	set	_	_	_	
109-18	16808-16823	vfs.zfs.arc_max	_	_	_	
109-19	16824-16826	to	_	_	_	
109-20	16827-16828	a	_	_	_	
109-21	16829-16833	sane	_	_	_	
109-22	16834-16839	value	_	_	_	
109-23	16840-16842	or	_	_	_	
109-24	16843-16846	ZFS	_	_	_	
109-25	16847-16851	will	_	_	_	
109-26	16852-16856	wire	_	_	_	
109-27	16857-16861	down	_	_	_	
109-28	16862-16864	as	_	_	_	
109-29	16865-16869	much	_	_	_	
109-30	16870-16879	available	_	_	_	
109-31	16880-16886	memory	_	_	_	
109-32	16887-16889	as	_	_	_	
109-33	16890-16898	possible	_	_	_	
109-34	16898-16899	,	_	_	_	
109-35	16900-16905	which	_	_	_	
109-36	16906-16909	can	_	_	_	
109-37	16910-16916	create	_	_	_	
109-38	16917-16923	memory	_	_	_	
109-39	16924-16934	starvation	_	_	_	
109-40	16935-16944	scenarios	_	_	_	
109-41	16944-16945	.	_	_	_	

#Text=It's a much better idea in general to use compression -- instead of deduplication -- if you're trying to save space, and you know that you can benefit from compression.
110-1	16946-16950	It's	_	_	_	
110-2	16951-16952	a	_	_	_	
110-3	16953-16957	much	_	_	_	
110-4	16958-16964	better	_	_	_	
110-5	16965-16969	idea	_	_	_	
110-6	16970-16972	in	_	_	_	
110-7	16973-16980	general	_	_	_	
110-8	16981-16983	to	_	_	_	
110-9	16984-16987	use	_	_	_	
110-10	16988-16999	compression	_	_	_	
110-11	17000-17001	-	_	_	_	
110-12	17001-17002	-	_	_	_	
110-13	17003-17010	instead	_	_	_	
110-14	17011-17013	of	_	_	_	
110-15	17014-17027	deduplication	_	_	_	
110-16	17028-17029	-	_	_	_	
110-17	17029-17030	-	_	_	_	
110-18	17031-17033	if	_	_	_	
110-19	17034-17040	you're	_	_	_	
110-20	17041-17047	trying	_	_	_	
110-21	17048-17050	to	_	_	_	
110-22	17051-17055	save	_	_	_	
110-23	17056-17061	space	_	_	_	
110-24	17061-17062	,	_	_	_	
110-25	17063-17066	and	_	_	_	
110-26	17067-17070	you	_	_	_	
110-27	17071-17075	know	_	_	_	
110-28	17076-17080	that	_	_	_	
110-29	17081-17084	you	_	_	_	
110-30	17085-17088	can	_	_	_	
110-31	17089-17096	benefit	_	_	_	
110-32	17097-17101	from	_	_	_	
110-33	17102-17113	compression	_	_	_	
110-34	17113-17114	.	_	_	_	

#Text=When in doubt, check how much you would actually gain from deduplication via zdb -S <zpool> instead of just turning it on.
111-1	17115-17119	When	_	_	_	
111-2	17120-17122	in	_	_	_	
111-3	17123-17128	doubt	_	_	_	
111-4	17128-17129	,	_	_	_	
111-5	17130-17135	check	_	_	_	
111-6	17136-17139	how	_	_	_	
111-7	17140-17144	much	_	_	_	
111-8	17145-17148	you	_	_	_	
111-9	17149-17154	would	_	_	_	
111-10	17155-17163	actually	_	_	_	
111-11	17164-17168	gain	_	_	_	
111-12	17169-17173	from	_	_	_	
111-13	17174-17187	deduplication	_	_	_	
111-14	17188-17191	via	_	_	_	
111-15	17192-17195	zdb	_	_	_	
111-16	17196-17197	-	_	_	_	
111-17	17197-17198	S	_	_	_	
111-18	17199-17200	<	_	_	_	
111-19	17200-17205	zpool	_	_	_	
111-20	17205-17206	>	_	_	_	
111-21	17207-17214	instead	_	_	_	
111-22	17215-17217	of	_	_	_	
111-23	17218-17222	just	_	_	_	
111-24	17223-17230	turning	_	_	_	
111-25	17231-17233	it	_	_	_	
111-26	17234-17236	on	_	_	_	
111-27	17236-17237	.	_	_	_	

#Text=Please note that this will take a while to run, depending on the dataset/zpool selected.
112-1	17238-17244	Please	_	_	_	
112-2	17245-17249	note	_	_	_	
112-3	17250-17254	that	_	_	_	
112-4	17255-17259	this	_	_	_	
112-5	17260-17264	will	_	_	_	
112-6	17265-17269	take	_	_	_	
112-7	17270-17271	a	_	_	_	
112-8	17272-17277	while	_	_	_	
112-9	17278-17280	to	_	_	_	
112-10	17281-17284	run	_	_	_	
112-11	17284-17285	,	_	_	_	
112-12	17286-17295	depending	_	_	_	
112-13	17296-17298	on	_	_	_	
112-14	17299-17302	the	_	_	_	
112-15	17303-17310	dataset	_	_	_	
112-16	17310-17311	/	_	_	_	
112-17	17311-17316	zpool	_	_	_	
112-18	17317-17325	selected	_	_	_	
112-19	17325-17326	.	_	_	_	

#Text=References
#Text=http://blogs.oracle.com/roch/entry/dedup_performance_considerations1
#Text=NFS tuning
#Text=The combination of ZFS and NFS stresses the ZIL to the point that performance falls significantly below expected levels.
113-1	17327-17337	References	_	_	_	
113-2	17338-17342	http	_	_	_	
113-3	17342-17343	:	_	_	_	
113-4	17343-17344	/	_	_	_	
113-5	17344-17345	/	_	_	_	
113-6	17345-17361	blogs.oracle.com	_	_	_	
113-7	17361-17362	/	_	_	_	
113-8	17362-17366	roch	_	_	_	
113-9	17366-17367	/	_	_	_	
113-10	17367-17372	entry	_	_	_	
113-11	17372-17373	/	_	_	_	
113-12	17373-17406	dedup_performance_considerations1	_	_	_	
113-13	17407-17410	NFS	_	_	_	
113-14	17411-17417	tuning	_	_	_	
113-15	17418-17421	The	_	_	_	
113-16	17422-17433	combination	_	_	_	
113-17	17434-17436	of	_	_	_	
113-18	17437-17440	ZFS	_	_	_	
113-19	17441-17444	and	_	_	_	
113-20	17445-17448	NFS	_	_	_	
113-21	17449-17457	stresses	_	_	_	
113-22	17458-17461	the	_	_	_	
113-23	17462-17465	ZIL	_	_	_	
113-24	17466-17468	to	_	_	_	
113-25	17469-17472	the	_	_	_	
113-26	17473-17478	point	_	_	_	
113-27	17479-17483	that	_	_	_	
113-28	17484-17495	performance	_	_	_	
113-29	17496-17501	falls	_	_	_	
113-30	17502-17515	significantly	_	_	_	
113-31	17516-17521	below	_	_	_	
113-32	17522-17530	expected	_	_	_	
113-33	17531-17537	levels	_	_	_	
113-34	17537-17538	.	_	_	_	

#Text=The best solution is to put the ZIL on a fast SSD (or a pair of SSDs in a mirror, for added redundancy).
114-1	17539-17542	The	_	_	_	
114-2	17543-17547	best	_	_	_	
114-3	17548-17556	solution	_	_	_	
114-4	17557-17559	is	_	_	_	
114-5	17560-17562	to	_	_	_	
114-6	17563-17566	put	_	_	_	
114-7	17567-17570	the	_	_	_	
114-8	17571-17574	ZIL	_	_	_	
114-9	17575-17577	on	_	_	_	
114-10	17578-17579	a	_	_	_	
114-11	17580-17584	fast	_	_	_	
114-12	17585-17588	SSD	_	_	_	
114-13	17589-17590	(	_	_	_	
114-14	17590-17592	or	_	_	_	
114-15	17593-17594	a	_	_	_	
114-16	17595-17599	pair	_	_	_	
114-17	17600-17602	of	_	_	_	
114-18	17603-17607	SSDs	_	_	_	
114-19	17608-17610	in	_	_	_	
114-20	17611-17612	a	_	_	_	
114-21	17613-17619	mirror	_	_	_	
114-22	17619-17620	,	_	_	_	
114-23	17621-17624	for	_	_	_	
114-24	17625-17630	added	_	_	_	
114-25	17631-17641	redundancy	_	_	_	
114-26	17641-17642	)	_	_	_	
114-27	17642-17643	.	_	_	_	

#Text=You can now enable/disable ZIL on a per-dataset basis (as of ZFS version 28 / FreeBSD 8.3+).  
115-1	17644-17647	You	_	_	_	
115-2	17648-17651	can	_	_	_	
115-3	17652-17655	now	_	_	_	
115-4	17656-17662	enable	_	_	_	
115-5	17662-17663	/	_	_	_	
115-6	17663-17670	disable	_	_	_	
115-7	17671-17674	ZIL	_	_	_	
115-8	17675-17677	on	_	_	_	
115-9	17678-17679	a	_	_	_	
115-10	17680-17691	per-dataset	_	_	_	
115-11	17692-17697	basis	_	_	_	
115-12	17698-17699	(	_	_	_	
115-13	17699-17701	as	_	_	_	
115-14	17702-17704	of	_	_	_	
115-15	17705-17708	ZFS	_	_	_	
115-16	17709-17716	version	_	_	_	
115-17	17717-17719	28	_	_	_	
115-18	17720-17721	/	_	_	_	
115-19	17722-17729	FreeBSD	_	_	_	
115-20	17730-17733	8.3	_	_	_	
115-21	17733-17734	+	_	_	_	
115-22	17734-17735	)	_	_	_	
115-23	17735-17736	.	_	_	_	
115-24	17737-17738	 	_	_	_	

#Text=zfs set sync=disabled tank/dataset  The next best solution is to disable ZIL with the following setting in loader.conf (up to ZFS version 15): vfs.zfs.zil_disable="1" the vfs.zfs.zil_disable loader tunable was replaced with the "sync" dataset property.
116-1	17738-17741	zfs	_	_	_	
116-2	17741-17742	 	_	_	_	
116-3	17742-17745	set	_	_	_	
116-4	17745-17746	 	_	_	_	
116-5	17746-17750	sync	_	_	_	
116-6	17750-17751	=	_	_	_	
116-7	17751-17759	disabled	_	_	_	
116-8	17759-17760	 	_	_	_	
116-9	17760-17764	tank	_	_	_	
116-10	17764-17765	/	_	_	_	
116-11	17765-17772	dataset	_	_	_	
116-12	17772-17773	 	_	_	_	
116-13	17774-17777	The	_	_	_	
116-14	17778-17782	next	_	_	_	
116-15	17783-17787	best	_	_	_	
116-16	17788-17796	solution	_	_	_	
116-17	17797-17799	is	_	_	_	
116-18	17800-17802	to	_	_	_	
116-19	17803-17810	disable	_	_	_	
116-20	17811-17814	ZIL	_	_	_	
116-21	17815-17819	with	_	_	_	
116-22	17820-17823	the	_	_	_	
116-23	17824-17833	following	_	_	_	
116-24	17834-17841	setting	_	_	_	
116-25	17842-17844	in	_	_	_	
116-26	17845-17856	loader.conf	_	_	_	
116-27	17857-17858	(	_	_	_	
116-28	17858-17860	up	_	_	_	
116-29	17861-17863	to	_	_	_	
116-30	17864-17867	ZFS	_	_	_	
116-31	17868-17875	version	_	_	_	
116-32	17876-17878	15	_	_	_	
116-33	17878-17879	)	_	_	_	
116-34	17879-17880	:	_	_	_	
116-35	17881-17900	vfs.zfs.zil_disable	_	_	_	
116-36	17900-17901	=	_	_	_	
116-37	17901-17902	"	_	_	_	
116-38	17902-17903	1	_	_	_	
116-39	17903-17904	"	_	_	_	
116-40	17905-17908	the	_	_	_	
116-41	17909-17928	vfs.zfs.zil_disable	_	_	_	
116-42	17929-17935	loader	_	_	_	
116-43	17936-17943	tunable	_	_	_	
116-44	17944-17947	was	_	_	_	
116-45	17948-17956	replaced	_	_	_	
116-46	17957-17961	with	_	_	_	
116-47	17962-17965	the	_	_	_	
116-48	17966-17967	"	_	_	_	
116-49	17967-17971	sync	_	_	_	
116-50	17971-17972	"	_	_	_	
116-51	17973-17980	dataset	_	_	_	
116-52	17981-17989	property	_	_	_	
116-53	17989-17990	.	_	_	_	

#Text=Disabling ZIL is not recommended where data consistency is required (such as database servers) but will not result in file system corruption.
117-1	17991-18000	Disabling	_	_	_	
117-2	18001-18004	ZIL	_	_	_	
117-3	18005-18007	is	_	_	_	
117-4	18008-18011	not	_	_	_	
117-5	18012-18023	recommended	_	_	_	
117-6	18024-18029	where	_	_	_	
117-7	18030-18034	data	_	_	_	
117-8	18035-18046	consistency	_	_	_	
117-9	18047-18049	is	_	_	_	
117-10	18050-18058	required	_	_	_	
117-11	18059-18060	(	_	_	_	
117-12	18060-18064	such	_	_	_	
117-13	18065-18067	as	_	_	_	
117-14	18068-18076	database	_	_	_	
117-15	18077-18084	servers	_	_	_	
117-16	18084-18085	)	_	_	_	
117-17	18086-18089	but	_	_	_	
117-18	18090-18094	will	_	_	_	
117-19	18095-18098	not	_	_	_	
117-20	18099-18105	result	_	_	_	
117-21	18106-18108	in	_	_	_	
117-22	18109-18113	file	_	_	_	
117-23	18114-18120	system	_	_	_	
117-24	18121-18131	corruption	_	_	_	
117-25	18131-18132	.	_	_	_	

#Text=See ZFS Evil Tuning Guide, section "Disabling the ZIL (Don't)".
118-1	18133-18136	See	_	_	_	
118-2	18137-18140	ZFS	_	_	_	
118-3	18141-18145	Evil	_	_	_	
118-4	18146-18152	Tuning	_	_	_	
118-5	18153-18158	Guide	_	_	_	
118-6	18158-18159	,	_	_	_	
118-7	18160-18167	section	_	_	_	
118-8	18168-18169	"	_	_	_	
118-9	18169-18178	Disabling	_	_	_	
118-10	18179-18182	the	_	_	_	
118-11	18183-18186	ZIL	_	_	_	
118-12	18187-18188	(	_	_	_	
118-13	18188-18193	Don't	_	_	_	
118-14	18193-18194	)	_	_	_	
118-15	18194-18195	"	_	_	_	
118-16	18195-18196	.	_	_	_	

#Text=ZFS is designed to be used with "raw" drives - i.e. not over already created hardware RAID volumes (this is sometimes called "JBOD" or "passthrough" mode when used with RAID controllers), but can benefit greatly from good and fast controllers.
119-1	18197-18200	ZFS	_	_	_	
119-2	18201-18203	is	_	_	_	
119-3	18204-18212	designed	_	_	_	
119-4	18213-18215	to	_	_	_	
119-5	18216-18218	be	_	_	_	
119-6	18219-18223	used	_	_	_	
119-7	18224-18228	with	_	_	_	
119-8	18229-18230	"	_	_	_	
119-9	18230-18233	raw	_	_	_	
119-10	18233-18234	"	_	_	_	
119-11	18235-18241	drives	_	_	_	
119-12	18242-18243	-	_	_	_	
119-13	18244-18247	i.e	_	_	_	
119-14	18247-18248	.	_	_	_	
119-15	18249-18252	not	_	_	_	
119-16	18253-18257	over	_	_	_	
119-17	18258-18265	already	_	_	_	
119-18	18266-18273	created	_	_	_	
119-19	18274-18282	hardware	_	_	_	
119-20	18283-18287	RAID	_	_	_	
119-21	18288-18295	volumes	_	_	_	
119-22	18296-18297	(	_	_	_	
119-23	18297-18301	this	_	_	_	
119-24	18302-18304	is	_	_	_	
119-25	18305-18314	sometimes	_	_	_	
119-26	18315-18321	called	_	_	_	
119-27	18322-18323	"	_	_	_	
119-28	18323-18327	JBOD	_	_	_	
119-29	18327-18328	"	_	_	_	
119-30	18329-18331	or	_	_	_	
119-31	18332-18333	"	_	_	_	
119-32	18333-18344	passthrough	_	_	_	
119-33	18344-18345	"	_	_	_	
119-34	18346-18350	mode	_	_	_	
119-35	18351-18355	when	_	_	_	
119-36	18356-18360	used	_	_	_	
119-37	18361-18365	with	_	_	_	
119-38	18366-18370	RAID	_	_	_	
119-39	18371-18382	controllers	_	_	_	
119-40	18382-18383	)	_	_	_	
119-41	18383-18384	,	_	_	_	
119-42	18385-18388	but	_	_	_	
119-43	18389-18392	can	_	_	_	
119-44	18393-18400	benefit	_	_	_	
119-45	18401-18408	greatly	_	_	_	
119-46	18409-18413	from	_	_	_	
119-47	18414-18418	good	_	_	_	
119-48	18419-18422	and	_	_	_	
119-49	18423-18427	fast	_	_	_	
119-50	18428-18439	controllers	_	_	_	
119-51	18439-18440	.	_	_	_	

#Text=MySQL
#Text=This assumes lots of RAM Tweaks for MySQL innodb_flush_log_at_trx_commit=2 skip-innodb_doublewrite Tweaks for ZFS zfs set primarycache=metadata tank/db zfs set atime=off tank/db zfs set recordsize=16k tank/db/innodb zfs set recordsize=128k tank/db/logs zfs set zfs:zfs_nocacheflush = 1 zfs set sync=disabled tank/db Note: MySQL 5.6.6 and newer (and related MariaDB / Percona forks)
#Text=has innodb_file_per_table = on as default, so IBD files are not created under tank/db/innodb (defined by innodb_data_home_dir in your my.cnf), they are created under tank/db/<db_name>/ and you should use recordsize=16k on this dataset too or switch back to innodb_file_per_table = off References MySQL Innodb ZFS Best Practices (Oracle)
#Text=Scrub and Resilver Performance
#Text=If you're getting horrible performance during a scrub or resilver, the following sysctls can be set: vfs.zfs.scrub_delay=0
#Text=vfs.zfs.top_maxinflight=128
#Text=vfs.zfs.resilver_min_time_ms=5000
#Text=vfs.zfs.resilver_delay=0Setting those sysctls to those values increased my (Shawn Webb's) resilver performance from 7MB/s to 230MB/s.
120-1	18441-18446	MySQL	_	_	_	
120-2	18447-18451	This	_	_	_	
120-3	18452-18459	assumes	_	_	_	
120-4	18460-18464	lots	_	_	_	
120-5	18465-18467	of	_	_	_	
120-6	18468-18471	RAM	_	_	_	
120-7	18472-18478	Tweaks	_	_	_	
120-8	18479-18482	for	_	_	_	
120-9	18483-18488	MySQL	_	_	_	
120-10	18489-18519	innodb_flush_log_at_trx_commit	parameter	_	_	
120-11	18519-18520	=	_	_	_	
120-12	18520-18521	2	value	Associated With	120-10	
120-13	18522-18545	skip-innodb_doublewrite	_	_	_	
120-14	18546-18552	Tweaks	_	_	_	
120-15	18553-18556	for	_	_	_	
120-16	18557-18560	ZFS	_	_	_	
120-17	18561-18564	zfs	_	_	_	
120-18	18565-18568	set	_	_	_	
120-19	18569-18581	primarycache	_	_	_	
120-20	18581-18582	=	_	_	_	
120-21	18582-18590	metadata	_	_	_	
120-22	18591-18595	tank	_	_	_	
120-23	18595-18596	/	_	_	_	
120-24	18596-18598	db	_	_	_	
120-25	18599-18602	zfs	_	_	_	
120-26	18603-18606	set	_	_	_	
120-27	18607-18612	atime	_	_	_	
120-28	18612-18613	=	_	_	_	
120-29	18613-18616	off	_	_	_	
120-30	18617-18621	tank	_	_	_	
120-31	18621-18622	/	_	_	_	
120-32	18622-18624	db	_	_	_	
120-33	18625-18628	zfs	_	_	_	
120-34	18629-18632	set	_	_	_	
120-35	18633-18643	recordsize	_	_	_	
120-36	18643-18644	=	_	_	_	
120-37	18644-18647	16k	_	_	_	
120-38	18648-18652	tank	_	_	_	
120-39	18652-18653	/	_	_	_	
120-40	18653-18655	db	_	_	_	
120-41	18655-18656	/	_	_	_	
120-42	18656-18662	innodb	_	_	_	
120-43	18663-18666	zfs	_	_	_	
120-44	18667-18670	set	_	_	_	
120-45	18671-18681	recordsize	_	_	_	
120-46	18681-18682	=	_	_	_	
120-47	18682-18686	128k	_	_	_	
120-48	18687-18691	tank	_	_	_	
120-49	18691-18692	/	_	_	_	
120-50	18692-18694	db	_	_	_	
120-51	18694-18695	/	_	_	_	
120-52	18695-18699	logs	_	_	_	
120-53	18700-18703	zfs	_	_	_	
120-54	18704-18707	set	_	_	_	
120-55	18708-18711	zfs	_	_	_	
120-56	18711-18712	:	_	_	_	
120-57	18712-18728	zfs_nocacheflush	_	_	_	
120-58	18729-18730	=	_	_	_	
120-59	18731-18732	1	_	_	_	
120-60	18733-18736	zfs	_	_	_	
120-61	18737-18740	set	_	_	_	
120-62	18741-18745	sync	_	_	_	
120-63	18745-18746	=	_	_	_	
120-64	18746-18754	disabled	_	_	_	
120-65	18755-18759	tank	_	_	_	
120-66	18759-18760	/	_	_	_	
120-67	18760-18762	db	_	_	_	
120-68	18763-18767	Note	_	_	_	
120-69	18767-18768	:	_	_	_	
120-70	18769-18774	MySQL	_	_	_	
120-71	18775-18780	5.6.6	_	_	_	
120-72	18781-18784	and	_	_	_	
120-73	18785-18790	newer	_	_	_	
120-74	18791-18792	(	_	_	_	
120-75	18792-18795	and	_	_	_	
120-76	18796-18803	related	_	_	_	
120-77	18804-18811	MariaDB	_	_	_	
120-78	18812-18813	/	_	_	_	
120-79	18814-18821	Percona	_	_	_	
120-80	18822-18827	forks	_	_	_	
120-81	18827-18828	)	_	_	_	
120-82	18829-18832	has	_	_	_	
120-83	18833-18854	innodb_file_per_table	parameter	_	_	
120-84	18854-18855	 	_	_	_	
120-85	18855-18856	=	_	_	_	
120-86	18856-18857	 	_	_	_	
120-87	18857-18859	on	value	Associated With	120-83	
120-88	18860-18862	as	_	_	_	
120-89	18863-18870	default	_	_	_	
120-90	18870-18871	,	_	_	_	
120-91	18872-18874	so	_	_	_	
120-92	18875-18878	IBD	_	_	_	
120-93	18879-18884	files	_	_	_	
120-94	18885-18888	are	_	_	_	
120-95	18889-18892	not	_	_	_	
120-96	18893-18900	created	_	_	_	
120-97	18901-18906	under	_	_	_	
120-98	18907-18911	tank	_	_	_	
120-99	18911-18912	/	_	_	_	
120-100	18912-18914	db	_	_	_	
120-101	18914-18915	/	_	_	_	
120-102	18915-18921	innodb	_	_	_	
120-103	18922-18923	(	_	_	_	
120-104	18923-18930	defined	_	_	_	
120-105	18931-18933	by	_	_	_	
120-106	18934-18954	innodb_data_home_dir	_	_	_	
120-107	18955-18957	in	_	_	_	
120-108	18958-18962	your	_	_	_	
120-109	18963-18969	my.cnf	_	_	_	
120-110	18969-18970	)	_	_	_	
120-111	18970-18971	,	_	_	_	
120-112	18972-18976	they	_	_	_	
120-113	18977-18980	are	_	_	_	
120-114	18981-18988	created	_	_	_	
120-115	18989-18994	under	_	_	_	
120-116	18995-18999	tank	_	_	_	
120-117	18999-19000	/	_	_	_	
120-118	19000-19002	db	_	_	_	
120-119	19002-19003	/	_	_	_	
120-120	19003-19004	<	_	_	_	
120-121	19004-19011	db_name	_	_	_	
120-122	19011-19012	>	_	_	_	
120-123	19012-19013	/	_	_	_	
120-124	19014-19017	and	_	_	_	
120-125	19018-19021	you	_	_	_	
120-126	19022-19028	should	_	_	_	
120-127	19029-19032	use	_	_	_	
120-128	19033-19043	recordsize	_	_	_	
120-129	19043-19044	=	_	_	_	
120-130	19044-19047	16k	_	_	_	
120-131	19048-19050	on	_	_	_	
120-132	19051-19055	this	_	_	_	
120-133	19056-19063	dataset	_	_	_	
120-134	19064-19067	too	_	_	_	
120-135	19068-19070	or	_	_	_	
120-136	19071-19077	switch	_	_	_	
120-137	19078-19082	back	_	_	_	
120-138	19083-19085	to	_	_	_	
120-139	19086-19107	innodb_file_per_table	parameter	_	_	
120-140	19107-19108	 	_	_	_	
120-141	19108-19109	=	_	_	_	
120-142	19109-19110	 	_	_	_	
120-143	19110-19113	off	value	Associated With	120-139	
120-144	19114-19124	References	_	_	_	
120-145	19125-19130	MySQL	_	_	_	
120-146	19131-19137	Innodb	_	_	_	
120-147	19138-19141	ZFS	_	_	_	
120-148	19142-19146	Best	_	_	_	
120-149	19147-19156	Practices	_	_	_	
120-150	19157-19158	(	_	_	_	
120-151	19158-19164	Oracle	_	_	_	
120-152	19164-19165	)	_	_	_	
120-153	19166-19171	Scrub	_	_	_	
120-154	19172-19175	and	_	_	_	
120-155	19176-19184	Resilver	_	_	_	
120-156	19185-19196	Performance	_	_	_	
120-157	19197-19199	If	_	_	_	
120-158	19200-19206	you're	_	_	_	
120-159	19207-19214	getting	_	_	_	
120-160	19215-19223	horrible	_	_	_	
120-161	19224-19235	performance	_	_	_	
120-162	19236-19242	during	_	_	_	
120-163	19243-19244	a	_	_	_	
120-164	19245-19250	scrub	_	_	_	
120-165	19251-19253	or	_	_	_	
120-166	19254-19262	resilver	_	_	_	
120-167	19262-19263	,	_	_	_	
120-168	19264-19267	the	_	_	_	
120-169	19268-19277	following	_	_	_	
120-170	19278-19285	sysctls	_	_	_	
120-171	19286-19289	can	_	_	_	
120-172	19290-19292	be	_	_	_	
120-173	19293-19296	set	_	_	_	
120-174	19296-19297	:	_	_	_	
120-175	19298-19317	vfs.zfs.scrub_delay	_	_	_	
120-176	19317-19318	=	_	_	_	
120-177	19318-19319	0	_	_	_	
120-178	19320-19343	vfs.zfs.top_maxinflight	_	_	_	
120-179	19343-19344	=	_	_	_	
120-180	19344-19347	128	_	_	_	
120-181	19348-19376	vfs.zfs.resilver_min_time_ms	_	_	_	
120-182	19376-19377	=	_	_	_	
120-183	19377-19381	5000	_	_	_	
120-184	19382-19404	vfs.zfs.resilver_delay	_	_	_	
120-185	19404-19405	=	_	_	_	
120-186	19405-19413	0Setting	_	_	_	
120-187	19414-19419	those	_	_	_	
120-188	19420-19427	sysctls	_	_	_	
120-189	19428-19430	to	_	_	_	
120-190	19431-19436	those	_	_	_	
120-191	19437-19443	values	_	_	_	
120-192	19444-19453	increased	_	_	_	
120-193	19454-19456	my	_	_	_	
120-194	19457-19458	(	_	_	_	
120-195	19458-19463	Shawn	_	_	_	
120-196	19464-19470	Webb's	_	_	_	
120-197	19470-19471	)	_	_	_	
120-198	19472-19480	resilver	_	_	_	
120-199	19481-19492	performance	_	_	_	
120-200	19493-19497	from	_	_	_	
120-201	19498-19501	7MB	_	_	_	
120-202	19501-19502	/	_	_	_	
120-203	19502-19503	s	_	_	_	
120-204	19504-19506	to	_	_	_	
120-205	19507-19512	230MB	_	_	_	
120-206	19512-19513	/	_	_	_	
120-207	19513-19514	s	_	_	_	
120-208	19514-19515	.	_	_	_	

#Text=CategoryZfs CategoryHowTo ZFSTuningGuide
#Text=(last edited 2021-01-16 19:54:35 by MateuszPiotrowski)
#Text=Immutable PageCommentsInfoAttachments
#Text=More Actions:
#Text=Raw Text
#Text=Print View
#Text=Render as Docbook
#Text=Delete Cache
#Text=------------------------
#Text=Check Spelling
#Text=Like Pages
#Text=Local Site Map
#Text=------------------------
#Text=Rename Page
#Text=Delete Page
#Text=------------------------
#Text=Subscribe User
#Text=------------------------
#Text=Remove Spam
#Text=Revert to this revision
#Text=Package Pages
#Text=Sync Pages
#Text=------------------------
#Text=Load
#Text=Save
#Text=SlideShow
#Text=MoinMoin PoweredPython PoweredGPL licensedValid HTML 4.01
121-1	19516-19527	CategoryZfs	_	_	_	
121-2	19528-19541	CategoryHowTo	_	_	_	
121-3	19542-19556	ZFSTuningGuide	_	_	_	
121-4	19557-19558	(	_	_	_	
121-5	19558-19562	last	_	_	_	
121-6	19563-19569	edited	_	_	_	
121-7	19570-19574	2021	_	_	_	
121-8	19574-19575	-	_	_	_	
121-9	19575-19577	01	_	_	_	
121-10	19577-19578	-	_	_	_	
121-11	19578-19580	16	_	_	_	
121-12	19581-19583	19	_	_	_	
121-13	19583-19584	:	_	_	_	
121-14	19584-19586	54	_	_	_	
121-15	19586-19587	:	_	_	_	
121-16	19587-19589	35	_	_	_	
121-17	19590-19592	by	_	_	_	
121-18	19593-19610	MateuszPiotrowski	_	_	_	
121-19	19610-19611	)	_	_	_	
121-20	19612-19621	Immutable	_	_	_	
121-21	19622-19649	PageCommentsInfoAttachments	_	_	_	
121-22	19650-19654	More	_	_	_	
121-23	19655-19662	Actions	_	_	_	
121-24	19662-19663	:	_	_	_	
121-25	19664-19667	Raw	_	_	_	
121-26	19668-19672	Text	_	_	_	
121-27	19673-19678	Print	_	_	_	
121-28	19679-19683	View	_	_	_	
121-29	19684-19690	Render	_	_	_	
121-30	19691-19693	as	_	_	_	
121-31	19694-19701	Docbook	_	_	_	
121-32	19702-19708	Delete	_	_	_	
121-33	19709-19714	Cache	_	_	_	
121-34	19715-19716	-	_	_	_	
121-35	19716-19717	-	_	_	_	
121-36	19717-19718	-	_	_	_	
121-37	19718-19719	-	_	_	_	
121-38	19719-19720	-	_	_	_	
121-39	19720-19721	-	_	_	_	
121-40	19721-19722	-	_	_	_	
121-41	19722-19723	-	_	_	_	
121-42	19723-19724	-	_	_	_	
121-43	19724-19725	-	_	_	_	
121-44	19725-19726	-	_	_	_	
121-45	19726-19727	-	_	_	_	
121-46	19727-19728	-	_	_	_	
121-47	19728-19729	-	_	_	_	
121-48	19729-19730	-	_	_	_	
121-49	19730-19731	-	_	_	_	
121-50	19731-19732	-	_	_	_	
121-51	19732-19733	-	_	_	_	
121-52	19733-19734	-	_	_	_	
121-53	19734-19735	-	_	_	_	
121-54	19735-19736	-	_	_	_	
121-55	19736-19737	-	_	_	_	
121-56	19737-19738	-	_	_	_	
121-57	19738-19739	-	_	_	_	
121-58	19740-19745	Check	_	_	_	
121-59	19746-19754	Spelling	_	_	_	
121-60	19755-19759	Like	_	_	_	
121-61	19760-19765	Pages	_	_	_	
121-62	19766-19771	Local	_	_	_	
121-63	19772-19776	Site	_	_	_	
121-64	19777-19780	Map	_	_	_	
121-65	19781-19782	-	_	_	_	
121-66	19782-19783	-	_	_	_	
121-67	19783-19784	-	_	_	_	
121-68	19784-19785	-	_	_	_	
121-69	19785-19786	-	_	_	_	
121-70	19786-19787	-	_	_	_	
121-71	19787-19788	-	_	_	_	
121-72	19788-19789	-	_	_	_	
121-73	19789-19790	-	_	_	_	
121-74	19790-19791	-	_	_	_	
121-75	19791-19792	-	_	_	_	
121-76	19792-19793	-	_	_	_	
121-77	19793-19794	-	_	_	_	
121-78	19794-19795	-	_	_	_	
121-79	19795-19796	-	_	_	_	
121-80	19796-19797	-	_	_	_	
121-81	19797-19798	-	_	_	_	
121-82	19798-19799	-	_	_	_	
121-83	19799-19800	-	_	_	_	
121-84	19800-19801	-	_	_	_	
121-85	19801-19802	-	_	_	_	
121-86	19802-19803	-	_	_	_	
121-87	19803-19804	-	_	_	_	
121-88	19804-19805	-	_	_	_	
121-89	19806-19812	Rename	_	_	_	
121-90	19813-19817	Page	_	_	_	
121-91	19818-19824	Delete	_	_	_	
121-92	19825-19829	Page	_	_	_	
121-93	19830-19831	-	_	_	_	
121-94	19831-19832	-	_	_	_	
121-95	19832-19833	-	_	_	_	
121-96	19833-19834	-	_	_	_	
121-97	19834-19835	-	_	_	_	
121-98	19835-19836	-	_	_	_	
121-99	19836-19837	-	_	_	_	
121-100	19837-19838	-	_	_	_	
121-101	19838-19839	-	_	_	_	
121-102	19839-19840	-	_	_	_	
121-103	19840-19841	-	_	_	_	
121-104	19841-19842	-	_	_	_	
121-105	19842-19843	-	_	_	_	
121-106	19843-19844	-	_	_	_	
121-107	19844-19845	-	_	_	_	
121-108	19845-19846	-	_	_	_	
121-109	19846-19847	-	_	_	_	
121-110	19847-19848	-	_	_	_	
121-111	19848-19849	-	_	_	_	
121-112	19849-19850	-	_	_	_	
121-113	19850-19851	-	_	_	_	
121-114	19851-19852	-	_	_	_	
121-115	19852-19853	-	_	_	_	
121-116	19853-19854	-	_	_	_	
121-117	19855-19864	Subscribe	_	_	_	
121-118	19865-19869	User	_	_	_	
121-119	19870-19871	-	_	_	_	
121-120	19871-19872	-	_	_	_	
121-121	19872-19873	-	_	_	_	
121-122	19873-19874	-	_	_	_	
121-123	19874-19875	-	_	_	_	
121-124	19875-19876	-	_	_	_	
121-125	19876-19877	-	_	_	_	
121-126	19877-19878	-	_	_	_	
121-127	19878-19879	-	_	_	_	
121-128	19879-19880	-	_	_	_	
121-129	19880-19881	-	_	_	_	
121-130	19881-19882	-	_	_	_	
121-131	19882-19883	-	_	_	_	
121-132	19883-19884	-	_	_	_	
121-133	19884-19885	-	_	_	_	
121-134	19885-19886	-	_	_	_	
121-135	19886-19887	-	_	_	_	
121-136	19887-19888	-	_	_	_	
121-137	19888-19889	-	_	_	_	
121-138	19889-19890	-	_	_	_	
121-139	19890-19891	-	_	_	_	
121-140	19891-19892	-	_	_	_	
121-141	19892-19893	-	_	_	_	
121-142	19893-19894	-	_	_	_	
121-143	19895-19901	Remove	_	_	_	
121-144	19902-19906	Spam	_	_	_	
121-145	19907-19913	Revert	_	_	_	
121-146	19914-19916	to	_	_	_	
121-147	19917-19921	this	_	_	_	
121-148	19922-19930	revision	_	_	_	
121-149	19931-19938	Package	_	_	_	
121-150	19939-19944	Pages	_	_	_	
121-151	19945-19949	Sync	_	_	_	
121-152	19950-19955	Pages	_	_	_	
121-153	19956-19957	-	_	_	_	
121-154	19957-19958	-	_	_	_	
121-155	19958-19959	-	_	_	_	
121-156	19959-19960	-	_	_	_	
121-157	19960-19961	-	_	_	_	
121-158	19961-19962	-	_	_	_	
121-159	19962-19963	-	_	_	_	
121-160	19963-19964	-	_	_	_	
121-161	19964-19965	-	_	_	_	
121-162	19965-19966	-	_	_	_	
121-163	19966-19967	-	_	_	_	
121-164	19967-19968	-	_	_	_	
121-165	19968-19969	-	_	_	_	
121-166	19969-19970	-	_	_	_	
121-167	19970-19971	-	_	_	_	
121-168	19971-19972	-	_	_	_	
121-169	19972-19973	-	_	_	_	
121-170	19973-19974	-	_	_	_	
121-171	19974-19975	-	_	_	_	
121-172	19975-19976	-	_	_	_	
121-173	19976-19977	-	_	_	_	
121-174	19977-19978	-	_	_	_	
121-175	19978-19979	-	_	_	_	
121-176	19979-19980	-	_	_	_	
121-177	19981-19985	Load	_	_	_	
121-178	19986-19990	Save	_	_	_	
121-179	19991-20000	SlideShow	_	_	_	
121-180	20001-20009	MoinMoin	_	_	_	
121-181	20010-20023	PoweredPython	_	_	_	
121-182	20024-20034	PoweredGPL	_	_	_	
121-183	20035-20048	licensedValid	_	_	_	
121-184	20049-20053	HTML	_	_	_	
121-185	20054-20058	4.01	_	_	_	
