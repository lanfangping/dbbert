#FORMAT=WebAnno TSV 3.3


#Text=ZFSTuningGuide - FreeBSD Wiki
#Text=Search:
#Text=Login
#Text=ZFSTuningGuide
#Text=RecentChangesFindPageHelpContentsZFSTuningGuide
#Text=Immutable PageCommentsInfoAttachments
#Text=More Actions:
#Text=Raw Text
#Text=Print View
#Text=Render as Docbook
#Text=Delete Cache
#Text=------------------------
#Text=Check Spelling
#Text=Like Pages
#Text=Local Site Map
#Text=------------------------
#Text=Rename Page
#Text=Delete Page
#Text=------------------------
#Text=Subscribe User
#Text=------------------------
#Text=Remove Spam
#Text=Revert to this revision
#Text=Package Pages
#Text=Sync Pages
#Text=------------------------
#Text=Load
#Text=Save
#Text=SlideShow
#Text=Contents
#Text=ZFS Tuning Guide
#Text=i386
#Text=amd64
#Text=Generic ARC discussion
#Text=L2ARC discussion
#Text=Application Issues
#Text=General Tuning
#Text=Deduplication
#Text=Suggestions
#Text=References
#Text=NFS tuning
#Text=MySQL
#Text=Scrub and Resilver Performance
#Text=See also: Solaris: ZFS Evil Tuning Guide, loader.conf(5), sysctl(8).
1-1	0-14	ZFSTuningGuide	
1-2	15-16	-	
1-3	17-24	FreeBSD	
1-4	25-29	Wiki	
1-5	30-36	Search	
1-6	36-37	:	
1-7	38-43	Login	
1-8	44-58	ZFSTuningGuide	
1-9	59-106	RecentChangesFindPageHelpContentsZFSTuningGuide	
1-10	107-116	Immutable	
1-11	117-144	PageCommentsInfoAttachments	
1-12	145-149	More	
1-13	150-157	Actions	
1-14	157-158	:	
1-15	159-162	Raw	
1-16	163-167	Text	
1-17	168-173	Print	
1-18	174-178	View	
1-19	179-185	Render	
1-20	186-188	as	
1-21	189-196	Docbook	
1-22	197-203	Delete	
1-23	204-209	Cache	
1-24	210-211	-	
1-25	211-212	-	
1-26	212-213	-	
1-27	213-214	-	
1-28	214-215	-	
1-29	215-216	-	
1-30	216-217	-	
1-31	217-218	-	
1-32	218-219	-	
1-33	219-220	-	
1-34	220-221	-	
1-35	221-222	-	
1-36	222-223	-	
1-37	223-224	-	
1-38	224-225	-	
1-39	225-226	-	
1-40	226-227	-	
1-41	227-228	-	
1-42	228-229	-	
1-43	229-230	-	
1-44	230-231	-	
1-45	231-232	-	
1-46	232-233	-	
1-47	233-234	-	
1-48	235-240	Check	
1-49	241-249	Spelling	
1-50	250-254	Like	
1-51	255-260	Pages	
1-52	261-266	Local	
1-53	267-271	Site	
1-54	272-275	Map	
1-55	276-277	-	
1-56	277-278	-	
1-57	278-279	-	
1-58	279-280	-	
1-59	280-281	-	
1-60	281-282	-	
1-61	282-283	-	
1-62	283-284	-	
1-63	284-285	-	
1-64	285-286	-	
1-65	286-287	-	
1-66	287-288	-	
1-67	288-289	-	
1-68	289-290	-	
1-69	290-291	-	
1-70	291-292	-	
1-71	292-293	-	
1-72	293-294	-	
1-73	294-295	-	
1-74	295-296	-	
1-75	296-297	-	
1-76	297-298	-	
1-77	298-299	-	
1-78	299-300	-	
1-79	301-307	Rename	
1-80	308-312	Page	
1-81	313-319	Delete	
1-82	320-324	Page	
1-83	325-326	-	
1-84	326-327	-	
1-85	327-328	-	
1-86	328-329	-	
1-87	329-330	-	
1-88	330-331	-	
1-89	331-332	-	
1-90	332-333	-	
1-91	333-334	-	
1-92	334-335	-	
1-93	335-336	-	
1-94	336-337	-	
1-95	337-338	-	
1-96	338-339	-	
1-97	339-340	-	
1-98	340-341	-	
1-99	341-342	-	
1-100	342-343	-	
1-101	343-344	-	
1-102	344-345	-	
1-103	345-346	-	
1-104	346-347	-	
1-105	347-348	-	
1-106	348-349	-	
1-107	350-359	Subscribe	
1-108	360-364	User	
1-109	365-366	-	
1-110	366-367	-	
1-111	367-368	-	
1-112	368-369	-	
1-113	369-370	-	
1-114	370-371	-	
1-115	371-372	-	
1-116	372-373	-	
1-117	373-374	-	
1-118	374-375	-	
1-119	375-376	-	
1-120	376-377	-	
1-121	377-378	-	
1-122	378-379	-	
1-123	379-380	-	
1-124	380-381	-	
1-125	381-382	-	
1-126	382-383	-	
1-127	383-384	-	
1-128	384-385	-	
1-129	385-386	-	
1-130	386-387	-	
1-131	387-388	-	
1-132	388-389	-	
1-133	390-396	Remove	
1-134	397-401	Spam	
1-135	402-408	Revert	
1-136	409-411	to	
1-137	412-416	this	
1-138	417-425	revision	
1-139	426-433	Package	
1-140	434-439	Pages	
1-141	440-444	Sync	
1-142	445-450	Pages	
1-143	451-452	-	
1-144	452-453	-	
1-145	453-454	-	
1-146	454-455	-	
1-147	455-456	-	
1-148	456-457	-	
1-149	457-458	-	
1-150	458-459	-	
1-151	459-460	-	
1-152	460-461	-	
1-153	461-462	-	
1-154	462-463	-	
1-155	463-464	-	
1-156	464-465	-	
1-157	465-466	-	
1-158	466-467	-	
1-159	467-468	-	
1-160	468-469	-	
1-161	469-470	-	
1-162	470-471	-	
1-163	471-472	-	
1-164	472-473	-	
1-165	473-474	-	
1-166	474-475	-	
1-167	476-480	Load	
1-168	481-485	Save	
1-169	486-495	SlideShow	
1-170	496-504	Contents	
1-171	505-508	ZFS	
1-172	509-515	Tuning	
1-173	516-521	Guide	
1-174	522-526	i386	
1-175	527-532	amd64	
1-176	533-540	Generic	
1-177	541-544	ARC	
1-178	545-555	discussion	
1-179	556-561	L2ARC	
1-180	562-572	discussion	
1-181	573-584	Application	
1-182	585-591	Issues	
1-183	592-599	General	
1-184	600-606	Tuning	
1-185	607-620	Deduplication	
1-186	621-632	Suggestions	
1-187	633-643	References	
1-188	644-647	NFS	
1-189	648-654	tuning	
1-190	655-660	MySQL	
1-191	661-666	Scrub	
1-192	667-670	and	
1-193	671-679	Resilver	
1-194	680-691	Performance	
1-195	692-695	See	
1-196	696-700	also	
1-197	700-701	:	
1-198	702-709	Solaris	
1-199	709-710	:	
1-200	711-714	ZFS	
1-201	715-719	Evil	
1-202	720-726	Tuning	
1-203	727-732	Guide	
1-204	732-733	,	
1-205	734-745	loader.conf	
1-206	745-746	(	
1-207	746-747	5	
1-208	747-748	)	
1-209	748-749	,	
1-210	750-756	sysctl	
1-211	756-757	(	
1-212	757-758	8	
1-213	758-759	)	
1-214	759-760	.	

#Text=ZFS Tuning Guide
#Text=(Work in Progress) To use ZFS, at least 1 GB of memory is recommended (for all architectures) but more is helpful as ZFS needs *lots* of memory.
2-1	761-764	ZFS	
2-2	765-771	Tuning	
2-3	772-777	Guide	
2-4	778-779	(	
2-5	779-783	Work	
2-6	784-786	in	
2-7	787-795	Progress	
2-8	795-796	)	
2-9	797-799	To	
2-10	800-803	use	
2-11	804-807	ZFS	
2-12	807-808	,	
2-13	809-811	at	
2-14	812-817	least	
2-15	818-819	1	
2-16	819-820	 	
2-17	820-822	GB	
2-18	823-825	of	
2-19	826-832	memory	
2-20	833-835	is	
2-21	836-847	recommended	
2-22	848-849	(	
2-23	849-852	for	
2-24	853-856	all	
2-25	857-870	architectures	
2-26	870-871	)	
2-27	872-875	but	
2-28	876-880	more	
2-29	881-883	is	
2-30	884-891	helpful	
2-31	892-894	as	
2-32	895-898	ZFS	
2-33	899-904	needs	
2-34	905-906	*	
2-35	906-910	lots	
2-36	910-911	*	
2-37	912-914	of	
2-38	915-921	memory	
2-39	921-922	.	

#Text=Depending on your workload, it may be possible to use ZFS on systems with less memory, but it requires careful tuning to avoid panics from memory exhaustion in the kernel.
3-1	923-932	Depending	
3-2	933-935	on	
3-3	936-940	your	
3-4	941-949	workload	
3-5	949-950	,	
3-6	951-953	it	
3-7	954-957	may	
3-8	958-960	be	
3-9	961-969	possible	
3-10	970-972	to	
3-11	973-976	use	
3-12	977-980	ZFS	
3-13	981-983	on	
3-14	984-991	systems	
3-15	992-996	with	
3-16	997-1001	less	
3-17	1002-1008	memory	
3-18	1008-1009	,	
3-19	1010-1013	but	
3-20	1014-1016	it	
3-21	1017-1025	requires	
3-22	1026-1033	careful	
3-23	1034-1040	tuning	
3-24	1041-1043	to	
3-25	1044-1049	avoid	
3-26	1050-1056	panics	
3-27	1057-1061	from	
3-28	1062-1068	memory	
3-29	1069-1079	exhaustion	
3-30	1080-1082	in	
3-31	1083-1086	the	
3-32	1087-1093	kernel	
3-33	1093-1094	.	

#Text=A 64-bit system is preferred due to its larger address space and better performance on 64-bit variables, which are used extensively by ZFS. 32-bit systems are supported though, with sufficient tuning.
4-1	1095-1096	A	
4-2	1097-1099	64	
4-3	1099-1100	-	
4-4	1100-1103	bit	
4-5	1104-1110	system	
4-6	1111-1113	is	
4-7	1114-1123	preferred	
4-8	1124-1127	due	
4-9	1128-1130	to	
4-10	1131-1134	its	
4-11	1135-1141	larger	
4-12	1142-1149	address	
4-13	1150-1155	space	
4-14	1156-1159	and	
4-15	1160-1166	better	
4-16	1167-1178	performance	
4-17	1179-1181	on	
4-18	1182-1184	64	
4-19	1184-1185	-	
4-20	1185-1188	bit	
4-21	1189-1198	variables	
4-22	1198-1199	,	
4-23	1200-1205	which	
4-24	1206-1209	are	
4-25	1210-1214	used	
4-26	1215-1226	extensively	
4-27	1227-1229	by	
4-28	1230-1233	ZFS	
4-29	1233-1234	.	
4-30	1235-1237	32	
4-31	1237-1238	-	
4-32	1238-1241	bit	
4-33	1242-1249	systems	
4-34	1250-1253	are	
4-35	1254-1263	supported	
4-36	1264-1270	though	
4-37	1270-1271	,	
4-38	1272-1276	with	
4-39	1277-1287	sufficient	
4-40	1288-1294	tuning	
4-41	1294-1295	.	

#Text=History of FreeBSD releases with ZFS is as follows: 7.0+ - original ZFS import, ZFS v6; requires significant tuning for stable operation (no longer supported) 7.2 - still ZFS v6, improved memory handling, amd64 may need no memory tuning (no longer supported) 7.3+ - backport of new ZFS v13 code, similar to the 8.0 code 8.0 - new ZFS v13 code, lots of bug fixes - recommended over all past versions.
5-1	1296-1303	History	
5-2	1304-1306	of	
5-3	1307-1314	FreeBSD	
5-4	1315-1323	releases	
5-5	1324-1328	with	
5-6	1329-1332	ZFS	
5-7	1333-1335	is	
5-8	1336-1338	as	
5-9	1339-1346	follows	
5-10	1346-1347	:	
5-11	1348-1351	7.0	
5-12	1351-1352	+	
5-13	1353-1354	-	
5-14	1355-1363	original	
5-15	1364-1367	ZFS	
5-16	1368-1374	import	
5-17	1374-1375	,	
5-18	1376-1379	ZFS	
5-19	1380-1382	v6	
5-20	1382-1383	;	
5-21	1384-1392	requires	
5-22	1393-1404	significant	
5-23	1405-1411	tuning	
5-24	1412-1415	for	
5-25	1416-1422	stable	
5-26	1423-1432	operation	
5-27	1433-1434	(	
5-28	1434-1436	no	
5-29	1437-1443	longer	
5-30	1444-1453	supported	
5-31	1453-1454	)	
5-32	1455-1458	7.2	
5-33	1459-1460	-	
5-34	1461-1466	still	
5-35	1467-1470	ZFS	
5-36	1471-1473	v6	
5-37	1473-1474	,	
5-38	1475-1483	improved	
5-39	1484-1490	memory	
5-40	1491-1499	handling	
5-41	1499-1500	,	
5-42	1501-1506	amd64	
5-43	1507-1510	may	
5-44	1511-1515	need	
5-45	1516-1518	no	
5-46	1519-1525	memory	
5-47	1526-1532	tuning	
5-48	1533-1534	(	
5-49	1534-1536	no	
5-50	1537-1543	longer	
5-51	1544-1553	supported	
5-52	1553-1554	)	
5-53	1555-1558	7.3	
5-54	1558-1559	+	
5-55	1560-1561	-	
5-56	1562-1570	backport	
5-57	1571-1573	of	
5-58	1574-1577	new	
5-59	1578-1581	ZFS	
5-60	1582-1585	v13	
5-61	1586-1590	code	
5-62	1590-1591	,	
5-63	1592-1599	similar	
5-64	1600-1602	to	
5-65	1603-1606	the	
5-66	1607-1610	8.0	
5-67	1611-1615	code	
5-68	1616-1619	8.0	
5-69	1620-1621	-	
5-70	1622-1625	new	
5-71	1626-1629	ZFS	
5-72	1630-1633	v13	
5-73	1634-1638	code	
5-74	1638-1639	,	
5-75	1640-1644	lots	
5-76	1645-1647	of	
5-77	1648-1651	bug	
5-78	1652-1657	fixes	
5-79	1658-1659	-	
5-80	1660-1671	recommended	
5-81	1672-1676	over	
5-82	1677-1680	all	
5-83	1681-1685	past	
5-84	1686-1694	versions	
5-85	1694-1695	.	

#Text=(no longer supported) 8.1+ - ZFS v14 8.2+ - ZFS v15 8.3+ - ZFS v28 9.0+ - ZFS v28
#Text=i386
#Text=Typically you need to increase vm.kmem_size_max and vm.kmem_size (with vm.kmem_size_max >= vm.kmem_size) to not get kernel panics (kmem too small).
6-1	1696-1697	(	
6-2	1697-1699	no	
6-3	1700-1706	longer	
6-4	1707-1716	supported	
6-5	1716-1717	)	
6-6	1718-1721	8.1	
6-7	1721-1722	+	
6-8	1723-1724	-	
6-9	1725-1728	ZFS	
6-10	1729-1732	v14	
6-11	1733-1736	8.2	
6-12	1736-1737	+	
6-13	1738-1739	-	
6-14	1740-1743	ZFS	
6-15	1744-1747	v15	
6-16	1748-1751	8.3	
6-17	1751-1752	+	
6-18	1753-1754	-	
6-19	1755-1758	ZFS	
6-20	1759-1762	v28	
6-21	1763-1766	9.0	
6-22	1766-1767	+	
6-23	1768-1769	-	
6-24	1770-1773	ZFS	
6-25	1774-1777	v28	
6-26	1778-1782	i386	
6-27	1783-1792	Typically	
6-28	1793-1796	you	
6-29	1797-1801	need	
6-30	1802-1804	to	
6-31	1805-1813	increase	
6-32	1814-1830	vm.kmem_size_max	
6-33	1831-1834	and	
6-34	1835-1847	vm.kmem_size	
6-35	1848-1849	(	
6-36	1849-1853	with	
6-37	1854-1870	vm.kmem_size_max	
6-38	1871-1872	>	
6-39	1872-1873	=	
6-40	1874-1886	vm.kmem_size	
6-41	1886-1887	)	
6-42	1888-1890	to	
6-43	1891-1894	not	
6-44	1895-1898	get	
6-45	1899-1905	kernel	
6-46	1906-1912	panics	
6-47	1913-1914	(	
6-48	1914-1918	kmem	
6-49	1919-1922	too	
6-50	1923-1928	small	
6-51	1928-1929	)	
6-52	1929-1930	.	

#Text=The value depends upon the workload.
7-1	1931-1934	The	
7-2	1935-1940	value	
7-3	1941-1948	depends	
7-4	1949-1953	upon	
7-5	1954-1957	the	
7-6	1958-1966	workload	
7-7	1966-1967	.	

#Text=If you need to extend them beyond 512M, you need to recompile your kernel with increased KVA_PAGES option, e.g. add the following line to your kernel configuration file to increase available space for vm.kmem_size beyond 1 GB: options KVA_PAGES=512 To chose a good value for KVA_PAGES read the explanation in the sys/i386/conf/NOTES file.
8-1	1968-1970	If	
8-2	1971-1974	you	
8-3	1975-1979	need	
8-4	1980-1982	to	
8-5	1983-1989	extend	
8-6	1990-1994	them	
8-7	1995-2001	beyond	
8-8	2002-2006	512M	
8-9	2006-2007	,	
8-10	2008-2011	you	
8-11	2012-2016	need	
8-12	2017-2019	to	
8-13	2020-2029	recompile	
8-14	2030-2034	your	
8-15	2035-2041	kernel	
8-16	2042-2046	with	
8-17	2047-2056	increased	
8-18	2057-2066	KVA_PAGES	
8-19	2067-2073	option	
8-20	2073-2074	,	
8-21	2075-2078	e.g	
8-22	2078-2079	.	
8-23	2080-2083	add	
8-24	2084-2087	the	
8-25	2088-2097	following	
8-26	2098-2102	line	
8-27	2103-2105	to	
8-28	2106-2110	your	
8-29	2111-2117	kernel	
8-30	2118-2131	configuration	
8-31	2132-2136	file	
8-32	2137-2139	to	
8-33	2140-2148	increase	
8-34	2149-2158	available	
8-35	2159-2164	space	
8-36	2165-2168	for	
8-37	2169-2181	vm.kmem_size	
8-38	2182-2188	beyond	
8-39	2189-2190	1	
8-40	2190-2191	 	
8-41	2191-2193	GB	
8-42	2193-2194	:	
8-43	2195-2202	options	
8-44	2202-2203	 	
8-45	2203-2212	KVA_PAGES	
8-46	2212-2213	=	
8-47	2213-2216	512	
8-48	2217-2219	To	
8-49	2220-2225	chose	
8-50	2226-2227	a	
8-51	2228-2232	good	
8-52	2233-2238	value	
8-53	2239-2242	for	
8-54	2243-2252	KVA_PAGES	
8-55	2253-2257	read	
8-56	2258-2261	the	
8-57	2262-2273	explanation	
8-58	2274-2276	in	
8-59	2277-2280	the	
8-60	2281-2284	sys	
8-61	2284-2285	/	
8-62	2285-2289	i386	
8-63	2289-2290	/	
8-64	2290-2294	conf	
8-65	2294-2295	/	
8-66	2295-2300	NOTES	
8-67	2301-2305	file	
8-68	2305-2306	.	

#Text=By default the kernel receives 1 GB of the 4 GB of address space available on the i386 architecture, and this is used for all of the kernel address space needs, not just the kmem map.
9-1	2307-2309	By	
9-2	2310-2317	default	
9-3	2318-2321	the	
9-4	2322-2328	kernel	
9-5	2329-2337	receives	
9-6	2338-2339	1	
9-7	2339-2340	 	
9-8	2340-2342	GB	
9-9	2343-2345	of	
9-10	2346-2349	the	
9-11	2350-2351	4	
9-12	2351-2352	 	
9-13	2352-2354	GB	
9-14	2355-2357	of	
9-15	2358-2365	address	
9-16	2366-2371	space	
9-17	2372-2381	available	
9-18	2382-2384	on	
9-19	2385-2388	the	
9-20	2389-2393	i386	
9-21	2394-2406	architecture	
9-22	2406-2407	,	
9-23	2408-2411	and	
9-24	2412-2416	this	
9-25	2417-2419	is	
9-26	2420-2424	used	
9-27	2425-2428	for	
9-28	2429-2432	all	
9-29	2433-2435	of	
9-30	2436-2439	the	
9-31	2440-2446	kernel	
9-32	2447-2454	address	
9-33	2455-2460	space	
9-34	2461-2466	needs	
9-35	2466-2467	,	
9-36	2468-2471	not	
9-37	2472-2476	just	
9-38	2477-2480	the	
9-39	2481-2485	kmem	
9-40	2486-2489	map	
9-41	2489-2490	.	

#Text=By increasing KVA_PAGES you can allocate a larger proportion of the 4 GB address space to the kernel (2 GB in the above example), allowing more room to increase vm.kmem_size.
10-1	2491-2493	By	
10-2	2494-2504	increasing	
10-3	2505-2514	KVA_PAGES	
10-4	2515-2518	you	
10-5	2519-2522	can	
10-6	2523-2531	allocate	
10-7	2532-2533	a	
10-8	2534-2540	larger	
10-9	2541-2551	proportion	
10-10	2552-2554	of	
10-11	2555-2558	the	
10-12	2559-2560	4	
10-13	2560-2561	 	
10-14	2561-2563	GB	
10-15	2564-2571	address	
10-16	2572-2577	space	
10-17	2578-2580	to	
10-18	2581-2584	the	
10-19	2585-2591	kernel	
10-20	2592-2593	(	
10-21	2593-2594	2	
10-22	2594-2595	 	
10-23	2595-2597	GB	
10-24	2598-2600	in	
10-25	2601-2604	the	
10-26	2605-2610	above	
10-27	2611-2618	example	
10-28	2618-2619	)	
10-29	2619-2620	,	
10-30	2621-2629	allowing	
10-31	2630-2634	more	
10-32	2635-2639	room	
10-33	2640-2642	to	
10-34	2643-2651	increase	
10-35	2652-2664	vm.kmem_size	
10-36	2664-2665	.	

#Text=The trade-off is that user applications have less address space available, and some programs (e.g. those that rely on mapping data at a fixed address that is now in the kernel address space, or which require close to the full 3 GB of address space themselves) may no longer run.
11-1	2666-2669	The	
11-2	2670-2679	trade-off	
11-3	2680-2682	is	
11-4	2683-2687	that	
11-5	2688-2692	user	
11-6	2693-2705	applications	
11-7	2706-2710	have	
11-8	2711-2715	less	
11-9	2716-2723	address	
11-10	2724-2729	space	
11-11	2730-2739	available	
11-12	2739-2740	,	
11-13	2741-2744	and	
11-14	2745-2749	some	
11-15	2750-2758	programs	
11-16	2759-2760	(	
11-17	2760-2763	e.g	
11-18	2763-2764	.	
11-19	2765-2770	those	
11-20	2771-2775	that	
11-21	2776-2780	rely	
11-22	2781-2783	on	
11-23	2784-2791	mapping	
11-24	2792-2796	data	
11-25	2797-2799	at	
11-26	2800-2801	a	
11-27	2802-2807	fixed	
11-28	2808-2815	address	
11-29	2816-2820	that	
11-30	2821-2823	is	
11-31	2824-2827	now	
11-32	2828-2830	in	
11-33	2831-2834	the	
11-34	2835-2841	kernel	
11-35	2842-2849	address	
11-36	2850-2855	space	
11-37	2855-2856	,	
11-38	2857-2859	or	
11-39	2860-2865	which	
11-40	2866-2873	require	
11-41	2874-2879	close	
11-42	2880-2882	to	
11-43	2883-2886	the	
11-44	2887-2891	full	
11-45	2892-2893	3	
11-46	2893-2894	 	
11-47	2894-2896	GB	
11-48	2897-2899	of	
11-49	2900-2907	address	
11-50	2908-2913	space	
11-51	2914-2924	themselves	
11-52	2924-2925	)	
11-53	2926-2929	may	
11-54	2930-2932	no	
11-55	2933-2939	longer	
11-56	2940-2943	run	
11-57	2943-2944	.	

#Text=If you change KVA_PAGES and the system reboots (no panic) after running a while this may be because the address space for userland applications is too small now.
12-1	2945-2947	If	
12-2	2948-2951	you	
12-3	2952-2958	change	
12-4	2959-2968	KVA_PAGES	
12-5	2969-2972	and	
12-6	2973-2976	the	
12-7	2977-2983	system	
12-8	2984-2991	reboots	
12-9	2992-2993	(	
12-10	2993-2995	no	
12-11	2996-3001	panic	
12-12	3001-3002	)	
12-13	3003-3008	after	
12-14	3009-3016	running	
12-15	3017-3018	a	
12-16	3019-3024	while	
12-17	3025-3029	this	
12-18	3030-3033	may	
12-19	3034-3036	be	
12-20	3037-3044	because	
12-21	3045-3048	the	
12-22	3049-3056	address	
12-23	3057-3062	space	
12-24	3063-3066	for	
12-25	3067-3075	userland	
12-26	3076-3088	applications	
12-27	3089-3091	is	
12-28	3092-3095	too	
12-29	3096-3101	small	
12-30	3102-3105	now	
12-31	3105-3106	.	

#Text=For *really* memory constrained systems it is also recommended to strip out as many unused drivers and options from the kernel (which will free a couple of MB of memory).
13-1	3107-3110	For	
13-2	3111-3112	*	
13-3	3112-3118	really	
13-4	3118-3119	*	
13-5	3120-3126	memory	
13-6	3127-3138	constrained	
13-7	3139-3146	systems	
13-8	3147-3149	it	
13-9	3150-3152	is	
13-10	3153-3157	also	
13-11	3158-3169	recommended	
13-12	3170-3172	to	
13-13	3173-3178	strip	
13-14	3179-3182	out	
13-15	3183-3185	as	
13-16	3186-3190	many	
13-17	3191-3197	unused	
13-18	3198-3205	drivers	
13-19	3206-3209	and	
13-20	3210-3217	options	
13-21	3218-3222	from	
13-22	3223-3226	the	
13-23	3227-3233	kernel	
13-24	3234-3235	(	
13-25	3235-3240	which	
13-26	3241-3245	will	
13-27	3246-3250	free	
13-28	3251-3252	a	
13-29	3253-3259	couple	
13-30	3260-3262	of	
13-31	3263-3265	MB	
13-32	3266-3268	of	
13-33	3269-3275	memory	
13-34	3275-3276	)	
13-35	3276-3277	.	

#Text=A stable configuration with vm.kmem_size="1536M" has been reported using an unmodified 7.0-RELEASE kernel, relatively sparse drivers as required for the hardware and options KVA_PAGES=512.
14-1	3278-3279	A	
14-2	3280-3286	stable	
14-3	3287-3300	configuration	
14-4	3301-3305	with	
14-5	3306-3318	vm.kmem_size	
14-6	3318-3319	=	
14-7	3319-3320	"	
14-8	3320-3325	1536M	
14-9	3325-3326	"	
14-10	3327-3330	has	
14-11	3331-3335	been	
14-12	3336-3344	reported	
14-13	3345-3350	using	
14-14	3351-3353	an	
14-15	3354-3364	unmodified	
14-16	3365-3368	7.0	
14-17	3368-3369	-	
14-18	3369-3376	RELEASE	
14-19	3377-3383	kernel	
14-20	3383-3384	,	
14-21	3385-3395	relatively	
14-22	3396-3402	sparse	
14-23	3403-3410	drivers	
14-24	3411-3413	as	
14-25	3414-3422	required	
14-26	3423-3426	for	
14-27	3427-3430	the	
14-28	3431-3439	hardware	
14-29	3440-3443	and	
14-30	3444-3451	options	
14-31	3451-3452	 	
14-32	3452-3461	KVA_PAGES	
14-33	3461-3462	=	
14-34	3462-3465	512	
14-35	3465-3466	.	

#Text=Some workloads need greatly reduced ARC size and the size of VDEV cache.
15-1	3467-3471	Some	
15-2	3472-3481	workloads	
15-3	3482-3486	need	
15-4	3487-3494	greatly	
15-5	3495-3502	reduced	
15-6	3503-3506	ARC	
15-7	3507-3511	size	
15-8	3512-3515	and	
15-9	3516-3519	the	
15-10	3520-3524	size	
15-11	3525-3527	of	
15-12	3528-3532	VDEV	
15-13	3533-3538	cache	
15-14	3538-3539	.	

#Text=ZFS manages the ARC through a multi-threaded process.
16-1	3540-3543	ZFS	
16-2	3544-3551	manages	
16-3	3552-3555	the	
16-4	3556-3559	ARC	
16-5	3560-3567	through	
16-6	3568-3569	a	
16-7	3570-3584	multi-threaded	
16-8	3585-3592	process	
16-9	3592-3593	.	

#Text=If it requires more memory for ARC ZFS will allocate it.
17-1	3594-3596	If	
17-2	3597-3599	it	
17-3	3600-3608	requires	
17-4	3609-3613	more	
17-5	3614-3620	memory	
17-6	3621-3624	for	
17-7	3625-3628	ARC	
17-8	3629-3632	ZFS	
17-9	3633-3637	will	
17-10	3638-3646	allocate	
17-11	3647-3649	it	
17-12	3649-3650	.	

#Text=Previously it exceeded arc_max (vfs.zfs.arc_max) from time to time, but with 7.3 and 8-stable as of mid-January 2010 this is not the case anymore.
18-1	3651-3661	Previously	
18-2	3662-3664	it	
18-3	3665-3673	exceeded	
18-4	3674-3681	arc_max	
18-5	3682-3683	(	
18-6	3683-3698	vfs.zfs.arc_max	
18-7	3698-3699	)	
18-8	3700-3704	from	
18-9	3705-3709	time	
18-10	3710-3712	to	
18-11	3713-3717	time	
18-12	3717-3718	,	
18-13	3719-3722	but	
18-14	3723-3727	with	
18-15	3728-3731	7.3	
18-16	3732-3735	and	
18-17	3736-3737	8	
18-18	3737-3738	-	
18-19	3738-3744	stable	
18-20	3745-3747	as	
18-21	3748-3750	of	
18-22	3751-3762	mid-January	
18-23	3763-3767	2010	
18-24	3768-3772	this	
18-25	3773-3775	is	
18-26	3776-3779	not	
18-27	3780-3783	the	
18-28	3784-3788	case	
18-29	3789-3796	anymore	
18-30	3796-3797	.	

#Text=On memory constrained systems it is safer to use an arbitrarily low arc_max.
19-1	3798-3800	On	
19-2	3801-3807	memory	
19-3	3808-3819	constrained	
19-4	3820-3827	systems	
19-5	3828-3830	it	
19-6	3831-3833	is	
19-7	3834-3839	safer	
19-8	3840-3842	to	
19-9	3843-3846	use	
19-10	3847-3849	an	
19-11	3850-3861	arbitrarily	
19-12	3862-3865	low	
19-13	3866-3873	arc_max	
19-14	3873-3874	.	

#Text=For example it is possible to set vm.kmem_size and vm.kmem_size_max to 512M, vfs.zfs.arc_max to 160M, keeping vfs.zfs.vdev.cache.size to half its default size of 10 Megs (setting it to 5 Megs can even achieve better stability, but this depends upon your workload).
20-1	3875-3878	For	
20-2	3879-3886	example	
20-3	3887-3889	it	
20-4	3890-3892	is	
20-5	3893-3901	possible	
20-6	3902-3904	to	
20-7	3905-3908	set	
20-8	3909-3921	vm.kmem_size	
20-9	3922-3925	and	
20-10	3926-3942	vm.kmem_size_max	
20-11	3942-3943	 	
20-12	3943-3945	to	
20-13	3945-3946	 	
20-14	3946-3950	512M	
20-15	3950-3951	,	
20-16	3952-3967	vfs.zfs.arc_max	
20-17	3968-3970	to	
20-18	3971-3975	160M	
20-19	3975-3976	,	
20-20	3977-3984	keeping	
20-21	3985-4008	vfs.zfs.vdev.cache.size	
20-22	4009-4011	to	
20-23	4012-4016	half	
20-24	4017-4020	its	
20-25	4021-4028	default	
20-26	4029-4033	size	
20-27	4034-4036	of	
20-28	4037-4039	10	
20-29	4040-4044	Megs	
20-30	4045-4046	(	
20-31	4046-4053	setting	
20-32	4054-4056	it	
20-33	4057-4059	to	
20-34	4060-4061	5	
20-35	4062-4066	Megs	
20-36	4067-4070	can	
20-37	4071-4075	even	
20-38	4076-4083	achieve	
20-39	4084-4090	better	
20-40	4091-4100	stability	
20-41	4100-4101	,	
20-42	4102-4105	but	
20-43	4106-4110	this	
20-44	4111-4118	depends	
20-45	4119-4123	upon	
20-46	4124-4128	your	
20-47	4129-4137	workload	
20-48	4137-4138	)	
20-49	4138-4139	.	

#Text=There is one example (CySchubert) of ZFS running nicely on a laptop with 768 Megs of physical RAM with the following settings in /boot/loader.conf: vm.kmem_size="330M" vm.kmem_size_max="330M" vfs.zfs.arc_max="40M" vfs.zfs.vdev.cache.size="5M" Kernel memory should be monitored while tuning to ensure a comfortable amount of free kernel address space.
21-1	4140-4145	There	
21-2	4146-4148	is	
21-3	4149-4152	one	
21-4	4153-4160	example	
21-5	4161-4162	(	
21-6	4162-4172	CySchubert	
21-7	4172-4173	)	
21-8	4174-4176	of	
21-9	4177-4180	ZFS	
21-10	4181-4188	running	
21-11	4189-4195	nicely	
21-12	4196-4198	on	
21-13	4199-4200	a	
21-14	4201-4207	laptop	
21-15	4208-4212	with	
21-16	4213-4216	768	
21-17	4217-4221	Megs	
21-18	4222-4224	of	
21-19	4225-4233	physical	
21-20	4234-4237	RAM	
21-21	4238-4242	with	
21-22	4243-4246	the	
21-23	4247-4256	following	
21-24	4257-4265	settings	
21-25	4266-4268	in	
21-26	4269-4270	/	
21-27	4270-4274	boot	
21-28	4274-4275	/	
21-29	4275-4286	loader.conf	
21-30	4286-4287	:	
21-31	4288-4300	vm.kmem_size	
21-32	4300-4301	=	
21-33	4301-4302	"	
21-34	4302-4306	330M	
21-35	4306-4307	"	
21-36	4308-4324	vm.kmem_size_max	
21-37	4324-4325	=	
21-38	4325-4326	"	
21-39	4326-4330	330M	
21-40	4330-4331	"	
21-41	4332-4347	vfs.zfs.arc_max	
21-42	4347-4348	=	
21-43	4348-4349	"	
21-44	4349-4352	40M	
21-45	4352-4353	"	
21-46	4354-4377	vfs.zfs.vdev.cache.size	
21-47	4377-4378	=	
21-48	4378-4379	"	
21-49	4379-4381	5M	
21-50	4381-4382	"	
21-51	4383-4389	Kernel	
21-52	4390-4396	memory	
21-53	4397-4403	should	
21-54	4404-4406	be	
21-55	4407-4416	monitored	
21-56	4417-4422	while	
21-57	4423-4429	tuning	
21-58	4430-4432	to	
21-59	4433-4439	ensure	
21-60	4440-4441	a	
21-61	4442-4453	comfortable	
21-62	4454-4460	amount	
21-63	4461-4463	of	
21-64	4464-4468	free	
21-65	4469-4475	kernel	
21-66	4476-4483	address	
21-67	4484-4489	space	
21-68	4489-4490	.	

#Text=The following script will summarize kernel memory utilization and assist in tuning arc_max and VDEV cache size. #!
22-1	4491-4494	The	
22-2	4495-4504	following	
22-3	4505-4511	script	
22-4	4512-4516	will	
22-5	4517-4526	summarize	
22-6	4527-4533	kernel	
22-7	4534-4540	memory	
22-8	4541-4552	utilization	
22-9	4553-4556	and	
22-10	4557-4563	assist	
22-11	4564-4566	in	
22-12	4567-4573	tuning	
22-13	4574-4581	arc_max	
22-14	4582-4585	and	
22-15	4586-4590	VDEV	
22-16	4591-4596	cache	
22-17	4597-4601	size	
22-18	4601-4602	.	
22-19	4603-4604	#	
22-20	4604-4605	!	

#Text=/bin/sh -
#Text=TEXT=`kldstat | awk 'BEGIN {print "16i 0";} NR>1 {print toupper($4) "+"} END {print "p"}' | dc`
#Text=DATA=`vmstat -m | sed -Ee '1s/.*/0/;s/.* ([0-9]+)K.*/\\1+/;$s/$/1024*p/' | dc`
#Text=TOTAL=$((DATA + TEXT))
#Text=echo TEXT=$TEXT, `echo $TEXT | awk '{print $1/1048576 " MB"}'`
#Text=echo DATA=$DATA, `echo $DATA | awk '{print $1/1048576 " MB"}'`
#Text=echo TOTAL=$TOTAL, `echo $TOTAL | awk '{print $1/1048576 " MB"}'`
#Text=Note: Perhaps there is a more precise way to calculate / measure how large of a vm.kmem_size setting can be used with a particular kernel, but the authors of this wiki do not know it.
23-1	4605-4606	/	
23-2	4606-4609	bin	
23-3	4609-4610	/	
23-4	4610-4612	sh	
23-5	4612-4613	 	
23-6	4613-4614	-	
23-7	4615-4619	TEXT	
23-8	4619-4620	=	
23-9	4620-4621	`	
23-10	4621-4628	kldstat	
23-11	4628-4629	 	
23-12	4629-4630	|	
23-13	4630-4631	 	
23-14	4631-4634	awk	
23-15	4634-4635	 	
23-16	4635-4636	'	
23-17	4636-4641	BEGIN	
23-18	4641-4642	 	
23-19	4642-4643	{	
23-20	4643-4648	print	
23-21	4648-4649	 	
23-22	4649-4650	"	
23-23	4650-4653	16i	
23-24	4653-4654	 	
23-25	4654-4655	0	
23-26	4655-4656	"	
23-27	4656-4657	;	
23-28	4657-4658	}	
23-29	4658-4659	 	
23-30	4659-4661	NR	
23-31	4661-4662	>	
23-32	4662-4663	1	
23-33	4663-4664	 	
23-34	4664-4665	{	
23-35	4665-4670	print	
23-36	4670-4671	 	
23-37	4671-4678	toupper	
23-38	4678-4679	(	
23-39	4679-4681	$4	
23-40	4681-4682	)	
23-41	4682-4683	 	
23-42	4683-4684	"	
23-43	4684-4685	+	
23-44	4685-4686	"	
23-45	4686-4687	}	
23-46	4687-4688	 	
23-47	4688-4691	END	
23-48	4691-4692	 	
23-49	4692-4693	{	
23-50	4693-4698	print	
23-51	4698-4699	 	
23-52	4699-4700	"	
23-53	4700-4701	p	
23-54	4701-4702	"	
23-55	4702-4703	}	
23-56	4703-4704	'	
23-57	4704-4705	 	
23-58	4705-4706	|	
23-59	4706-4707	 	
23-60	4707-4709	dc	
23-61	4709-4710	`	
23-62	4711-4715	DATA	
23-63	4715-4716	=	
23-64	4716-4717	`	
23-65	4717-4723	vmstat	
23-66	4723-4724	 	
23-67	4724-4725	-	
23-68	4725-4726	m	
23-69	4726-4727	 	
23-70	4727-4728	|	
23-71	4728-4729	 	
23-72	4729-4732	sed	
23-73	4732-4733	 	
23-74	4733-4734	-	
23-75	4734-4736	Ee	
23-76	4736-4737	 	
23-77	4737-4738	'	
23-78	4738-4740	1s	
23-79	4740-4741	/	
23-80	4741-4742	.	
23-81	4742-4743	*	
23-82	4743-4744	/	
23-83	4744-4745	0	
23-84	4745-4746	/	
23-85	4746-4747	;	
23-86	4747-4748	s	
23-87	4748-4749	/	
23-88	4749-4750	.	
23-89	4750-4751	*	
23-90	4751-4752	 	
23-91	4752-4753	(	
23-92	4753-4754	[	
23-93	4754-4755	0	
23-94	4755-4756	-	
23-95	4756-4757	9	
23-96	4757-4758	]	
23-97	4758-4759	+	
23-98	4759-4760	)	
23-99	4760-4761	K	
23-100	4761-4762	.	
23-101	4762-4763	*	
23-102	4763-4764	/	
23-103	4764-4765	\	
23-104	4765-4766	1	
23-105	4766-4767	+	
23-106	4767-4768	/	
23-107	4768-4769	;	
23-108	4769-4770	$	
23-109	4770-4771	s	
23-110	4771-4772	/	
23-111	4772-4773	$	
23-112	4773-4774	/	
23-113	4774-4778	1024	
23-114	4778-4779	*	
23-115	4779-4780	p	
23-116	4780-4781	/	
23-117	4781-4782	'	
23-118	4782-4783	 	
23-119	4783-4784	|	
23-120	4784-4785	 	
23-121	4785-4787	dc	
23-122	4787-4788	`	
23-123	4789-4794	TOTAL	
23-124	4794-4795	=	
23-125	4795-4796	$	
23-126	4796-4797	(	
23-127	4797-4798	(	
23-128	4798-4802	DATA	
23-129	4802-4803	 	
23-130	4803-4804	+	
23-131	4804-4805	 	
23-132	4805-4809	TEXT	
23-133	4809-4810	)	
23-134	4810-4811	)	
23-135	4812-4816	echo	
23-136	4816-4817	 	
23-137	4817-4821	TEXT	
23-138	4821-4822	=	
23-139	4822-4823	$	
23-140	4823-4827	TEXT	
23-141	4827-4828	,	
23-142	4828-4829	 	
23-143	4829-4830	`	
23-144	4830-4834	echo	
23-145	4834-4835	 	
23-146	4835-4836	$	
23-147	4836-4840	TEXT	
23-148	4840-4841	 	
23-149	4841-4842	|	
23-150	4842-4843	 	
23-151	4843-4846	awk	
23-152	4846-4847	 	
23-153	4847-4848	'	
23-154	4848-4849	{	
23-155	4849-4854	print	
23-156	4854-4855	 	
23-157	4855-4857	$1	
23-158	4857-4858	/	
23-159	4858-4865	1048576	
23-160	4865-4866	 	
23-161	4866-4867	"	
23-162	4867-4868	 	
23-163	4868-4870	MB	
23-164	4870-4871	"	
23-165	4871-4872	}	
23-166	4872-4873	'	
23-167	4873-4874	`	
23-168	4875-4879	echo	
23-169	4879-4880	 	
23-170	4880-4884	DATA	
23-171	4884-4885	=	
23-172	4885-4886	$	
23-173	4886-4890	DATA	
23-174	4890-4891	,	
23-175	4891-4892	 	
23-176	4892-4893	`	
23-177	4893-4897	echo	
23-178	4897-4898	 	
23-179	4898-4899	$	
23-180	4899-4903	DATA	
23-181	4903-4904	 	
23-182	4904-4905	|	
23-183	4905-4906	 	
23-184	4906-4909	awk	
23-185	4909-4910	 	
23-186	4910-4911	'	
23-187	4911-4912	{	
23-188	4912-4917	print	
23-189	4917-4918	 	
23-190	4918-4920	$1	
23-191	4920-4921	/	
23-192	4921-4928	1048576	
23-193	4928-4929	 	
23-194	4929-4930	"	
23-195	4930-4931	 	
23-196	4931-4933	MB	
23-197	4933-4934	"	
23-198	4934-4935	}	
23-199	4935-4936	'	
23-200	4936-4937	`	
23-201	4938-4942	echo	
23-202	4942-4943	 	
23-203	4943-4948	TOTAL	
23-204	4948-4949	=	
23-205	4949-4950	$	
23-206	4950-4955	TOTAL	
23-207	4955-4956	,	
23-208	4956-4957	 	
23-209	4957-4958	`	
23-210	4958-4962	echo	
23-211	4962-4963	 	
23-212	4963-4964	$	
23-213	4964-4969	TOTAL	
23-214	4969-4970	 	
23-215	4970-4971	|	
23-216	4971-4972	 	
23-217	4972-4975	awk	
23-218	4975-4976	 	
23-219	4976-4977	'	
23-220	4977-4978	{	
23-221	4978-4983	print	
23-222	4983-4984	 	
23-223	4984-4986	$1	
23-224	4986-4987	/	
23-225	4987-4994	1048576	
23-226	4994-4995	 	
23-227	4995-4996	"	
23-228	4996-4997	 	
23-229	4997-4999	MB	
23-230	4999-5000	"	
23-231	5000-5001	}	
23-232	5001-5002	'	
23-233	5002-5003	`	
23-234	5004-5008	Note	
23-235	5008-5009	:	
23-236	5010-5017	Perhaps	
23-237	5018-5023	there	
23-238	5024-5026	is	
23-239	5027-5028	a	
23-240	5029-5033	more	
23-241	5034-5041	precise	
23-242	5042-5045	way	
23-243	5046-5048	to	
23-244	5049-5058	calculate	
23-245	5059-5060	/	
23-246	5061-5068	measure	
23-247	5069-5072	how	
23-248	5073-5078	large	
23-249	5079-5081	of	
23-250	5082-5083	a	
23-251	5084-5096	vm.kmem_size	
23-252	5097-5104	setting	
23-253	5105-5108	can	
23-254	5109-5111	be	
23-255	5112-5116	used	
23-256	5117-5121	with	
23-257	5122-5123	a	
23-258	5124-5134	particular	
23-259	5135-5141	kernel	
23-260	5141-5142	,	
23-261	5143-5146	but	
23-262	5147-5150	the	
23-263	5151-5158	authors	
23-264	5159-5161	of	
23-265	5162-5166	this	
23-266	5167-5171	wiki	
23-267	5172-5174	do	
23-268	5175-5178	not	
23-269	5179-5183	know	
23-270	5184-5186	it	
23-271	5186-5187	.	

#Text=Experimentation does work.
24-1	5188-5203	Experimentation	
24-2	5204-5208	does	
24-3	5209-5213	work	
24-4	5213-5214	.	

#Text=However, if you set vm.kmem_size too high in loader.conf, the kernel will panic on boot.
25-1	5215-5222	However	
25-2	5222-5223	,	
25-3	5224-5226	if	
25-4	5227-5230	you	
25-5	5231-5234	set	
25-6	5235-5247	vm.kmem_size	
25-7	5248-5251	too	
25-8	5252-5256	high	
25-9	5257-5259	in	
25-10	5260-5271	loader.conf	
25-11	5271-5272	,	
25-12	5273-5276	the	
25-13	5277-5283	kernel	
25-14	5284-5288	will	
25-15	5289-5294	panic	
25-16	5295-5297	on	
25-17	5298-5302	boot	
25-18	5302-5303	.	

#Text=You can fix this by dropping to the boot loader prompt and typing set vm.kmem_size="512M" (or a similar smaller number known to work.)
26-1	5304-5307	You	
26-2	5308-5311	can	
26-3	5312-5315	fix	
26-4	5316-5320	this	
26-5	5321-5323	by	
26-6	5324-5332	dropping	
26-7	5333-5335	to	
26-8	5336-5339	the	
26-9	5340-5344	boot	
26-10	5345-5351	loader	
26-11	5352-5358	prompt	
26-12	5359-5362	and	
26-13	5363-5369	typing	
26-14	5370-5373	set	
26-15	5373-5374	 	
26-16	5374-5386	vm.kmem_size	
26-17	5386-5387	=	
26-18	5387-5388	"	
26-19	5388-5392	512M	
26-20	5392-5393	"	
26-21	5394-5395	(	
26-22	5395-5397	or	
26-23	5398-5399	a	
26-24	5400-5407	similar	
26-25	5408-5415	smaller	
26-26	5416-5422	number	
26-27	5423-5428	known	
26-28	5429-5431	to	
26-29	5432-5436	work	
26-30	5436-5437	.	
26-31	5437-5438	)	

#Text=The vm.kmem_size_max setting is not used directly during the system operation (i.e. it is not a limit which kmem can "grow" into) but for initial autoconfiguration of various system settings, the most important of which for this discussion is the ARC size.
27-1	5439-5442	The	
27-2	5443-5459	vm.kmem_size_max	
27-3	5460-5467	setting	
27-4	5468-5470	is	
27-5	5471-5474	not	
27-6	5475-5479	used	
27-7	5480-5488	directly	
27-8	5489-5495	during	
27-9	5496-5499	the	
27-10	5500-5506	system	
27-11	5507-5516	operation	
27-12	5517-5518	(	
27-13	5518-5521	i.e	
27-14	5521-5522	.	
27-15	5523-5525	it	
27-16	5526-5528	is	
27-17	5529-5532	not	
27-18	5533-5534	a	
27-19	5535-5540	limit	
27-20	5541-5546	which	
27-21	5547-5551	kmem	
27-22	5552-5555	can	
27-23	5556-5557	"	
27-24	5557-5561	grow	
27-25	5561-5562	"	
27-26	5563-5567	into	
27-27	5567-5568	)	
27-28	5569-5572	but	
27-29	5573-5576	for	
27-30	5577-5584	initial	
27-31	5585-5602	autoconfiguration	
27-32	5603-5605	of	
27-33	5606-5613	various	
27-34	5614-5620	system	
27-35	5621-5629	settings	
27-36	5629-5630	,	
27-37	5631-5634	the	
27-38	5635-5639	most	
27-39	5640-5649	important	
27-40	5650-5652	of	
27-41	5653-5658	which	
27-42	5659-5662	for	
27-43	5663-5667	this	
27-44	5668-5678	discussion	
27-45	5679-5681	is	
27-46	5682-5685	the	
27-47	5686-5689	ARC	
27-48	5690-5694	size	
27-49	5694-5695	.	

#Text=If kmem_size and arc_max are tuned manually, kmem_size_max will be ignored, but it is still required to be set.
28-1	5696-5698	If	
28-2	5699-5708	kmem_size	
28-3	5709-5712	and	
28-4	5713-5720	arc_max	
28-5	5721-5724	are	
28-6	5725-5730	tuned	
28-7	5731-5739	manually	
28-8	5739-5740	,	
28-9	5741-5754	kmem_size_max	
28-10	5755-5759	will	
28-11	5760-5762	be	
28-12	5763-5770	ignored	
28-13	5770-5771	,	
28-14	5772-5775	but	
28-15	5776-5778	it	
28-16	5779-5781	is	
28-17	5782-5787	still	
28-18	5788-5796	required	
28-19	5797-5799	to	
28-20	5800-5802	be	
28-21	5803-5806	set	
28-22	5806-5807	.	

#Text=The issue of kernel memory exhaustion is a complex one, involving the interaction between disk speeds, application loads and the special caching ZFS does.
29-1	5808-5811	The	
29-2	5812-5817	issue	
29-3	5818-5820	of	
29-4	5821-5827	kernel	
29-5	5828-5834	memory	
29-6	5835-5845	exhaustion	
29-7	5846-5848	is	
29-8	5849-5850	a	
29-9	5851-5858	complex	
29-10	5859-5862	one	
29-11	5862-5863	,	
29-12	5864-5873	involving	
29-13	5874-5877	the	
29-14	5878-5889	interaction	
29-15	5890-5897	between	
29-16	5898-5902	disk	
29-17	5903-5909	speeds	
29-18	5909-5910	,	
29-19	5911-5922	application	
29-20	5923-5928	loads	
29-21	5929-5932	and	
29-22	5933-5936	the	
29-23	5937-5944	special	
29-24	5945-5952	caching	
29-25	5953-5956	ZFS	
29-26	5957-5961	does	
29-27	5961-5962	.	

#Text=Faster drives will write the cached data faster but will also fill the caches up faster.
30-1	5963-5969	Faster	
30-2	5970-5976	drives	
30-3	5977-5981	will	
30-4	5982-5987	write	
30-5	5988-5991	the	
30-6	5992-5998	cached	
30-7	5999-6003	data	
30-8	6004-6010	faster	
30-9	6011-6014	but	
30-10	6015-6019	will	
30-11	6020-6024	also	
30-12	6025-6029	fill	
30-13	6030-6033	the	
30-14	6034-6040	caches	
30-15	6041-6043	up	
30-16	6044-6050	faster	
30-17	6050-6051	.	

#Text=Generally, larger and faster drives will need more memory for ZFS.
31-1	6052-6061	Generally	
31-2	6061-6062	,	
31-3	6063-6069	larger	
31-4	6070-6073	and	
31-5	6074-6080	faster	
31-6	6081-6087	drives	
31-7	6088-6092	will	
31-8	6093-6097	need	
31-9	6098-6102	more	
31-10	6103-6109	memory	
31-11	6110-6113	for	
31-12	6114-6117	ZFS	
31-13	6117-6118	.	

#Text=To increase performance, you may increase kern.maxvnodes (in /etc/sysctl.conf) way up if you have the RAM for it (e.g. 400000 for a 2GB system).
32-1	6119-6121	To	
32-2	6122-6130	increase	
32-3	6131-6142	performance	
32-4	6142-6143	,	
32-5	6144-6147	you	
32-6	6148-6151	may	
32-7	6152-6160	increase	
32-8	6161-6175	kern.maxvnodes	
32-9	6176-6177	(	
32-10	6177-6179	in	
32-11	6180-6181	/	
32-12	6181-6184	etc	
32-13	6184-6185	/	
32-14	6185-6196	sysctl.conf	
32-15	6196-6197	)	
32-16	6198-6201	way	
32-17	6202-6204	up	
32-18	6205-6207	if	
32-19	6208-6211	you	
32-20	6212-6216	have	
32-21	6217-6220	the	
32-22	6221-6224	RAM	
32-23	6225-6228	for	
32-24	6229-6231	it	
32-25	6232-6233	(	
32-26	6233-6236	e.g	
32-27	6236-6237	.	
32-28	6238-6244	400000	
32-29	6245-6248	for	
32-30	6249-6250	a	
32-31	6251-6254	2GB	
32-32	6255-6261	system	
32-33	6261-6262	)	
32-34	6262-6263	.	

#Text=On i386, keep an eye on vfs.numvnodes during production to see where it stabilizes.
33-1	6264-6266	On	
33-2	6267-6271	i386	
33-3	6271-6272	,	
33-4	6273-6277	keep	
33-5	6278-6280	an	
33-6	6281-6284	eye	
33-7	6285-6287	on	
33-8	6288-6301	vfs.numvnodes	
33-9	6302-6308	during	
33-10	6309-6319	production	
33-11	6320-6322	to	
33-12	6323-6326	see	
33-13	6327-6332	where	
33-14	6333-6335	it	
33-15	6336-6346	stabilizes	
33-16	6346-6347	.	

#Text=(AMD64 uses direct mapping for vnodes, so you don't have to worry about address space for vnodes on this architecture).
#Text=amd64
#Text=NOTE (gcooper): this blanket statement is far from true 100% of the time, depending on how the system with ZFS is being used.
34-1	6348-6349	(	
34-2	6349-6354	AMD64	
34-3	6355-6359	uses	
34-4	6360-6366	direct	
34-5	6367-6374	mapping	
34-6	6375-6378	for	
34-7	6379-6385	vnodes	
34-8	6385-6386	,	
34-9	6387-6389	so	
34-10	6390-6393	you	
34-11	6394-6399	don't	
34-12	6400-6404	have	
34-13	6405-6407	to	
34-14	6408-6413	worry	
34-15	6414-6419	about	
34-16	6420-6427	address	
34-17	6428-6433	space	
34-18	6434-6437	for	
34-19	6438-6444	vnodes	
34-20	6445-6447	on	
34-21	6448-6452	this	
34-22	6453-6465	architecture	
34-23	6465-6466	)	
34-24	6466-6467	.	
34-25	6468-6473	amd64	
34-26	6474-6478	NOTE	
34-27	6479-6480	(	
34-28	6480-6487	gcooper	
34-29	6487-6488	)	
34-30	6488-6489	:	
34-31	6490-6494	this	
34-32	6495-6502	blanket	
34-33	6503-6512	statement	
34-34	6513-6515	is	
34-35	6516-6519	far	
34-36	6520-6524	from	
34-37	6525-6529	true	
34-38	6530-6534	100%	
34-39	6535-6537	of	
34-40	6538-6541	the	
34-41	6542-6546	time	
34-42	6546-6547	,	
34-43	6548-6557	depending	
34-44	6558-6560	on	
34-45	6561-6564	how	
34-46	6565-6568	the	
34-47	6569-6575	system	
34-48	6576-6580	with	
34-49	6581-6584	ZFS	
34-50	6585-6587	is	
34-51	6588-6593	being	
34-52	6594-6598	used	
34-53	6598-6599	.	

#Text=FreeBSD 7.2+ has improved kernel memory allocation strategy and no tuning may be necessary on systems with more than 2 GB of RAM.
35-1	6600-6607	FreeBSD	
35-2	6608-6611	7.2	
35-3	6611-6612	+	
35-4	6613-6616	has	
35-5	6617-6625	improved	
35-6	6626-6632	kernel	
35-7	6633-6639	memory	
35-8	6640-6650	allocation	
35-9	6651-6659	strategy	
35-10	6660-6663	and	
35-11	6664-6666	no	
35-12	6667-6673	tuning	
35-13	6674-6677	may	
35-14	6678-6680	be	
35-15	6681-6690	necessary	
35-16	6691-6693	on	
35-17	6694-6701	systems	
35-18	6702-6706	with	
35-19	6707-6711	more	
35-20	6712-6716	than	
35-21	6717-6718	2	
35-22	6718-6719	 	
35-23	6719-6721	GB	
35-24	6722-6724	of	
35-25	6725-6728	RAM	
35-26	6728-6729	.	

#Text=Generic ARC discussion
#Text=The value for vfs.zfs.arc_max needs to be smaller than the value for vm.kmem_size (not only ZFS is using the kmem).
36-1	6730-6737	Generic	
36-2	6738-6741	ARC	
36-3	6742-6752	discussion	
36-4	6753-6756	The	
36-5	6757-6762	value	
36-6	6763-6766	for	
36-7	6767-6782	vfs.zfs.arc_max	
36-8	6783-6788	needs	
36-9	6789-6791	to	
36-10	6792-6794	be	
36-11	6795-6802	smaller	
36-12	6803-6807	than	
36-13	6808-6811	the	
36-14	6812-6817	value	
36-15	6818-6821	for	
36-16	6822-6834	vm.kmem_size	
36-17	6835-6836	(	
36-18	6836-6839	not	
36-19	6840-6844	only	
36-20	6845-6848	ZFS	
36-21	6849-6851	is	
36-22	6852-6857	using	
36-23	6858-6861	the	
36-24	6862-6866	kmem	
36-25	6866-6867	)	
36-26	6867-6868	.	

#Text=To monitor the ARC, you should install the sysutils/zfs-stats port;
#Text=the port is an evolution of the arc_stat.pl script available in Solaris that was ported to FreeBSD by FreeBSD contributor, jhell.
37-1	6869-6871	To	
37-2	6872-6879	monitor	
37-3	6880-6883	the	
37-4	6884-6887	ARC	
37-5	6887-6888	,	
37-6	6889-6892	you	
37-7	6893-6899	should	
37-8	6900-6907	install	
37-9	6908-6911	the	
37-10	6912-6920	sysutils	
37-11	6920-6921	/	
37-12	6921-6930	zfs-stats	
37-13	6931-6935	port	
37-14	6935-6936	;	
37-15	6937-6940	the	
37-16	6941-6945	port	
37-17	6946-6948	is	
37-18	6949-6951	an	
37-19	6952-6961	evolution	
37-20	6962-6964	of	
37-21	6965-6968	the	
37-22	6969-6980	arc_stat.pl	
37-23	6981-6987	script	
37-24	6988-6997	available	
37-25	6998-7000	in	
37-26	7001-7008	Solaris	
37-27	7009-7013	that	
37-28	7014-7017	was	
37-29	7018-7024	ported	
37-30	7025-7027	to	
37-31	7028-7035	FreeBSD	
37-32	7036-7038	by	
37-33	7039-7046	FreeBSD	
37-34	7047-7058	contributor	
37-35	7058-7059	,	
37-36	7060-7065	jhell	
37-37	7065-7066	.	

#Text=To improve the random read performance, a separate L2ARC device can be used (zpool add <pool> cache <device>).
38-1	7067-7069	To	
38-2	7070-7077	improve	
38-3	7078-7081	the	
38-4	7082-7088	random	
38-5	7089-7093	read	
38-6	7094-7105	performance	
38-7	7105-7106	,	
38-8	7107-7108	a	
38-9	7109-7117	separate	
38-10	7118-7123	L2ARC	
38-11	7124-7130	device	
38-12	7131-7134	can	
38-13	7135-7137	be	
38-14	7138-7142	used	
38-15	7143-7144	(	
38-16	7144-7149	zpool	
38-17	7150-7153	add	
38-18	7154-7155	<	
38-19	7155-7159	pool	
38-20	7159-7160	>	
38-21	7161-7166	cache	
38-22	7167-7168	<	
38-23	7168-7174	device	
38-24	7174-7175	>	
38-25	7175-7176	)	
38-26	7176-7177	.	

#Text=A cheap solution is to add an USB memory stick (see http://www.leidinger.net/blog/2010/02/10/making-zfs-faster/).
39-1	7178-7179	A	
39-2	7180-7185	cheap	
39-3	7186-7194	solution	
39-4	7195-7197	is	
39-5	7198-7200	to	
39-6	7201-7204	add	
39-7	7205-7207	an	
39-8	7208-7211	USB	
39-9	7212-7218	memory	
39-10	7219-7224	stick	
39-11	7225-7226	(	
39-12	7226-7229	see	
39-13	7230-7234	http	
39-14	7234-7235	:	
39-15	7235-7236	/	
39-16	7236-7237	/	
39-17	7237-7254	www.leidinger.net	
39-18	7254-7255	/	
39-19	7255-7259	blog	
39-20	7259-7260	/	
39-21	7260-7264	2010	
39-22	7264-7265	/	
39-23	7265-7267	02	
39-24	7267-7268	/	
39-25	7268-7270	10	
39-26	7270-7271	/	
39-27	7271-7288	making-zfs-faster	
39-28	7288-7289	/	
39-29	7289-7290	)	
39-30	7290-7291	.	

#Text=The high performance solution is to add a SSD.
40-1	7292-7295	The	
40-2	7296-7300	high	
40-3	7301-7312	performance	
40-4	7313-7321	solution	
40-5	7322-7324	is	
40-6	7325-7327	to	
40-7	7328-7331	add	
40-8	7332-7333	a	
40-9	7334-7337	SSD	
40-10	7337-7338	.	

#Text=Using a L2ARC device will increase the amount of memory ZFS needs to allocate, see http://www.mail-archive.com/zfs-discuss@opensolaris.org/msg34674.html for more info.
41-1	7339-7344	Using	
41-2	7345-7346	a	
41-3	7347-7352	L2ARC	
41-4	7353-7359	device	
41-5	7360-7364	will	
41-6	7365-7373	increase	
41-7	7374-7377	the	
41-8	7378-7384	amount	
41-9	7385-7387	of	
41-10	7388-7394	memory	
41-11	7395-7398	ZFS	
41-12	7399-7404	needs	
41-13	7405-7407	to	
41-14	7408-7416	allocate	
41-15	7416-7417	,	
41-16	7418-7421	see	
41-17	7422-7426	http	
41-18	7426-7427	:	
41-19	7427-7428	/	
41-20	7428-7429	/	
41-21	7429-7449	www.mail-archive.com	
41-22	7449-7450	/	
41-23	7450-7461	zfs-discuss	
41-24	7461-7462	@	
41-25	7462-7477	opensolaris.org	
41-26	7477-7478	/	
41-27	7478-7486	msg34674	
41-28	7486-7487	.	
41-29	7487-7491	html	
41-30	7492-7495	for	
41-31	7496-7500	more	
41-32	7501-7505	info	
41-33	7505-7506	.	

#Text=L2ARC discussion
#Text=ZFS has the ability to extend the ARC with one or more L2ARC devices, which provides the best benefit for random read workloads.
42-1	7507-7512	L2ARC	
42-2	7513-7523	discussion	
42-3	7524-7527	ZFS	
42-4	7528-7531	has	
42-5	7532-7535	the	
42-6	7536-7543	ability	
42-7	7544-7546	to	
42-8	7547-7553	extend	
42-9	7554-7557	the	
42-10	7558-7561	ARC	
42-11	7562-7566	with	
42-12	7567-7570	one	
42-13	7571-7573	or	
42-14	7574-7578	more	
42-15	7579-7584	L2ARC	
42-16	7585-7592	devices	
42-17	7592-7593	,	
42-18	7594-7599	which	
42-19	7600-7608	provides	
42-20	7609-7612	the	
42-21	7613-7617	best	
42-22	7618-7625	benefit	
42-23	7626-7629	for	
42-24	7630-7636	random	
42-25	7637-7641	read	
42-26	7642-7651	workloads	
42-27	7651-7652	.	

#Text=These L2ARC devices should be faster and/or lower latency than the storage pool.
43-1	7653-7658	These	
43-2	7659-7664	L2ARC	
43-3	7665-7672	devices	
43-4	7673-7679	should	
43-5	7680-7682	be	
43-6	7683-7689	faster	
43-7	7690-7693	and	
43-8	7693-7694	/	
43-9	7694-7696	or	
43-10	7697-7702	lower	
43-11	7703-7710	latency	
43-12	7711-7715	than	
43-13	7716-7719	the	
43-14	7720-7727	storage	
43-15	7728-7732	pool	
43-16	7732-7733	.	

#Text=Generally speaking this limits the useful choices to flash based devices.
44-1	7734-7743	Generally	
44-2	7744-7752	speaking	
44-3	7753-7757	this	
44-4	7758-7764	limits	
44-5	7765-7768	the	
44-6	7769-7775	useful	
44-7	7776-7783	choices	
44-8	7784-7786	to	
44-9	7787-7792	flash	
44-10	7793-7798	based	
44-11	7799-7806	devices	
44-12	7806-7807	.	

#Text=In very large pools the ability to have devices faster than the pool may be problematic.
45-1	7808-7810	In	
45-2	7811-7815	very	
45-3	7816-7821	large	
45-4	7822-7827	pools	
45-5	7828-7831	the	
45-6	7832-7839	ability	
45-7	7840-7842	to	
45-8	7843-7847	have	
45-9	7848-7855	devices	
45-10	7856-7862	faster	
45-11	7863-7867	than	
45-12	7868-7871	the	
45-13	7872-7876	pool	
45-14	7877-7880	may	
45-15	7881-7883	be	
45-16	7884-7895	problematic	
45-17	7895-7896	.	

#Text=In smaller pools it may be tempting to use a spinning disk as a dedicated L2ARC device.
46-1	7897-7899	In	
46-2	7900-7907	smaller	
46-3	7908-7913	pools	
46-4	7914-7916	it	
46-5	7917-7920	may	
46-6	7921-7923	be	
46-7	7924-7932	tempting	
46-8	7933-7935	to	
46-9	7936-7939	use	
46-10	7940-7941	a	
46-11	7942-7950	spinning	
46-12	7951-7955	disk	
46-13	7956-7958	as	
46-14	7959-7960	a	
46-15	7961-7970	dedicated	
46-16	7971-7976	L2ARC	
46-17	7977-7983	device	
46-18	7983-7984	.	

#Text=Generally this will result in lower pool performance (and definitely capacity) than if it was just placed in the pool.
47-1	7985-7994	Generally	
47-2	7995-7999	this	
47-3	8000-8004	will	
47-4	8005-8011	result	
47-5	8012-8014	in	
47-6	8015-8020	lower	
47-7	8021-8025	pool	
47-8	8026-8037	performance	
47-9	8038-8039	(	
47-10	8039-8042	and	
47-11	8043-8053	definitely	
47-12	8054-8062	capacity	
47-13	8062-8063	)	
47-14	8064-8068	than	
47-15	8069-8071	if	
47-16	8072-8074	it	
47-17	8075-8078	was	
47-18	8079-8083	just	
47-19	8084-8090	placed	
47-20	8091-8093	in	
47-21	8094-8097	the	
47-22	8098-8102	pool	
47-23	8102-8103	.	

#Text=There may be scenarios in lower memory systems where a single 15K SAS disk can improve the performance of a small pool of 5.4k or 7.2 drives, but this is not a typical case.
48-1	8104-8109	There	
48-2	8110-8113	may	
48-3	8114-8116	be	
48-4	8117-8126	scenarios	
48-5	8127-8129	in	
48-6	8130-8135	lower	
48-7	8136-8142	memory	
48-8	8143-8150	systems	
48-9	8151-8156	where	
48-10	8157-8158	a	
48-11	8159-8165	single	
48-12	8166-8169	15K	
48-13	8170-8173	SAS	
48-14	8174-8178	disk	
48-15	8179-8182	can	
48-16	8183-8190	improve	
48-17	8191-8194	the	
48-18	8195-8206	performance	
48-19	8207-8209	of	
48-20	8210-8211	a	
48-21	8212-8217	small	
48-22	8218-8222	pool	
48-23	8223-8225	of	
48-24	8226-8230	5.4k	
48-25	8231-8233	or	
48-26	8234-8237	7.2	
48-27	8238-8244	drives	
48-28	8244-8245	,	
48-29	8246-8249	but	
48-30	8250-8254	this	
48-31	8255-8257	is	
48-32	8258-8261	not	
48-33	8262-8263	a	
48-34	8264-8271	typical	
48-35	8272-8276	case	
48-36	8276-8277	.	

#Text=By default the L2ARC does not attempt to cache prefetched/streaming workloads, on the assumption that most data of this type is sequential and the combined throughput of your pool disks exceeds the throughput of the L2ARC devices, and therefore, this workload is best left for the pool disks to serve.
49-1	8278-8280	By	
49-2	8281-8288	default	
49-3	8289-8292	the	
49-4	8293-8298	L2ARC	
49-5	8299-8303	does	
49-6	8304-8307	not	
49-7	8308-8315	attempt	
49-8	8316-8318	to	
49-9	8319-8324	cache	
49-10	8325-8335	prefetched	
49-11	8335-8336	/	
49-12	8336-8345	streaming	
49-13	8346-8355	workloads	
49-14	8355-8356	,	
49-15	8357-8359	on	
49-16	8360-8363	the	
49-17	8364-8374	assumption	
49-18	8375-8379	that	
49-19	8380-8384	most	
49-20	8385-8389	data	
49-21	8390-8392	of	
49-22	8393-8397	this	
49-23	8398-8402	type	
49-24	8403-8405	is	
49-25	8406-8416	sequential	
49-26	8417-8420	and	
49-27	8421-8424	the	
49-28	8425-8433	combined	
49-29	8434-8444	throughput	
49-30	8445-8447	of	
49-31	8448-8452	your	
49-32	8453-8457	pool	
49-33	8458-8463	disks	
49-34	8464-8471	exceeds	
49-35	8472-8475	the	
49-36	8476-8486	throughput	
49-37	8487-8489	of	
49-38	8490-8493	the	
49-39	8494-8499	L2ARC	
49-40	8500-8507	devices	
49-41	8507-8508	,	
49-42	8509-8512	and	
49-43	8513-8522	therefore	
49-44	8522-8523	,	
49-45	8524-8528	this	
49-46	8529-8537	workload	
49-47	8538-8540	is	
49-48	8541-8545	best	
49-49	8546-8550	left	
49-50	8551-8554	for	
49-51	8555-8558	the	
49-52	8559-8563	pool	
49-53	8564-8569	disks	
49-54	8570-8572	to	
49-55	8573-8578	serve	
49-56	8578-8579	.	

#Text=This is usually the case.
50-1	8580-8584	This	
50-2	8585-8587	is	
50-3	8588-8595	usually	
50-4	8596-8599	the	
50-5	8600-8604	case	
50-6	8604-8605	.	

#Text=If you believe otherwise (number of L2ARC devices X their max throughput > number of pool disks X their max throughput, or you are not doing large amounts of sequential access), then this can be toggled with the following sysctl: vfs.zfs.l2arc_noprefetchThe default value of 1 does not allow caching of streaming and/or sequential workloads, and will not read from L2ARC when prefetching blocks.
51-1	8606-8608	If	
51-2	8609-8612	you	
51-3	8613-8620	believe	
51-4	8621-8630	otherwise	
51-5	8631-8632	(	
51-6	8632-8638	number	
51-7	8639-8641	of	
51-8	8642-8647	L2ARC	
51-9	8648-8655	devices	
51-10	8656-8657	X	
51-11	8658-8663	their	
51-12	8664-8667	max	
51-13	8668-8678	throughput	
51-14	8679-8680	>	
51-15	8681-8687	number	
51-16	8688-8690	of	
51-17	8691-8695	pool	
51-18	8696-8701	disks	
51-19	8702-8703	X	
51-20	8704-8709	their	
51-21	8710-8713	max	
51-22	8714-8724	throughput	
51-23	8724-8725	,	
51-24	8726-8728	or	
51-25	8729-8732	you	
51-26	8733-8736	are	
51-27	8737-8740	not	
51-28	8741-8746	doing	
51-29	8747-8752	large	
51-30	8753-8760	amounts	
51-31	8761-8763	of	
51-32	8764-8774	sequential	
51-33	8775-8781	access	
51-34	8781-8782	)	
51-35	8782-8783	,	
51-36	8784-8788	then	
51-37	8789-8793	this	
51-38	8794-8797	can	
51-39	8798-8800	be	
51-40	8801-8808	toggled	
51-41	8809-8813	with	
51-42	8814-8817	the	
51-43	8818-8827	following	
51-44	8828-8834	sysctl	
51-45	8834-8835	:	
51-46	8836-8863	vfs.zfs.l2arc_noprefetchThe	
51-47	8864-8871	default	
51-48	8872-8877	value	
51-49	8878-8880	of	
51-50	8881-8882	1	
51-51	8883-8887	does	
51-52	8888-8891	not	
51-53	8892-8897	allow	
51-54	8898-8905	caching	
51-55	8906-8908	of	
51-56	8909-8918	streaming	
51-57	8919-8922	and	
51-58	8922-8923	/	
51-59	8923-8925	or	
51-60	8926-8936	sequential	
51-61	8937-8946	workloads	
51-62	8946-8947	,	
51-63	8948-8951	and	
51-64	8952-8956	will	
51-65	8957-8960	not	
51-66	8961-8965	read	
51-67	8966-8970	from	
51-68	8971-8976	L2ARC	
51-69	8977-8981	when	
51-70	8982-8993	prefetching	
51-71	8994-9000	blocks	
51-72	9000-9001	.	

#Text=Switching it to 0 will allow prefetched/streaming reads to be cached, and may significantly improve performance if you are storing many small files in a large directory hierarchy (since many metadata blocks are read via the prefetcher and would ordinarily always be read from pool disks).
52-1	9002-9011	Switching	
52-2	9012-9014	it	
52-3	9015-9017	to	
52-4	9018-9019	0	
52-5	9020-9024	will	
52-6	9025-9030	allow	
52-7	9031-9041	prefetched	
52-8	9041-9042	/	
52-9	9042-9051	streaming	
52-10	9052-9057	reads	
52-11	9058-9060	to	
52-12	9061-9063	be	
52-13	9064-9070	cached	
52-14	9070-9071	,	
52-15	9072-9075	and	
52-16	9076-9079	may	
52-17	9080-9093	significantly	
52-18	9094-9101	improve	
52-19	9102-9113	performance	
52-20	9114-9116	if	
52-21	9117-9120	you	
52-22	9121-9124	are	
52-23	9125-9132	storing	
52-24	9133-9137	many	
52-25	9138-9143	small	
52-26	9144-9149	files	
52-27	9150-9152	in	
52-28	9153-9154	a	
52-29	9155-9160	large	
52-30	9161-9170	directory	
52-31	9171-9180	hierarchy	
52-32	9181-9182	(	
52-33	9182-9187	since	
52-34	9188-9192	many	
52-35	9193-9201	metadata	
52-36	9202-9208	blocks	
52-37	9209-9212	are	
52-38	9213-9217	read	
52-39	9218-9221	via	
52-40	9222-9225	the	
52-41	9226-9236	prefetcher	
52-42	9237-9240	and	
52-43	9241-9246	would	
52-44	9247-9257	ordinarily	
52-45	9258-9264	always	
52-46	9265-9267	be	
52-47	9268-9272	read	
52-48	9273-9277	from	
52-49	9278-9282	pool	
52-50	9283-9288	disks	
52-51	9288-9289	)	
52-52	9289-9290	.	

#Text=The default throttling of loading the L2ARC device is 8 Mbytes/sec, on the assumption that the L2ARC is warming up from a random read workload from spinning disks, for which 8 Mbytes/sec is usually more than the spinning disks can provide.
53-1	9291-9294	The	
53-2	9295-9302	default	
53-3	9303-9313	throttling	
53-4	9314-9316	of	
53-5	9317-9324	loading	
53-6	9325-9328	the	
53-7	9329-9334	L2ARC	
53-8	9335-9341	device	
53-9	9342-9344	is	
53-10	9345-9346	8	
53-11	9347-9353	Mbytes	
53-12	9353-9354	/	
53-13	9354-9357	sec	
53-14	9357-9358	,	
53-15	9359-9361	on	
53-16	9362-9365	the	
53-17	9366-9376	assumption	
53-18	9377-9381	that	
53-19	9382-9385	the	
53-20	9386-9391	L2ARC	
53-21	9392-9394	is	
53-22	9395-9402	warming	
53-23	9403-9405	up	
53-24	9406-9410	from	
53-25	9411-9412	a	
53-26	9413-9419	random	
53-27	9420-9424	read	
53-28	9425-9433	workload	
53-29	9434-9438	from	
53-30	9439-9447	spinning	
53-31	9448-9453	disks	
53-32	9453-9454	,	
53-33	9455-9458	for	
53-34	9459-9464	which	
53-35	9465-9466	8	
53-36	9467-9473	Mbytes	
53-37	9473-9474	/	
53-38	9474-9477	sec	
53-39	9478-9480	is	
53-40	9481-9488	usually	
53-41	9489-9493	more	
53-42	9494-9498	than	
53-43	9499-9502	the	
53-44	9503-9511	spinning	
53-45	9512-9517	disks	
53-46	9518-9521	can	
53-47	9522-9529	provide	
53-48	9529-9530	.	

#Text=For example, at a 4 Kbyte I/O size, this is 2048 random disk IOPS, which may take at least 20 pool disks to drive.
54-1	9531-9534	For	
54-2	9535-9542	example	
54-3	9542-9543	,	
54-4	9544-9546	at	
54-5	9547-9548	a	
54-6	9549-9550	4	
54-7	9551-9556	Kbyte	
54-8	9557-9558	I	
54-9	9558-9559	/	
54-10	9559-9560	O	
54-11	9561-9565	size	
54-12	9565-9566	,	
54-13	9567-9571	this	
54-14	9572-9574	is	
54-15	9575-9579	2048	
54-16	9580-9586	random	
54-17	9587-9591	disk	
54-18	9592-9596	IOPS	
54-19	9596-9597	,	
54-20	9598-9603	which	
54-21	9604-9607	may	
54-22	9608-9612	take	
54-23	9613-9615	at	
54-24	9616-9621	least	
54-25	9622-9624	20	
54-26	9625-9629	pool	
54-27	9630-9635	disks	
54-28	9636-9638	to	
54-29	9639-9644	drive	
54-30	9644-9645	.	

#Text=Should the L2ARC throttling be increased from 8 Mbytes, it would make no difference in many configurations, which cannot provide more random IOPS.
55-1	9646-9652	Should	
55-2	9653-9656	the	
55-3	9657-9662	L2ARC	
55-4	9663-9673	throttling	
55-5	9674-9676	be	
55-6	9677-9686	increased	
55-7	9687-9691	from	
55-8	9692-9693	8	
55-9	9694-9700	Mbytes	
55-10	9700-9701	,	
55-11	9702-9704	it	
55-12	9705-9710	would	
55-13	9711-9715	make	
55-14	9716-9718	no	
55-15	9719-9729	difference	
55-16	9730-9732	in	
55-17	9733-9737	many	
55-18	9738-9752	configurations	
55-19	9752-9753	,	
55-20	9754-9759	which	
55-21	9760-9766	cannot	
55-22	9767-9774	provide	
55-23	9775-9779	more	
55-24	9780-9786	random	
55-25	9787-9791	IOPS	
55-26	9791-9792	.	

#Text=The downside of increasing the throttling is CPU consumption: the L2ARC periodically scans the ARC to find buffers to cache, based on the throttling size.
56-1	9793-9796	The	
56-2	9797-9805	downside	
56-3	9806-9808	of	
56-4	9809-9819	increasing	
56-5	9820-9823	the	
56-6	9824-9834	throttling	
56-7	9835-9837	is	
56-8	9838-9841	CPU	
56-9	9842-9853	consumption	
56-10	9853-9854	:	
56-11	9855-9858	the	
56-12	9859-9864	L2ARC	
56-13	9865-9877	periodically	
56-14	9878-9883	scans	
56-15	9884-9887	the	
56-16	9888-9891	ARC	
56-17	9892-9894	to	
56-18	9895-9899	find	
56-19	9900-9907	buffers	
56-20	9908-9910	to	
56-21	9911-9916	cache	
56-22	9916-9917	,	
56-23	9918-9923	based	
56-24	9924-9926	on	
56-25	9927-9930	the	
56-26	9931-9941	throttling	
56-27	9942-9946	size	
56-28	9946-9947	.	

#Text=If you increase the throttling but the pool disks cannot keep up, you burn CPU needlessly.
57-1	9948-9950	If	
57-2	9951-9954	you	
57-3	9955-9963	increase	
57-4	9964-9967	the	
57-5	9968-9978	throttling	
57-6	9979-9982	but	
57-7	9983-9986	the	
57-8	9987-9991	pool	
57-9	9992-9997	disks	
57-10	9998-10004	cannot	
57-11	10005-10009	keep	
57-12	10010-10012	up	
57-13	10012-10013	,	
57-14	10014-10017	you	
57-15	10018-10022	burn	
57-16	10023-10026	CPU	
57-17	10027-10037	needlessly	
57-18	10037-10038	.	

#Text=In extreme cases of tuning, this can consume an entire CPU for the ARC scan.
58-1	10039-10041	In	
58-2	10042-10049	extreme	
58-3	10050-10055	cases	
58-4	10056-10058	of	
58-5	10059-10065	tuning	
58-6	10065-10066	,	
58-7	10067-10071	this	
58-8	10072-10075	can	
58-9	10076-10083	consume	
58-10	10084-10086	an	
58-11	10087-10093	entire	
58-12	10094-10097	CPU	
58-13	10098-10101	for	
58-14	10102-10105	the	
58-15	10106-10109	ARC	
58-16	10110-10114	scan	
58-17	10114-10115	.	

#Text=If you are using the L2ARC in its typical use case: say, fewer than 30 pool disks, and caching a random read workload for ~4 Kbyte I/O which is mostly being pulled from the pool disks, then 8 Mbytes is usually sufficient.
59-1	10116-10118	If	
59-2	10119-10122	you	
59-3	10123-10126	are	
59-4	10127-10132	using	
59-5	10133-10136	the	
59-6	10137-10142	L2ARC	
59-7	10143-10145	in	
59-8	10146-10149	its	
59-9	10150-10157	typical	
59-10	10158-10161	use	
59-11	10162-10166	case	
59-12	10166-10167	:	
59-13	10168-10171	say	
59-14	10171-10172	,	
59-15	10173-10178	fewer	
59-16	10179-10183	than	
59-17	10184-10186	30	
59-18	10187-10191	pool	
59-19	10192-10197	disks	
59-20	10197-10198	,	
59-21	10199-10202	and	
59-22	10203-10210	caching	
59-23	10211-10212	a	
59-24	10213-10219	random	
59-25	10220-10224	read	
59-26	10225-10233	workload	
59-27	10234-10237	for	
59-28	10238-10239	~	
59-29	10239-10240	4	
59-30	10241-10246	Kbyte	
59-31	10247-10248	I	
59-32	10248-10249	/	
59-33	10249-10250	O	
59-34	10251-10256	which	
59-35	10257-10259	is	
59-36	10260-10266	mostly	
59-37	10267-10272	being	
59-38	10273-10279	pulled	
59-39	10280-10284	from	
59-40	10285-10288	the	
59-41	10289-10293	pool	
59-42	10294-10299	disks	
59-43	10299-10300	,	
59-44	10301-10305	then	
59-45	10306-10307	8	
59-46	10308-10314	Mbytes	
59-47	10315-10317	is	
59-48	10318-10325	usually	
59-49	10326-10336	sufficient	
59-50	10336-10337	.	

#Text=If you are not this typical use case: say, you are caching streaming workloads, or have several dozens of disks, then you may want to consider tuning the rate.
60-1	10338-10340	If	
60-2	10341-10344	you	
60-3	10345-10348	are	
60-4	10349-10352	not	
60-5	10353-10357	this	
60-6	10358-10365	typical	
60-7	10366-10369	use	
60-8	10370-10374	case	
60-9	10374-10375	:	
60-10	10376-10379	say	
60-11	10379-10380	,	
60-12	10381-10384	you	
60-13	10385-10388	are	
60-14	10389-10396	caching	
60-15	10397-10406	streaming	
60-16	10407-10416	workloads	
60-17	10416-10417	,	
60-18	10418-10420	or	
60-19	10421-10425	have	
60-20	10426-10433	several	
60-21	10434-10440	dozens	
60-22	10441-10443	of	
60-23	10444-10449	disks	
60-24	10449-10450	,	
60-25	10451-10455	then	
60-26	10456-10459	you	
60-27	10460-10463	may	
60-28	10464-10468	want	
60-29	10469-10471	to	
60-30	10472-10480	consider	
60-31	10481-10487	tuning	
60-32	10488-10491	the	
60-33	10492-10496	rate	
60-34	10496-10497	.	

#Text=Modern L2ARC devices (SSDs) can handle an order of magnitude higher than the default.
61-1	10498-10504	Modern	
61-2	10505-10510	L2ARC	
61-3	10511-10518	devices	
61-4	10519-10520	(	
61-5	10520-10524	SSDs	
61-6	10524-10525	)	
61-7	10526-10529	can	
61-8	10530-10536	handle	
61-9	10537-10539	an	
61-10	10540-10545	order	
61-11	10546-10548	of	
61-12	10549-10558	magnitude	
61-13	10559-10565	higher	
61-14	10566-10570	than	
61-15	10571-10574	the	
61-16	10575-10582	default	
61-17	10582-10583	.	

#Text=It can be tuned by setting the following sysctls: vfs.zfs.l2arc_write_max
#Text=vfs.zfs.l2arc_write_boostThe former value sets the runtime max that data will be loaded into L2ARC.
62-1	10584-10586	It	
62-2	10587-10590	can	
62-3	10591-10593	be	
62-4	10594-10599	tuned	
62-5	10600-10602	by	
62-6	10603-10610	setting	
62-7	10611-10614	the	
62-8	10615-10624	following	
62-9	10625-10632	sysctls	
62-10	10632-10633	:	
62-11	10634-10657	vfs.zfs.l2arc_write_max	
62-12	10658-10686	vfs.zfs.l2arc_write_boostThe	
62-13	10687-10693	former	
62-14	10694-10699	value	
62-15	10700-10704	sets	
62-16	10705-10708	the	
62-17	10709-10716	runtime	
62-18	10717-10720	max	
62-19	10721-10725	that	
62-20	10726-10730	data	
62-21	10731-10735	will	
62-22	10736-10738	be	
62-23	10739-10745	loaded	
62-24	10746-10750	into	
62-25	10751-10756	L2ARC	
62-26	10756-10757	.	

#Text=The latter can be used to accelerate the loading of a freshly booted system.
63-1	10758-10761	The	
63-2	10762-10768	latter	
63-3	10769-10772	can	
63-4	10773-10775	be	
63-5	10776-10780	used	
63-6	10781-10783	to	
63-7	10784-10794	accelerate	
63-8	10795-10798	the	
63-9	10799-10806	loading	
63-10	10807-10809	of	
63-11	10810-10811	a	
63-12	10812-10819	freshly	
63-13	10820-10826	booted	
63-14	10827-10833	system	
63-15	10833-10834	.	

#Text=Note that the same caveats apply about these sysctls and pool imports as the previous one.
64-1	10835-10839	Note	
64-2	10840-10844	that	
64-3	10845-10848	the	
64-4	10849-10853	same	
64-5	10854-10861	caveats	
64-6	10862-10867	apply	
64-7	10868-10873	about	
64-8	10874-10879	these	
64-9	10880-10887	sysctls	
64-10	10888-10891	and	
64-11	10892-10896	pool	
64-12	10897-10904	imports	
64-13	10905-10907	as	
64-14	10908-10911	the	
64-15	10912-10920	previous	
64-16	10921-10924	one	
64-17	10924-10925	.	

#Text=While you can improve the L2ARC warmup rate, keep an eye on increased CPU consumption due to scanning by the l2arc_feed_thread().
65-1	10926-10931	While	
65-2	10932-10935	you	
65-3	10936-10939	can	
65-4	10940-10947	improve	
65-5	10948-10951	the	
65-6	10952-10957	L2ARC	
65-7	10958-10964	warmup	
65-8	10965-10969	rate	
65-9	10969-10970	,	
65-10	10971-10975	keep	
65-11	10976-10978	an	
65-12	10979-10982	eye	
65-13	10983-10985	on	
65-14	10986-10995	increased	
65-15	10996-10999	CPU	
65-16	11000-11011	consumption	
65-17	11012-11015	due	
65-18	11016-11018	to	
65-19	11019-11027	scanning	
65-20	11028-11030	by	
65-21	11031-11034	the	
65-22	11035-11052	l2arc_feed_thread	
65-23	11052-11053	(	
65-24	11053-11054	)	
65-25	11054-11055	.	

#Text=Eg, use DTrace to profile on-CPU thread names (see DTrace One-Liners).
66-1	11056-11058	Eg	
66-2	11058-11059	,	
66-3	11060-11063	use	
66-4	11064-11070	DTrace	
66-5	11071-11073	to	
66-6	11074-11081	profile	
66-7	11082-11088	on-CPU	
66-8	11089-11095	thread	
66-9	11096-11101	names	
66-10	11102-11103	(	
66-11	11103-11106	see	
66-12	11107-11113	DTrace	
66-13	11114-11124	One-Liners	
66-14	11124-11125	)	
66-15	11125-11126	.	

#Text=The known caveats: There's no free lunch.
67-1	11127-11130	The	
67-2	11131-11136	known	
67-3	11137-11144	caveats	
67-4	11144-11145	:	
67-5	11146-11153	There's	
67-6	11154-11156	no	
67-7	11157-11161	free	
67-8	11162-11167	lunch	
67-9	11167-11168	.	

#Text=A properly tuned L2ARC will increase read performance, but it comes at the price of decreased write performance.
68-1	11169-11170	A	
68-2	11171-11179	properly	
68-3	11180-11185	tuned	
68-4	11186-11191	L2ARC	
68-5	11192-11196	will	
68-6	11197-11205	increase	
68-7	11206-11210	read	
68-8	11211-11222	performance	
68-9	11222-11223	,	
68-10	11224-11227	but	
68-11	11228-11230	it	
68-12	11231-11236	comes	
68-13	11237-11239	at	
68-14	11240-11243	the	
68-15	11244-11249	price	
68-16	11250-11252	of	
68-17	11253-11262	decreased	
68-18	11263-11268	write	
68-19	11269-11280	performance	
68-20	11280-11281	.	

#Text=The pool essentially magnifies writes by writing them to the pool as well as the L2ARC device.
69-1	11282-11285	The	
69-2	11286-11290	pool	
69-3	11291-11302	essentially	
69-4	11303-11312	magnifies	
69-5	11313-11319	writes	
69-6	11320-11322	by	
69-7	11323-11330	writing	
69-8	11331-11335	them	
69-9	11336-11338	to	
69-10	11339-11342	the	
69-11	11343-11347	pool	
69-12	11348-11350	as	
69-13	11351-11355	well	
69-14	11356-11358	as	
69-15	11359-11362	the	
69-16	11363-11368	L2ARC	
69-17	11369-11375	device	
69-18	11375-11376	.	

#Text=Another interesting effect that's been observed is a falloff in L2ARC performance when doing a streaming read from L2ARC while simultaneously doing a heavy write workload.
70-1	11377-11384	Another	
70-2	11385-11396	interesting	
70-3	11397-11403	effect	
70-4	11404-11410	that's	
70-5	11411-11415	been	
70-6	11416-11424	observed	
70-7	11425-11427	is	
70-8	11428-11429	a	
70-9	11430-11437	falloff	
70-10	11438-11440	in	
70-11	11441-11446	L2ARC	
70-12	11447-11458	performance	
70-13	11459-11463	when	
70-14	11464-11469	doing	
70-15	11470-11471	a	
70-16	11472-11481	streaming	
70-17	11482-11486	read	
70-18	11487-11491	from	
70-19	11492-11497	L2ARC	
70-20	11498-11503	while	
70-21	11504-11518	simultaneously	
70-22	11519-11524	doing	
70-23	11525-11526	a	
70-24	11527-11532	heavy	
70-25	11533-11538	write	
70-26	11539-11547	workload	
70-27	11547-11548	.	

#Text=My conjecture is that the write can cause cache thrashing but this hasn't been confirmed at this time.
71-1	11549-11551	My	
71-2	11552-11562	conjecture	
71-3	11563-11565	is	
71-4	11566-11570	that	
71-5	11571-11574	the	
71-6	11575-11580	write	
71-7	11581-11584	can	
71-8	11585-11590	cause	
71-9	11591-11596	cache	
71-10	11597-11606	thrashing	
71-11	11607-11610	but	
71-12	11611-11615	this	
71-13	11616-11622	hasn't	
71-14	11623-11627	been	
71-15	11628-11637	confirmed	
71-16	11638-11640	at	
71-17	11641-11645	this	
71-18	11646-11650	time	
71-19	11650-11651	.	

#Text=Given a working set close to ARC size an L2ARC can actually hurt performance.
72-1	11652-11657	Given	
72-2	11658-11659	a	
72-3	11660-11667	working	
72-4	11668-11671	set	
72-5	11672-11677	close	
72-6	11678-11680	to	
72-7	11681-11684	ARC	
72-8	11685-11689	size	
72-9	11690-11692	an	
72-10	11693-11698	L2ARC	
72-11	11699-11702	can	
72-12	11703-11711	actually	
72-13	11712-11716	hurt	
72-14	11717-11728	performance	
72-15	11728-11729	.	

#Text=If a system has a 14GB ARC and a 13GB working set, adding an L2ARC device will rob ARC space to map the L2ARC.
73-1	11730-11732	If	
73-2	11733-11734	a	
73-3	11735-11741	system	
73-4	11742-11745	has	
73-5	11746-11747	a	
73-6	11748-11752	14GB	
73-7	11753-11756	ARC	
73-8	11757-11760	and	
73-9	11761-11762	a	
73-10	11763-11767	13GB	
73-11	11768-11775	working	
73-12	11776-11779	set	
73-13	11779-11780	,	
73-14	11781-11787	adding	
73-15	11788-11790	an	
73-16	11791-11796	L2ARC	
73-17	11797-11803	device	
73-18	11804-11808	will	
73-19	11809-11812	rob	
73-20	11813-11816	ARC	
73-21	11817-11822	space	
73-22	11823-11825	to	
73-23	11826-11829	map	
73-24	11830-11833	the	
73-25	11834-11839	L2ARC	
73-26	11839-11840	.	

#Text=If the reduced ARC size is smaller than the working set reads will be evicted from the ARC into the (ostensibly slower) L2ARC.
74-1	11841-11843	If	
74-2	11844-11847	the	
74-3	11848-11855	reduced	
74-4	11856-11859	ARC	
74-5	11860-11864	size	
74-6	11865-11867	is	
74-7	11868-11875	smaller	
74-8	11876-11880	than	
74-9	11881-11884	the	
74-10	11885-11892	working	
74-11	11893-11896	set	
74-12	11897-11902	reads	
74-13	11903-11907	will	
74-14	11908-11910	be	
74-15	11911-11918	evicted	
74-16	11919-11923	from	
74-17	11924-11927	the	
74-18	11928-11931	ARC	
74-19	11932-11936	into	
74-20	11937-11940	the	
74-21	11941-11942	(	
74-22	11942-11952	ostensibly	
74-23	11953-11959	slower	
74-24	11959-11960	)	
74-25	11961-11966	L2ARC	
74-26	11966-11967	.	

#Text=Multiple L2ARC devices are concatenated, there's no provision for mirroring them.
75-1	11968-11976	Multiple	
75-2	11977-11982	L2ARC	
75-3	11983-11990	devices	
75-4	11991-11994	are	
75-5	11995-12007	concatenated	
75-6	12007-12008	,	
75-7	12009-12016	there's	
75-8	12017-12019	no	
75-9	12020-12029	provision	
75-10	12030-12033	for	
75-11	12034-12043	mirroring	
75-12	12044-12048	them	
75-13	12048-12049	.	

#Text=If a heavily used L2ARC device fails the pool will continue to operate with reduced performance.
76-1	12050-12052	If	
76-2	12053-12054	a	
76-3	12055-12062	heavily	
76-4	12063-12067	used	
76-5	12068-12073	L2ARC	
76-6	12074-12080	device	
76-7	12081-12086	fails	
76-8	12087-12090	the	
76-9	12091-12095	pool	
76-10	12096-12100	will	
76-11	12101-12109	continue	
76-12	12110-12112	to	
76-13	12113-12120	operate	
76-14	12121-12125	with	
76-15	12126-12133	reduced	
76-16	12134-12145	performance	
76-17	12145-12146	.	

#Text=There's also no provision for striping reads across multiple devices.
77-1	12147-12154	There's	
77-2	12155-12159	also	
77-3	12160-12162	no	
77-4	12163-12172	provision	
77-5	12173-12176	for	
77-6	12177-12185	striping	
77-7	12186-12191	reads	
77-8	12192-12198	across	
77-9	12199-12207	multiple	
77-10	12208-12215	devices	
77-11	12215-12216	.	

#Text=If the blocks for a file end up in multiple devices you'll see striping but there's no way to force this behavior.
78-1	12217-12219	If	
78-2	12220-12223	the	
78-3	12224-12230	blocks	
78-4	12231-12234	for	
78-5	12235-12236	a	
78-6	12237-12241	file	
78-7	12242-12245	end	
78-8	12246-12248	up	
78-9	12249-12251	in	
78-10	12252-12260	multiple	
78-11	12261-12268	devices	
78-12	12269-12275	you'll	
78-13	12276-12279	see	
78-14	12280-12288	striping	
78-15	12289-12292	but	
78-16	12293-12300	there's	
78-17	12301-12303	no	
78-18	12304-12307	way	
78-19	12308-12310	to	
78-20	12311-12316	force	
78-21	12317-12321	this	
78-22	12322-12330	behavior	
78-23	12330-12331	.	

#Text=Be very careful when adding devices to a production pool.
79-1	12332-12334	Be	
79-2	12335-12339	very	
79-3	12340-12347	careful	
79-4	12348-12352	when	
79-5	12353-12359	adding	
79-6	12360-12367	devices	
79-7	12368-12370	to	
79-8	12371-12372	a	
79-9	12373-12383	production	
79-10	12384-12388	pool	
79-11	12388-12389	.	

#Text=By default zpool add stripes vdevs to the pool.
80-1	12390-12392	By	
80-2	12393-12400	default	
80-3	12401-12406	zpool	
80-4	12407-12410	add	
80-5	12411-12418	stripes	
80-6	12419-12424	vdevs	
80-7	12425-12427	to	
80-8	12428-12431	the	
80-9	12432-12436	pool	
80-10	12436-12437	.	

#Text=If you do this you'll end up striping the device you intended to add as an L2ARC to the pool, and the only way to remove it will be backing up the pool, destroying it, and recreating it.
81-1	12438-12440	If	
81-2	12441-12444	you	
81-3	12445-12447	do	
81-4	12448-12452	this	
81-5	12453-12459	you'll	
81-6	12460-12463	end	
81-7	12464-12466	up	
81-8	12467-12475	striping	
81-9	12476-12479	the	
81-10	12480-12486	device	
81-11	12487-12490	you	
81-12	12491-12499	intended	
81-13	12500-12502	to	
81-14	12503-12506	add	
81-15	12507-12509	as	
81-16	12510-12512	an	
81-17	12513-12518	L2ARC	
81-18	12519-12521	to	
81-19	12522-12525	the	
81-20	12526-12530	pool	
81-21	12530-12531	,	
81-22	12532-12535	and	
81-23	12536-12539	the	
81-24	12540-12544	only	
81-25	12545-12548	way	
81-26	12549-12551	to	
81-27	12552-12558	remove	
81-28	12559-12561	it	
81-29	12562-12566	will	
81-30	12567-12569	be	
81-31	12570-12577	backing	
81-32	12578-12580	up	
81-33	12581-12584	the	
81-34	12585-12589	pool	
81-35	12589-12590	,	
81-36	12591-12601	destroying	
81-37	12602-12604	it	
81-38	12604-12605	,	
81-39	12606-12609	and	
81-40	12610-12620	recreating	
81-41	12621-12623	it	
81-42	12623-12624	.	

#Text=Many SSDs benefit from 4K alignment.
82-1	12625-12629	Many	
82-2	12630-12634	SSDs	
82-3	12635-12642	benefit	
82-4	12643-12647	from	
82-5	12648-12650	4K	
82-6	12651-12660	alignment	
82-7	12660-12661	.	

#Text=Using gpart and gnop on L2ARC devices can help with accomplishing this.
83-1	12662-12667	Using	
83-2	12668-12673	gpart	
83-3	12674-12677	and	
83-4	12678-12682	gnop	
83-5	12683-12685	on	
83-6	12686-12691	L2ARC	
83-7	12692-12699	devices	
83-8	12700-12703	can	
83-9	12704-12708	help	
83-10	12709-12713	with	
83-11	12714-12727	accomplishing	
83-12	12728-12732	this	
83-13	12732-12733	.	

#Text=Because the pool ID isn't stored on hot spare or L2ARC devices they can get lost if the system changes device names.
84-1	12734-12741	Because	
84-2	12742-12745	the	
84-3	12746-12750	pool	
84-4	12751-12753	ID	
84-5	12754-12759	isn't	
84-6	12760-12766	stored	
84-7	12767-12769	on	
84-8	12770-12773	hot	
84-9	12774-12779	spare	
84-10	12780-12782	or	
84-11	12783-12788	L2ARC	
84-12	12789-12796	devices	
84-13	12797-12801	they	
84-14	12802-12805	can	
84-15	12806-12809	get	
84-16	12810-12814	lost	
84-17	12815-12817	if	
84-18	12818-12821	the	
84-19	12822-12828	system	
84-20	12829-12836	changes	
84-21	12837-12843	device	
84-22	12844-12849	names	
84-23	12849-12850	.	

#Text=The caveat about only giving ZFS full devices is a solarism that doesn't apply to FreeBSD.
85-1	12851-12854	The	
85-2	12855-12861	caveat	
85-3	12862-12867	about	
85-4	12868-12872	only	
85-5	12873-12879	giving	
85-6	12880-12883	ZFS	
85-7	12884-12888	full	
85-8	12889-12896	devices	
85-9	12897-12899	is	
85-10	12900-12901	a	
85-11	12902-12910	solarism	
85-12	12911-12915	that	
85-13	12916-12923	doesn't	
85-14	12924-12929	apply	
85-15	12930-12932	to	
85-16	12933-12940	FreeBSD	
85-17	12940-12941	.	

#Text=On Solaris write caches are disabled on drives if partitions are handed to ZFS.
86-1	12942-12944	On	
86-2	12945-12952	Solaris	
86-3	12953-12958	write	
86-4	12959-12965	caches	
86-5	12966-12969	are	
86-6	12970-12978	disabled	
86-7	12979-12981	on	
86-8	12982-12988	drives	
86-9	12989-12991	if	
86-10	12992-13002	partitions	
86-11	13003-13006	are	
86-12	13007-13013	handed	
86-13	13014-13016	to	
86-14	13017-13020	ZFS	
86-15	13020-13021	.	

#Text=On FreeBSD this isn't the case.
87-1	13022-13024	On	
87-2	13025-13032	FreeBSD	
87-3	13033-13037	this	
87-4	13038-13043	isn't	
87-5	13044-13047	the	
87-6	13048-13052	case	
87-7	13052-13053	.	

#Text=Application Issues
#Text=ZFS is a copy-on-write filesystem.
88-1	13054-13065	Application	
88-2	13066-13072	Issues	
88-3	13073-13076	ZFS	
88-4	13077-13079	is	
88-5	13080-13081	a	
88-6	13082-13095	copy-on-write	
88-7	13096-13106	filesystem	
88-8	13106-13107	.	

#Text=As such metadata from the top of the hierarchy is copied in order to maintain consistency in case of sudden failure, i.e. loss of power during a write operation.
89-1	13108-13110	As	
89-2	13111-13115	such	
89-3	13116-13124	metadata	
89-4	13125-13129	from	
89-5	13130-13133	the	
89-6	13134-13137	top	
89-7	13138-13140	of	
89-8	13141-13144	the	
89-9	13145-13154	hierarchy	
89-10	13155-13157	is	
89-11	13158-13164	copied	
89-12	13165-13167	in	
89-13	13168-13173	order	
89-14	13174-13176	to	
89-15	13177-13185	maintain	
89-16	13186-13197	consistency	
89-17	13198-13200	in	
89-18	13201-13205	case	
89-19	13206-13208	of	
89-20	13209-13215	sudden	
89-21	13216-13223	failure	
89-22	13223-13224	,	
89-23	13225-13228	i.e	
89-24	13228-13229	.	
89-25	13230-13234	loss	
89-26	13235-13237	of	
89-27	13238-13243	power	
89-28	13244-13250	during	
89-29	13251-13252	a	
89-30	13253-13258	write	
89-31	13259-13268	operation	
89-32	13268-13269	.	

#Text=This obviates the need for an fsck-like requirement of ZFS filesystems at boot.
90-1	13270-13274	This	
90-2	13275-13283	obviates	
90-3	13284-13287	the	
90-4	13288-13292	need	
90-5	13293-13296	for	
90-6	13297-13299	an	
90-7	13300-13309	fsck-like	
90-8	13310-13321	requirement	
90-9	13322-13324	of	
90-10	13325-13328	ZFS	
90-11	13329-13340	filesystems	
90-12	13341-13343	at	
90-13	13344-13348	boot	
90-14	13348-13349	.	

#Text=However the downside to this is that applications which perform updates in place to large files, e.g. databases, will likely perform poorly in this application of the filesystem due to excessive I/O from copy-on-write (a fast SLOG device -- e.g. a SSD -- can help regarding the write performance of databases or any application which is doing synchronous writes (e.g. open with O_FSYNC) to the FS to make sure the data is on non-volatile storage when the write-call returns).
91-1	13350-13357	However	
91-2	13358-13361	the	
91-3	13362-13370	downside	
91-4	13371-13373	to	
91-5	13374-13378	this	
91-6	13379-13381	is	
91-7	13382-13386	that	
91-8	13387-13399	applications	
91-9	13400-13405	which	
91-10	13406-13413	perform	
91-11	13414-13421	updates	
91-12	13422-13424	in	
91-13	13425-13430	place	
91-14	13431-13433	to	
91-15	13434-13439	large	
91-16	13440-13445	files	
91-17	13445-13446	,	
91-18	13447-13450	e.g	
91-19	13450-13451	.	
91-20	13452-13461	databases	
91-21	13461-13462	,	
91-22	13463-13467	will	
91-23	13468-13474	likely	
91-24	13475-13482	perform	
91-25	13483-13489	poorly	
91-26	13490-13492	in	
91-27	13493-13497	this	
91-28	13498-13509	application	
91-29	13510-13512	of	
91-30	13513-13516	the	
91-31	13517-13527	filesystem	
91-32	13528-13531	due	
91-33	13532-13534	to	
91-34	13535-13544	excessive	
91-35	13545-13546	I	
91-36	13546-13547	/	
91-37	13547-13548	O	
91-38	13549-13553	from	
91-39	13554-13567	copy-on-write	
91-40	13568-13569	(	
91-41	13569-13570	a	
91-42	13571-13575	fast	
91-43	13576-13580	SLOG	
91-44	13581-13587	device	
91-45	13588-13589	-	
91-46	13589-13590	-	
91-47	13591-13594	e.g	
91-48	13594-13595	.	
91-49	13596-13597	a	
91-50	13598-13601	SSD	
91-51	13602-13603	-	
91-52	13603-13604	-	
91-53	13605-13608	can	
91-54	13609-13613	help	
91-55	13614-13623	regarding	
91-56	13624-13627	the	
91-57	13628-13633	write	
91-58	13634-13645	performance	
91-59	13646-13648	of	
91-60	13649-13658	databases	
91-61	13659-13661	or	
91-62	13662-13665	any	
91-63	13666-13677	application	
91-64	13678-13683	which	
91-65	13684-13686	is	
91-66	13687-13692	doing	
91-67	13693-13704	synchronous	
91-68	13705-13711	writes	
91-69	13712-13713	(	
91-70	13713-13716	e.g	
91-71	13716-13717	.	
91-72	13718-13722	open	
91-73	13723-13727	with	
91-74	13728-13735	O_FSYNC	
91-75	13735-13736	)	
91-76	13737-13739	to	
91-77	13740-13743	the	
91-78	13744-13746	FS	
91-79	13747-13749	to	
91-80	13750-13754	make	
91-81	13755-13759	sure	
91-82	13760-13763	the	
91-83	13764-13768	data	
91-84	13769-13771	is	
91-85	13772-13774	on	
91-86	13775-13787	non-volatile	
91-87	13788-13795	storage	
91-88	13796-13800	when	
91-89	13801-13804	the	
91-90	13805-13815	write-call	
91-91	13816-13823	returns	
91-92	13823-13824	)	
91-93	13824-13825	.	

#Text=Additionally, database applications, such as Oracle, maintain a large cache (called the SGA in Oracle) in memory will perform poorly due to double caching of data in the ARC and in the application's own cache.
92-1	13826-13838	Additionally	
92-2	13838-13839	,	
92-3	13840-13848	database	
92-4	13849-13861	applications	
92-5	13861-13862	,	
92-6	13863-13867	such	
92-7	13868-13870	as	
92-8	13871-13877	Oracle	
92-9	13877-13878	,	
92-10	13879-13887	maintain	
92-11	13888-13889	a	
92-12	13890-13895	large	
92-13	13896-13901	cache	
92-14	13902-13903	(	
92-15	13903-13909	called	
92-16	13910-13913	the	
92-17	13914-13917	SGA	
92-18	13918-13920	in	
92-19	13921-13927	Oracle	
92-20	13927-13928	)	
92-21	13929-13931	in	
92-22	13932-13938	memory	
92-23	13939-13943	will	
92-24	13944-13951	perform	
92-25	13952-13958	poorly	
92-26	13959-13962	due	
92-27	13963-13965	to	
92-28	13966-13972	double	
92-29	13973-13980	caching	
92-30	13981-13983	of	
92-31	13984-13988	data	
92-32	13989-13991	in	
92-33	13992-13995	the	
92-34	13996-13999	ARC	
92-35	14000-14003	and	
92-36	14004-14006	in	
92-37	14007-14010	the	
92-38	14011-14024	application's	
92-39	14025-14028	own	
92-40	14029-14034	cache	
92-41	14034-14035	.	

#Text=Reducing the ARC to a minimum can improve performance of applications which maintain their own cache.
93-1	14036-14044	Reducing	
93-2	14045-14048	the	
93-3	14049-14052	ARC	
93-4	14053-14055	to	
93-5	14056-14057	a	
93-6	14058-14065	minimum	
93-7	14066-14069	can	
93-8	14070-14077	improve	
93-9	14078-14089	performance	
93-10	14090-14092	of	
93-11	14093-14105	applications	
93-12	14106-14111	which	
93-13	14112-14120	maintain	
93-14	14121-14126	their	
93-15	14127-14130	own	
93-16	14131-14136	cache	
93-17	14136-14137	.	

#Text=At ZFS Best Practices Guide there are some generic recommendations for ZFS on Solaris which mostly apply to FreeBSD too.
94-1	14138-14140	At	
94-2	14141-14144	ZFS	
94-3	14145-14149	Best	
94-4	14150-14159	Practices	
94-5	14160-14165	Guide	
94-6	14166-14171	there	
94-7	14172-14175	are	
94-8	14176-14180	some	
94-9	14181-14188	generic	
94-10	14189-14204	recommendations	
94-11	14205-14208	for	
94-12	14209-14212	ZFS	
94-13	14213-14215	on	
94-14	14216-14223	Solaris	
94-15	14224-14229	which	
94-16	14230-14236	mostly	
94-17	14237-14242	apply	
94-18	14243-14245	to	
94-19	14246-14253	FreeBSD	
94-20	14254-14257	too	
94-21	14257-14258	.	

#Text=General Tuning
#Text=There are some changes that can be made to improve performance in certain situations and avoid the bursty IO that's often seen with ZFS.
95-1	14259-14266	General	
95-2	14267-14273	Tuning	
95-3	14274-14279	There	
95-4	14280-14283	are	
95-5	14284-14288	some	
95-6	14289-14296	changes	
95-7	14297-14301	that	
95-8	14302-14305	can	
95-9	14306-14308	be	
95-10	14309-14313	made	
95-11	14314-14316	to	
95-12	14317-14324	improve	
95-13	14325-14336	performance	
95-14	14337-14339	in	
95-15	14340-14347	certain	
95-16	14348-14358	situations	
95-17	14359-14362	and	
95-18	14363-14368	avoid	
95-19	14369-14372	the	
95-20	14373-14379	bursty	
95-21	14380-14382	IO	
95-22	14383-14389	that's	
95-23	14390-14395	often	
95-24	14396-14400	seen	
95-25	14401-14405	with	
95-26	14406-14409	ZFS	
95-27	14409-14410	.	

#Text=Loader tunables (in /boot/loader.conf): # Disable ZFS prefetching
#Text=# http://southbrain.com/south/2008/04/the-nightmare-comes-slowly-zfs.html
#Text=# Increases overall speed of ZFS, but when disk flushing/writes occur,
#Text=# system is less responsive (due to extreme disk I/O).
#Text=# NOTE: Systems with 4 GB of RAM or more have prefetch enabled by default.
#Text=vfs.zfs.prefetch_disable="1"
#Text=# Decrease ZFS txg timeout value from 30 (default) to 5 seconds.
96-1	14411-14417	Loader	
96-2	14418-14426	tunables	
96-3	14427-14428	(	
96-4	14428-14430	in	
96-5	14431-14432	/	
96-6	14432-14436	boot	
96-7	14436-14437	/	
96-8	14437-14448	loader.conf	
96-9	14448-14449	)	
96-10	14449-14450	:	
96-11	14451-14452	#	
96-12	14453-14460	Disable	
96-13	14461-14464	ZFS	
96-14	14465-14476	prefetching	
96-15	14477-14478	#	
96-16	14479-14483	http	
96-17	14483-14484	:	
96-18	14484-14485	/	
96-19	14485-14486	/	
96-20	14486-14500	southbrain.com	
96-21	14500-14501	/	
96-22	14501-14506	south	
96-23	14506-14507	/	
96-24	14507-14511	2008	
96-25	14511-14512	/	
96-26	14512-14514	04	
96-27	14514-14515	/	
96-28	14515-14550	the-nightmare-comes-slowly-zfs.html	
96-29	14551-14552	#	
96-30	14553-14562	Increases	
96-31	14563-14570	overall	
96-32	14571-14576	speed	
96-33	14577-14579	of	
96-34	14580-14583	ZFS	
96-35	14583-14584	,	
96-36	14585-14588	but	
96-37	14589-14593	when	
96-38	14594-14598	disk	
96-39	14599-14607	flushing	
96-40	14607-14608	/	
96-41	14608-14614	writes	
96-42	14615-14620	occur	
96-43	14620-14621	,	
96-44	14622-14623	#	
96-45	14624-14630	system	
96-46	14631-14633	is	
96-47	14634-14638	less	
96-48	14639-14649	responsive	
96-49	14650-14651	(	
96-50	14651-14654	due	
96-51	14655-14657	to	
96-52	14658-14665	extreme	
96-53	14666-14670	disk	
96-54	14671-14672	I	
96-55	14672-14673	/	
96-56	14673-14674	O	
96-57	14674-14675	)	
96-58	14675-14676	.	
96-59	14677-14678	#	
96-60	14679-14683	NOTE	
96-61	14683-14684	:	
96-62	14685-14692	Systems	
96-63	14693-14697	with	
96-64	14698-14699	4	
96-65	14700-14702	GB	
96-66	14703-14705	of	
96-67	14706-14709	RAM	
96-68	14710-14712	or	
96-69	14713-14717	more	
96-70	14718-14722	have	
96-71	14723-14731	prefetch	
96-72	14732-14739	enabled	
96-73	14740-14742	by	
96-74	14743-14750	default	
96-75	14750-14751	.	
96-76	14752-14776	vfs.zfs.prefetch_disable	
96-77	14776-14777	=	
96-78	14777-14778	"	
96-79	14778-14779	1	
96-80	14779-14780	"	
96-81	14781-14782	#	
96-82	14783-14791	Decrease	
96-83	14792-14795	ZFS	
96-84	14796-14799	txg	
96-85	14800-14807	timeout	
96-86	14808-14813	value	
96-87	14814-14818	from	
96-88	14819-14821	30	
96-89	14822-14823	(	
96-90	14823-14830	default	
96-91	14830-14831	)	
96-92	14832-14834	to	
96-93	14835-14836	5	
96-94	14837-14844	seconds	
96-95	14844-14845	.	

#Text=This
#Text=# should increase throughput and decrease the "bursty" stalls that
#Text=# happen during immense I/O with ZFS.
#Text=# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007343.html
#Text=# http://lists.freebsd.org/pipermail/freebsd-fs/2009-December/007355.html
#Text=# default in FreeBSD since ZFS v28
#Text=vfs.zfs.txg.timeout="5"
#Text=Sysctl variables (/etc/sysctl.conf):
#Text=# Increase number of vnodes; we've seen vfs.numvnodes reach 115,000
#Text=# at times.
97-1	14846-14850	This	
97-2	14851-14852	#	
97-3	14853-14859	should	
97-4	14860-14868	increase	
97-5	14869-14879	throughput	
97-6	14880-14883	and	
97-7	14884-14892	decrease	
97-8	14893-14896	the	
97-9	14897-14898	"	
97-10	14898-14904	bursty	
97-11	14904-14905	"	
97-12	14906-14912	stalls	
97-13	14913-14917	that	
97-14	14918-14919	#	
97-15	14920-14926	happen	
97-16	14927-14933	during	
97-17	14934-14941	immense	
97-18	14942-14943	I	
97-19	14943-14944	/	
97-20	14944-14945	O	
97-21	14946-14950	with	
97-22	14951-14954	ZFS	
97-23	14954-14955	.	
97-24	14956-14957	#	
97-25	14958-14962	http	
97-26	14962-14963	:	
97-27	14963-14964	/	
97-28	14964-14965	/	
97-29	14965-14982	lists.freebsd.org	
97-30	14982-14983	/	
97-31	14983-14992	pipermail	
97-32	14992-14993	/	
97-33	14993-15003	freebsd-fs	
97-34	15003-15004	/	
97-35	15004-15008	2009	
97-36	15008-15009	-	
97-37	15009-15017	December	
97-38	15017-15018	/	
97-39	15018-15024	007343	
97-40	15024-15025	.	
97-41	15025-15029	html	
97-42	15030-15031	#	
97-43	15032-15036	http	
97-44	15036-15037	:	
97-45	15037-15038	/	
97-46	15038-15039	/	
97-47	15039-15056	lists.freebsd.org	
97-48	15056-15057	/	
97-49	15057-15066	pipermail	
97-50	15066-15067	/	
97-51	15067-15077	freebsd-fs	
97-52	15077-15078	/	
97-53	15078-15082	2009	
97-54	15082-15083	-	
97-55	15083-15091	December	
97-56	15091-15092	/	
97-57	15092-15098	007355	
97-58	15098-15099	.	
97-59	15099-15103	html	
97-60	15104-15105	#	
97-61	15106-15113	default	
97-62	15114-15116	in	
97-63	15117-15124	FreeBSD	
97-64	15125-15130	since	
97-65	15131-15134	ZFS	
97-66	15135-15138	v28	
97-67	15139-15158	vfs.zfs.txg.timeout	
97-68	15158-15159	=	
97-69	15159-15160	"	
97-70	15160-15161	5	
97-71	15161-15162	"	
97-72	15163-15169	Sysctl	
97-73	15170-15179	variables	
97-74	15180-15181	(	
97-75	15181-15182	/	
97-76	15182-15185	etc	
97-77	15185-15186	/	
97-78	15186-15197	sysctl.conf	
97-79	15197-15198	)	
97-80	15198-15199	:	
97-81	15200-15201	#	
97-82	15202-15210	Increase	
97-83	15211-15217	number	
97-84	15218-15220	of	
97-85	15221-15227	vnodes	
97-86	15227-15228	;	
97-87	15229-15234	we've	
97-88	15235-15239	seen	
97-89	15240-15253	vfs.numvnodes	
97-90	15254-15259	reach	
97-91	15260-15267	115,000	
97-92	15268-15269	#	
97-93	15270-15272	at	
97-94	15273-15278	times	
97-95	15278-15279	.	

#Text=Default max is a little over 200,000.
98-1	15280-15287	Default	
98-2	15288-15291	max	
98-3	15292-15294	is	
98-4	15295-15296	a	
98-5	15297-15303	little	
98-6	15304-15308	over	
98-7	15309-15316	200,000	
98-8	15316-15317	.	

#Text=Playing it safe...
#Text=# If numvnodes reaches maxvnode performance substantially decreases.
#Text=kern.maxvnodes=250000
#Text=# Set TXG write limit to a lower threshold.
99-1	15318-15325	Playing	
99-2	15326-15328	it	
99-3	15329-15333	safe	
99-4	15333-15334	.	
99-5	15334-15335	.	
99-6	15335-15336	.	
99-7	15337-15338	#	
99-8	15339-15341	If	
99-9	15342-15351	numvnodes	
99-10	15352-15359	reaches	
99-11	15360-15368	maxvnode	
99-12	15369-15380	performance	
99-13	15381-15394	substantially	
99-14	15395-15404	decreases	
99-15	15404-15405	.	
99-16	15406-15420	kern.maxvnodes	
99-17	15420-15421	=	
99-18	15421-15427	250000	
99-19	15428-15429	#	
99-20	15430-15433	Set	
99-21	15434-15437	TXG	
99-22	15438-15443	write	
99-23	15444-15449	limit	
99-24	15450-15452	to	
99-25	15453-15454	a	
99-26	15455-15460	lower	
99-27	15461-15470	threshold	
99-28	15470-15471	.	

#Text=This helps "level out"
#Text=# the throughput rate (see "zpool iostat").
100-1	15472-15476	This	
100-2	15477-15482	helps	
100-3	15483-15484	"	
100-4	15484-15489	level	
100-5	15490-15493	out	
100-6	15493-15494	"	
100-7	15495-15496	#	
100-8	15497-15500	the	
100-9	15501-15511	throughput	
100-10	15512-15516	rate	
100-11	15517-15518	(	
100-12	15518-15521	see	
100-13	15522-15523	"	
100-14	15523-15528	zpool	
100-15	15529-15535	iostat	
100-16	15535-15536	"	
100-17	15536-15537	)	
100-18	15537-15538	.	

#Text=A value of 256MB works well
#Text=# for systems with 4 GB of RAM, while 1 GB works well for us w/ 8 GB on
#Text=# disks which have 64 MB cache.
101-1	15539-15540	A	
101-2	15541-15546	value	
101-3	15547-15549	of	
101-4	15550-15555	256MB	
101-5	15556-15561	works	
101-6	15562-15566	well	
101-7	15567-15568	#	
101-8	15569-15572	for	
101-9	15573-15580	systems	
101-10	15581-15585	with	
101-11	15586-15587	4	
101-12	15588-15590	GB	
101-13	15591-15593	of	
101-14	15594-15597	RAM	
101-15	15597-15598	,	
101-16	15599-15604	while	
101-17	15605-15606	1	
101-18	15607-15609	GB	
101-19	15610-15615	works	
101-20	15616-15620	well	
101-21	15621-15624	for	
101-22	15625-15627	us	
101-23	15628-15629	w	
101-24	15629-15630	/	
101-25	15631-15632	8	
101-26	15633-15635	GB	
101-27	15636-15638	on	
101-28	15639-15640	#	
101-29	15641-15646	disks	
101-30	15647-15652	which	
101-31	15653-15657	have	
101-32	15658-15660	64	
101-33	15661-15663	MB	
101-34	15664-15669	cache	
101-35	15669-15670	.	

#Text=<<BR>>
#Text=# NOTE: in <v28, this tunable is called 'vfs.zfs.txg.write_limit_override'.
#Text=vfs.zfs.write_limit_override=1073741824
#Text=Be aware that the vfs.zfs.write_limit_override tuning you see above
#Text=may need to be adjusted for your system.
102-1	15671-15672	<	
102-2	15672-15673	<	
102-3	15673-15675	BR	
102-4	15675-15676	>	
102-5	15676-15677	>	
102-6	15678-15679	#	
102-7	15680-15684	NOTE	
102-8	15684-15685	:	
102-9	15686-15688	in	
102-10	15689-15690	<	
102-11	15690-15693	v28	
102-12	15693-15694	,	
102-13	15695-15699	this	
102-14	15700-15707	tunable	
102-15	15708-15710	is	
102-16	15711-15717	called	
102-17	15718-15719	'	
102-18	15719-15751	vfs.zfs.txg.write_limit_override	
102-19	15751-15752	'	
102-20	15752-15753	.	
102-21	15754-15782	vfs.zfs.write_limit_override	
102-22	15782-15783	=	
102-23	15783-15793	1073741824	
102-24	15794-15796	Be	
102-25	15797-15802	aware	
102-26	15803-15807	that	
102-27	15808-15811	the	
102-28	15812-15840	vfs.zfs.write_limit_override	
102-29	15841-15847	tuning	
102-30	15848-15851	you	
102-31	15852-15855	see	
102-32	15856-15861	above	
102-33	15862-15865	may	
102-34	15866-15870	need	
102-35	15871-15873	to	
102-36	15874-15876	be	
102-37	15877-15885	adjusted	
102-38	15886-15889	for	
102-39	15890-15894	your	
102-40	15895-15901	system	
102-41	15901-15902	.	

#Text=It's up to you to figure out
#Text=what works best in your environment.
103-1	15903-15907	It's	
103-2	15908-15910	up	
103-3	15911-15913	to	
103-4	15914-15917	you	
103-5	15918-15920	to	
103-6	15921-15927	figure	
103-7	15928-15931	out	
103-8	15932-15936	what	
103-9	15937-15942	works	
103-10	15943-15947	best	
103-11	15948-15950	in	
103-12	15951-15955	your	
103-13	15956-15967	environment	
103-14	15967-15968	.	

#Text=Deduplication
#Text=Deduplication is a misunderstood feature in ZFS v21+; some users see it as a silver bullet for increasing capacity by reducing redundancies in data.
104-1	15969-15982	Deduplication	
104-2	15983-15996	Deduplication	
104-3	15997-15999	is	
104-4	16000-16001	a	
104-5	16002-16015	misunderstood	
104-6	16016-16023	feature	
104-7	16024-16026	in	
104-8	16027-16030	ZFS	
104-9	16031-16034	v21	
104-10	16034-16035	+	
104-11	16035-16036	;	
104-12	16037-16041	some	
104-13	16042-16047	users	
104-14	16048-16051	see	
104-15	16052-16054	it	
104-16	16055-16057	as	
104-17	16058-16059	a	
104-18	16060-16066	silver	
104-19	16067-16073	bullet	
104-20	16074-16077	for	
104-21	16078-16088	increasing	
104-22	16089-16097	capacity	
104-23	16098-16100	by	
104-24	16101-16109	reducing	
104-25	16110-16122	redundancies	
104-26	16123-16125	in	
104-27	16126-16130	data	
104-28	16130-16131	.	

#Text=Here are the author's (gcooper's) observations: There are some resources that suggest that one needs 2GB per TB of storage with deduplication [i] (in fact this is a misinterpretation of the text).
105-1	16132-16136	Here	
105-2	16137-16140	are	
105-3	16141-16144	the	
105-4	16145-16153	author's	
105-5	16154-16155	(	
105-6	16155-16164	gcooper's	
105-7	16164-16165	)	
105-8	16166-16178	observations	
105-9	16178-16179	:	
105-10	16180-16185	There	
105-11	16186-16189	are	
105-12	16190-16194	some	
105-13	16195-16204	resources	
105-14	16205-16209	that	
105-15	16210-16217	suggest	
105-16	16218-16222	that	
105-17	16223-16226	one	
105-18	16227-16232	needs	
105-19	16233-16236	2GB	
105-20	16237-16240	per	
105-21	16241-16243	TB	
105-22	16244-16246	of	
105-23	16247-16254	storage	
105-24	16255-16259	with	
105-25	16260-16273	deduplication	
105-26	16274-16275	[	
105-27	16275-16276	i	
105-28	16276-16277	]	
105-29	16278-16279	(	
105-30	16279-16281	in	
105-31	16282-16286	fact	
105-32	16287-16291	this	
105-33	16292-16294	is	
105-34	16295-16296	a	
105-35	16297-16314	misinterpretation	
105-36	16315-16317	of	
105-37	16318-16321	the	
105-38	16322-16326	text	
105-39	16326-16327	)	
105-40	16327-16328	.	

#Text=In practice with FreeBSD, based on empirical testing and additional reading, it's closer to 5GB per TB.
106-1	16329-16331	In	
106-2	16332-16340	practice	
106-3	16341-16345	with	
106-4	16346-16353	FreeBSD	
106-5	16353-16354	,	
106-6	16355-16360	based	
106-7	16361-16363	on	
106-8	16364-16373	empirical	
106-9	16374-16381	testing	
106-10	16382-16385	and	
106-11	16386-16396	additional	
106-12	16397-16404	reading	
106-13	16404-16405	,	
106-14	16406-16410	it's	
106-15	16411-16417	closer	
106-16	16418-16420	to	
106-17	16421-16424	5GB	
106-18	16425-16428	per	
106-19	16429-16431	TB	
106-20	16431-16432	.	

#Text=Using deduplication is slower than not running it.
107-1	16433-16438	Using	
107-2	16439-16452	deduplication	
107-3	16453-16455	is	
107-4	16456-16462	slower	
107-5	16463-16467	than	
107-6	16468-16471	not	
107-7	16472-16479	running	
107-8	16480-16482	it	
107-9	16482-16483	.	

#Text=Deduplication [on 8.x/9.x at least] lies via stat(2) / statvfs(2); it reports the theoretical used space -- not the actual used space -- which can confuse scripts that look at df output, etc (TODO: find PR that mentions this).
108-1	16484-16497	Deduplication	
108-2	16498-16499	[	
108-3	16499-16501	on	
108-4	16502-16503	8	
108-5	16503-16504	.	
108-6	16504-16505	x	
108-7	16505-16506	/	
108-8	16506-16507	9	
108-9	16507-16508	.	
108-10	16508-16509	x	
108-11	16510-16512	at	
108-12	16513-16518	least	
108-13	16518-16519	]	
108-14	16520-16524	lies	
108-15	16525-16528	via	
108-16	16529-16533	stat	
108-17	16533-16534	(	
108-18	16534-16535	2	
108-19	16535-16536	)	
108-20	16537-16538	/	
108-21	16539-16546	statvfs	
108-22	16546-16547	(	
108-23	16547-16548	2	
108-24	16548-16549	)	
108-25	16549-16550	;	
108-26	16551-16553	it	
108-27	16554-16561	reports	
108-28	16562-16565	the	
108-29	16566-16577	theoretical	
108-30	16578-16582	used	
108-31	16583-16588	space	
108-32	16589-16590	-	
108-33	16590-16591	-	
108-34	16592-16595	not	
108-35	16596-16599	the	
108-36	16600-16606	actual	
108-37	16607-16611	used	
108-38	16612-16617	space	
108-39	16618-16619	-	
108-40	16619-16620	-	
108-41	16621-16626	which	
108-42	16627-16630	can	
108-43	16631-16638	confuse	
108-44	16639-16646	scripts	
108-45	16647-16651	that	
108-46	16652-16656	look	
108-47	16657-16659	at	
108-48	16660-16662	df	
108-49	16663-16669	output	
108-50	16669-16670	,	
108-51	16671-16674	etc	
108-52	16675-16676	(	
108-53	16676-16680	TODO	
108-54	16680-16681	:	
108-55	16682-16686	find	
108-56	16687-16689	PR	
108-57	16690-16694	that	
108-58	16695-16703	mentions	
108-59	16704-16708	this	
108-60	16708-16709	)	
108-61	16709-16710	.	

#Text=Suggestions
#Text=If you are going to use deduplication and your machine is underspec'ed, you must set vfs.zfs.arc_max to a sane value or ZFS will wire down as much available memory as possible, which can create memory starvation scenarios.
109-1	16711-16722	Suggestions	
109-2	16723-16725	If	
109-3	16726-16729	you	
109-4	16730-16733	are	
109-5	16734-16739	going	
109-6	16740-16742	to	
109-7	16743-16746	use	
109-8	16747-16760	deduplication	
109-9	16761-16764	and	
109-10	16765-16769	your	
109-11	16770-16777	machine	
109-12	16778-16780	is	
109-13	16781-16793	underspec'ed	
109-14	16793-16794	,	
109-15	16795-16798	you	
109-16	16799-16803	must	
109-17	16804-16807	set	
109-18	16808-16823	vfs.zfs.arc_max	
109-19	16824-16826	to	
109-20	16827-16828	a	
109-21	16829-16833	sane	
109-22	16834-16839	value	
109-23	16840-16842	or	
109-24	16843-16846	ZFS	
109-25	16847-16851	will	
109-26	16852-16856	wire	
109-27	16857-16861	down	
109-28	16862-16864	as	
109-29	16865-16869	much	
109-30	16870-16879	available	
109-31	16880-16886	memory	
109-32	16887-16889	as	
109-33	16890-16898	possible	
109-34	16898-16899	,	
109-35	16900-16905	which	
109-36	16906-16909	can	
109-37	16910-16916	create	
109-38	16917-16923	memory	
109-39	16924-16934	starvation	
109-40	16935-16944	scenarios	
109-41	16944-16945	.	

#Text=It's a much better idea in general to use compression -- instead of deduplication -- if you're trying to save space, and you know that you can benefit from compression.
110-1	16946-16950	It's	
110-2	16951-16952	a	
110-3	16953-16957	much	
110-4	16958-16964	better	
110-5	16965-16969	idea	
110-6	16970-16972	in	
110-7	16973-16980	general	
110-8	16981-16983	to	
110-9	16984-16987	use	
110-10	16988-16999	compression	
110-11	17000-17001	-	
110-12	17001-17002	-	
110-13	17003-17010	instead	
110-14	17011-17013	of	
110-15	17014-17027	deduplication	
110-16	17028-17029	-	
110-17	17029-17030	-	
110-18	17031-17033	if	
110-19	17034-17040	you're	
110-20	17041-17047	trying	
110-21	17048-17050	to	
110-22	17051-17055	save	
110-23	17056-17061	space	
110-24	17061-17062	,	
110-25	17063-17066	and	
110-26	17067-17070	you	
110-27	17071-17075	know	
110-28	17076-17080	that	
110-29	17081-17084	you	
110-30	17085-17088	can	
110-31	17089-17096	benefit	
110-32	17097-17101	from	
110-33	17102-17113	compression	
110-34	17113-17114	.	

#Text=When in doubt, check how much you would actually gain from deduplication via zdb -S <zpool> instead of just turning it on.
111-1	17115-17119	When	
111-2	17120-17122	in	
111-3	17123-17128	doubt	
111-4	17128-17129	,	
111-5	17130-17135	check	
111-6	17136-17139	how	
111-7	17140-17144	much	
111-8	17145-17148	you	
111-9	17149-17154	would	
111-10	17155-17163	actually	
111-11	17164-17168	gain	
111-12	17169-17173	from	
111-13	17174-17187	deduplication	
111-14	17188-17191	via	
111-15	17192-17195	zdb	
111-16	17196-17197	-	
111-17	17197-17198	S	
111-18	17199-17200	<	
111-19	17200-17205	zpool	
111-20	17205-17206	>	
111-21	17207-17214	instead	
111-22	17215-17217	of	
111-23	17218-17222	just	
111-24	17223-17230	turning	
111-25	17231-17233	it	
111-26	17234-17236	on	
111-27	17236-17237	.	

#Text=Please note that this will take a while to run, depending on the dataset/zpool selected.
112-1	17238-17244	Please	
112-2	17245-17249	note	
112-3	17250-17254	that	
112-4	17255-17259	this	
112-5	17260-17264	will	
112-6	17265-17269	take	
112-7	17270-17271	a	
112-8	17272-17277	while	
112-9	17278-17280	to	
112-10	17281-17284	run	
112-11	17284-17285	,	
112-12	17286-17295	depending	
112-13	17296-17298	on	
112-14	17299-17302	the	
112-15	17303-17310	dataset	
112-16	17310-17311	/	
112-17	17311-17316	zpool	
112-18	17317-17325	selected	
112-19	17325-17326	.	

#Text=References
#Text=http://blogs.oracle.com/roch/entry/dedup_performance_considerations1
#Text=NFS tuning
#Text=The combination of ZFS and NFS stresses the ZIL to the point that performance falls significantly below expected levels.
113-1	17327-17337	References	
113-2	17338-17342	http	
113-3	17342-17343	:	
113-4	17343-17344	/	
113-5	17344-17345	/	
113-6	17345-17361	blogs.oracle.com	
113-7	17361-17362	/	
113-8	17362-17366	roch	
113-9	17366-17367	/	
113-10	17367-17372	entry	
113-11	17372-17373	/	
113-12	17373-17406	dedup_performance_considerations1	
113-13	17407-17410	NFS	
113-14	17411-17417	tuning	
113-15	17418-17421	The	
113-16	17422-17433	combination	
113-17	17434-17436	of	
113-18	17437-17440	ZFS	
113-19	17441-17444	and	
113-20	17445-17448	NFS	
113-21	17449-17457	stresses	
113-22	17458-17461	the	
113-23	17462-17465	ZIL	
113-24	17466-17468	to	
113-25	17469-17472	the	
113-26	17473-17478	point	
113-27	17479-17483	that	
113-28	17484-17495	performance	
113-29	17496-17501	falls	
113-30	17502-17515	significantly	
113-31	17516-17521	below	
113-32	17522-17530	expected	
113-33	17531-17537	levels	
113-34	17537-17538	.	

#Text=The best solution is to put the ZIL on a fast SSD (or a pair of SSDs in a mirror, for added redundancy).
114-1	17539-17542	The	
114-2	17543-17547	best	
114-3	17548-17556	solution	
114-4	17557-17559	is	
114-5	17560-17562	to	
114-6	17563-17566	put	
114-7	17567-17570	the	
114-8	17571-17574	ZIL	
114-9	17575-17577	on	
114-10	17578-17579	a	
114-11	17580-17584	fast	
114-12	17585-17588	SSD	
114-13	17589-17590	(	
114-14	17590-17592	or	
114-15	17593-17594	a	
114-16	17595-17599	pair	
114-17	17600-17602	of	
114-18	17603-17607	SSDs	
114-19	17608-17610	in	
114-20	17611-17612	a	
114-21	17613-17619	mirror	
114-22	17619-17620	,	
114-23	17621-17624	for	
114-24	17625-17630	added	
114-25	17631-17641	redundancy	
114-26	17641-17642	)	
114-27	17642-17643	.	

#Text=You can now enable/disable ZIL on a per-dataset basis (as of ZFS version 28 / FreeBSD 8.3+).  
115-1	17644-17647	You	
115-2	17648-17651	can	
115-3	17652-17655	now	
115-4	17656-17662	enable	
115-5	17662-17663	/	
115-6	17663-17670	disable	
115-7	17671-17674	ZIL	
115-8	17675-17677	on	
115-9	17678-17679	a	
115-10	17680-17691	per-dataset	
115-11	17692-17697	basis	
115-12	17698-17699	(	
115-13	17699-17701	as	
115-14	17702-17704	of	
115-15	17705-17708	ZFS	
115-16	17709-17716	version	
115-17	17717-17719	28	
115-18	17720-17721	/	
115-19	17722-17729	FreeBSD	
115-20	17730-17733	8.3	
115-21	17733-17734	+	
115-22	17734-17735	)	
115-23	17735-17736	.	
115-24	17737-17738	 	

#Text=zfs set sync=disabled tank/dataset  The next best solution is to disable ZIL with the following setting in loader.conf (up to ZFS version 15): vfs.zfs.zil_disable="1" the vfs.zfs.zil_disable loader tunable was replaced with the "sync" dataset property.
116-1	17738-17741	zfs	
116-2	17741-17742	 	
116-3	17742-17745	set	
116-4	17745-17746	 	
116-5	17746-17750	sync	
116-6	17750-17751	=	
116-7	17751-17759	disabled	
116-8	17759-17760	 	
116-9	17760-17764	tank	
116-10	17764-17765	/	
116-11	17765-17772	dataset	
116-12	17772-17773	 	
116-13	17774-17777	The	
116-14	17778-17782	next	
116-15	17783-17787	best	
116-16	17788-17796	solution	
116-17	17797-17799	is	
116-18	17800-17802	to	
116-19	17803-17810	disable	
116-20	17811-17814	ZIL	
116-21	17815-17819	with	
116-22	17820-17823	the	
116-23	17824-17833	following	
116-24	17834-17841	setting	
116-25	17842-17844	in	
116-26	17845-17856	loader.conf	
116-27	17857-17858	(	
116-28	17858-17860	up	
116-29	17861-17863	to	
116-30	17864-17867	ZFS	
116-31	17868-17875	version	
116-32	17876-17878	15	
116-33	17878-17879	)	
116-34	17879-17880	:	
116-35	17881-17900	vfs.zfs.zil_disable	
116-36	17900-17901	=	
116-37	17901-17902	"	
116-38	17902-17903	1	
116-39	17903-17904	"	
116-40	17905-17908	the	
116-41	17909-17928	vfs.zfs.zil_disable	
116-42	17929-17935	loader	
116-43	17936-17943	tunable	
116-44	17944-17947	was	
116-45	17948-17956	replaced	
116-46	17957-17961	with	
116-47	17962-17965	the	
116-48	17966-17967	"	
116-49	17967-17971	sync	
116-50	17971-17972	"	
116-51	17973-17980	dataset	
116-52	17981-17989	property	
116-53	17989-17990	.	

#Text=Disabling ZIL is not recommended where data consistency is required (such as database servers) but will not result in file system corruption.
117-1	17991-18000	Disabling	
117-2	18001-18004	ZIL	
117-3	18005-18007	is	
117-4	18008-18011	not	
117-5	18012-18023	recommended	
117-6	18024-18029	where	
117-7	18030-18034	data	
117-8	18035-18046	consistency	
117-9	18047-18049	is	
117-10	18050-18058	required	
117-11	18059-18060	(	
117-12	18060-18064	such	
117-13	18065-18067	as	
117-14	18068-18076	database	
117-15	18077-18084	servers	
117-16	18084-18085	)	
117-17	18086-18089	but	
117-18	18090-18094	will	
117-19	18095-18098	not	
117-20	18099-18105	result	
117-21	18106-18108	in	
117-22	18109-18113	file	
117-23	18114-18120	system	
117-24	18121-18131	corruption	
117-25	18131-18132	.	

#Text=See ZFS Evil Tuning Guide, section "Disabling the ZIL (Don't)".
118-1	18133-18136	See	
118-2	18137-18140	ZFS	
118-3	18141-18145	Evil	
118-4	18146-18152	Tuning	
118-5	18153-18158	Guide	
118-6	18158-18159	,	
118-7	18160-18167	section	
118-8	18168-18169	"	
118-9	18169-18178	Disabling	
118-10	18179-18182	the	
118-11	18183-18186	ZIL	
118-12	18187-18188	(	
118-13	18188-18193	Don't	
118-14	18193-18194	)	
118-15	18194-18195	"	
118-16	18195-18196	.	

#Text=ZFS is designed to be used with "raw" drives - i.e. not over already created hardware RAID volumes (this is sometimes called "JBOD" or "passthrough" mode when used with RAID controllers), but can benefit greatly from good and fast controllers.
119-1	18197-18200	ZFS	
119-2	18201-18203	is	
119-3	18204-18212	designed	
119-4	18213-18215	to	
119-5	18216-18218	be	
119-6	18219-18223	used	
119-7	18224-18228	with	
119-8	18229-18230	"	
119-9	18230-18233	raw	
119-10	18233-18234	"	
119-11	18235-18241	drives	
119-12	18242-18243	-	
119-13	18244-18247	i.e	
119-14	18247-18248	.	
119-15	18249-18252	not	
119-16	18253-18257	over	
119-17	18258-18265	already	
119-18	18266-18273	created	
119-19	18274-18282	hardware	
119-20	18283-18287	RAID	
119-21	18288-18295	volumes	
119-22	18296-18297	(	
119-23	18297-18301	this	
119-24	18302-18304	is	
119-25	18305-18314	sometimes	
119-26	18315-18321	called	
119-27	18322-18323	"	
119-28	18323-18327	JBOD	
119-29	18327-18328	"	
119-30	18329-18331	or	
119-31	18332-18333	"	
119-32	18333-18344	passthrough	
119-33	18344-18345	"	
119-34	18346-18350	mode	
119-35	18351-18355	when	
119-36	18356-18360	used	
119-37	18361-18365	with	
119-38	18366-18370	RAID	
119-39	18371-18382	controllers	
119-40	18382-18383	)	
119-41	18383-18384	,	
119-42	18385-18388	but	
119-43	18389-18392	can	
119-44	18393-18400	benefit	
119-45	18401-18408	greatly	
119-46	18409-18413	from	
119-47	18414-18418	good	
119-48	18419-18422	and	
119-49	18423-18427	fast	
119-50	18428-18439	controllers	
119-51	18439-18440	.	

#Text=MySQL
#Text=This assumes lots of RAM Tweaks for MySQL innodb_flush_log_at_trx_commit=2 skip-innodb_doublewrite Tweaks for ZFS zfs set primarycache=metadata tank/db zfs set atime=off tank/db zfs set recordsize=16k tank/db/innodb zfs set recordsize=128k tank/db/logs zfs set zfs:zfs_nocacheflush = 1 zfs set sync=disabled tank/db Note: MySQL 5.6.6 and newer (and related MariaDB / Percona forks)
#Text=has innodb_file_per_table = on as default, so IBD files are not created under tank/db/innodb (defined by innodb_data_home_dir in your my.cnf), they are created under tank/db/<db_name>/ and you should use recordsize=16k on this dataset too or switch back to innodb_file_per_table = off References MySQL Innodb ZFS Best Practices (Oracle)
#Text=Scrub and Resilver Performance
#Text=If you're getting horrible performance during a scrub or resilver, the following sysctls can be set: vfs.zfs.scrub_delay=0
#Text=vfs.zfs.top_maxinflight=128
#Text=vfs.zfs.resilver_min_time_ms=5000
#Text=vfs.zfs.resilver_delay=0Setting those sysctls to those values increased my (Shawn Webb's) resilver performance from 7MB/s to 230MB/s.
120-1	18441-18446	MySQL	
120-2	18447-18451	This	
120-3	18452-18459	assumes	
120-4	18460-18464	lots	
120-5	18465-18467	of	
120-6	18468-18471	RAM	
120-7	18472-18478	Tweaks	
120-8	18479-18482	for	
120-9	18483-18488	MySQL	
120-10	18489-18519	innodb_flush_log_at_trx_commit	
120-11	18519-18520	=	
120-12	18520-18521	2	
120-13	18522-18545	skip-innodb_doublewrite	
120-14	18546-18552	Tweaks	
120-15	18553-18556	for	
120-16	18557-18560	ZFS	
120-17	18561-18564	zfs	
120-18	18565-18568	set	
120-19	18569-18581	primarycache	
120-20	18581-18582	=	
120-21	18582-18590	metadata	
120-22	18591-18595	tank	
120-23	18595-18596	/	
120-24	18596-18598	db	
120-25	18599-18602	zfs	
120-26	18603-18606	set	
120-27	18607-18612	atime	
120-28	18612-18613	=	
120-29	18613-18616	off	
120-30	18617-18621	tank	
120-31	18621-18622	/	
120-32	18622-18624	db	
120-33	18625-18628	zfs	
120-34	18629-18632	set	
120-35	18633-18643	recordsize	
120-36	18643-18644	=	
120-37	18644-18647	16k	
120-38	18648-18652	tank	
120-39	18652-18653	/	
120-40	18653-18655	db	
120-41	18655-18656	/	
120-42	18656-18662	innodb	
120-43	18663-18666	zfs	
120-44	18667-18670	set	
120-45	18671-18681	recordsize	
120-46	18681-18682	=	
120-47	18682-18686	128k	
120-48	18687-18691	tank	
120-49	18691-18692	/	
120-50	18692-18694	db	
120-51	18694-18695	/	
120-52	18695-18699	logs	
120-53	18700-18703	zfs	
120-54	18704-18707	set	
120-55	18708-18711	zfs	
120-56	18711-18712	:	
120-57	18712-18728	zfs_nocacheflush	
120-58	18729-18730	=	
120-59	18731-18732	1	
120-60	18733-18736	zfs	
120-61	18737-18740	set	
120-62	18741-18745	sync	
120-63	18745-18746	=	
120-64	18746-18754	disabled	
120-65	18755-18759	tank	
120-66	18759-18760	/	
120-67	18760-18762	db	
120-68	18763-18767	Note	
120-69	18767-18768	:	
120-70	18769-18774	MySQL	
120-71	18775-18780	5.6.6	
120-72	18781-18784	and	
120-73	18785-18790	newer	
120-74	18791-18792	(	
120-75	18792-18795	and	
120-76	18796-18803	related	
120-77	18804-18811	MariaDB	
120-78	18812-18813	/	
120-79	18814-18821	Percona	
120-80	18822-18827	forks	
120-81	18827-18828	)	
120-82	18829-18832	has	
120-83	18833-18854	innodb_file_per_table	
120-84	18854-18855	 	
120-85	18855-18856	=	
120-86	18856-18857	 	
120-87	18857-18859	on	
120-88	18860-18862	as	
120-89	18863-18870	default	
120-90	18870-18871	,	
120-91	18872-18874	so	
120-92	18875-18878	IBD	
120-93	18879-18884	files	
120-94	18885-18888	are	
120-95	18889-18892	not	
120-96	18893-18900	created	
120-97	18901-18906	under	
120-98	18907-18911	tank	
120-99	18911-18912	/	
120-100	18912-18914	db	
120-101	18914-18915	/	
120-102	18915-18921	innodb	
120-103	18922-18923	(	
120-104	18923-18930	defined	
120-105	18931-18933	by	
120-106	18934-18954	innodb_data_home_dir	
120-107	18955-18957	in	
120-108	18958-18962	your	
120-109	18963-18969	my.cnf	
120-110	18969-18970	)	
120-111	18970-18971	,	
120-112	18972-18976	they	
120-113	18977-18980	are	
120-114	18981-18988	created	
120-115	18989-18994	under	
120-116	18995-18999	tank	
120-117	18999-19000	/	
120-118	19000-19002	db	
120-119	19002-19003	/	
120-120	19003-19004	<	
120-121	19004-19011	db_name	
120-122	19011-19012	>	
120-123	19012-19013	/	
120-124	19014-19017	and	
120-125	19018-19021	you	
120-126	19022-19028	should	
120-127	19029-19032	use	
120-128	19033-19043	recordsize	
120-129	19043-19044	=	
120-130	19044-19047	16k	
120-131	19048-19050	on	
120-132	19051-19055	this	
120-133	19056-19063	dataset	
120-134	19064-19067	too	
120-135	19068-19070	or	
120-136	19071-19077	switch	
120-137	19078-19082	back	
120-138	19083-19085	to	
120-139	19086-19107	innodb_file_per_table	
120-140	19107-19108	 	
120-141	19108-19109	=	
120-142	19109-19110	 	
120-143	19110-19113	off	
120-144	19114-19124	References	
120-145	19125-19130	MySQL	
120-146	19131-19137	Innodb	
120-147	19138-19141	ZFS	
120-148	19142-19146	Best	
120-149	19147-19156	Practices	
120-150	19157-19158	(	
120-151	19158-19164	Oracle	
120-152	19164-19165	)	
120-153	19166-19171	Scrub	
120-154	19172-19175	and	
120-155	19176-19184	Resilver	
120-156	19185-19196	Performance	
120-157	19197-19199	If	
120-158	19200-19206	you're	
120-159	19207-19214	getting	
120-160	19215-19223	horrible	
120-161	19224-19235	performance	
120-162	19236-19242	during	
120-163	19243-19244	a	
120-164	19245-19250	scrub	
120-165	19251-19253	or	
120-166	19254-19262	resilver	
120-167	19262-19263	,	
120-168	19264-19267	the	
120-169	19268-19277	following	
120-170	19278-19285	sysctls	
120-171	19286-19289	can	
120-172	19290-19292	be	
120-173	19293-19296	set	
120-174	19296-19297	:	
120-175	19298-19317	vfs.zfs.scrub_delay	
120-176	19317-19318	=	
120-177	19318-19319	0	
120-178	19320-19343	vfs.zfs.top_maxinflight	
120-179	19343-19344	=	
120-180	19344-19347	128	
120-181	19348-19376	vfs.zfs.resilver_min_time_ms	
120-182	19376-19377	=	
120-183	19377-19381	5000	
120-184	19382-19404	vfs.zfs.resilver_delay	
120-185	19404-19405	=	
120-186	19405-19413	0Setting	
120-187	19414-19419	those	
120-188	19420-19427	sysctls	
120-189	19428-19430	to	
120-190	19431-19436	those	
120-191	19437-19443	values	
120-192	19444-19453	increased	
120-193	19454-19456	my	
120-194	19457-19458	(	
120-195	19458-19463	Shawn	
120-196	19464-19470	Webb's	
120-197	19470-19471	)	
120-198	19472-19480	resilver	
120-199	19481-19492	performance	
120-200	19493-19497	from	
120-201	19498-19501	7MB	
120-202	19501-19502	/	
120-203	19502-19503	s	
120-204	19504-19506	to	
120-205	19507-19512	230MB	
120-206	19512-19513	/	
120-207	19513-19514	s	
120-208	19514-19515	.	

#Text=CategoryZfs CategoryHowTo ZFSTuningGuide
#Text=(last edited 2021-01-16 19:54:35 by MateuszPiotrowski)
#Text=Immutable PageCommentsInfoAttachments
#Text=More Actions:
#Text=Raw Text
#Text=Print View
#Text=Render as Docbook
#Text=Delete Cache
#Text=------------------------
#Text=Check Spelling
#Text=Like Pages
#Text=Local Site Map
#Text=------------------------
#Text=Rename Page
#Text=Delete Page
#Text=------------------------
#Text=Subscribe User
#Text=------------------------
#Text=Remove Spam
#Text=Revert to this revision
#Text=Package Pages
#Text=Sync Pages
#Text=------------------------
#Text=Load
#Text=Save
#Text=SlideShow
#Text=MoinMoin PoweredPython PoweredGPL licensedValid HTML 4.01
121-1	19516-19527	CategoryZfs	
121-2	19528-19541	CategoryHowTo	
121-3	19542-19556	ZFSTuningGuide	
121-4	19557-19558	(	
121-5	19558-19562	last	
121-6	19563-19569	edited	
121-7	19570-19574	2021	
121-8	19574-19575	-	
121-9	19575-19577	01	
121-10	19577-19578	-	
121-11	19578-19580	16	
121-12	19581-19583	19	
121-13	19583-19584	:	
121-14	19584-19586	54	
121-15	19586-19587	:	
121-16	19587-19589	35	
121-17	19590-19592	by	
121-18	19593-19610	MateuszPiotrowski	
121-19	19610-19611	)	
121-20	19612-19621	Immutable	
121-21	19622-19649	PageCommentsInfoAttachments	
121-22	19650-19654	More	
121-23	19655-19662	Actions	
121-24	19662-19663	:	
121-25	19664-19667	Raw	
121-26	19668-19672	Text	
121-27	19673-19678	Print	
121-28	19679-19683	View	
121-29	19684-19690	Render	
121-30	19691-19693	as	
121-31	19694-19701	Docbook	
121-32	19702-19708	Delete	
121-33	19709-19714	Cache	
121-34	19715-19716	-	
121-35	19716-19717	-	
121-36	19717-19718	-	
121-37	19718-19719	-	
121-38	19719-19720	-	
121-39	19720-19721	-	
121-40	19721-19722	-	
121-41	19722-19723	-	
121-42	19723-19724	-	
121-43	19724-19725	-	
121-44	19725-19726	-	
121-45	19726-19727	-	
121-46	19727-19728	-	
121-47	19728-19729	-	
121-48	19729-19730	-	
121-49	19730-19731	-	
121-50	19731-19732	-	
121-51	19732-19733	-	
121-52	19733-19734	-	
121-53	19734-19735	-	
121-54	19735-19736	-	
121-55	19736-19737	-	
121-56	19737-19738	-	
121-57	19738-19739	-	
121-58	19740-19745	Check	
121-59	19746-19754	Spelling	
121-60	19755-19759	Like	
121-61	19760-19765	Pages	
121-62	19766-19771	Local	
121-63	19772-19776	Site	
121-64	19777-19780	Map	
121-65	19781-19782	-	
121-66	19782-19783	-	
121-67	19783-19784	-	
121-68	19784-19785	-	
121-69	19785-19786	-	
121-70	19786-19787	-	
121-71	19787-19788	-	
121-72	19788-19789	-	
121-73	19789-19790	-	
121-74	19790-19791	-	
121-75	19791-19792	-	
121-76	19792-19793	-	
121-77	19793-19794	-	
121-78	19794-19795	-	
121-79	19795-19796	-	
121-80	19796-19797	-	
121-81	19797-19798	-	
121-82	19798-19799	-	
121-83	19799-19800	-	
121-84	19800-19801	-	
121-85	19801-19802	-	
121-86	19802-19803	-	
121-87	19803-19804	-	
121-88	19804-19805	-	
121-89	19806-19812	Rename	
121-90	19813-19817	Page	
121-91	19818-19824	Delete	
121-92	19825-19829	Page	
121-93	19830-19831	-	
121-94	19831-19832	-	
121-95	19832-19833	-	
121-96	19833-19834	-	
121-97	19834-19835	-	
121-98	19835-19836	-	
121-99	19836-19837	-	
121-100	19837-19838	-	
121-101	19838-19839	-	
121-102	19839-19840	-	
121-103	19840-19841	-	
121-104	19841-19842	-	
121-105	19842-19843	-	
121-106	19843-19844	-	
121-107	19844-19845	-	
121-108	19845-19846	-	
121-109	19846-19847	-	
121-110	19847-19848	-	
121-111	19848-19849	-	
121-112	19849-19850	-	
121-113	19850-19851	-	
121-114	19851-19852	-	
121-115	19852-19853	-	
121-116	19853-19854	-	
121-117	19855-19864	Subscribe	
121-118	19865-19869	User	
121-119	19870-19871	-	
121-120	19871-19872	-	
121-121	19872-19873	-	
121-122	19873-19874	-	
121-123	19874-19875	-	
121-124	19875-19876	-	
121-125	19876-19877	-	
121-126	19877-19878	-	
121-127	19878-19879	-	
121-128	19879-19880	-	
121-129	19880-19881	-	
121-130	19881-19882	-	
121-131	19882-19883	-	
121-132	19883-19884	-	
121-133	19884-19885	-	
121-134	19885-19886	-	
121-135	19886-19887	-	
121-136	19887-19888	-	
121-137	19888-19889	-	
121-138	19889-19890	-	
121-139	19890-19891	-	
121-140	19891-19892	-	
121-141	19892-19893	-	
121-142	19893-19894	-	
121-143	19895-19901	Remove	
121-144	19902-19906	Spam	
121-145	19907-19913	Revert	
121-146	19914-19916	to	
121-147	19917-19921	this	
121-148	19922-19930	revision	
121-149	19931-19938	Package	
121-150	19939-19944	Pages	
121-151	19945-19949	Sync	
121-152	19950-19955	Pages	
121-153	19956-19957	-	
121-154	19957-19958	-	
121-155	19958-19959	-	
121-156	19959-19960	-	
121-157	19960-19961	-	
121-158	19961-19962	-	
121-159	19962-19963	-	
121-160	19963-19964	-	
121-161	19964-19965	-	
121-162	19965-19966	-	
121-163	19966-19967	-	
121-164	19967-19968	-	
121-165	19968-19969	-	
121-166	19969-19970	-	
121-167	19970-19971	-	
121-168	19971-19972	-	
121-169	19972-19973	-	
121-170	19973-19974	-	
121-171	19974-19975	-	
121-172	19975-19976	-	
121-173	19976-19977	-	
121-174	19977-19978	-	
121-175	19978-19979	-	
121-176	19979-19980	-	
121-177	19981-19985	Load	
121-178	19986-19990	Save	
121-179	19991-20000	SlideShow	
121-180	20001-20009	MoinMoin	
121-181	20010-20023	PoweredPython	
121-182	20024-20034	PoweredGPL	
121-183	20035-20048	licensedValid	
121-184	20049-20053	HTML	
121-185	20054-20058	4.01	
