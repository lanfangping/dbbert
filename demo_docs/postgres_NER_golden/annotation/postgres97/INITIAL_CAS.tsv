#FORMAT=WebAnno TSV 3.3


#Text=Spark tips.
1-1	0-5	Spark	
1-2	6-10	tips	
1-3	10-11	.	

#Text=DataFrame API - Blog | luminousmen
#Text=Spark tips.
2-1	12-21	DataFrame	
2-2	22-25	API	
2-3	26-27	-	
2-4	28-32	Blog	
2-5	33-34	|	
2-6	35-46	luminousmen	
2-7	47-52	Spark	
2-8	53-57	tips	
2-9	57-58	.	

#Text=DataFrame API
#Text=Last updated
#Text=Thu May 07 2020
#Text=There are many different tools in the world, each of which solves a range of problems.
3-1	59-68	DataFrame	
3-2	69-72	API	
3-3	73-77	Last	
3-4	78-85	updated	
3-5	86-89	Thu	
3-6	90-93	May	
3-7	94-96	07	
3-8	97-101	2020	
3-9	102-107	There	
3-10	108-111	are	
3-11	112-116	many	
3-12	117-126	different	
3-13	127-132	tools	
3-14	133-135	in	
3-15	136-139	the	
3-16	140-145	world	
3-17	145-146	,	
3-18	147-151	each	
3-19	152-154	of	
3-20	155-160	which	
3-21	161-167	solves	
3-22	168-169	a	
3-23	170-175	range	
3-24	176-178	of	
3-25	179-187	problems	
3-26	187-188	.	

#Text=Many of them are judged by how well and correct they solve this or that problem, but there are tools that you just like, you want to use them.
4-1	189-193	Many	
4-2	194-196	of	
4-3	197-201	them	
4-4	202-205	are	
4-5	206-212	judged	
4-6	213-215	by	
4-7	216-219	how	
4-8	220-224	well	
4-9	225-228	and	
4-10	229-236	correct	
4-11	237-241	they	
4-12	242-247	solve	
4-13	248-252	this	
4-14	253-255	or	
4-15	256-260	that	
4-16	261-268	problem	
4-17	268-269	,	
4-18	270-273	but	
4-19	274-279	there	
4-20	280-283	are	
4-21	284-289	tools	
4-22	290-294	that	
4-23	295-298	you	
4-24	299-303	just	
4-25	304-308	like	
4-26	308-309	,	
4-27	310-313	you	
4-28	314-318	want	
4-29	319-321	to	
4-30	322-325	use	
4-31	326-330	them	
4-32	330-331	.	

#Text=They are properly designed and fit well in your hand, you do not need to dig into the documentation and understand how to do this or that simple action.
5-1	332-336	They	
5-2	337-340	are	
5-3	341-349	properly	
5-4	350-358	designed	
5-5	359-362	and	
5-6	363-366	fit	
5-7	367-371	well	
5-8	372-374	in	
5-9	375-379	your	
5-10	380-384	hand	
5-11	384-385	,	
5-12	386-389	you	
5-13	390-392	do	
5-14	393-396	not	
5-15	397-401	need	
5-16	402-404	to	
5-17	405-408	dig	
5-18	409-413	into	
5-19	414-417	the	
5-20	418-431	documentation	
5-21	432-435	and	
5-22	436-446	understand	
5-23	447-450	how	
5-24	451-453	to	
5-25	454-456	do	
5-26	457-461	this	
5-27	462-464	or	
5-28	465-469	that	
5-29	470-476	simple	
5-30	477-483	action	
5-31	483-484	.	

#Text=About one of these tools for me I will be writing this series of posts.
6-1	485-490	About	
6-2	491-494	one	
6-3	495-497	of	
6-4	498-503	these	
6-5	504-509	tools	
6-6	510-513	for	
6-7	514-516	me	
6-8	517-518	I	
6-9	519-523	will	
6-10	524-526	be	
6-11	527-534	writing	
6-12	535-539	this	
6-13	540-546	series	
6-14	547-549	of	
6-15	550-555	posts	
6-16	555-556	.	

#Text=I will describe the optimization methods and tips that help me solve certain technical problems and achieve high efficiency using Apache Spark.
7-1	557-558	I	
7-2	559-563	will	
7-3	564-572	describe	
7-4	573-576	the	
7-5	577-589	optimization	
7-6	590-597	methods	
7-7	598-601	and	
7-8	602-606	tips	
7-9	607-611	that	
7-10	612-616	help	
7-11	617-619	me	
7-12	620-625	solve	
7-13	626-633	certain	
7-14	634-643	technical	
7-15	644-652	problems	
7-16	653-656	and	
7-17	657-664	achieve	
7-18	665-669	high	
7-19	670-680	efficiency	
7-20	681-686	using	
7-21	687-693	Apache	
7-22	694-699	Spark	
7-23	699-700	.	

#Text=This is my updated collection.
8-1	701-705	This	
8-2	706-708	is	
8-3	709-711	my	
8-4	712-719	updated	
8-5	720-730	collection	
8-6	730-731	.	

#Text=Many of the optimizations that I will describe will not affect the JVM languages ​​so much, but without these methods, many Python applications may simply not work.
9-1	732-736	Many	
9-2	737-739	of	
9-3	740-743	the	
9-4	744-757	optimizations	
9-5	758-762	that	
9-6	763-764	I	
9-7	765-769	will	
9-8	770-778	describe	
9-9	779-783	will	
9-10	784-787	not	
9-11	788-794	affect	
9-12	795-798	the	
9-13	799-802	JVM	
9-14	803-812	languages	
9-15	813-815	​​	
9-16	815-817	so	
9-17	818-822	much	
9-18	822-823	,	
9-19	824-827	but	
9-20	828-835	without	
9-21	836-841	these	
9-22	842-849	methods	
9-23	849-850	,	
9-24	851-855	many	
9-25	856-862	Python	
9-26	863-875	applications	
9-27	876-879	may	
9-28	880-886	simply	
9-29	887-890	not	
9-30	891-895	work	
9-31	895-896	.	

#Text=Whole series:
#Text=Spark tips.
10-1	897-902	Whole	
10-2	903-909	series	
10-3	909-910	:	
10-4	911-916	Spark	
10-5	917-921	tips	
10-6	921-922	.	

#Text=DataFrame API
#Text=Spark Tips.
11-1	923-932	DataFrame	
11-2	933-936	API	
11-3	937-942	Spark	
11-4	943-947	Tips	
11-5	947-948	.	

#Text=Don't collect data on driver
#Text=The 5-minute guide to using bucketing in Pyspark
#Text=Spark Tips.
12-1	949-954	Don't	
12-2	955-962	collect	
12-3	963-967	data	
12-4	968-970	on	
12-5	971-977	driver	
12-6	978-981	The	
12-7	982-983	5	
12-8	983-984	-	
12-9	984-990	minute	
12-10	991-996	guide	
12-11	997-999	to	
12-12	1000-1005	using	
12-13	1006-1015	bucketing	
12-14	1016-1018	in	
12-15	1019-1026	Pyspark	
12-16	1027-1032	Spark	
12-17	1033-1037	Tips	
12-18	1037-1038	.	

#Text=Partition Tuning
#Text=Use DataFrame API
#Text=We know that RDD is a fault-tolerant collection of elements that can be processed in parallel.
13-1	1039-1048	Partition	
13-2	1049-1055	Tuning	
13-3	1056-1059	Use	
13-4	1060-1069	DataFrame	
13-5	1070-1073	API	
13-6	1074-1076	We	
13-7	1077-1081	know	
13-8	1082-1086	that	
13-9	1087-1090	RDD	
13-10	1091-1093	is	
13-11	1094-1095	a	
13-12	1096-1110	fault-tolerant	
13-13	1111-1121	collection	
13-14	1122-1124	of	
13-15	1125-1133	elements	
13-16	1134-1138	that	
13-17	1139-1142	can	
13-18	1143-1145	be	
13-19	1146-1155	processed	
13-20	1156-1158	in	
13-21	1159-1167	parallel	
13-22	1167-1168	.	

#Text=But RDDs actually kind of black box of data — we know that it holds some data but we do not know the type of the data or any other properties of the data.
14-1	1169-1172	But	
14-2	1173-1177	RDDs	
14-3	1178-1186	actually	
14-4	1187-1191	kind	
14-5	1192-1194	of	
14-6	1195-1200	black	
14-7	1201-1204	box	
14-8	1205-1207	of	
14-9	1208-1212	data	
14-10	1213-1214	—	
14-11	1215-1217	we	
14-12	1218-1222	know	
14-13	1223-1227	that	
14-14	1228-1230	it	
14-15	1231-1236	holds	
14-16	1237-1241	some	
14-17	1242-1246	data	
14-18	1247-1250	but	
14-19	1251-1253	we	
14-20	1254-1256	do	
14-21	1257-1260	not	
14-22	1261-1265	know	
14-23	1266-1269	the	
14-24	1270-1274	type	
14-25	1275-1277	of	
14-26	1278-1281	the	
14-27	1282-1286	data	
14-28	1287-1289	or	
14-29	1290-1293	any	
14-30	1294-1299	other	
14-31	1300-1310	properties	
14-32	1311-1313	of	
14-33	1314-1317	the	
14-34	1318-1322	data	
14-35	1322-1323	.	

#Text=Hence it's data cannot be optimized as well as the operations on it.
15-1	1324-1329	Hence	
15-2	1330-1334	it's	
15-3	1335-1339	data	
15-4	1340-1346	cannot	
15-5	1347-1349	be	
15-6	1350-1359	optimized	
15-7	1360-1362	as	
15-8	1363-1367	well	
15-9	1368-1370	as	
15-10	1371-1374	the	
15-11	1375-1385	operations	
15-12	1386-1388	on	
15-13	1389-1391	it	
15-14	1391-1392	.	

#Text=Spark 1.3 introduced a new abstraction — a DataFrame, in Spark 1.6 the Project Tungsten was introduced, an initiative which seeks to improve the performance and scalability of Spark.
16-1	1393-1398	Spark	
16-2	1399-1402	1.3	
16-3	1403-1413	introduced	
16-4	1414-1415	a	
16-5	1416-1419	new	
16-6	1420-1431	abstraction	
16-7	1432-1433	—	
16-8	1434-1435	a	
16-9	1436-1445	DataFrame	
16-10	1445-1446	,	
16-11	1447-1449	in	
16-12	1450-1455	Spark	
16-13	1456-1459	1.6	
16-14	1460-1463	the	
16-15	1464-1471	Project	
16-16	1472-1480	Tungsten	
16-17	1481-1484	was	
16-18	1485-1495	introduced	
16-19	1495-1496	,	
16-20	1497-1499	an	
16-21	1500-1510	initiative	
16-22	1511-1516	which	
16-23	1517-1522	seeks	
16-24	1523-1525	to	
16-25	1526-1533	improve	
16-26	1534-1537	the	
16-27	1538-1549	performance	
16-28	1550-1553	and	
16-29	1554-1565	scalability	
16-30	1566-1568	of	
16-31	1569-1574	Spark	
16-32	1574-1575	.	

#Text=DataFrame data is organized into named columns.
17-1	1576-1585	DataFrame	
17-2	1586-1590	data	
17-3	1591-1593	is	
17-4	1594-1603	organized	
17-5	1604-1608	into	
17-6	1609-1614	named	
17-7	1615-1622	columns	
17-8	1622-1623	.	

#Text=It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.
18-1	1624-1626	It	
18-2	1627-1629	is	
18-3	1630-1642	conceptually	
18-4	1643-1653	equivalent	
18-5	1654-1656	to	
18-6	1657-1658	a	
18-7	1659-1664	table	
18-8	1665-1667	in	
18-9	1668-1669	a	
18-10	1670-1680	relational	
18-11	1681-1689	database	
18-12	1690-1692	or	
18-13	1693-1694	a	
18-14	1695-1699	data	
18-15	1700-1705	frame	
18-16	1706-1708	in	
18-17	1709-1710	R	
18-18	1710-1711	/	
18-19	1711-1717	Python	
18-20	1717-1718	,	
18-21	1719-1722	but	
18-22	1723-1727	with	
18-23	1728-1734	richer	
18-24	1735-1748	optimizations	
18-25	1749-1754	under	
18-26	1755-1758	the	
18-27	1759-1763	hood	
18-28	1763-1764	.	

#Text=Using DataFrame API you get:
#Text=Efficient serialization/deserialization and memory usage
#Text=In RDDs Spark uses Java serialization when it is necessary to distribute data across a cluster.
19-1	1765-1770	Using	
19-2	1771-1780	DataFrame	
19-3	1781-1784	API	
19-4	1785-1788	you	
19-5	1789-1792	get	
19-6	1792-1793	:	
19-7	1794-1803	Efficient	
19-8	1804-1817	serialization	
19-9	1817-1818	/	
19-10	1818-1833	deserialization	
19-11	1834-1837	and	
19-12	1838-1844	memory	
19-13	1845-1850	usage	
19-14	1851-1853	In	
19-15	1854-1858	RDDs	
19-16	1859-1864	Spark	
19-17	1865-1869	uses	
19-18	1870-1874	Java	
19-19	1875-1888	serialization	
19-20	1889-1893	when	
19-21	1894-1896	it	
19-22	1897-1899	is	
19-23	1900-1909	necessary	
19-24	1910-1912	to	
19-25	1913-1923	distribute	
19-26	1924-1928	data	
19-27	1929-1935	across	
19-28	1936-1937	a	
19-29	1938-1945	cluster	
19-30	1945-1946	.	

#Text=The serialization of individual Scala and Java objects is expensive.
20-1	1947-1950	The	
20-2	1951-1964	serialization	
20-3	1965-1967	of	
20-4	1968-1978	individual	
20-5	1979-1984	Scala	
20-6	1985-1988	and	
20-7	1989-1993	Java	
20-8	1994-2001	objects	
20-9	2002-2004	is	
20-10	2005-2014	expensive	
20-11	2014-2015	.	

#Text=In Pyspark, it has become more expensive when all data is double-serialized/deserialized to Java/Scala and then to Python (using cloudpickle) and back.
21-1	2016-2018	In	
21-2	2019-2026	Pyspark	
21-3	2026-2027	,	
21-4	2028-2030	it	
21-5	2031-2034	has	
21-6	2035-2041	become	
21-7	2042-2046	more	
21-8	2047-2056	expensive	
21-9	2057-2061	when	
21-10	2062-2065	all	
21-11	2066-2070	data	
21-12	2071-2073	is	
21-13	2074-2091	double-serialized	
21-14	2091-2092	/	
21-15	2092-2104	deserialized	
21-16	2105-2107	to	
21-17	2108-2112	Java	
21-18	2112-2113	/	
21-19	2113-2118	Scala	
21-20	2119-2122	and	
21-21	2123-2127	then	
21-22	2128-2130	to	
21-23	2131-2137	Python	
21-24	2138-2139	(	
21-25	2139-2144	using	
21-26	2145-2156	cloudpickle	
21-27	2156-2157	)	
21-28	2158-2161	and	
21-29	2162-2166	back	
21-30	2166-2167	.	

#Text=The cost of double serialization is the most expensive part when working with Pyspark.
22-1	2168-2171	The	
22-2	2172-2176	cost	
22-3	2177-2179	of	
22-4	2180-2186	double	
22-5	2187-2200	serialization	
22-6	2201-2203	is	
22-7	2204-2207	the	
22-8	2208-2212	most	
22-9	2213-2222	expensive	
22-10	2223-2227	part	
22-11	2228-2232	when	
22-12	2233-2240	working	
22-13	2241-2245	with	
22-14	2246-2253	Pyspark	
22-15	2253-2254	.	

#Text=For distributed systems such as Spark, most of our time is spent only on serialization of data.
23-1	2255-2258	For	
23-2	2259-2270	distributed	
23-3	2271-2278	systems	
23-4	2279-2283	such	
23-5	2284-2286	as	
23-6	2287-2292	Spark	
23-7	2292-2293	,	
23-8	2294-2298	most	
23-9	2299-2301	of	
23-10	2302-2305	our	
23-11	2306-2310	time	
23-12	2311-2313	is	
23-13	2314-2319	spent	
23-14	2320-2324	only	
23-15	2325-2327	on	
23-16	2328-2341	serialization	
23-17	2342-2344	of	
23-18	2345-2349	data	
23-19	2349-2350	.	

#Text=Actual computing is often not a big blocking part — most of it is just serializing things and shuffling them around.
24-1	2351-2357	Actual	
24-2	2358-2367	computing	
24-3	2368-2370	is	
24-4	2371-2376	often	
24-5	2377-2380	not	
24-6	2381-2382	a	
24-7	2383-2386	big	
24-8	2387-2395	blocking	
24-9	2396-2400	part	
24-10	2401-2402	—	
24-11	2403-2407	most	
24-12	2408-2410	of	
24-13	2411-2413	it	
24-14	2414-2416	is	
24-15	2417-2421	just	
24-16	2422-2433	serializing	
24-17	2434-2440	things	
24-18	2441-2444	and	
24-19	2445-2454	shuffling	
24-20	2455-2459	them	
24-21	2460-2466	around	
24-22	2466-2467	.	

#Text=We want to avoid this double cost of serialization.
25-1	2468-2470	We	
25-2	2471-2475	want	
25-3	2476-2478	to	
25-4	2479-2484	avoid	
25-5	2485-2489	this	
25-6	2490-2496	double	
25-7	2497-2501	cost	
25-8	2502-2504	of	
25-9	2505-2518	serialization	
25-10	2518-2519	.	

#Text=In RDD we have this double serialization, and we pay for it on every operation — wherever we need to distribute data into a cluster or call the python function.
26-1	2520-2522	In	
26-2	2523-2526	RDD	
26-3	2527-2529	we	
26-4	2530-2534	have	
26-5	2535-2539	this	
26-6	2540-2546	double	
26-7	2547-2560	serialization	
26-8	2560-2561	,	
26-9	2562-2565	and	
26-10	2566-2568	we	
26-11	2569-2572	pay	
26-12	2573-2576	for	
26-13	2577-2579	it	
26-14	2580-2582	on	
26-15	2583-2588	every	
26-16	2589-2598	operation	
26-17	2599-2600	—	
26-18	2601-2609	wherever	
26-19	2610-2612	we	
26-20	2613-2617	need	
26-21	2618-2620	to	
26-22	2621-2631	distribute	
26-23	2632-2636	data	
26-24	2637-2641	into	
26-25	2642-2643	a	
26-26	2644-2651	cluster	
26-27	2652-2654	or	
26-28	2655-2659	call	
26-29	2660-2663	the	
26-30	2664-2670	python	
26-31	2671-2679	function	
26-32	2679-2680	.	

#Text=It also requires both data and structure to be transferred between nodes.
27-1	2681-2683	It	
27-2	2684-2688	also	
27-3	2689-2697	requires	
27-4	2698-2702	both	
27-5	2703-2707	data	
27-6	2708-2711	and	
27-7	2712-2721	structure	
27-8	2722-2724	to	
27-9	2725-2727	be	
27-10	2728-2739	transferred	
27-11	2740-2747	between	
27-12	2748-2753	nodes	
27-13	2753-2754	.	

#Text=With the DataFrame API, everything is a bit different.
28-1	2755-2759	With	
28-2	2760-2763	the	
28-3	2764-2773	DataFrame	
28-4	2774-2777	API	
28-5	2777-2778	,	
28-6	2779-2789	everything	
28-7	2790-2792	is	
28-8	2793-2794	a	
28-9	2795-2798	bit	
28-10	2799-2808	different	
28-11	2808-2809	.	

#Text=Since Spark understands data structure and column types if they are represented as DataFrame, he can understand how to store and manage them more efficiently(Java objects have a large inherent memory overhead as well as JVM GC).
29-1	2810-2815	Since	
29-2	2816-2821	Spark	
29-3	2822-2833	understands	
29-4	2834-2838	data	
29-5	2839-2848	structure	
29-6	2849-2852	and	
29-7	2853-2859	column	
29-8	2860-2865	types	
29-9	2866-2868	if	
29-10	2869-2873	they	
29-11	2874-2877	are	
29-12	2878-2889	represented	
29-13	2890-2892	as	
29-14	2893-2902	DataFrame	
29-15	2902-2903	,	
29-16	2904-2906	he	
29-17	2907-2910	can	
29-18	2911-2921	understand	
29-19	2922-2925	how	
29-20	2926-2928	to	
29-21	2929-2934	store	
29-22	2935-2938	and	
29-23	2939-2945	manage	
29-24	2946-2950	them	
29-25	2951-2955	more	
29-26	2956-2967	efficiently	
29-27	2967-2968	(	
29-28	2968-2972	Java	
29-29	2973-2980	objects	
29-30	2981-2985	have	
29-31	2986-2987	a	
29-32	2988-2993	large	
29-33	2994-3002	inherent	
29-34	3003-3009	memory	
29-35	3010-3018	overhead	
29-36	3019-3021	as	
29-37	3022-3026	well	
29-38	3027-3029	as	
29-39	3030-3033	JVM	
29-40	3034-3036	GC	
29-41	3036-3037	)	
29-42	3037-3038	.	

#Text=The DataFrame API does two things that help to do this (through the Tungsten project).
30-1	3039-3042	The	
30-2	3043-3052	DataFrame	
30-3	3053-3056	API	
30-4	3057-3061	does	
30-5	3062-3065	two	
30-6	3066-3072	things	
30-7	3073-3077	that	
30-8	3078-3082	help	
30-9	3083-3085	to	
30-10	3086-3088	do	
30-11	3089-3093	this	
30-12	3094-3095	(	
30-13	3095-3102	through	
30-14	3103-3106	the	
30-15	3107-3115	Tungsten	
30-16	3116-3123	project	
30-17	3123-3124	)	
30-18	3124-3125	.	

#Text=First, using off-heap storage for data in binary format.
31-1	3126-3131	First	
31-2	3131-3132	,	
31-3	3133-3138	using	
31-4	3139-3147	off-heap	
31-5	3148-3155	storage	
31-6	3156-3159	for	
31-7	3160-3164	data	
31-8	3165-3167	in	
31-9	3168-3174	binary	
31-10	3175-3181	format	
31-11	3181-3182	.	

#Text=Second, generating encoder code on the fly to work with this binary format for your specific objects.
32-1	3183-3189	Second	
32-2	3189-3190	,	
32-3	3191-3201	generating	
32-4	3202-3209	encoder	
32-5	3210-3214	code	
32-6	3215-3217	on	
32-7	3218-3221	the	
32-8	3222-3225	fly	
32-9	3226-3228	to	
32-10	3229-3233	work	
32-11	3234-3238	with	
32-12	3239-3243	this	
32-13	3244-3250	binary	
32-14	3251-3257	format	
32-15	3258-3261	for	
32-16	3262-3266	your	
32-17	3267-3275	specific	
32-18	3276-3283	objects	
32-19	3283-3284	.	

#Text=In simple words, Spark says:
#Text=I'm going to generate these efficient encoders so that I can pull the data right out in this compact format and process it in that format right through the transformation chain.
33-1	3285-3287	In	
33-2	3288-3294	simple	
33-3	3295-3300	words	
33-4	3300-3301	,	
33-5	3302-3307	Spark	
33-6	3308-3312	says	
33-7	3312-3313	:	
33-8	3314-3317	I'm	
33-9	3318-3323	going	
33-10	3324-3326	to	
33-11	3327-3335	generate	
33-12	3336-3341	these	
33-13	3342-3351	efficient	
33-14	3352-3360	encoders	
33-15	3361-3363	so	
33-16	3364-3368	that	
33-17	3369-3370	I	
33-18	3371-3374	can	
33-19	3375-3379	pull	
33-20	3380-3383	the	
33-21	3384-3388	data	
33-22	3389-3394	right	
33-23	3395-3398	out	
33-24	3399-3401	in	
33-25	3402-3406	this	
33-26	3407-3414	compact	
33-27	3415-3421	format	
33-28	3422-3425	and	
33-29	3426-3433	process	
33-30	3434-3436	it	
33-31	3437-3439	in	
33-32	3440-3444	that	
33-33	3445-3451	format	
33-34	3452-3457	right	
33-35	3458-3465	through	
33-36	3466-3469	the	
33-37	3470-3484	transformation	
33-38	3485-3490	chain	
33-39	3490-3491	.	

#Text=I'm going to minimize the amount of work I'm required to do in JVM (no, I'm not going to do anything in Python) and maximize the amount of data I can process in my compact format stored in off-heap memory.
34-1	3492-3495	I'm	
34-2	3496-3501	going	
34-3	3502-3504	to	
34-4	3505-3513	minimize	
34-5	3514-3517	the	
34-6	3518-3524	amount	
34-7	3525-3527	of	
34-8	3528-3532	work	
34-9	3533-3536	I'm	
34-10	3537-3545	required	
34-11	3546-3548	to	
34-12	3549-3551	do	
34-13	3552-3554	in	
34-14	3555-3558	JVM	
34-15	3559-3560	(	
34-16	3560-3562	no	
34-17	3562-3563	,	
34-18	3564-3567	I'm	
34-19	3568-3571	not	
34-20	3572-3577	going	
34-21	3578-3580	to	
34-22	3581-3583	do	
34-23	3584-3592	anything	
34-24	3593-3595	in	
34-25	3596-3602	Python	
34-26	3602-3603	)	
34-27	3604-3607	and	
34-28	3608-3616	maximize	
34-29	3617-3620	the	
34-30	3621-3627	amount	
34-31	3628-3630	of	
34-32	3631-3635	data	
34-33	3636-3637	I	
34-34	3638-3641	can	
34-35	3642-3649	process	
34-36	3650-3652	in	
34-37	3653-3655	my	
34-38	3656-3663	compact	
34-39	3664-3670	format	
34-40	3671-3677	stored	
34-41	3678-3680	in	
34-42	3681-3689	off-heap	
34-43	3690-3696	memory	
34-44	3696-3697	.	

#Text=It turns out that you don't pay a penalty for serialization/deserialization — you work directly on this compact format, and using off-heap memory is a great way to reduce GC pauses because it is not in the scope of GC.
35-1	3698-3700	It	
35-2	3701-3706	turns	
35-3	3707-3710	out	
35-4	3711-3715	that	
35-5	3716-3719	you	
35-6	3720-3725	don't	
35-7	3726-3729	pay	
35-8	3730-3731	a	
35-9	3732-3739	penalty	
35-10	3740-3743	for	
35-11	3744-3757	serialization	
35-12	3757-3758	/	
35-13	3758-3773	deserialization	
35-14	3774-3775	—	
35-15	3776-3779	you	
35-16	3780-3784	work	
35-17	3785-3793	directly	
35-18	3794-3796	on	
35-19	3797-3801	this	
35-20	3802-3809	compact	
35-21	3810-3816	format	
35-22	3816-3817	,	
35-23	3818-3821	and	
35-24	3822-3827	using	
35-25	3828-3836	off-heap	
35-26	3837-3843	memory	
35-27	3844-3846	is	
35-28	3847-3848	a	
35-29	3849-3854	great	
35-30	3855-3858	way	
35-31	3859-3861	to	
35-32	3862-3868	reduce	
35-33	3869-3871	GC	
35-34	3872-3878	pauses	
35-35	3879-3886	because	
35-36	3887-3889	it	
35-37	3890-3892	is	
35-38	3893-3896	not	
35-39	3897-3899	in	
35-40	3900-3903	the	
35-41	3904-3909	scope	
35-42	3910-3912	of	
35-43	3913-3915	GC	
35-44	3915-3916	.	

#Text=So it's great that we can store our data inside the JVM, but we can still write code in Python using a very clear API and it will still be very efficient!
36-1	3917-3919	So	
36-2	3920-3924	it's	
36-3	3925-3930	great	
36-4	3931-3935	that	
36-5	3936-3938	we	
36-6	3939-3942	can	
36-7	3943-3948	store	
36-8	3949-3952	our	
36-9	3953-3957	data	
36-10	3958-3964	inside	
36-11	3965-3968	the	
36-12	3969-3972	JVM	
36-13	3972-3973	,	
36-14	3974-3977	but	
36-15	3978-3980	we	
36-16	3981-3984	can	
36-17	3985-3990	still	
36-18	3991-3996	write	
36-19	3997-4001	code	
36-20	4002-4004	in	
36-21	4005-4011	Python	
36-22	4012-4017	using	
36-23	4018-4019	a	
36-24	4020-4024	very	
36-25	4025-4030	clear	
36-26	4031-4034	API	
36-27	4035-4038	and	
36-28	4039-4041	it	
36-29	4042-4046	will	
36-30	4047-4052	still	
36-31	4053-4055	be	
36-32	4056-4060	very	
36-33	4061-4070	efficient	
36-34	4070-4071	!	

#Text=Schema Projection
#Text=The RDD API explicitly uses schematic projection.
37-1	4072-4078	Schema	
37-2	4079-4089	Projection	
37-3	4090-4093	The	
37-4	4094-4097	RDD	
37-5	4098-4101	API	
37-6	4102-4112	explicitly	
37-7	4113-4117	uses	
37-8	4118-4127	schematic	
37-9	4128-4138	projection	
37-10	4138-4139	.	

#Text=Therefore the user needs to define the schema manually.
38-1	4140-4149	Therefore	
38-2	4150-4153	the	
38-3	4154-4158	user	
38-4	4159-4164	needs	
38-5	4165-4167	to	
38-6	4168-4174	define	
38-7	4175-4178	the	
38-8	4179-4185	schema	
38-9	4186-4194	manually	
38-10	4194-4195	.	

#Text=There's no need to specify the schema explicitly in DataFrame.
39-1	4196-4203	There's	
39-2	4204-4206	no	
39-3	4207-4211	need	
39-4	4212-4214	to	
39-5	4215-4222	specify	
39-6	4223-4226	the	
39-7	4227-4233	schema	
39-8	4234-4244	explicitly	
39-9	4245-4247	in	
39-10	4248-4257	DataFrame	
39-11	4257-4258	.	

#Text=As a rule, Spark can detect the schema automatically(inferSchema option).
40-1	4259-4261	As	
40-2	4262-4263	a	
40-3	4264-4268	rule	
40-4	4268-4269	,	
40-5	4270-4275	Spark	
40-6	4276-4279	can	
40-7	4280-4286	detect	
40-8	4287-4290	the	
40-9	4291-4297	schema	
40-10	4298-4311	automatically	
40-11	4311-4312	(	
40-12	4312-4323	inferSchema	
40-13	4324-4330	option	
40-14	4330-4331	)	
40-15	4331-4332	.	

#Text=But the schema's resolution depends mainly on the data sources.
41-1	4333-4336	But	
41-2	4337-4340	the	
41-3	4341-4349	schema's	
41-4	4350-4360	resolution	
41-5	4361-4368	depends	
41-6	4369-4375	mainly	
41-7	4376-4378	on	
41-8	4379-4382	the	
41-9	4383-4387	data	
41-10	4388-4395	sources	
41-11	4395-4396	.	

#Text=If the source should contain structured data (i.e. relational database), the schema is extracted directly without any guesswork.
42-1	4397-4399	If	
42-2	4400-4403	the	
42-3	4404-4410	source	
42-4	4411-4417	should	
42-5	4418-4425	contain	
42-6	4426-4436	structured	
42-7	4437-4441	data	
42-8	4442-4443	(	
42-9	4443-4446	i.e	
42-10	4446-4447	.	
42-11	4448-4458	relational	
42-12	4459-4467	database	
42-13	4467-4468	)	
42-14	4468-4469	,	
42-15	4470-4473	the	
42-16	4474-4480	schema	
42-17	4481-4483	is	
42-18	4484-4493	extracted	
42-19	4494-4502	directly	
42-20	4503-4510	without	
42-21	4511-4514	any	
42-22	4515-4524	guesswork	
42-23	4524-4525	.	

#Text=More complex operations apply to semi-structured data, such as JSON files.
43-1	4526-4530	More	
43-2	4531-4538	complex	
43-3	4539-4549	operations	
43-4	4550-4555	apply	
43-5	4556-4558	to	
43-6	4559-4574	semi-structured	
43-7	4575-4579	data	
43-8	4579-4580	,	
43-9	4581-4585	such	
43-10	4586-4588	as	
43-11	4589-4593	JSON	
43-12	4594-4599	files	
43-13	4599-4600	.	

#Text=In these cases the schema is guesswork.
44-1	4601-4603	In	
44-2	4604-4609	these	
44-3	4610-4615	cases	
44-4	4616-4619	the	
44-5	4620-4626	schema	
44-6	4627-4629	is	
44-7	4630-4639	guesswork	
44-8	4639-4640	.	

#Text=Optimizations
#Text=RDD cannot be optimized by Spark — they are fully lambda driven.
45-1	4641-4654	Optimizations	
45-2	4655-4658	RDD	
45-3	4659-4665	cannot	
45-4	4666-4668	be	
45-5	4669-4678	optimized	
45-6	4679-4681	by	
45-7	4682-4687	Spark	
45-8	4688-4689	—	
45-9	4690-4694	they	
45-10	4695-4698	are	
45-11	4699-4704	fully	
45-12	4705-4711	lambda	
45-13	4712-4718	driven	
45-14	4718-4719	.	

#Text=RDD is rather a "black box" of data, which cannot be optimized because Spark can' t look inside what the data actually consists of.
46-1	4720-4723	RDD	
46-2	4724-4726	is	
46-3	4727-4733	rather	
46-4	4734-4735	a	
46-5	4736-4737	"	
46-6	4737-4742	black	
46-7	4743-4746	box	
46-8	4746-4747	"	
46-9	4748-4750	of	
46-10	4751-4755	data	
46-11	4755-4756	,	
46-12	4757-4762	which	
46-13	4763-4769	cannot	
46-14	4770-4772	be	
46-15	4773-4782	optimized	
46-16	4783-4790	because	
46-17	4791-4796	Spark	
46-18	4797-4800	can	
46-19	4800-4801	'	
46-20	4802-4803	t	
46-21	4804-4808	look	
46-22	4809-4815	inside	
46-23	4816-4820	what	
46-24	4821-4824	the	
46-25	4825-4829	data	
46-26	4830-4838	actually	
46-27	4839-4847	consists	
46-28	4848-4850	of	
46-29	4850-4851	.	

#Text=So Spark can't do any optimizations on your behalf.
47-1	4852-4854	So	
47-2	4855-4860	Spark	
47-3	4861-4866	can't	
47-4	4867-4869	do	
47-5	4870-4873	any	
47-6	4874-4887	optimizations	
47-7	4888-4890	on	
47-8	4891-4895	your	
47-9	4896-4902	behalf	
47-10	4902-4903	.	

#Text=This feels more acute in non-JVM languages such as Python.
48-1	4904-4908	This	
48-2	4909-4914	feels	
48-3	4915-4919	more	
48-4	4920-4925	acute	
48-5	4926-4928	in	
48-6	4929-4936	non-JVM	
48-7	4937-4946	languages	
48-8	4947-4951	such	
48-9	4952-4954	as	
48-10	4955-4961	Python	
48-11	4961-4962	.	

#Text=DataFrame has additional metadata due to its column format, which allows Spark to perform certain optimizations on a completed request.
49-1	4963-4972	DataFrame	
49-2	4973-4976	has	
49-3	4977-4987	additional	
49-4	4988-4996	metadata	
49-5	4997-5000	due	
49-6	5001-5003	to	
49-7	5004-5007	its	
49-8	5008-5014	column	
49-9	5015-5021	format	
49-10	5021-5022	,	
49-11	5023-5028	which	
49-12	5029-5035	allows	
49-13	5036-5041	Spark	
49-14	5042-5044	to	
49-15	5045-5052	perform	
49-16	5053-5060	certain	
49-17	5061-5074	optimizations	
49-18	5075-5077	on	
49-19	5078-5079	a	
49-20	5080-5089	completed	
49-21	5090-5097	request	
49-22	5097-5098	.	

#Text=Before your query is run, a logical plan is created using Catalyst Optimizer and then it's executed using the Tungsten execution engine.
50-1	5099-5105	Before	
50-2	5106-5110	your	
50-3	5111-5116	query	
50-4	5117-5119	is	
50-5	5120-5123	run	
50-6	5123-5124	,	
50-7	5125-5126	a	
50-8	5127-5134	logical	
50-9	5135-5139	plan	
50-10	5140-5142	is	
50-11	5143-5150	created	
50-12	5151-5156	using	
50-13	5157-5165	Catalyst	
50-14	5166-5175	Optimizer	
50-15	5176-5179	and	
50-16	5180-5184	then	
50-17	5185-5189	it's	
50-18	5190-5198	executed	
50-19	5199-5204	using	
50-20	5205-5208	the	
50-21	5209-5217	Tungsten	
50-22	5218-5227	execution	
50-23	5228-5234	engine	
50-24	5234-5235	.	

#Text=What is Catalyst?
51-1	5236-5240	What	
51-2	5241-5243	is	
51-3	5244-5252	Catalyst	
51-4	5252-5253	?	

#Text=Catalyst Optimizer — the name of the integrated query optimizer and execution scheduler for Spark Datasets/DataFrame.
52-1	5254-5262	Catalyst	
52-2	5263-5272	Optimizer	
52-3	5273-5274	—	
52-4	5275-5278	the	
52-5	5279-5283	name	
52-6	5284-5286	of	
52-7	5287-5290	the	
52-8	5291-5301	integrated	
52-9	5302-5307	query	
52-10	5308-5317	optimizer	
52-11	5318-5321	and	
52-12	5322-5331	execution	
52-13	5332-5341	scheduler	
52-14	5342-5345	for	
52-15	5346-5351	Spark	
52-16	5352-5360	Datasets	
52-17	5360-5361	/	
52-18	5361-5370	DataFrame	
52-19	5370-5371	.	

#Text=Catalyst Optimizer is the place where most of the "magic" tends to improve the speed of your code execution by logically improving it.
53-1	5372-5380	Catalyst	
53-2	5381-5390	Optimizer	
53-3	5391-5393	is	
53-4	5394-5397	the	
53-5	5398-5403	place	
53-6	5404-5409	where	
53-7	5410-5414	most	
53-8	5415-5417	of	
53-9	5418-5421	the	
53-10	5422-5423	"	
53-11	5423-5428	magic	
53-12	5428-5429	"	
53-13	5430-5435	tends	
53-14	5436-5438	to	
53-15	5439-5446	improve	
53-16	5447-5450	the	
53-17	5451-5456	speed	
53-18	5457-5459	of	
53-19	5460-5464	your	
53-20	5465-5469	code	
53-21	5470-5479	execution	
53-22	5480-5482	by	
53-23	5483-5492	logically	
53-24	5493-5502	improving	
53-25	5503-5505	it	
53-26	5505-5506	.	

#Text=But in any complex system, unfortunately, "magic" is not enough to always guarantee optimal performance.
54-1	5507-5510	But	
54-2	5511-5513	in	
54-3	5514-5517	any	
54-4	5518-5525	complex	
54-5	5526-5532	system	
54-6	5532-5533	,	
54-7	5534-5547	unfortunately	
54-8	5547-5548	,	
54-9	5549-5550	"	
54-10	5550-5555	magic	
54-11	5555-5556	"	
54-12	5557-5559	is	
54-13	5560-5563	not	
54-14	5564-5570	enough	
54-15	5571-5573	to	
54-16	5574-5580	always	
54-17	5581-5590	guarantee	
54-18	5591-5598	optimal	
54-19	5599-5610	performance	
54-20	5610-5611	.	

#Text=As with relational databases, it is useful to learn a little bit about how the optimizer works to understand how it plans and customizes your applications.
55-1	5612-5614	As	
55-2	5615-5619	with	
55-3	5620-5630	relational	
55-4	5631-5640	databases	
55-5	5640-5641	,	
55-6	5642-5644	it	
55-7	5645-5647	is	
55-8	5648-5654	useful	
55-9	5655-5657	to	
55-10	5658-5663	learn	
55-11	5664-5665	a	
55-12	5666-5672	little	
55-13	5673-5676	bit	
55-14	5677-5682	about	
55-15	5683-5686	how	
55-16	5687-5690	the	
55-17	5691-5700	optimizer	
55-18	5701-5706	works	
55-19	5707-5709	to	
55-20	5710-5720	understand	
55-21	5721-5724	how	
55-22	5725-5727	it	
55-23	5728-5733	plans	
55-24	5734-5737	and	
55-25	5738-5748	customizes	
55-26	5749-5753	your	
55-27	5754-5766	applications	
55-28	5766-5767	.	

#Text=In particular, Catalyst Optimizer can perform refactoring of complex queries.
56-1	5768-5770	In	
56-2	5771-5781	particular	
56-3	5781-5782	,	
56-4	5783-5791	Catalyst	
56-5	5792-5801	Optimizer	
56-6	5802-5805	can	
56-7	5806-5813	perform	
56-8	5814-5825	refactoring	
56-9	5826-5828	of	
56-10	5829-5836	complex	
56-11	5837-5844	queries	
56-12	5844-5845	.	

#Text=However, almost all of its optimizations are qualitative and rule-based rather than quantitative and statistical.
57-1	5846-5853	However	
57-2	5853-5854	,	
57-3	5855-5861	almost	
57-4	5862-5865	all	
57-5	5866-5868	of	
57-6	5869-5872	its	
57-7	5873-5886	optimizations	
57-8	5887-5890	are	
57-9	5891-5902	qualitative	
57-10	5903-5906	and	
57-11	5907-5917	rule-based	
57-12	5918-5924	rather	
57-13	5925-5929	than	
57-14	5930-5942	quantitative	
57-15	5943-5946	and	
57-16	5947-5958	statistical	
57-17	5958-5959	.	

#Text=For example, Spark knows how and when to do things like combine filters or move filters before joining.
58-1	5960-5963	For	
58-2	5964-5971	example	
58-3	5971-5972	,	
58-4	5973-5978	Spark	
58-5	5979-5984	knows	
58-6	5985-5988	how	
58-7	5989-5992	and	
58-8	5993-5997	when	
58-9	5998-6000	to	
58-10	6001-6003	do	
58-11	6004-6010	things	
58-12	6011-6015	like	
58-13	6016-6023	combine	
58-14	6024-6031	filters	
58-15	6032-6034	or	
58-16	6035-6039	move	
58-17	6040-6047	filters	
58-18	6048-6054	before	
58-19	6055-6062	joining	
58-20	6062-6063	.	

#Text=Spark 2.0 even allows you to define, add, and test your own additional optimization rules at runtime.
59-1	6064-6069	Spark	
59-2	6070-6073	2.0	
59-3	6074-6078	even	
59-4	6079-6085	allows	
59-5	6086-6089	you	
59-6	6090-6092	to	
59-7	6093-6099	define	
59-8	6099-6100	,	
59-9	6101-6104	add	
59-10	6104-6105	,	
59-11	6106-6109	and	
59-12	6110-6114	test	
59-13	6115-6119	your	
59-14	6120-6123	own	
59-15	6124-6134	additional	
59-16	6135-6147	optimization	
59-17	6148-6153	rules	
59-18	6154-6156	at	
59-19	6157-6164	runtime	
59-20	6164-6165	.	

#Text=Catalyst Optimizer supports both rule-based and cost-based optimization.
60-1	6166-6174	Catalyst	
60-2	6175-6184	Optimizer	
60-3	6185-6193	supports	
60-4	6194-6198	both	
60-5	6199-6209	rule-based	
60-6	6210-6213	and	
60-7	6214-6224	cost-based	
60-8	6225-6237	optimization	
60-9	6237-6238	.	

#Text=Cost-Based Optimizer(CBO): If an SQL query can be executed in two different ways (e.g. #1 and #2 for the same original query), then CBO essentially calculates the cost of each path and analyzes which path is cheaper, and then executes that path to improve the query execution.
61-1	6239-6249	Cost-Based	
61-2	6250-6259	Optimizer	
61-3	6259-6260	(	
61-4	6260-6263	CBO	
61-5	6263-6264	)	
61-6	6264-6265	:	
61-7	6266-6268	If	
61-8	6269-6271	an	
61-9	6272-6275	SQL	
61-10	6276-6281	query	
61-11	6282-6285	can	
61-12	6286-6288	be	
61-13	6289-6297	executed	
61-14	6298-6300	in	
61-15	6301-6304	two	
61-16	6305-6314	different	
61-17	6315-6319	ways	
61-18	6320-6321	(	
61-19	6321-6324	e.g	
61-20	6324-6325	.	
61-21	6326-6328	#1	
61-22	6329-6332	and	
61-23	6333-6335	#2	
61-24	6336-6339	for	
61-25	6340-6343	the	
61-26	6344-6348	same	
61-27	6349-6357	original	
61-28	6358-6363	query	
61-29	6363-6364	)	
61-30	6364-6365	,	
61-31	6366-6370	then	
61-32	6371-6374	CBO	
61-33	6375-6386	essentially	
61-34	6387-6397	calculates	
61-35	6398-6401	the	
61-36	6402-6406	cost	
61-37	6407-6409	of	
61-38	6410-6414	each	
61-39	6415-6419	path	
61-40	6420-6423	and	
61-41	6424-6432	analyzes	
61-42	6433-6438	which	
61-43	6439-6443	path	
61-44	6444-6446	is	
61-45	6447-6454	cheaper	
61-46	6454-6455	,	
61-47	6456-6459	and	
61-48	6460-6464	then	
61-49	6465-6473	executes	
61-50	6474-6478	that	
61-51	6479-6483	path	
61-52	6484-6486	to	
61-53	6487-6494	improve	
61-54	6495-6498	the	
61-55	6499-6504	query	
61-56	6505-6514	execution	
61-57	6514-6515	.	

#Text=Rule-Based optimizer(RBO): follows different optimization rules that apply depending on the query.
62-1	6516-6526	Rule-Based	
62-2	6527-6536	optimizer	
62-3	6536-6537	(	
62-4	6537-6540	RBO	
62-5	6540-6541	)	
62-6	6541-6542	:	
62-7	6543-6550	follows	
62-8	6551-6560	different	
62-9	6561-6573	optimization	
62-10	6574-6579	rules	
62-11	6580-6584	that	
62-12	6585-6590	apply	
62-13	6591-6600	depending	
62-14	6601-6603	on	
62-15	6604-6607	the	
62-16	6608-6613	query	
62-17	6613-6614	.	

#Text=These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean simplification, and other rules.
63-1	6615-6620	These	
63-2	6621-6628	include	
63-3	6629-6637	constant	
63-4	6638-6645	folding	
63-5	6645-6646	,	
63-6	6647-6656	predicate	
63-7	6657-6665	pushdown	
63-8	6665-6666	,	
63-9	6667-6677	projection	
63-10	6678-6685	pruning	
63-11	6685-6686	,	
63-12	6687-6691	null	
63-13	6692-6703	propagation	
63-14	6703-6704	,	
63-15	6705-6712	Boolean	
63-16	6713-6727	simplification	
63-17	6727-6728	,	
63-18	6729-6732	and	
63-19	6733-6738	other	
63-20	6739-6744	rules	
63-21	6744-6745	.	

#Text=In fact, there is no separation inside the Spark, the two approaches work together, cost-based optimization is performed by generating multiple plans using rules, and then computing their costs.
64-1	6746-6748	In	
64-2	6749-6753	fact	
64-3	6753-6754	,	
64-4	6755-6760	there	
64-5	6761-6763	is	
64-6	6764-6766	no	
64-7	6767-6777	separation	
64-8	6778-6784	inside	
64-9	6785-6788	the	
64-10	6789-6794	Spark	
64-11	6794-6795	,	
64-12	6796-6799	the	
64-13	6800-6803	two	
64-14	6804-6814	approaches	
64-15	6815-6819	work	
64-16	6820-6828	together	
64-17	6828-6829	,	
64-18	6830-6840	cost-based	
64-19	6841-6853	optimization	
64-20	6854-6856	is	
64-21	6857-6866	performed	
64-22	6867-6869	by	
64-23	6870-6880	generating	
64-24	6881-6889	multiple	
64-25	6890-6895	plans	
64-26	6896-6901	using	
64-27	6902-6907	rules	
64-28	6907-6908	,	
64-29	6909-6912	and	
64-30	6913-6917	then	
64-31	6918-6927	computing	
64-32	6928-6933	their	
64-33	6934-6939	costs	
64-34	6939-6940	.	

#Text=As an example, imagine that we have users (they come from the database) and their transactions (I will generate some random values, but it could be a database as well):
#Text=transactions = spark.range(160000)\\
#Text=.select(F.col('id').alias('key'), F.rand(12).alias('value'))
#Text=users = spark.read.jdbc(
#Text=table='users', url='jdbc:postgresql://localhost:5432/postgres')
#Text=users.join(transactions, 'key')\\
#Text=.filter(F.col('key') > 100)\\
#Text=.explain()
#Text=We see here that our developer in a programming rush wrote user filtering after join.
65-1	6941-6943	As	
65-2	6944-6946	an	
65-3	6947-6954	example	
65-4	6954-6955	,	
65-5	6956-6963	imagine	
65-6	6964-6968	that	
65-7	6969-6971	we	
65-8	6972-6976	have	
65-9	6977-6982	users	
65-10	6983-6984	(	
65-11	6984-6988	they	
65-12	6989-6993	come	
65-13	6994-6998	from	
65-14	6999-7002	the	
65-15	7003-7011	database	
65-16	7011-7012	)	
65-17	7013-7016	and	
65-18	7017-7022	their	
65-19	7023-7035	transactions	
65-20	7036-7037	(	
65-21	7037-7038	I	
65-22	7039-7043	will	
65-23	7044-7052	generate	
65-24	7053-7057	some	
65-25	7058-7064	random	
65-26	7065-7071	values	
65-27	7071-7072	,	
65-28	7073-7076	but	
65-29	7077-7079	it	
65-30	7080-7085	could	
65-31	7086-7088	be	
65-32	7089-7090	a	
65-33	7091-7099	database	
65-34	7100-7102	as	
65-35	7103-7107	well	
65-36	7107-7108	)	
65-37	7108-7109	:	
65-38	7110-7122	transactions	
65-39	7123-7124	=	
65-40	7125-7136	spark.range	
65-41	7136-7137	(	
65-42	7137-7143	160000	
65-43	7143-7144	)	
65-44	7144-7145	\	
65-45	7146-7147	.	
65-46	7147-7153	select	
65-47	7153-7154	(	
65-48	7154-7159	F.col	
65-49	7159-7160	(	
65-50	7160-7161	'	
65-51	7161-7163	id	
65-52	7163-7164	'	
65-53	7164-7165	)	
65-54	7165-7166	.	
65-55	7166-7171	alias	
65-56	7171-7172	(	
65-57	7172-7173	'	
65-58	7173-7176	key	
65-59	7176-7177	'	
65-60	7177-7178	)	
65-61	7178-7179	,	
65-62	7180-7186	F.rand	
65-63	7186-7187	(	
65-64	7187-7189	12	
65-65	7189-7190	)	
65-66	7190-7191	.	
65-67	7191-7196	alias	
65-68	7196-7197	(	
65-69	7197-7198	'	
65-70	7198-7203	value	
65-71	7203-7204	'	
65-72	7204-7205	)	
65-73	7205-7206	)	
65-74	7207-7212	users	
65-75	7213-7214	=	
65-76	7215-7230	spark.read.jdbc	
65-77	7230-7231	(	
65-78	7232-7237	table	
65-79	7237-7238	=	
65-80	7238-7239	'	
65-81	7239-7244	users	
65-82	7244-7245	'	
65-83	7245-7246	,	
65-84	7247-7250	url	
65-85	7250-7251	=	
65-86	7251-7252	'	
65-87	7252-7256	jdbc	
65-88	7256-7257	:	
65-89	7257-7267	postgresql	
65-90	7267-7268	:	
65-91	7268-7269	/	
65-92	7269-7270	/	
65-93	7270-7279	localhost	
65-94	7279-7280	:	
65-95	7280-7284	5432	
65-96	7284-7285	/	
65-97	7285-7293	postgres	
65-98	7293-7294	'	
65-99	7294-7295	)	
65-100	7296-7306	users.join	
65-101	7306-7307	(	
65-102	7307-7319	transactions	
65-103	7319-7320	,	
65-104	7321-7322	'	
65-105	7322-7325	key	
65-106	7325-7326	'	
65-107	7326-7327	)	
65-108	7327-7328	\	
65-109	7329-7330	.	
65-110	7330-7336	filter	
65-111	7336-7337	(	
65-112	7337-7342	F.col	
65-113	7342-7343	(	
65-114	7343-7344	'	
65-115	7344-7347	key	
65-116	7347-7348	'	
65-117	7348-7349	)	
65-118	7350-7351	>	
65-119	7352-7355	100	
65-120	7355-7356	)	
65-121	7356-7357	\	
65-122	7358-7359	.	
65-123	7359-7366	explain	
65-124	7366-7367	(	
65-125	7367-7368	)	
65-126	7369-7371	We	
65-127	7372-7375	see	
65-128	7376-7380	here	
65-129	7381-7385	that	
65-130	7386-7389	our	
65-131	7390-7399	developer	
65-132	7400-7402	in	
65-133	7403-7404	a	
65-134	7405-7416	programming	
65-135	7417-7421	rush	
65-136	7422-7427	wrote	
65-137	7428-7432	user	
65-138	7433-7442	filtering	
65-139	7443-7448	after	
65-140	7449-7453	join	
65-141	7453-7454	.	

#Text=It would be more reasonable to do the filter first and then join because the shuffle data will be reduced.
66-1	7455-7457	It	
66-2	7458-7463	would	
66-3	7464-7466	be	
66-4	7467-7471	more	
66-5	7472-7482	reasonable	
66-6	7483-7485	to	
66-7	7486-7488	do	
66-8	7489-7492	the	
66-9	7493-7499	filter	
66-10	7500-7505	first	
66-11	7506-7509	and	
66-12	7510-7514	then	
66-13	7515-7519	join	
66-14	7520-7527	because	
66-15	7528-7531	the	
66-16	7532-7539	shuffle	
66-17	7540-7544	data	
66-18	7545-7549	will	
66-19	7550-7552	be	
66-20	7553-7560	reduced	
66-21	7560-7561	.	

#Text=But if we see the physical plan, we see that Catalyst Optimizer did it for us.
67-1	7562-7565	But	
67-2	7566-7568	if	
67-3	7569-7571	we	
67-4	7572-7575	see	
67-5	7576-7579	the	
67-6	7580-7588	physical	
67-7	7589-7593	plan	
67-8	7593-7594	,	
67-9	7595-7597	we	
67-10	7598-7601	see	
67-11	7602-7606	that	
67-12	7607-7615	Catalyst	
67-13	7616-7625	Optimizer	
67-14	7626-7629	did	
67-15	7630-7632	it	
67-16	7633-7636	for	
67-17	7637-7639	us	
67-18	7639-7640	.	

#Text=What's more, it did a predicate pushdown of our filter to the database (Spark will try to move the filtering data as close to the source as possible to avoid loading unnecessary data into memory, see PushedFilters).
#Text=== Physical Plan ==
#Text=*(5) SortMergeJoin [cast(key#6 as bigint)], [key#2L], Inner
#Text=:- *(2) Sort [cast(key#6 as bigint) ASC NULLS FIRST], false, 0
#Text=+- Exchange hashpartitioning(cast(key#6 as bigint), 200)
#Text=+- *(1) Filter (cast(key#6 as int) > 100)
#Text=+- *(1) Scan JDBCRelation(users) [numPartitions=1] [key#6,name#7] PushedFilters: [*IsNotNull(key)], ReadSchema: struct<key:string,name:string>
#Text=+- *(4) Sort [key#2L ASC NULLS FIRST], false, 0
#Text=+- Exchange hashpartitioning(key#2L, 200)
#Text=+- *(3) Project [id#0L AS key#2L, rand(12) AS value#3]
#Text=+- *(3) Range (0, 160000, step=1, splits=1)
#Text=Spark does not "own" any storage, so it does not build indexes, B-trees, etc. on the disk.
68-1	7641-7647	What's	
68-2	7648-7652	more	
68-3	7652-7653	,	
68-4	7654-7656	it	
68-5	7657-7660	did	
68-6	7661-7662	a	
68-7	7663-7672	predicate	
68-8	7673-7681	pushdown	
68-9	7682-7684	of	
68-10	7685-7688	our	
68-11	7689-7695	filter	
68-12	7696-7698	to	
68-13	7699-7702	the	
68-14	7703-7711	database	
68-15	7712-7713	(	
68-16	7713-7718	Spark	
68-17	7719-7723	will	
68-18	7724-7727	try	
68-19	7728-7730	to	
68-20	7731-7735	move	
68-21	7736-7739	the	
68-22	7740-7749	filtering	
68-23	7750-7754	data	
68-24	7755-7757	as	
68-25	7758-7763	close	
68-26	7764-7766	to	
68-27	7767-7770	the	
68-28	7771-7777	source	
68-29	7778-7780	as	
68-30	7781-7789	possible	
68-31	7790-7792	to	
68-32	7793-7798	avoid	
68-33	7799-7806	loading	
68-34	7807-7818	unnecessary	
68-35	7819-7823	data	
68-36	7824-7828	into	
68-37	7829-7835	memory	
68-38	7835-7836	,	
68-39	7837-7840	see	
68-40	7841-7854	PushedFilters	
68-41	7854-7855	)	
68-42	7855-7856	.	
68-43	7857-7858	=	
68-44	7858-7859	=	
68-45	7860-7868	Physical	
68-46	7869-7873	Plan	
68-47	7874-7875	=	
68-48	7875-7876	=	
68-49	7877-7878	*	
68-50	7878-7879	(	
68-51	7879-7880	5	
68-52	7880-7881	)	
68-53	7882-7895	SortMergeJoin	
68-54	7896-7897	[	
68-55	7897-7901	cast	
68-56	7901-7902	(	
68-57	7902-7905	key	
68-58	7905-7907	#6	
68-59	7908-7910	as	
68-60	7911-7917	bigint	
68-61	7917-7918	)	
68-62	7918-7919	]	
68-63	7919-7920	,	
68-64	7921-7922	[	
68-65	7922-7925	key	
68-66	7925-7928	#2L	
68-67	7928-7929	]	
68-68	7929-7930	,	
68-69	7931-7936	Inner	
68-70	7937-7938	:	
68-71	7938-7939	-	
68-72	7940-7941	*	
68-73	7941-7942	(	
68-74	7942-7943	2	
68-75	7943-7944	)	
68-76	7945-7949	Sort	
68-77	7950-7951	[	
68-78	7951-7955	cast	
68-79	7955-7956	(	
68-80	7956-7959	key	
68-81	7959-7961	#6	
68-82	7962-7964	as	
68-83	7965-7971	bigint	
68-84	7971-7972	)	
68-85	7973-7976	ASC	
68-86	7977-7982	NULLS	
68-87	7983-7988	FIRST	
68-88	7988-7989	]	
68-89	7989-7990	,	
68-90	7991-7996	false	
68-91	7996-7997	,	
68-92	7998-7999	0	
68-93	8000-8001	+	
68-94	8001-8002	-	
68-95	8003-8011	Exchange	
68-96	8012-8028	hashpartitioning	
68-97	8028-8029	(	
68-98	8029-8033	cast	
68-99	8033-8034	(	
68-100	8034-8037	key	
68-101	8037-8039	#6	
68-102	8040-8042	as	
68-103	8043-8049	bigint	
68-104	8049-8050	)	
68-105	8050-8051	,	
68-106	8052-8055	200	
68-107	8055-8056	)	
68-108	8057-8058	+	
68-109	8058-8059	-	
68-110	8060-8061	*	
68-111	8061-8062	(	
68-112	8062-8063	1	
68-113	8063-8064	)	
68-114	8065-8071	Filter	
68-115	8072-8073	(	
68-116	8073-8077	cast	
68-117	8077-8078	(	
68-118	8078-8081	key	
68-119	8081-8083	#6	
68-120	8084-8086	as	
68-121	8087-8090	int	
68-122	8090-8091	)	
68-123	8092-8093	>	
68-124	8094-8097	100	
68-125	8097-8098	)	
68-126	8099-8100	+	
68-127	8100-8101	-	
68-128	8102-8103	*	
68-129	8103-8104	(	
68-130	8104-8105	1	
68-131	8105-8106	)	
68-132	8107-8111	Scan	
68-133	8112-8124	JDBCRelation	
68-134	8124-8125	(	
68-135	8125-8130	users	
68-136	8130-8131	)	
68-137	8132-8133	[	
68-138	8133-8146	numPartitions	
68-139	8146-8147	=	
68-140	8147-8148	1	
68-141	8148-8149	]	
68-142	8150-8151	[	
68-143	8151-8154	key	
68-144	8154-8156	#6	
68-145	8156-8157	,	
68-146	8157-8161	name	
68-147	8161-8163	#7	
68-148	8163-8164	]	
68-149	8165-8178	PushedFilters	
68-150	8178-8179	:	
68-151	8180-8181	[	
68-152	8181-8182	*	
68-153	8182-8191	IsNotNull	
68-154	8191-8192	(	
68-155	8192-8195	key	
68-156	8195-8196	)	
68-157	8196-8197	]	
68-158	8197-8198	,	
68-159	8199-8209	ReadSchema	
68-160	8209-8210	:	
68-161	8211-8217	struct	
68-162	8217-8218	<	
68-163	8218-8221	key	
68-164	8221-8222	:	
68-165	8222-8228	string	
68-166	8228-8229	,	
68-167	8229-8233	name	
68-168	8233-8234	:	
68-169	8234-8240	string	
68-170	8240-8241	>	
68-171	8242-8243	+	
68-172	8243-8244	-	
68-173	8245-8246	*	
68-174	8246-8247	(	
68-175	8247-8248	4	
68-176	8248-8249	)	
68-177	8250-8254	Sort	
68-178	8255-8256	[	
68-179	8256-8259	key	
68-180	8259-8262	#2L	
68-181	8263-8266	ASC	
68-182	8267-8272	NULLS	
68-183	8273-8278	FIRST	
68-184	8278-8279	]	
68-185	8279-8280	,	
68-186	8281-8286	false	
68-187	8286-8287	,	
68-188	8288-8289	0	
68-189	8290-8291	+	
68-190	8291-8292	-	
68-191	8293-8301	Exchange	
68-192	8302-8318	hashpartitioning	
68-193	8318-8319	(	
68-194	8319-8322	key	
68-195	8322-8325	#2L	
68-196	8325-8326	,	
68-197	8327-8330	200	
68-198	8330-8331	)	
68-199	8332-8333	+	
68-200	8333-8334	-	
68-201	8335-8336	*	
68-202	8336-8337	(	
68-203	8337-8338	3	
68-204	8338-8339	)	
68-205	8340-8347	Project	
68-206	8348-8349	[	
68-207	8349-8351	id	
68-208	8351-8354	#0L	
68-209	8355-8357	AS	
68-210	8358-8361	key	
68-211	8361-8364	#2L	
68-212	8364-8365	,	
68-213	8366-8370	rand	
68-214	8370-8371	(	
68-215	8371-8373	12	
68-216	8373-8374	)	
68-217	8375-8377	AS	
68-218	8378-8383	value	
68-219	8383-8385	#3	
68-220	8385-8386	]	
68-221	8387-8388	+	
68-222	8388-8389	-	
68-223	8390-8391	*	
68-224	8391-8392	(	
68-225	8392-8393	3	
68-226	8393-8394	)	
68-227	8395-8400	Range	
68-228	8401-8402	(	
68-229	8402-8403	0	
68-230	8403-8404	,	
68-231	8405-8411	160000	
68-232	8411-8412	,	
68-233	8413-8417	step	
68-234	8417-8418	=	
68-235	8418-8419	1	
68-236	8419-8420	,	
68-237	8421-8427	splits	
68-238	8427-8428	=	
68-239	8428-8429	1	
68-240	8429-8430	)	
68-241	8431-8436	Spark	
68-242	8437-8441	does	
68-243	8442-8445	not	
68-244	8446-8447	"	
68-245	8447-8450	own	
68-246	8450-8451	"	
68-247	8452-8455	any	
68-248	8456-8463	storage	
68-249	8463-8464	,	
68-250	8465-8467	so	
68-251	8468-8470	it	
68-252	8471-8475	does	
68-253	8476-8479	not	
68-254	8480-8485	build	
68-255	8486-8493	indexes	
68-256	8493-8494	,	
68-257	8495-8502	B-trees	
68-258	8502-8503	,	
68-259	8504-8507	etc	
68-260	8507-8508	.	
68-261	8509-8511	on	
68-262	8512-8515	the	
68-263	8516-8520	disk	
68-264	8520-8521	.	

#Text=(although support for parquet files, if used well, may give you some related features).
69-1	8522-8523	(	
69-2	8523-8531	although	
69-3	8532-8539	support	
69-4	8540-8543	for	
69-5	8544-8551	parquet	
69-6	8552-8557	files	
69-7	8557-8558	,	
69-8	8559-8561	if	
69-9	8562-8566	used	
69-10	8567-8571	well	
69-11	8571-8572	,	
69-12	8573-8576	may	
69-13	8577-8581	give	
69-14	8582-8585	you	
69-15	8586-8590	some	
69-16	8591-8598	related	
69-17	8599-8607	features	
69-18	8607-8608	)	
69-19	8608-8609	.	

#Text=Spark has been optimized for the volume, variety, etc. of big data — so, traditionally, it is not designed to maintain and use statistics. about the dataset.
70-1	8610-8615	Spark	
70-2	8616-8619	has	
70-3	8620-8624	been	
70-4	8625-8634	optimized	
70-5	8635-8638	for	
70-6	8639-8642	the	
70-7	8643-8649	volume	
70-8	8649-8650	,	
70-9	8651-8658	variety	
70-10	8658-8659	,	
70-11	8660-8663	etc	
70-12	8663-8664	.	
70-13	8665-8667	of	
70-14	8668-8671	big	
70-15	8672-8676	data	
70-16	8677-8678	—	
70-17	8679-8681	so	
70-18	8681-8682	,	
70-19	8683-8696	traditionally	
70-20	8696-8697	,	
70-21	8698-8700	it	
70-22	8701-8703	is	
70-23	8704-8707	not	
70-24	8708-8716	designed	
70-25	8717-8719	to	
70-26	8720-8728	maintain	
70-27	8729-8732	and	
70-28	8733-8736	use	
70-29	8737-8747	statistics	
70-30	8747-8748	.	
70-31	8749-8754	about	
70-32	8755-8758	the	
70-33	8759-8766	dataset	
70-34	8766-8767	.	

#Text=For example, in cases where the DBMS may know that a particular filter will remove most records and apply it early in the query, Spark does not know this fact and will not perform such optimization.
#Text=transactions = spark.range(160000) \\
#Text=.select(F.col('id').alias('key'), F.rand(12).alias('value'))
#Text=users = spark.read.jdbc(
#Text=table='users', url='jdbc:postgresql://localhost:5432/postgres')
#Text=users.join(transactions, 'key') \\
#Text=.drop_duplicates(['key']) \\
#Text=.explain()
#Text=This gives us one more Exchange(shuffle) but not a predicate pushdown of distinct operation into the database.
71-1	8768-8771	For	
71-2	8772-8779	example	
71-3	8779-8780	,	
71-4	8781-8783	in	
71-5	8784-8789	cases	
71-6	8790-8795	where	
71-7	8796-8799	the	
71-8	8800-8804	DBMS	
71-9	8805-8808	may	
71-10	8809-8813	know	
71-11	8814-8818	that	
71-12	8819-8820	a	
71-13	8821-8831	particular	
71-14	8832-8838	filter	
71-15	8839-8843	will	
71-16	8844-8850	remove	
71-17	8851-8855	most	
71-18	8856-8863	records	
71-19	8864-8867	and	
71-20	8868-8873	apply	
71-21	8874-8876	it	
71-22	8877-8882	early	
71-23	8883-8885	in	
71-24	8886-8889	the	
71-25	8890-8895	query	
71-26	8895-8896	,	
71-27	8897-8902	Spark	
71-28	8903-8907	does	
71-29	8908-8911	not	
71-30	8912-8916	know	
71-31	8917-8921	this	
71-32	8922-8926	fact	
71-33	8927-8930	and	
71-34	8931-8935	will	
71-35	8936-8939	not	
71-36	8940-8947	perform	
71-37	8948-8952	such	
71-38	8953-8965	optimization	
71-39	8965-8966	.	
71-40	8967-8979	transactions	
71-41	8980-8981	=	
71-42	8982-8993	spark.range	
71-43	8993-8994	(	
71-44	8994-9000	160000	
71-45	9000-9001	)	
71-46	9002-9003	\	
71-47	9004-9005	.	
71-48	9005-9011	select	
71-49	9011-9012	(	
71-50	9012-9017	F.col	
71-51	9017-9018	(	
71-52	9018-9019	'	
71-53	9019-9021	id	
71-54	9021-9022	'	
71-55	9022-9023	)	
71-56	9023-9024	.	
71-57	9024-9029	alias	
71-58	9029-9030	(	
71-59	9030-9031	'	
71-60	9031-9034	key	
71-61	9034-9035	'	
71-62	9035-9036	)	
71-63	9036-9037	,	
71-64	9038-9044	F.rand	
71-65	9044-9045	(	
71-66	9045-9047	12	
71-67	9047-9048	)	
71-68	9048-9049	.	
71-69	9049-9054	alias	
71-70	9054-9055	(	
71-71	9055-9056	'	
71-72	9056-9061	value	
71-73	9061-9062	'	
71-74	9062-9063	)	
71-75	9063-9064	)	
71-76	9065-9070	users	
71-77	9071-9072	=	
71-78	9073-9088	spark.read.jdbc	
71-79	9088-9089	(	
71-80	9090-9095	table	
71-81	9095-9096	=	
71-82	9096-9097	'	
71-83	9097-9102	users	
71-84	9102-9103	'	
71-85	9103-9104	,	
71-86	9105-9108	url	
71-87	9108-9109	=	
71-88	9109-9110	'	
71-89	9110-9114	jdbc	
71-90	9114-9115	:	
71-91	9115-9125	postgresql	
71-92	9125-9126	:	
71-93	9126-9127	/	
71-94	9127-9128	/	
71-95	9128-9137	localhost	
71-96	9137-9138	:	
71-97	9138-9142	5432	
71-98	9142-9143	/	
71-99	9143-9151	postgres	
71-100	9151-9152	'	
71-101	9152-9153	)	
71-102	9154-9164	users.join	
71-103	9164-9165	(	
71-104	9165-9177	transactions	
71-105	9177-9178	,	
71-106	9179-9180	'	
71-107	9180-9183	key	
71-108	9183-9184	'	
71-109	9184-9185	)	
71-110	9186-9187	\	
71-111	9188-9189	.	
71-112	9189-9204	drop_duplicates	
71-113	9204-9205	(	
71-114	9205-9206	[	
71-115	9206-9207	'	
71-116	9207-9210	key	
71-117	9210-9211	'	
71-118	9211-9212	]	
71-119	9212-9213	)	
71-120	9214-9215	\	
71-121	9216-9217	.	
71-122	9217-9224	explain	
71-123	9224-9225	(	
71-124	9225-9226	)	
71-125	9227-9231	This	
71-126	9232-9237	gives	
71-127	9238-9240	us	
71-128	9241-9244	one	
71-129	9245-9249	more	
71-130	9250-9258	Exchange	
71-131	9258-9259	(	
71-132	9259-9266	shuffle	
71-133	9266-9267	)	
71-134	9268-9271	but	
71-135	9272-9275	not	
71-136	9276-9277	a	
71-137	9278-9287	predicate	
71-138	9288-9296	pushdown	
71-139	9297-9299	of	
71-140	9300-9308	distinct	
71-141	9309-9318	operation	
71-142	9319-9323	into	
71-143	9324-9327	the	
71-144	9328-9336	database	
71-145	9336-9337	.	

#Text=We need to rewrite it ourselves to do distinct first.
#Text=== Physical Plan ==
#Text=*(6) HashAggregate(keys=[key#6], functions=[first(key#2L, false), first(value#3, false)])
#Text=+- Exchange hashpartitioning(key#6, 200)
#Text=+- *(5) HashAggregate(keys=[key#6], functions=[partial_first(key#2L, false), partial_first(value#3, false)])
#Text=+- *(5) SortMergeJoin [cast(key#6 as bigint)], [key#2L], Inner
#Text=:- *(2) Sort [cast(key#6 as bigint) ASC NULLS FIRST], false, 0
#Text=+- Exchange hashpartitioning(cast(key#6 as bigint), 200)
#Text=+- *(1) Scan JDBCRelation(users) [numPartitions=1] [key#6] PushedFilters: [*IsNotNull(key)], ReadSchema: struct<key:string>
#Text=+- *(4) Sort [key#2L ASC NULLS FIRST], false, 0
#Text=+- Exchange hashpartitioning(key#2L, 200)
#Text=+- *(3) Project [id#0L AS key#2L, rand(12) AS value#3]
#Text=+- *(3) Range (0, 160000, step=1, splits=1)
#Text=To sum up, use Spark DataFrames, release the power of [Catalyst Optimizer] (https://databricks.com/glossary/catalyst-optimizer) and [Tungsten execution engine] (https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html).
72-1	9338-9340	We	
72-2	9341-9345	need	
72-3	9346-9348	to	
72-4	9349-9356	rewrite	
72-5	9357-9359	it	
72-6	9360-9369	ourselves	
72-7	9370-9372	to	
72-8	9373-9375	do	
72-9	9376-9384	distinct	
72-10	9385-9390	first	
72-11	9390-9391	.	
72-12	9392-9393	=	
72-13	9393-9394	=	
72-14	9395-9403	Physical	
72-15	9404-9408	Plan	
72-16	9409-9410	=	
72-17	9410-9411	=	
72-18	9412-9413	*	
72-19	9413-9414	(	
72-20	9414-9415	6	
72-21	9415-9416	)	
72-22	9417-9430	HashAggregate	
72-23	9430-9431	(	
72-24	9431-9435	keys	
72-25	9435-9436	=	
72-26	9436-9437	[	
72-27	9437-9440	key	
72-28	9440-9442	#6	
72-29	9442-9443	]	
72-30	9443-9444	,	
72-31	9445-9454	functions	
72-32	9454-9455	=	
72-33	9455-9456	[	
72-34	9456-9461	first	
72-35	9461-9462	(	
72-36	9462-9465	key	
72-37	9465-9468	#2L	
72-38	9468-9469	,	
72-39	9470-9475	false	
72-40	9475-9476	)	
72-41	9476-9477	,	
72-42	9478-9483	first	
72-43	9483-9484	(	
72-44	9484-9489	value	
72-45	9489-9491	#3	
72-46	9491-9492	,	
72-47	9493-9498	false	
72-48	9498-9499	)	
72-49	9499-9500	]	
72-50	9500-9501	)	
72-51	9502-9503	+	
72-52	9503-9504	-	
72-53	9505-9513	Exchange	
72-54	9514-9530	hashpartitioning	
72-55	9530-9531	(	
72-56	9531-9534	key	
72-57	9534-9536	#6	
72-58	9536-9537	,	
72-59	9538-9541	200	
72-60	9541-9542	)	
72-61	9543-9544	+	
72-62	9544-9545	-	
72-63	9546-9547	*	
72-64	9547-9548	(	
72-65	9548-9549	5	
72-66	9549-9550	)	
72-67	9551-9564	HashAggregate	
72-68	9564-9565	(	
72-69	9565-9569	keys	
72-70	9569-9570	=	
72-71	9570-9571	[	
72-72	9571-9574	key	
72-73	9574-9576	#6	
72-74	9576-9577	]	
72-75	9577-9578	,	
72-76	9579-9588	functions	
72-77	9588-9589	=	
72-78	9589-9590	[	
72-79	9590-9603	partial_first	
72-80	9603-9604	(	
72-81	9604-9607	key	
72-82	9607-9610	#2L	
72-83	9610-9611	,	
72-84	9612-9617	false	
72-85	9617-9618	)	
72-86	9618-9619	,	
72-87	9620-9633	partial_first	
72-88	9633-9634	(	
72-89	9634-9639	value	
72-90	9639-9641	#3	
72-91	9641-9642	,	
72-92	9643-9648	false	
72-93	9648-9649	)	
72-94	9649-9650	]	
72-95	9650-9651	)	
72-96	9652-9653	+	
72-97	9653-9654	-	
72-98	9655-9656	*	
72-99	9656-9657	(	
72-100	9657-9658	5	
72-101	9658-9659	)	
72-102	9660-9673	SortMergeJoin	
72-103	9674-9675	[	
72-104	9675-9679	cast	
72-105	9679-9680	(	
72-106	9680-9683	key	
72-107	9683-9685	#6	
72-108	9686-9688	as	
72-109	9689-9695	bigint	
72-110	9695-9696	)	
72-111	9696-9697	]	
72-112	9697-9698	,	
72-113	9699-9700	[	
72-114	9700-9703	key	
72-115	9703-9706	#2L	
72-116	9706-9707	]	
72-117	9707-9708	,	
72-118	9709-9714	Inner	
72-119	9715-9716	:	
72-120	9716-9717	-	
72-121	9718-9719	*	
72-122	9719-9720	(	
72-123	9720-9721	2	
72-124	9721-9722	)	
72-125	9723-9727	Sort	
72-126	9728-9729	[	
72-127	9729-9733	cast	
72-128	9733-9734	(	
72-129	9734-9737	key	
72-130	9737-9739	#6	
72-131	9740-9742	as	
72-132	9743-9749	bigint	
72-133	9749-9750	)	
72-134	9751-9754	ASC	
72-135	9755-9760	NULLS	
72-136	9761-9766	FIRST	
72-137	9766-9767	]	
72-138	9767-9768	,	
72-139	9769-9774	false	
72-140	9774-9775	,	
72-141	9776-9777	0	
72-142	9778-9779	+	
72-143	9779-9780	-	
72-144	9781-9789	Exchange	
72-145	9790-9806	hashpartitioning	
72-146	9806-9807	(	
72-147	9807-9811	cast	
72-148	9811-9812	(	
72-149	9812-9815	key	
72-150	9815-9817	#6	
72-151	9818-9820	as	
72-152	9821-9827	bigint	
72-153	9827-9828	)	
72-154	9828-9829	,	
72-155	9830-9833	200	
72-156	9833-9834	)	
72-157	9835-9836	+	
72-158	9836-9837	-	
72-159	9838-9839	*	
72-160	9839-9840	(	
72-161	9840-9841	1	
72-162	9841-9842	)	
72-163	9843-9847	Scan	
72-164	9848-9860	JDBCRelation	
72-165	9860-9861	(	
72-166	9861-9866	users	
72-167	9866-9867	)	
72-168	9868-9869	[	
72-169	9869-9882	numPartitions	
72-170	9882-9883	=	
72-171	9883-9884	1	
72-172	9884-9885	]	
72-173	9886-9887	[	
72-174	9887-9890	key	
72-175	9890-9892	#6	
72-176	9892-9893	]	
72-177	9894-9907	PushedFilters	
72-178	9907-9908	:	
72-179	9909-9910	[	
72-180	9910-9911	*	
72-181	9911-9920	IsNotNull	
72-182	9920-9921	(	
72-183	9921-9924	key	
72-184	9924-9925	)	
72-185	9925-9926	]	
72-186	9926-9927	,	
72-187	9928-9938	ReadSchema	
72-188	9938-9939	:	
72-189	9940-9946	struct	
72-190	9946-9947	<	
72-191	9947-9950	key	
72-192	9950-9951	:	
72-193	9951-9957	string	
72-194	9957-9958	>	
72-195	9959-9960	+	
72-196	9960-9961	-	
72-197	9962-9963	*	
72-198	9963-9964	(	
72-199	9964-9965	4	
72-200	9965-9966	)	
72-201	9967-9971	Sort	
72-202	9972-9973	[	
72-203	9973-9976	key	
72-204	9976-9979	#2L	
72-205	9980-9983	ASC	
72-206	9984-9989	NULLS	
72-207	9990-9995	FIRST	
72-208	9995-9996	]	
72-209	9996-9997	,	
72-210	9998-10003	false	
72-211	10003-10004	,	
72-212	10005-10006	0	
72-213	10007-10008	+	
72-214	10008-10009	-	
72-215	10010-10018	Exchange	
72-216	10019-10035	hashpartitioning	
72-217	10035-10036	(	
72-218	10036-10039	key	
72-219	10039-10042	#2L	
72-220	10042-10043	,	
72-221	10044-10047	200	
72-222	10047-10048	)	
72-223	10049-10050	+	
72-224	10050-10051	-	
72-225	10052-10053	*	
72-226	10053-10054	(	
72-227	10054-10055	3	
72-228	10055-10056	)	
72-229	10057-10064	Project	
72-230	10065-10066	[	
72-231	10066-10068	id	
72-232	10068-10071	#0L	
72-233	10072-10074	AS	
72-234	10075-10078	key	
72-235	10078-10081	#2L	
72-236	10081-10082	,	
72-237	10083-10087	rand	
72-238	10087-10088	(	
72-239	10088-10090	12	
72-240	10090-10091	)	
72-241	10092-10094	AS	
72-242	10095-10100	value	
72-243	10100-10102	#3	
72-244	10102-10103	]	
72-245	10104-10105	+	
72-246	10105-10106	-	
72-247	10107-10108	*	
72-248	10108-10109	(	
72-249	10109-10110	3	
72-250	10110-10111	)	
72-251	10112-10117	Range	
72-252	10118-10119	(	
72-253	10119-10120	0	
72-254	10120-10121	,	
72-255	10122-10128	160000	
72-256	10128-10129	,	
72-257	10130-10134	step	
72-258	10134-10135	=	
72-259	10135-10136	1	
72-260	10136-10137	,	
72-261	10138-10144	splits	
72-262	10144-10145	=	
72-263	10145-10146	1	
72-264	10146-10147	)	
72-265	10148-10150	To	
72-266	10151-10154	sum	
72-267	10155-10157	up	
72-268	10157-10158	,	
72-269	10159-10162	use	
72-270	10163-10168	Spark	
72-271	10169-10179	DataFrames	
72-272	10179-10180	,	
72-273	10181-10188	release	
72-274	10189-10192	the	
72-275	10193-10198	power	
72-276	10199-10201	of	
72-277	10202-10203	[	
72-278	10203-10211	Catalyst	
72-279	10212-10221	Optimizer	
72-280	10221-10222	]	
72-281	10223-10224	(	
72-282	10224-10229	https	
72-283	10229-10230	:	
72-284	10230-10231	/	
72-285	10231-10232	/	
72-286	10232-10246	databricks.com	
72-287	10246-10247	/	
72-288	10247-10255	glossary	
72-289	10255-10256	/	
72-290	10256-10274	catalyst-optimizer	
72-291	10274-10275	)	
72-292	10276-10279	and	
72-293	10280-10281	[	
72-294	10281-10289	Tungsten	
72-295	10290-10299	execution	
72-296	10300-10306	engine	
72-297	10306-10307	]	
72-298	10308-10309	(	
72-299	10309-10314	https	
72-300	10314-10315	:	
72-301	10315-10316	/	
72-302	10316-10317	/	
72-303	10317-10331	databricks.com	
72-304	10331-10332	/	
72-305	10332-10336	blog	
72-306	10336-10337	/	
72-307	10337-10341	2015	
72-308	10341-10342	/	
72-309	10342-10344	04	
72-310	10344-10345	/	
72-311	10345-10347	28	
72-312	10347-10348	/	
72-313	10348-10405	project-tungsten-bringing-spark-closer-to-bare-metal.html	
72-314	10405-10406	)	
72-315	10406-10407	.	

#Text=You do not have to waste time tuning your RDDs, you can focus on the business problem.
73-1	10408-10411	You	
73-2	10412-10414	do	
73-3	10415-10418	not	
73-4	10419-10423	have	
73-5	10424-10426	to	
73-6	10427-10432	waste	
73-7	10433-10437	time	
73-8	10438-10444	tuning	
73-9	10445-10449	your	
73-10	10450-10454	RDDs	
73-11	10454-10455	,	
73-12	10456-10459	you	
73-13	10460-10463	can	
73-14	10464-10469	focus	
73-15	10470-10472	on	
73-16	10473-10476	the	
73-17	10477-10485	business	
73-18	10486-10493	problem	
73-19	10493-10494	.	

#Text=Understandable code
#Text=The code in RDD expresses more how of a solution better than what.
74-1	10495-10509	Understandable	
74-2	10510-10514	code	
74-3	10515-10518	The	
74-4	10519-10523	code	
74-5	10524-10526	in	
74-6	10527-10530	RDD	
74-7	10531-10540	expresses	
74-8	10541-10545	more	
74-9	10546-10549	how	
74-10	10550-10552	of	
74-11	10553-10554	a	
74-12	10555-10563	solution	
74-13	10564-10570	better	
74-14	10571-10575	than	
74-15	10576-10580	what	
74-16	10580-10581	.	

#Text=It is a bit difficult to read the code based on RDD and understand what is going on.
75-1	10582-10584	It	
75-2	10585-10587	is	
75-3	10588-10589	a	
75-4	10590-10593	bit	
75-5	10594-10603	difficult	
75-6	10604-10606	to	
75-7	10607-10611	read	
75-8	10612-10615	the	
75-9	10616-10620	code	
75-10	10621-10626	based	
75-11	10627-10629	on	
75-12	10630-10633	RDD	
75-13	10634-10637	and	
75-14	10638-10648	understand	
75-15	10649-10653	what	
75-16	10654-10656	is	
75-17	10657-10662	going	
75-18	10663-10665	on	
75-19	10665-10666	.	

#Text=Using DataFrame API it's easy to go one level up — to business logic.
76-1	10667-10672	Using	
76-2	10673-10682	DataFrame	
76-3	10683-10686	API	
76-4	10687-10691	it's	
76-5	10692-10696	easy	
76-6	10697-10699	to	
76-7	10700-10702	go	
76-8	10703-10706	one	
76-9	10707-10712	level	
76-10	10713-10715	up	
76-11	10716-10717	—	
76-12	10718-10720	to	
76-13	10721-10729	business	
76-14	10730-10735	logic	
76-15	10735-10736	.	

#Text=Since DataFrame API looks like SQL and there is a schema, it's easy to see the real work with data.
77-1	10737-10742	Since	
77-2	10743-10752	DataFrame	
77-3	10753-10756	API	
77-4	10757-10762	looks	
77-5	10763-10767	like	
77-6	10768-10771	SQL	
77-7	10772-10775	and	
77-8	10776-10781	there	
77-9	10782-10784	is	
77-10	10785-10786	a	
77-11	10787-10793	schema	
77-12	10793-10794	,	
77-13	10795-10799	it's	
77-14	10800-10804	easy	
77-15	10805-10807	to	
77-16	10808-10811	see	
77-17	10812-10815	the	
77-18	10816-10820	real	
77-19	10821-10825	work	
77-20	10826-10830	with	
77-21	10831-10835	data	
77-22	10835-10836	.	

#Text=So, using DataFrame API, you are closer to what you are trying to achieve and not to how you are doing it.
78-1	10837-10839	So	
78-2	10839-10840	,	
78-3	10841-10846	using	
78-4	10847-10856	DataFrame	
78-5	10857-10860	API	
78-6	10860-10861	,	
78-7	10862-10865	you	
78-8	10866-10869	are	
78-9	10870-10876	closer	
78-10	10877-10879	to	
78-11	10880-10884	what	
78-12	10885-10888	you	
78-13	10889-10892	are	
78-14	10893-10899	trying	
78-15	10900-10902	to	
78-16	10903-10910	achieve	
78-17	10911-10914	and	
78-18	10915-10918	not	
78-19	10919-10921	to	
78-20	10922-10925	how	
78-21	10926-10929	you	
78-22	10930-10933	are	
78-23	10934-10939	doing	
78-24	10940-10942	it	
78-25	10942-10943	.	

#Text=You don't have to worry about how the DBMS find out if you need to perform a table scan or which indexes to use — all you need is a result.
79-1	10944-10947	You	
79-2	10948-10953	don't	
79-3	10954-10958	have	
79-4	10959-10961	to	
79-5	10962-10967	worry	
79-6	10968-10973	about	
79-7	10974-10977	how	
79-8	10978-10981	the	
79-9	10982-10986	DBMS	
79-10	10987-10991	find	
79-11	10992-10995	out	
79-12	10996-10998	if	
79-13	10999-11002	you	
79-14	11003-11007	need	
79-15	11008-11010	to	
79-16	11011-11018	perform	
79-17	11019-11020	a	
79-18	11021-11026	table	
79-19	11027-11031	scan	
79-20	11032-11034	or	
79-21	11035-11040	which	
79-22	11041-11048	indexes	
79-23	11049-11051	to	
79-24	11052-11055	use	
79-25	11056-11057	—	
79-26	11058-11061	all	
79-27	11062-11065	you	
79-28	11066-11070	need	
79-29	11071-11073	is	
79-30	11074-11075	a	
79-31	11076-11082	result	
79-32	11082-11083	.	

#Text=You express what you want, and you let Spark under the cover find the most effective way to do it.
80-1	11084-11087	You	
80-2	11088-11095	express	
80-3	11096-11100	what	
80-4	11101-11104	you	
80-5	11105-11109	want	
80-6	11109-11110	,	
80-7	11111-11114	and	
80-8	11115-11118	you	
80-9	11119-11122	let	
80-10	11123-11128	Spark	
80-11	11129-11134	under	
80-12	11135-11138	the	
80-13	11139-11144	cover	
80-14	11145-11149	find	
80-15	11150-11153	the	
80-16	11154-11158	most	
80-17	11159-11168	effective	
80-18	11169-11172	way	
80-19	11173-11175	to	
80-20	11176-11178	do	
80-21	11179-11181	it	
80-22	11181-11182	.	

#Text=DataFrame-based API is the primary API for MLlib
#Text=As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode.
81-1	11183-11198	DataFrame-based	
81-2	11199-11202	API	
81-3	11203-11205	is	
81-4	11206-11209	the	
81-5	11210-11217	primary	
81-6	11218-11221	API	
81-7	11222-11225	for	
81-8	11226-11231	MLlib	
81-9	11232-11234	As	
81-10	11235-11237	of	
81-11	11238-11243	Spark	
81-12	11244-11247	2.0	
81-13	11247-11248	,	
81-14	11249-11252	the	
81-15	11253-11262	RDD-based	
81-16	11263-11267	APIs	
81-17	11268-11270	in	
81-18	11271-11274	the	
81-19	11275-11286	spark.mllib	
81-20	11287-11294	package	
81-21	11295-11299	have	
81-22	11300-11307	entered	
81-23	11308-11319	maintenance	
81-24	11320-11324	mode	
81-25	11324-11325	.	

#Text=The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.
82-1	11326-11329	The	
82-2	11330-11337	primary	
82-3	11338-11345	Machine	
82-4	11346-11354	Learning	
82-5	11355-11358	API	
82-6	11359-11362	for	
82-7	11363-11368	Spark	
82-8	11369-11371	is	
82-9	11372-11375	now	
82-10	11376-11379	the	
82-11	11380-11395	DataFrame-based	
82-12	11396-11399	API	
82-13	11400-11402	in	
82-14	11403-11406	the	
82-15	11407-11415	spark.ml	
82-16	11416-11423	package	
82-17	11423-11424	.	

#Text=Recommended books
#Text=Spark: The Definitive Guide
#Text=Learning Spark: Lightning-Fast Data Analytics
#Text=#big_data
#Text=#spark
#Text=#python
#Text=Buy me a coffee
#Text=More?
83-1	11425-11436	Recommended	
83-2	11437-11442	books	
83-3	11443-11448	Spark	
83-4	11448-11449	:	
83-5	11450-11453	The	
83-6	11454-11464	Definitive	
83-7	11465-11470	Guide	
83-8	11471-11479	Learning	
83-9	11480-11485	Spark	
83-10	11485-11486	:	
83-11	11487-11501	Lightning-Fast	
83-12	11502-11506	Data	
83-13	11507-11516	Analytics	
83-14	11517-11518	#	
83-15	11518-11526	big_data	
83-16	11527-11528	#	
83-17	11528-11533	spark	
83-18	11534-11535	#	
83-19	11535-11541	python	
83-20	11542-11545	Buy	
83-21	11546-11548	me	
83-22	11549-11550	a	
83-23	11551-11557	coffee	
83-24	11558-11562	More	
83-25	11562-11563	?	

#Text=Well, there you go:
#Text=Azure Blob Storage with Pyspark
#Text=Data Challenges in Big Data
#Text=The ultimate Python style guidelines
#Text=Tags
#Text=About
#Text=License
#Text=Subscribe
#Text=Support author
#Text=Telegram channel
#Text=RSS
#Text=Feedly
#Text=© Copyright luminousmen.com All Rights Reserved
84-1	11564-11568	Well	
84-2	11568-11569	,	
84-3	11570-11575	there	
84-4	11576-11579	you	
84-5	11580-11582	go	
84-6	11582-11583	:	
84-7	11584-11589	Azure	
84-8	11590-11594	Blob	
84-9	11595-11602	Storage	
84-10	11603-11607	with	
84-11	11608-11615	Pyspark	
84-12	11616-11620	Data	
84-13	11621-11631	Challenges	
84-14	11632-11634	in	
84-15	11635-11638	Big	
84-16	11639-11643	Data	
84-17	11644-11647	The	
84-18	11648-11656	ultimate	
84-19	11657-11663	Python	
84-20	11664-11669	style	
84-21	11670-11680	guidelines	
84-22	11681-11685	Tags	
84-23	11686-11691	About	
84-24	11692-11699	License	
84-25	11700-11709	Subscribe	
84-26	11710-11717	Support	
84-27	11718-11724	author	
84-28	11725-11733	Telegram	
84-29	11734-11741	channel	
84-30	11742-11745	RSS	
84-31	11746-11752	Feedly	
84-32	11753-11754	©	
84-33	11755-11764	Copyright	
84-34	11765-11780	luminousmen.com	
84-35	11781-11784	All	
84-36	11785-11791	Rights	
84-37	11792-11800	Reserved	
