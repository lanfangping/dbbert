#FORMAT=WebAnno TSV 3.3


#Text=JDBC To Other Databases - Spark 2.4.7 Documentation
#Text=2.4.7
#Text=Overview
#Text=Programming Guides
#Text=Quick Start
#Text=RDDs, Accumulators, Broadcasts Vars
#Text=SQL, DataFrames, and Datasets
#Text=Structured Streaming
#Text=Spark Streaming (DStreams)
#Text=MLlib (Machine Learning)
#Text=GraphX (Graph Processing)
#Text=SparkR (R on Spark)
#Text=API Docs
#Text=Scala
#Text=Java
#Text=Python
#Text=SQL, Built-in Functions
#Text=Deploying
#Text=Overview
#Text=Submitting Applications
#Text=Spark Standalone
#Text=Mesos
#Text=YARN
#Text=Kubernetes
#Text=More
#Text=Configuration
#Text=Monitoring
#Text=Tuning Guide
#Text=Job Scheduling
#Text=Security
#Text=Hardware Provisioning
#Text=Building Spark
#Text=Contributing to Spark
#Text=Third Party Projects
#Text=Spark SQL Guide
#Text=Getting Started
#Text=Data Sources
#Text=Generic Load/Save Functions
#Text=Parquet Files
#Text=ORC Files
#Text=JSON Files
#Text=Hive Tables
#Text=JDBC To Other Databases
#Text=Avro Files
#Text=Troubleshooting
#Text=Performance Tuning
#Text=Distributed SQL Engine
#Text=PySpark Usage Guide for Pandas with Apache Arrow
#Text=Migration Guide
#Text=Reference
#Text=JDBC To Other Databases
#Text=Spark SQL also includes a data source that can read data from other databases using JDBC.
1-1	0-4	JDBC	
1-2	5-7	To	
1-3	8-13	Other	
1-4	14-23	Databases	
1-5	24-25	-	
1-6	26-31	Spark	
1-7	32-37	2.4.7	
1-8	38-51	Documentation	
1-9	52-57	2.4.7	
1-10	58-66	Overview	
1-11	67-78	Programming	
1-12	79-85	Guides	
1-13	86-91	Quick	
1-14	92-97	Start	
1-15	98-102	RDDs	
1-16	102-103	,	
1-17	104-116	Accumulators	
1-18	116-117	,	
1-19	118-128	Broadcasts	
1-20	129-133	Vars	
1-21	134-137	SQL	
1-22	137-138	,	
1-23	139-149	DataFrames	
1-24	149-150	,	
1-25	151-154	and	
1-26	155-163	Datasets	
1-27	164-174	Structured	
1-28	175-184	Streaming	
1-29	185-190	Spark	
1-30	191-200	Streaming	
1-31	201-202	(	
1-32	202-210	DStreams	
1-33	210-211	)	
1-34	212-217	MLlib	
1-35	218-219	(	
1-36	219-226	Machine	
1-37	227-235	Learning	
1-38	235-236	)	
1-39	237-243	GraphX	
1-40	244-245	(	
1-41	245-250	Graph	
1-42	251-261	Processing	
1-43	261-262	)	
1-44	263-269	SparkR	
1-45	270-271	(	
1-46	271-272	R	
1-47	273-275	on	
1-48	276-281	Spark	
1-49	281-282	)	
1-50	283-286	API	
1-51	287-291	Docs	
1-52	292-297	Scala	
1-53	298-302	Java	
1-54	303-309	Python	
1-55	310-313	SQL	
1-56	313-314	,	
1-57	315-323	Built-in	
1-58	324-333	Functions	
1-59	334-343	Deploying	
1-60	344-352	Overview	
1-61	353-363	Submitting	
1-62	364-376	Applications	
1-63	377-382	Spark	
1-64	383-393	Standalone	
1-65	394-399	Mesos	
1-66	400-404	YARN	
1-67	405-415	Kubernetes	
1-68	416-420	More	
1-69	421-434	Configuration	
1-70	435-445	Monitoring	
1-71	446-452	Tuning	
1-72	453-458	Guide	
1-73	459-462	Job	
1-74	463-473	Scheduling	
1-75	474-482	Security	
1-76	483-491	Hardware	
1-77	492-504	Provisioning	
1-78	505-513	Building	
1-79	514-519	Spark	
1-80	520-532	Contributing	
1-81	533-535	to	
1-82	536-541	Spark	
1-83	542-547	Third	
1-84	548-553	Party	
1-85	554-562	Projects	
1-86	563-568	Spark	
1-87	569-572	SQL	
1-88	573-578	Guide	
1-89	579-586	Getting	
1-90	587-594	Started	
1-91	595-599	Data	
1-92	600-607	Sources	
1-93	608-615	Generic	
1-94	616-620	Load	
1-95	620-621	/	
1-96	621-625	Save	
1-97	626-635	Functions	
1-98	636-643	Parquet	
1-99	644-649	Files	
1-100	650-653	ORC	
1-101	654-659	Files	
1-102	660-664	JSON	
1-103	665-670	Files	
1-104	671-675	Hive	
1-105	676-682	Tables	
1-106	683-687	JDBC	
1-107	688-690	To	
1-108	691-696	Other	
1-109	697-706	Databases	
1-110	707-711	Avro	
1-111	712-717	Files	
1-112	718-733	Troubleshooting	
1-113	734-745	Performance	
1-114	746-752	Tuning	
1-115	753-764	Distributed	
1-116	765-768	SQL	
1-117	769-775	Engine	
1-118	776-783	PySpark	
1-119	784-789	Usage	
1-120	790-795	Guide	
1-121	796-799	for	
1-122	800-806	Pandas	
1-123	807-811	with	
1-124	812-818	Apache	
1-125	819-824	Arrow	
1-126	825-834	Migration	
1-127	835-840	Guide	
1-128	841-850	Reference	
1-129	851-855	JDBC	
1-130	856-858	To	
1-131	859-864	Other	
1-132	865-874	Databases	
1-133	875-880	Spark	
1-134	881-884	SQL	
1-135	885-889	also	
1-136	890-898	includes	
1-137	899-900	a	
1-138	901-905	data	
1-139	906-912	source	
1-140	913-917	that	
1-141	918-921	can	
1-142	922-926	read	
1-143	927-931	data	
1-144	932-936	from	
1-145	937-942	other	
1-146	943-952	databases	
1-147	953-958	using	
1-148	959-963	JDBC	
1-149	963-964	.	

#Text=This
#Text=functionality should be preferred over using JdbcRDD.
2-1	965-969	This	
2-2	970-983	functionality	
2-3	984-990	should	
2-4	991-993	be	
2-5	994-1003	preferred	
2-6	1004-1008	over	
2-7	1009-1014	using	
2-8	1015-1022	JdbcRDD	
2-9	1022-1023	.	

#Text=This is because the results are returned
#Text=as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.
3-1	1024-1028	This	
3-2	1029-1031	is	
3-3	1032-1039	because	
3-4	1040-1043	the	
3-5	1044-1051	results	
3-6	1052-1055	are	
3-7	1056-1064	returned	
3-8	1065-1067	as	
3-9	1068-1069	a	
3-10	1070-1079	DataFrame	
3-11	1080-1083	and	
3-12	1084-1088	they	
3-13	1089-1092	can	
3-14	1093-1099	easily	
3-15	1100-1102	be	
3-16	1103-1112	processed	
3-17	1113-1115	in	
3-18	1116-1121	Spark	
3-19	1122-1125	SQL	
3-20	1126-1128	or	
3-21	1129-1135	joined	
3-22	1136-1140	with	
3-23	1141-1146	other	
3-24	1147-1151	data	
3-25	1152-1159	sources	
3-26	1159-1160	.	

#Text=The JDBC data source is also easier to use from Java or Python as it does not require the user to
#Text=provide a ClassTag.
4-1	1161-1164	The	
4-2	1165-1169	JDBC	
4-3	1170-1174	data	
4-4	1175-1181	source	
4-5	1182-1184	is	
4-6	1185-1189	also	
4-7	1190-1196	easier	
4-8	1197-1199	to	
4-9	1200-1203	use	
4-10	1204-1208	from	
4-11	1209-1213	Java	
4-12	1214-1216	or	
4-13	1217-1223	Python	
4-14	1224-1226	as	
4-15	1227-1229	it	
4-16	1230-1234	does	
4-17	1235-1238	not	
4-18	1239-1246	require	
4-19	1247-1250	the	
4-20	1251-1255	user	
4-21	1256-1258	to	
4-22	1259-1266	provide	
4-23	1267-1268	a	
4-24	1269-1277	ClassTag	
4-25	1277-1278	.	

#Text=(Note that this is different than the Spark SQL JDBC server, which allows other applications to
#Text=run queries using Spark SQL).
5-1	1279-1280	(	
5-2	1280-1284	Note	
5-3	1285-1289	that	
5-4	1290-1294	this	
5-5	1295-1297	is	
5-6	1298-1307	different	
5-7	1308-1312	than	
5-8	1313-1316	the	
5-9	1317-1322	Spark	
5-10	1323-1326	SQL	
5-11	1327-1331	JDBC	
5-12	1332-1338	server	
5-13	1338-1339	,	
5-14	1340-1345	which	
5-15	1346-1352	allows	
5-16	1353-1358	other	
5-17	1359-1371	applications	
5-18	1372-1374	to	
5-19	1375-1378	run	
5-20	1379-1386	queries	
5-21	1387-1392	using	
5-22	1393-1398	Spark	
5-23	1399-1402	SQL	
5-24	1402-1403	)	
5-25	1403-1404	.	

#Text=To get started you will need to include the JDBC driver for your particular database on the
#Text=spark classpath.
6-1	1405-1407	To	
6-2	1408-1411	get	
6-3	1412-1419	started	
6-4	1420-1423	you	
6-5	1424-1428	will	
6-6	1429-1433	need	
6-7	1434-1436	to	
6-8	1437-1444	include	
6-9	1445-1448	the	
6-10	1449-1453	JDBC	
6-11	1454-1460	driver	
6-12	1461-1464	for	
6-13	1465-1469	your	
6-14	1470-1480	particular	
6-15	1481-1489	database	
6-16	1490-1492	on	
6-17	1493-1496	the	
6-18	1497-1502	spark	
6-19	1503-1512	classpath	
6-20	1512-1513	.	

#Text=For example, to connect to postgres from the Spark Shell you would run the
#Text=following command:
#Text=bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar
#Text=Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using
#Text=the Data Sources API.
7-1	1514-1517	For	
7-2	1518-1525	example	
7-3	1525-1526	,	
7-4	1527-1529	to	
7-5	1530-1537	connect	
7-6	1538-1540	to	
7-7	1541-1549	postgres	
7-8	1550-1554	from	
7-9	1555-1558	the	
7-10	1559-1564	Spark	
7-11	1565-1570	Shell	
7-12	1571-1574	you	
7-13	1575-1580	would	
7-14	1581-1584	run	
7-15	1585-1588	the	
7-16	1589-1598	following	
7-17	1599-1606	command	
7-18	1606-1607	:	
7-19	1608-1611	bin	
7-20	1611-1612	/	
7-21	1612-1623	spark-shell	
7-22	1624-1625	-	
7-23	1625-1626	-	
7-24	1626-1643	driver-class-path	
7-25	1644-1654	postgresql	
7-26	1654-1655	-	
7-27	1655-1663	9.4.1207	
7-28	1663-1664	.	
7-29	1664-1667	jar	
7-30	1668-1669	-	
7-31	1669-1670	-	
7-32	1670-1674	jars	
7-33	1675-1685	postgresql	
7-34	1685-1686	-	
7-35	1686-1694	9.4.1207	
7-36	1694-1695	.	
7-37	1695-1698	jar	
7-38	1699-1705	Tables	
7-39	1706-1710	from	
7-40	1711-1714	the	
7-41	1715-1721	remote	
7-42	1722-1730	database	
7-43	1731-1734	can	
7-44	1735-1737	be	
7-45	1738-1744	loaded	
7-46	1745-1747	as	
7-47	1748-1749	a	
7-48	1750-1759	DataFrame	
7-49	1760-1762	or	
7-50	1763-1768	Spark	
7-51	1769-1772	SQL	
7-52	1773-1782	temporary	
7-53	1783-1787	view	
7-54	1788-1793	using	
7-55	1794-1797	the	
7-56	1798-1802	Data	
7-57	1803-1810	Sources	
7-58	1811-1814	API	
7-59	1814-1815	.	

#Text=Users can specify the JDBC connection properties in the data source options.
#Text=user and password are normally provided as connection properties for
#Text=logging into the data sources.
8-1	1816-1821	Users	
8-2	1822-1825	can	
8-3	1826-1833	specify	
8-4	1834-1837	the	
8-5	1838-1842	JDBC	
8-6	1843-1853	connection	
8-7	1854-1864	properties	
8-8	1865-1867	in	
8-9	1868-1871	the	
8-10	1872-1876	data	
8-11	1877-1883	source	
8-12	1884-1891	options	
8-13	1891-1892	.	
8-14	1893-1897	user	
8-15	1898-1901	and	
8-16	1902-1910	password	
8-17	1911-1914	are	
8-18	1915-1923	normally	
8-19	1924-1932	provided	
8-20	1933-1935	as	
8-21	1936-1946	connection	
8-22	1947-1957	properties	
8-23	1958-1961	for	
8-24	1962-1969	logging	
8-25	1970-1974	into	
8-26	1975-1978	the	
8-27	1979-1983	data	
8-28	1984-1991	sources	
8-29	1991-1992	.	

#Text=In addition to the connection properties, Spark also supports
#Text=the following case-insensitive options:
#Text=Property NameMeaning
#Text=url
#Text=The JDBC URL to connect to.
9-1	1993-1995	In	
9-2	1996-2004	addition	
9-3	2005-2007	to	
9-4	2008-2011	the	
9-5	2012-2022	connection	
9-6	2023-2033	properties	
9-7	2033-2034	,	
9-8	2035-2040	Spark	
9-9	2041-2045	also	
9-10	2046-2054	supports	
9-11	2055-2058	the	
9-12	2059-2068	following	
9-13	2069-2085	case-insensitive	
9-14	2086-2093	options	
9-15	2093-2094	:	
9-16	2095-2103	Property	
9-17	2104-2115	NameMeaning	
9-18	2116-2119	url	
9-19	2120-2123	The	
9-20	2124-2128	JDBC	
9-21	2129-2132	URL	
9-22	2133-2135	to	
9-23	2136-2143	connect	
9-24	2144-2146	to	
9-25	2146-2147	.	

#Text=The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?
10-1	2148-2151	The	
10-2	2152-2167	source-specific	
10-3	2168-2178	connection	
10-4	2179-2189	properties	
10-5	2190-2193	may	
10-6	2194-2196	be	
10-7	2197-2206	specified	
10-8	2207-2209	in	
10-9	2210-2213	the	
10-10	2214-2217	URL	
10-11	2217-2218	.	
10-12	2219-2222	e.g	
10-13	2222-2223	.	
10-14	2223-2224	,	
10-15	2225-2229	jdbc	
10-16	2229-2230	:	
10-17	2230-2240	postgresql	
10-18	2240-2241	:	
10-19	2241-2242	/	
10-20	2242-2243	/	
10-21	2243-2252	localhost	
10-22	2252-2253	/	
10-23	2253-2257	test	
10-24	2257-2258	?	

#Text=user=fred&password=secret
#Text=dbtable
#Text=The JDBC table that should be read from or written into.
11-1	2258-2262	user	
11-2	2262-2263	=	
11-3	2263-2267	fred	
11-4	2267-2268	&	
11-5	2268-2276	password	
11-6	2276-2277	=	
11-7	2277-2283	secret	
11-8	2284-2291	dbtable	
11-9	2292-2295	The	
11-10	2296-2300	JDBC	
11-11	2301-2306	table	
11-12	2307-2311	that	
11-13	2312-2318	should	
11-14	2319-2321	be	
11-15	2322-2326	read	
11-16	2327-2331	from	
11-17	2332-2334	or	
11-18	2335-2342	written	
11-19	2343-2347	into	
11-20	2347-2348	.	

#Text=Note that when using it in the read
#Text=path anything that is valid in a FROM clause of a SQL query can be used.
12-1	2349-2353	Note	
12-2	2354-2358	that	
12-3	2359-2363	when	
12-4	2364-2369	using	
12-5	2370-2372	it	
12-6	2373-2375	in	
12-7	2376-2379	the	
12-8	2380-2384	read	
12-9	2385-2389	path	
12-10	2390-2398	anything	
12-11	2399-2403	that	
12-12	2404-2406	is	
12-13	2407-2412	valid	
12-14	2413-2415	in	
12-15	2416-2417	a	
12-16	2418-2422	FROM	
12-17	2423-2429	clause	
12-18	2430-2432	of	
12-19	2433-2434	a	
12-20	2435-2438	SQL	
12-21	2439-2444	query	
12-22	2445-2448	can	
12-23	2449-2451	be	
12-24	2452-2456	used	
12-25	2456-2457	.	

#Text=For example, instead of a full table you could also use a subquery in parentheses.
13-1	2458-2461	For	
13-2	2462-2469	example	
13-3	2469-2470	,	
13-4	2471-2478	instead	
13-5	2479-2481	of	
13-6	2482-2483	a	
13-7	2484-2488	full	
13-8	2489-2494	table	
13-9	2495-2498	you	
13-10	2499-2504	could	
13-11	2505-2509	also	
13-12	2510-2513	use	
13-13	2514-2515	a	
13-14	2516-2524	subquery	
13-15	2525-2527	in	
13-16	2528-2539	parentheses	
13-17	2539-2540	.	

#Text=It is not
#Text=allowed to specify `dbtable` and `query` options at the same time.
#Text=query
#Text=A query that will be used to read data into Spark.
14-1	2541-2543	It	
14-2	2544-2546	is	
14-3	2547-2550	not	
14-4	2551-2558	allowed	
14-5	2559-2561	to	
14-6	2562-2569	specify	
14-7	2570-2571	`	
14-8	2571-2578	dbtable	
14-9	2578-2579	`	
14-10	2580-2583	and	
14-11	2584-2585	`	
14-12	2585-2590	query	
14-13	2590-2591	`	
14-14	2592-2599	options	
14-15	2600-2602	at	
14-16	2603-2606	the	
14-17	2607-2611	same	
14-18	2612-2616	time	
14-19	2616-2617	.	
14-20	2618-2623	query	
14-21	2624-2625	A	
14-22	2626-2631	query	
14-23	2632-2636	that	
14-24	2637-2641	will	
14-25	2642-2644	be	
14-26	2645-2649	used	
14-27	2650-2652	to	
14-28	2653-2657	read	
14-29	2658-2662	data	
14-30	2663-2667	into	
14-31	2668-2673	Spark	
14-32	2673-2674	.	

#Text=The specified query will be parenthesized and used
#Text=as a subquery in the FROM clause.
15-1	2675-2678	The	
15-2	2679-2688	specified	
15-3	2689-2694	query	
15-4	2695-2699	will	
15-5	2700-2702	be	
15-6	2703-2716	parenthesized	
15-7	2717-2720	and	
15-8	2721-2725	used	
15-9	2726-2728	as	
15-10	2729-2730	a	
15-11	2731-2739	subquery	
15-12	2740-2742	in	
15-13	2743-2746	the	
15-14	2747-2751	FROM	
15-15	2752-2758	clause	
15-16	2758-2759	.	

#Text=Spark will also assign an alias to the subquery clause.
16-1	2760-2765	Spark	
16-2	2766-2770	will	
16-3	2771-2775	also	
16-4	2776-2782	assign	
16-5	2783-2785	an	
16-6	2786-2791	alias	
16-7	2792-2794	to	
16-8	2795-2798	the	
16-9	2799-2807	subquery	
16-10	2808-2814	clause	
16-11	2814-2815	.	

#Text=As an example, spark will issue a query of the following form to the JDBC Source.
17-1	2816-2818	As	
17-2	2819-2821	an	
17-3	2822-2829	example	
17-4	2829-2830	,	
17-5	2831-2836	spark	
17-6	2837-2841	will	
17-7	2842-2847	issue	
17-8	2848-2849	a	
17-9	2850-2855	query	
17-10	2856-2858	of	
17-11	2859-2862	the	
17-12	2863-2872	following	
17-13	2873-2877	form	
17-14	2878-2880	to	
17-15	2881-2884	the	
17-16	2885-2889	JDBC	
17-17	2890-2896	Source	
17-18	2896-2897	.	

#Text=SELECT <columns> FROM (<user_specified_query>) spark_gen_alias
#Text=Below are couple of restrictions while using this option.
18-1	2898-2904	SELECT	
18-2	2905-2906	<	
18-3	2906-2913	columns	
18-4	2913-2914	>	
18-5	2915-2919	FROM	
18-6	2920-2921	(	
18-7	2921-2922	<	
18-8	2922-2942	user_specified_query	
18-9	2942-2943	>	
18-10	2943-2944	)	
18-11	2945-2960	spark_gen_alias	
18-12	2961-2966	Below	
18-13	2967-2970	are	
18-14	2971-2977	couple	
18-15	2978-2980	of	
18-16	2981-2993	restrictions	
18-17	2994-2999	while	
18-18	3000-3005	using	
18-19	3006-3010	this	
18-20	3011-3017	option	
18-21	3017-3018	.	

#Text=It is not allowed to specify `dbtable` and `query` options at the same time.
19-1	3019-3021	It	
19-2	3022-3024	is	
19-3	3025-3028	not	
19-4	3029-3036	allowed	
19-5	3037-3039	to	
19-6	3040-3047	specify	
19-7	3048-3049	`	
19-8	3049-3056	dbtable	
19-9	3056-3057	`	
19-10	3058-3061	and	
19-11	3062-3063	`	
19-12	3063-3068	query	
19-13	3068-3069	`	
19-14	3070-3077	options	
19-15	3078-3080	at	
19-16	3081-3084	the	
19-17	3085-3089	same	
19-18	3090-3094	time	
19-19	3094-3095	.	

#Text=It is not allowed to specify `query` and `partitionColumn` options at the same time.
20-1	3096-3098	It	
20-2	3099-3101	is	
20-3	3102-3105	not	
20-4	3106-3113	allowed	
20-5	3114-3116	to	
20-6	3117-3124	specify	
20-7	3125-3126	`	
20-8	3126-3131	query	
20-9	3131-3132	`	
20-10	3133-3136	and	
20-11	3137-3138	`	
20-12	3138-3153	partitionColumn	
20-13	3153-3154	`	
20-14	3155-3162	options	
20-15	3163-3165	at	
20-16	3166-3169	the	
20-17	3170-3174	same	
20-18	3175-3179	time	
20-19	3179-3180	.	

#Text=When specifying
#Text=`partitionColumn` option is required, the subquery can be specified using `dbtable` option instead and
#Text=partition columns can be qualified using the subquery alias provided as part of `dbtable`.
21-1	3181-3185	When	
21-2	3186-3196	specifying	
21-3	3197-3198	`	
21-4	3198-3213	partitionColumn	
21-5	3213-3214	`	
21-6	3215-3221	option	
21-7	3222-3224	is	
21-8	3225-3233	required	
21-9	3233-3234	,	
21-10	3235-3238	the	
21-11	3239-3247	subquery	
21-12	3248-3251	can	
21-13	3252-3254	be	
21-14	3255-3264	specified	
21-15	3265-3270	using	
21-16	3271-3272	`	
21-17	3272-3279	dbtable	
21-18	3279-3280	`	
21-19	3281-3287	option	
21-20	3288-3295	instead	
21-21	3296-3299	and	
21-22	3300-3309	partition	
21-23	3310-3317	columns	
21-24	3318-3321	can	
21-25	3322-3324	be	
21-26	3325-3334	qualified	
21-27	3335-3340	using	
21-28	3341-3344	the	
21-29	3345-3353	subquery	
21-30	3354-3359	alias	
21-31	3360-3368	provided	
21-32	3369-3371	as	
21-33	3372-3376	part	
21-34	3377-3379	of	
21-35	3380-3381	`	
21-36	3381-3388	dbtable	
21-37	3388-3389	`	
21-38	3389-3390	.	

#Text=Example:
#Text=spark.read.format("jdbc")
#Text=.option("url", jdbcUrl)
#Text=.option("query", "select c1, c2 from t1")
#Text=.load()
#Text=driver
#Text=The class name of the JDBC driver to use to connect to this URL.
#Text=partitionColumn, lowerBound, upperBound
#Text=These options must all be specified if any of them is specified.
22-1	3391-3398	Example	
22-2	3398-3399	:	
22-3	3400-3417	spark.read.format	
22-4	3417-3418	(	
22-5	3418-3419	"	
22-6	3419-3423	jdbc	
22-7	3423-3424	"	
22-8	3424-3425	)	
22-9	3426-3427	.	
22-10	3427-3433	option	
22-11	3433-3434	(	
22-12	3434-3435	"	
22-13	3435-3438	url	
22-14	3438-3439	"	
22-15	3439-3440	,	
22-16	3441-3448	jdbcUrl	
22-17	3448-3449	)	
22-18	3450-3451	.	
22-19	3451-3457	option	
22-20	3457-3458	(	
22-21	3458-3459	"	
22-22	3459-3464	query	
22-23	3464-3465	"	
22-24	3465-3466	,	
22-25	3467-3468	"	
22-26	3468-3474	select	
22-27	3475-3477	c1	
22-28	3477-3478	,	
22-29	3479-3481	c2	
22-30	3482-3486	from	
22-31	3487-3489	t1	
22-32	3489-3490	"	
22-33	3490-3491	)	
22-34	3492-3493	.	
22-35	3493-3497	load	
22-36	3497-3498	(	
22-37	3498-3499	)	
22-38	3500-3506	driver	
22-39	3507-3510	The	
22-40	3511-3516	class	
22-41	3517-3521	name	
22-42	3522-3524	of	
22-43	3525-3528	the	
22-44	3529-3533	JDBC	
22-45	3534-3540	driver	
22-46	3541-3543	to	
22-47	3544-3547	use	
22-48	3548-3550	to	
22-49	3551-3558	connect	
22-50	3559-3561	to	
22-51	3562-3566	this	
22-52	3567-3570	URL	
22-53	3570-3571	.	
22-54	3572-3587	partitionColumn	
22-55	3587-3588	,	
22-56	3589-3599	lowerBound	
22-57	3599-3600	,	
22-58	3601-3611	upperBound	
22-59	3612-3617	These	
22-60	3618-3625	options	
22-61	3626-3630	must	
22-62	3631-3634	all	
22-63	3635-3637	be	
22-64	3638-3647	specified	
22-65	3648-3650	if	
22-66	3651-3654	any	
22-67	3655-3657	of	
22-68	3658-3662	them	
22-69	3663-3665	is	
22-70	3666-3675	specified	
22-71	3675-3676	.	

#Text=In addition,
#Text=numPartitions must be specified.
23-1	3677-3679	In	
23-2	3680-3688	addition	
23-3	3688-3689	,	
23-4	3690-3703	numPartitions	
23-5	3704-3708	must	
23-6	3709-3711	be	
23-7	3712-3721	specified	
23-8	3721-3722	.	

#Text=They describe how to partition the table when
#Text=reading in parallel from multiple workers.
#Text=partitionColumn must be a numeric, date, or timestamp column from the table in question.
24-1	3723-3727	They	
24-2	3728-3736	describe	
24-3	3737-3740	how	
24-4	3741-3743	to	
24-5	3744-3753	partition	
24-6	3754-3757	the	
24-7	3758-3763	table	
24-8	3764-3768	when	
24-9	3769-3776	reading	
24-10	3777-3779	in	
24-11	3780-3788	parallel	
24-12	3789-3793	from	
24-13	3794-3802	multiple	
24-14	3803-3810	workers	
24-15	3810-3811	.	
24-16	3812-3827	partitionColumn	
24-17	3828-3832	must	
24-18	3833-3835	be	
24-19	3836-3837	a	
24-20	3838-3845	numeric	
24-21	3845-3846	,	
24-22	3847-3851	date	
24-23	3851-3852	,	
24-24	3853-3855	or	
24-25	3856-3865	timestamp	
24-26	3866-3872	column	
24-27	3873-3877	from	
24-28	3878-3881	the	
24-29	3882-3887	table	
24-30	3888-3890	in	
24-31	3891-3899	question	
24-32	3899-3900	.	

#Text=Notice that lowerBound and upperBound are just used to decide the
#Text=partition stride, not for filtering the rows in table.
25-1	3901-3907	Notice	
25-2	3908-3912	that	
25-3	3913-3923	lowerBound	
25-4	3924-3927	and	
25-5	3928-3938	upperBound	
25-6	3939-3942	are	
25-7	3943-3947	just	
25-8	3948-3952	used	
25-9	3953-3955	to	
25-10	3956-3962	decide	
25-11	3963-3966	the	
25-12	3967-3976	partition	
25-13	3977-3983	stride	
25-14	3983-3984	,	
25-15	3985-3988	not	
25-16	3989-3992	for	
25-17	3993-4002	filtering	
25-18	4003-4006	the	
25-19	4007-4011	rows	
25-20	4012-4014	in	
25-21	4015-4020	table	
25-22	4020-4021	.	

#Text=So all rows in the table will be
#Text=partitioned and returned.
26-1	4022-4024	So	
26-2	4025-4028	all	
26-3	4029-4033	rows	
26-4	4034-4036	in	
26-5	4037-4040	the	
26-6	4041-4046	table	
26-7	4047-4051	will	
26-8	4052-4054	be	
26-9	4055-4066	partitioned	
26-10	4067-4070	and	
26-11	4071-4079	returned	
26-12	4079-4080	.	

#Text=This option applies only to reading.
#Text=numPartitions
#Text=The maximum number of partitions that can be used for parallelism in table reading and
#Text=writing.
27-1	4081-4085	This	
27-2	4086-4092	option	
27-3	4093-4100	applies	
27-4	4101-4105	only	
27-5	4106-4108	to	
27-6	4109-4116	reading	
27-7	4116-4117	.	
27-8	4118-4131	numPartitions	
27-9	4132-4135	The	
27-10	4136-4143	maximum	
27-11	4144-4150	number	
27-12	4151-4153	of	
27-13	4154-4164	partitions	
27-14	4165-4169	that	
27-15	4170-4173	can	
27-16	4174-4176	be	
27-17	4177-4181	used	
27-18	4182-4185	for	
27-19	4186-4197	parallelism	
27-20	4198-4200	in	
27-21	4201-4206	table	
27-22	4207-4214	reading	
27-23	4215-4218	and	
27-24	4219-4226	writing	
27-25	4226-4227	.	

#Text=This also determines the maximum number of concurrent JDBC connections.
28-1	4228-4232	This	
28-2	4233-4237	also	
28-3	4238-4248	determines	
28-4	4249-4252	the	
28-5	4253-4260	maximum	
28-6	4261-4267	number	
28-7	4268-4270	of	
28-8	4271-4281	concurrent	
28-9	4282-4286	JDBC	
28-10	4287-4298	connections	
28-11	4298-4299	.	

#Text=If the number of partitions to write exceeds this limit, we decrease it to this limit by
#Text=calling coalesce(numPartitions) before writing.
#Text=queryTimeout
#Text=The number of seconds the driver will wait for a Statement object to execute to the given
#Text=number of seconds.
29-1	4300-4302	If	
29-2	4303-4306	the	
29-3	4307-4313	number	
29-4	4314-4316	of	
29-5	4317-4327	partitions	
29-6	4328-4330	to	
29-7	4331-4336	write	
29-8	4337-4344	exceeds	
29-9	4345-4349	this	
29-10	4350-4355	limit	
29-11	4355-4356	,	
29-12	4357-4359	we	
29-13	4360-4368	decrease	
29-14	4369-4371	it	
29-15	4372-4374	to	
29-16	4375-4379	this	
29-17	4380-4385	limit	
29-18	4386-4388	by	
29-19	4389-4396	calling	
29-20	4397-4405	coalesce	
29-21	4405-4406	(	
29-22	4406-4419	numPartitions	
29-23	4419-4420	)	
29-24	4421-4427	before	
29-25	4428-4435	writing	
29-26	4435-4436	.	
29-27	4437-4449	queryTimeout	
29-28	4450-4453	The	
29-29	4454-4460	number	
29-30	4461-4463	of	
29-31	4464-4471	seconds	
29-32	4472-4475	the	
29-33	4476-4482	driver	
29-34	4483-4487	will	
29-35	4488-4492	wait	
29-36	4493-4496	for	
29-37	4497-4498	a	
29-38	4499-4508	Statement	
29-39	4509-4515	object	
29-40	4516-4518	to	
29-41	4519-4526	execute	
29-42	4527-4529	to	
29-43	4530-4533	the	
29-44	4534-4539	given	
29-45	4540-4546	number	
29-46	4547-4549	of	
29-47	4550-4557	seconds	
29-48	4557-4558	.	

#Text=Zero means there is no limit.
30-1	4559-4563	Zero	
30-2	4564-4569	means	
30-3	4570-4575	there	
30-4	4576-4578	is	
30-5	4579-4581	no	
30-6	4582-4587	limit	
30-7	4587-4588	.	

#Text=In the write path, this option depends on
#Text=how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver
#Text=checks the timeout of each query instead of an entire JDBC batch.
31-1	4589-4591	In	
31-2	4592-4595	the	
31-3	4596-4601	write	
31-4	4602-4606	path	
31-5	4606-4607	,	
31-6	4608-4612	this	
31-7	4613-4619	option	
31-8	4620-4627	depends	
31-9	4628-4630	on	
31-10	4631-4634	how	
31-11	4635-4639	JDBC	
31-12	4640-4647	drivers	
31-13	4648-4657	implement	
31-14	4658-4661	the	
31-15	4662-4665	API	
31-16	4666-4681	setQueryTimeout	
31-17	4681-4682	,	
31-18	4683-4686	e.g	
31-19	4686-4687	.	
31-20	4687-4688	,	
31-21	4689-4692	the	
31-22	4693-4695	h2	
31-23	4696-4700	JDBC	
31-24	4701-4707	driver	
31-25	4708-4714	checks	
31-26	4715-4718	the	
31-27	4719-4726	timeout	
31-28	4727-4729	of	
31-29	4730-4734	each	
31-30	4735-4740	query	
31-31	4741-4748	instead	
31-32	4749-4751	of	
31-33	4752-4754	an	
31-34	4755-4761	entire	
31-35	4762-4766	JDBC	
31-36	4767-4772	batch	
31-37	4772-4773	.	

#Text=It defaults to 0.
#Text=fetchsize
#Text=The JDBC fetch size, which determines how many rows to fetch per round trip.
32-1	4774-4776	It	
32-2	4777-4785	defaults	
32-3	4786-4788	to	
32-4	4789-4790	0	
32-5	4790-4791	.	
32-6	4792-4801	fetchsize	
32-7	4802-4805	The	
32-8	4806-4810	JDBC	
32-9	4811-4816	fetch	
32-10	4817-4821	size	
32-11	4821-4822	,	
32-12	4823-4828	which	
32-13	4829-4839	determines	
32-14	4840-4843	how	
32-15	4844-4848	many	
32-16	4849-4853	rows	
32-17	4854-4856	to	
32-18	4857-4862	fetch	
32-19	4863-4866	per	
32-20	4867-4872	round	
32-21	4873-4877	trip	
32-22	4877-4878	.	

#Text=This can help performance on JDBC drivers which default to low fetch size (eg.
33-1	4879-4883	This	
33-2	4884-4887	can	
33-3	4888-4892	help	
33-4	4893-4904	performance	
33-5	4905-4907	on	
33-6	4908-4912	JDBC	
33-7	4913-4920	drivers	
33-8	4921-4926	which	
33-9	4927-4934	default	
33-10	4935-4937	to	
33-11	4938-4941	low	
33-12	4942-4947	fetch	
33-13	4948-4952	size	
33-14	4953-4954	(	
33-15	4954-4956	eg	
33-16	4956-4957	.	

#Text=Oracle with 10 rows).
34-1	4958-4964	Oracle	
34-2	4965-4969	with	
34-3	4970-4972	10	
34-4	4973-4977	rows	
34-5	4977-4978	)	
34-6	4978-4979	.	

#Text=This option applies only to reading.
#Text=batchsize
#Text=The JDBC batch size, which determines how many rows to insert per round trip.
35-1	4980-4984	This	
35-2	4985-4991	option	
35-3	4992-4999	applies	
35-4	5000-5004	only	
35-5	5005-5007	to	
35-6	5008-5015	reading	
35-7	5015-5016	.	
35-8	5017-5026	batchsize	
35-9	5027-5030	The	
35-10	5031-5035	JDBC	
35-11	5036-5041	batch	
35-12	5042-5046	size	
35-13	5046-5047	,	
35-14	5048-5053	which	
35-15	5054-5064	determines	
35-16	5065-5068	how	
35-17	5069-5073	many	
35-18	5074-5078	rows	
35-19	5079-5081	to	
35-20	5082-5088	insert	
35-21	5089-5092	per	
35-22	5093-5098	round	
35-23	5099-5103	trip	
35-24	5103-5104	.	

#Text=This can help performance on JDBC drivers.
36-1	5105-5109	This	
36-2	5110-5113	can	
36-3	5114-5118	help	
36-4	5119-5130	performance	
36-5	5131-5133	on	
36-6	5134-5138	JDBC	
36-7	5139-5146	drivers	
36-8	5146-5147	.	

#Text=This option applies only to writing.
37-1	5148-5152	This	
37-2	5153-5159	option	
37-3	5160-5167	applies	
37-4	5168-5172	only	
37-5	5173-5175	to	
37-6	5176-5183	writing	
37-7	5183-5184	.	

#Text=It defaults to 1000.
#Text=isolationLevel
#Text=The transaction isolation level, which applies to current connection.
38-1	5185-5187	It	
38-2	5188-5196	defaults	
38-3	5197-5199	to	
38-4	5200-5204	1000	
38-5	5204-5205	.	
38-6	5206-5220	isolationLevel	
38-7	5221-5224	The	
38-8	5225-5236	transaction	
38-9	5237-5246	isolation	
38-10	5247-5252	level	
38-11	5252-5253	,	
38-12	5254-5259	which	
38-13	5260-5267	applies	
38-14	5268-5270	to	
38-15	5271-5278	current	
38-16	5279-5289	connection	
38-17	5289-5290	.	

#Text=It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED.
39-1	5291-5293	It	
39-2	5294-5297	can	
39-3	5298-5300	be	
39-4	5301-5304	one	
39-5	5305-5307	of	
39-6	5308-5312	NONE	
39-7	5312-5313	,	
39-8	5314-5328	READ_COMMITTED	
39-9	5328-5329	,	
39-10	5330-5346	READ_UNCOMMITTED	
39-11	5346-5347	,	
39-12	5348-5363	REPEATABLE_READ	
39-13	5363-5364	,	
39-14	5365-5367	or	
39-15	5368-5380	SERIALIZABLE	
39-16	5380-5381	,	
39-17	5382-5395	corresponding	
39-18	5396-5398	to	
39-19	5399-5407	standard	
39-20	5408-5419	transaction	
39-21	5420-5429	isolation	
39-22	5430-5436	levels	
39-23	5437-5444	defined	
39-24	5445-5447	by	
39-25	5448-5454	JDBC's	
39-26	5455-5465	Connection	
39-27	5466-5472	object	
39-28	5472-5473	,	
39-29	5474-5478	with	
39-30	5479-5486	default	
39-31	5487-5489	of	
39-32	5490-5506	READ_UNCOMMITTED	
39-33	5506-5507	.	

#Text=This option applies only to writing.
40-1	5508-5512	This	
40-2	5513-5519	option	
40-3	5520-5527	applies	
40-4	5528-5532	only	
40-5	5533-5535	to	
40-6	5536-5543	writing	
40-7	5543-5544	.	

#Text=Please refer the documentation in java.sql.Connection.
#Text=sessionInitStatement
#Text=After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block).
41-1	5545-5551	Please	
41-2	5552-5557	refer	
41-3	5558-5561	the	
41-4	5562-5575	documentation	
41-5	5576-5578	in	
41-6	5579-5598	java.sql.Connection	
41-7	5598-5599	.	
41-8	5600-5620	sessionInitStatement	
41-9	5621-5626	After	
41-10	5627-5631	each	
41-11	5632-5640	database	
41-12	5641-5648	session	
41-13	5649-5651	is	
41-14	5652-5658	opened	
41-15	5659-5661	to	
41-16	5662-5665	the	
41-17	5666-5672	remote	
41-18	5673-5675	DB	
41-19	5676-5679	and	
41-20	5680-5686	before	
41-21	5687-5695	starting	
41-22	5696-5698	to	
41-23	5699-5703	read	
41-24	5704-5708	data	
41-25	5708-5709	,	
41-26	5710-5714	this	
41-27	5715-5721	option	
41-28	5722-5730	executes	
41-29	5731-5732	a	
41-30	5733-5739	custom	
41-31	5740-5743	SQL	
41-32	5744-5753	statement	
41-33	5754-5755	(	
41-34	5755-5757	or	
41-35	5758-5759	a	
41-36	5760-5762	PL	
41-37	5762-5763	/	
41-38	5763-5766	SQL	
41-39	5767-5772	block	
41-40	5772-5773	)	
41-41	5773-5774	.	

#Text=Use this to implement session initialization code.
42-1	5775-5778	Use	
42-2	5779-5783	this	
42-3	5784-5786	to	
42-4	5787-5796	implement	
42-5	5797-5804	session	
42-6	5805-5819	initialization	
42-7	5820-5824	code	
42-8	5824-5825	.	

#Text=Example: option("sessionInitStatement", """BEGIN execute immediate 'alter session set "_serial_direct_read"=true'; END;""")
#Text=truncate
#Text=This is a JDBC writer related option.
43-1	5826-5833	Example	
43-2	5833-5834	:	
43-3	5835-5841	option	
43-4	5841-5842	(	
43-5	5842-5843	"	
43-6	5843-5863	sessionInitStatement	
43-7	5863-5864	"	
43-8	5864-5865	,	
43-9	5866-5867	"	
43-10	5867-5868	"	
43-11	5868-5869	"	
43-12	5869-5874	BEGIN	
43-13	5875-5882	execute	
43-14	5883-5892	immediate	
43-15	5893-5894	'	
43-16	5894-5899	alter	
43-17	5900-5907	session	
43-18	5908-5911	set	
43-19	5912-5913	"	
43-20	5913-5914	_	
43-21	5914-5932	serial_direct_read	
43-22	5932-5933	"	
43-23	5933-5934	=	
43-24	5934-5938	true	
43-25	5938-5939	'	
43-26	5939-5940	;	
43-27	5941-5944	END	
43-28	5944-5945	;	
43-29	5945-5946	"	
43-30	5946-5947	"	
43-31	5947-5948	"	
43-32	5948-5949	)	
43-33	5950-5958	truncate	
43-34	5959-5963	This	
43-35	5964-5966	is	
43-36	5967-5968	a	
43-37	5969-5973	JDBC	
43-38	5974-5980	writer	
43-39	5981-5988	related	
43-40	5989-5995	option	
43-41	5995-5996	.	

#Text=When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it.
44-1	5997-6001	When	
44-2	6002-6020	SaveMode.Overwrite	
44-3	6021-6023	is	
44-4	6024-6031	enabled	
44-5	6031-6032	,	
44-6	6033-6037	this	
44-7	6038-6044	option	
44-8	6045-6051	causes	
44-9	6052-6057	Spark	
44-10	6058-6060	to	
44-11	6061-6069	truncate	
44-12	6070-6072	an	
44-13	6073-6081	existing	
44-14	6082-6087	table	
44-15	6088-6095	instead	
44-16	6096-6098	of	
44-17	6099-6107	dropping	
44-18	6108-6111	and	
44-19	6112-6122	recreating	
44-20	6123-6125	it	
44-21	6125-6126	.	

#Text=This can be more efficient, and prevents the table metadata (e.g., indices) from being removed.
45-1	6127-6131	This	
45-2	6132-6135	can	
45-3	6136-6138	be	
45-4	6139-6143	more	
45-5	6144-6153	efficient	
45-6	6153-6154	,	
45-7	6155-6158	and	
45-8	6159-6167	prevents	
45-9	6168-6171	the	
45-10	6172-6177	table	
45-11	6178-6186	metadata	
45-12	6187-6188	(	
45-13	6188-6191	e.g	
45-14	6191-6192	.	
45-15	6192-6193	,	
45-16	6194-6201	indices	
45-17	6201-6202	)	
45-18	6203-6207	from	
45-19	6208-6213	being	
45-20	6214-6221	removed	
45-21	6221-6222	.	

#Text=However, it will not work in some cases, such as when the new data has a different schema.
46-1	6223-6230	However	
46-2	6230-6231	,	
46-3	6232-6234	it	
46-4	6235-6239	will	
46-5	6240-6243	not	
46-6	6244-6248	work	
46-7	6249-6251	in	
46-8	6252-6256	some	
46-9	6257-6262	cases	
46-10	6262-6263	,	
46-11	6264-6268	such	
46-12	6269-6271	as	
46-13	6272-6276	when	
46-14	6277-6280	the	
46-15	6281-6284	new	
46-16	6285-6289	data	
46-17	6290-6293	has	
46-18	6294-6295	a	
46-19	6296-6305	different	
46-20	6306-6312	schema	
46-21	6312-6313	.	

#Text=It defaults to false.
47-1	6314-6316	It	
47-2	6317-6325	defaults	
47-3	6326-6328	to	
47-4	6329-6334	false	
47-5	6334-6335	.	

#Text=This option applies only to writing.
#Text=cascadeTruncate
#Text=This is a JDBC writer related option.
48-1	6336-6340	This	
48-2	6341-6347	option	
48-3	6348-6355	applies	
48-4	6356-6360	only	
48-5	6361-6363	to	
48-6	6364-6371	writing	
48-7	6371-6372	.	
48-8	6373-6388	cascadeTruncate	
48-9	6389-6393	This	
48-10	6394-6396	is	
48-11	6397-6398	a	
48-12	6399-6403	JDBC	
48-13	6404-6410	writer	
48-14	6411-6418	related	
48-15	6419-6425	option	
48-16	6425-6426	.	

#Text=If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a TRUNCATE TABLE t CASCADE (in the case of PostgreSQL a TRUNCATE TABLE ONLY t CASCADE is executed to prevent inadvertently truncating descendant tables).
49-1	6427-6429	If	
49-2	6430-6437	enabled	
49-3	6438-6441	and	
49-4	6442-6451	supported	
49-5	6452-6454	by	
49-6	6455-6458	the	
49-7	6459-6463	JDBC	
49-8	6464-6472	database	
49-9	6473-6474	(	
49-10	6474-6484	PostgreSQL	
49-11	6485-6488	and	
49-12	6489-6495	Oracle	
49-13	6496-6498	at	
49-14	6499-6502	the	
49-15	6503-6509	moment	
49-16	6509-6510	)	
49-17	6510-6511	,	
49-18	6512-6516	this	
49-19	6517-6524	options	
49-20	6525-6531	allows	
49-21	6532-6541	execution	
49-22	6542-6544	of	
49-23	6545-6546	a	
49-24	6547-6555	TRUNCATE	
49-25	6556-6561	TABLE	
49-26	6562-6563	t	
49-27	6564-6571	CASCADE	
49-28	6572-6573	(	
49-29	6573-6575	in	
49-30	6576-6579	the	
49-31	6580-6584	case	
49-32	6585-6587	of	
49-33	6588-6598	PostgreSQL	
49-34	6599-6600	a	
49-35	6601-6609	TRUNCATE	
49-36	6610-6615	TABLE	
49-37	6616-6620	ONLY	
49-38	6621-6622	t	
49-39	6623-6630	CASCADE	
49-40	6631-6633	is	
49-41	6634-6642	executed	
49-42	6643-6645	to	
49-43	6646-6653	prevent	
49-44	6654-6667	inadvertently	
49-45	6668-6678	truncating	
49-46	6679-6689	descendant	
49-47	6690-6696	tables	
49-48	6696-6697	)	
49-49	6697-6698	.	

#Text=This will affect other tables, and thus should be used with care.
50-1	6699-6703	This	
50-2	6704-6708	will	
50-3	6709-6715	affect	
50-4	6716-6721	other	
50-5	6722-6728	tables	
50-6	6728-6729	,	
50-7	6730-6733	and	
50-8	6734-6738	thus	
50-9	6739-6745	should	
50-10	6746-6748	be	
50-11	6749-6753	used	
50-12	6754-6758	with	
50-13	6759-6763	care	
50-14	6763-6764	.	

#Text=This option applies only to writing.
51-1	6765-6769	This	
51-2	6770-6776	option	
51-3	6777-6784	applies	
51-4	6785-6789	only	
51-5	6790-6792	to	
51-6	6793-6800	writing	
51-7	6800-6801	.	

#Text=It defaults to the default cascading truncate behaviour of the JDBC database in question, specified in the isCascadeTruncate in each JDBCDialect.
#Text=createTableOptions
#Text=This is a JDBC writer related option.
52-1	6802-6804	It	
52-2	6805-6813	defaults	
52-3	6814-6816	to	
52-4	6817-6820	the	
52-5	6821-6828	default	
52-6	6829-6838	cascading	
52-7	6839-6847	truncate	
52-8	6848-6857	behaviour	
52-9	6858-6860	of	
52-10	6861-6864	the	
52-11	6865-6869	JDBC	
52-12	6870-6878	database	
52-13	6879-6881	in	
52-14	6882-6890	question	
52-15	6890-6891	,	
52-16	6892-6901	specified	
52-17	6902-6904	in	
52-18	6905-6908	the	
52-19	6909-6926	isCascadeTruncate	
52-20	6927-6929	in	
52-21	6930-6934	each	
52-22	6935-6946	JDBCDialect	
52-23	6946-6947	.	
52-24	6948-6966	createTableOptions	
52-25	6967-6971	This	
52-26	6972-6974	is	
52-27	6975-6976	a	
52-28	6977-6981	JDBC	
52-29	6982-6988	writer	
52-30	6989-6996	related	
52-31	6997-7003	option	
52-32	7003-7004	.	

#Text=If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.).
53-1	7005-7007	If	
53-2	7008-7017	specified	
53-3	7017-7018	,	
53-4	7019-7023	this	
53-5	7024-7030	option	
53-6	7031-7037	allows	
53-7	7038-7045	setting	
53-8	7046-7048	of	
53-9	7049-7066	database-specific	
53-10	7067-7072	table	
53-11	7073-7076	and	
53-12	7077-7086	partition	
53-13	7087-7094	options	
53-14	7095-7099	when	
53-15	7100-7108	creating	
53-16	7109-7110	a	
53-17	7111-7116	table	
53-18	7117-7118	(	
53-19	7118-7121	e.g	
53-20	7121-7122	.	
53-21	7122-7123	,	
53-22	7124-7130	CREATE	
53-23	7131-7136	TABLE	
53-24	7137-7138	t	
53-25	7139-7140	(	
53-26	7140-7144	name	
53-27	7145-7151	string	
53-28	7151-7152	)	
53-29	7153-7159	ENGINE	
53-30	7159-7160	=	
53-31	7160-7166	InnoDB	
53-32	7166-7167	.	
53-33	7167-7168	)	
53-34	7168-7169	.	

#Text=This option applies only to writing.
#Text=createTableColumnTypes
#Text=The database column data types to use instead of the defaults, when creating the table.
54-1	7170-7174	This	
54-2	7175-7181	option	
54-3	7182-7189	applies	
54-4	7190-7194	only	
54-5	7195-7197	to	
54-6	7198-7205	writing	
54-7	7205-7206	.	
54-8	7207-7229	createTableColumnTypes	
54-9	7230-7233	The	
54-10	7234-7242	database	
54-11	7243-7249	column	
54-12	7250-7254	data	
54-13	7255-7260	types	
54-14	7261-7263	to	
54-15	7264-7267	use	
54-16	7268-7275	instead	
54-17	7276-7278	of	
54-18	7279-7282	the	
54-19	7283-7291	defaults	
54-20	7291-7292	,	
54-21	7293-7297	when	
54-22	7298-7306	creating	
54-23	7307-7310	the	
54-24	7311-7316	table	
54-25	7316-7317	.	

#Text=Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: "name CHAR(64), comments VARCHAR(1024)").
55-1	7318-7322	Data	
55-2	7323-7327	type	
55-3	7328-7339	information	
55-4	7340-7346	should	
55-5	7347-7349	be	
55-6	7350-7359	specified	
55-7	7360-7362	in	
55-8	7363-7366	the	
55-9	7367-7371	same	
55-10	7372-7378	format	
55-11	7379-7381	as	
55-12	7382-7388	CREATE	
55-13	7389-7394	TABLE	
55-14	7395-7402	columns	
55-15	7403-7409	syntax	
55-16	7410-7411	(	
55-17	7411-7414	e.g	
55-18	7414-7415	:	
55-19	7416-7417	"	
55-20	7417-7421	name	
55-21	7422-7426	CHAR	
55-22	7426-7427	(	
55-23	7427-7429	64	
55-24	7429-7430	)	
55-25	7430-7431	,	
55-26	7432-7440	comments	
55-27	7441-7448	VARCHAR	
55-28	7448-7449	(	
55-29	7449-7453	1024	
55-30	7453-7454	)	
55-31	7454-7455	"	
55-32	7455-7456	)	
55-33	7456-7457	.	

#Text=The specified types should be valid spark sql data types.
56-1	7458-7461	The	
56-2	7462-7471	specified	
56-3	7472-7477	types	
56-4	7478-7484	should	
56-5	7485-7487	be	
56-6	7488-7493	valid	
56-7	7494-7499	spark	
56-8	7500-7503	sql	
56-9	7504-7508	data	
56-10	7509-7514	types	
56-11	7514-7515	.	

#Text=This option applies only to writing.
#Text=customSchema
#Text=The custom schema to use for reading data from JDBC connectors.
57-1	7516-7520	This	
57-2	7521-7527	option	
57-3	7528-7535	applies	
57-4	7536-7540	only	
57-5	7541-7543	to	
57-6	7544-7551	writing	
57-7	7551-7552	.	
57-8	7553-7565	customSchema	
57-9	7566-7569	The	
57-10	7570-7576	custom	
57-11	7577-7583	schema	
57-12	7584-7586	to	
57-13	7587-7590	use	
57-14	7591-7594	for	
57-15	7595-7602	reading	
57-16	7603-7607	data	
57-17	7608-7612	from	
57-18	7613-7617	JDBC	
57-19	7618-7628	connectors	
57-20	7628-7629	.	

#Text=For example, "id DECIMAL(38, 0), name STRING".
58-1	7630-7633	For	
58-2	7634-7641	example	
58-3	7641-7642	,	
58-4	7643-7644	"	
58-5	7644-7646	id	
58-6	7647-7654	DECIMAL	
58-7	7654-7655	(	
58-8	7655-7657	38	
58-9	7657-7658	,	
58-10	7659-7660	0	
58-11	7660-7661	)	
58-12	7661-7662	,	
58-13	7663-7667	name	
58-14	7668-7674	STRING	
58-15	7674-7675	"	
58-16	7675-7676	.	

#Text=You can also specify partial fields, and the others use the default type mapping.
59-1	7677-7680	You	
59-2	7681-7684	can	
59-3	7685-7689	also	
59-4	7690-7697	specify	
59-5	7698-7705	partial	
59-6	7706-7712	fields	
59-7	7712-7713	,	
59-8	7714-7717	and	
59-9	7718-7721	the	
59-10	7722-7728	others	
59-11	7729-7732	use	
59-12	7733-7736	the	
59-13	7737-7744	default	
59-14	7745-7749	type	
59-15	7750-7757	mapping	
59-16	7757-7758	.	

#Text=For example, "id DECIMAL(38, 0)".
60-1	7759-7762	For	
60-2	7763-7770	example	
60-3	7770-7771	,	
60-4	7772-7773	"	
60-5	7773-7775	id	
60-6	7776-7783	DECIMAL	
60-7	7783-7784	(	
60-8	7784-7786	38	
60-9	7786-7787	,	
60-10	7788-7789	0	
60-11	7789-7790	)	
60-12	7790-7791	"	
60-13	7791-7792	.	

#Text=The column names should be identical to the corresponding column names of JDBC table.
61-1	7793-7796	The	
61-2	7797-7803	column	
61-3	7804-7809	names	
61-4	7810-7816	should	
61-5	7817-7819	be	
61-6	7820-7829	identical	
61-7	7830-7832	to	
61-8	7833-7836	the	
61-9	7837-7850	corresponding	
61-10	7851-7857	column	
61-11	7858-7863	names	
61-12	7864-7866	of	
61-13	7867-7871	JDBC	
61-14	7872-7877	table	
61-15	7877-7878	.	

#Text=Users can specify the corresponding data types of Spark SQL instead of using the defaults.
62-1	7879-7884	Users	
62-2	7885-7888	can	
62-3	7889-7896	specify	
62-4	7897-7900	the	
62-5	7901-7914	corresponding	
62-6	7915-7919	data	
62-7	7920-7925	types	
62-8	7926-7928	of	
62-9	7929-7934	Spark	
62-10	7935-7938	SQL	
62-11	7939-7946	instead	
62-12	7947-7949	of	
62-13	7950-7955	using	
62-14	7956-7959	the	
62-15	7960-7968	defaults	
62-16	7968-7969	.	

#Text=This option applies only to reading.
#Text=pushDownPredicate
#Text=The option to enable or disable predicate push-down into the JDBC data source.
63-1	7970-7974	This	
63-2	7975-7981	option	
63-3	7982-7989	applies	
63-4	7990-7994	only	
63-5	7995-7997	to	
63-6	7998-8005	reading	
63-7	8005-8006	.	
63-8	8007-8024	pushDownPredicate	
63-9	8025-8028	The	
63-10	8029-8035	option	
63-11	8036-8038	to	
63-12	8039-8045	enable	
63-13	8046-8048	or	
63-14	8049-8056	disable	
63-15	8057-8066	predicate	
63-16	8067-8076	push-down	
63-17	8077-8081	into	
63-18	8082-8085	the	
63-19	8086-8090	JDBC	
63-20	8091-8095	data	
63-21	8096-8102	source	
63-22	8102-8103	.	

#Text=The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible.
64-1	8104-8107	The	
64-2	8108-8115	default	
64-3	8116-8121	value	
64-4	8122-8124	is	
64-5	8125-8129	true	
64-6	8129-8130	,	
64-7	8131-8133	in	
64-8	8134-8139	which	
64-9	8140-8144	case	
64-10	8145-8150	Spark	
64-11	8151-8155	will	
64-12	8156-8160	push	
64-13	8161-8165	down	
64-14	8166-8173	filters	
64-15	8174-8176	to	
64-16	8177-8180	the	
64-17	8181-8185	JDBC	
64-18	8186-8190	data	
64-19	8191-8197	source	
64-20	8198-8200	as	
64-21	8201-8205	much	
64-22	8206-8208	as	
64-23	8209-8217	possible	
64-24	8217-8218	.	

#Text=Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark.
65-1	8219-8228	Otherwise	
65-2	8228-8229	,	
65-3	8230-8232	if	
65-4	8233-8236	set	
65-5	8237-8239	to	
65-6	8240-8245	false	
65-7	8245-8246	,	
65-8	8247-8249	no	
65-9	8250-8256	filter	
65-10	8257-8261	will	
65-11	8262-8264	be	
65-12	8265-8271	pushed	
65-13	8272-8276	down	
65-14	8277-8279	to	
65-15	8280-8283	the	
65-16	8284-8288	JDBC	
65-17	8289-8293	data	
65-18	8294-8300	source	
65-19	8301-8304	and	
65-20	8305-8309	thus	
65-21	8310-8313	all	
65-22	8314-8321	filters	
65-23	8322-8326	will	
65-24	8327-8329	be	
65-25	8330-8337	handled	
65-26	8338-8340	by	
65-27	8341-8346	Spark	
65-28	8346-8347	.	

#Text=Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.
#Text=// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
#Text=// Loading data from a JDBC source
#Text=val jdbcDF = spark.read
#Text=.format("jdbc")
#Text=.option("url", "jdbc:postgresql:dbserver")
#Text=.option("dbtable", "schema.tablename")
#Text=.option("user", "username")
#Text=.option("password", "password")
#Text=.load()
#Text=val connectionProperties = new Properties()
#Text=connectionProperties.put("user", "username")
#Text=connectionProperties.put("password", "password")
#Text=val jdbcDF2 = spark.read
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
#Text=// Specifying the custom data types of the read schema
#Text=connectionProperties.put("customSchema", "id DECIMAL(38, 0), name STRING")
#Text=val jdbcDF3 = spark.read
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
#Text=// Saving data to a JDBC source
#Text=jdbcDF.write
#Text=.format("jdbc")
#Text=.option("url", "jdbc:postgresql:dbserver")
#Text=.option("dbtable", "schema.tablename")
#Text=.option("user", "username")
#Text=.option("password", "password")
#Text=.save()
#Text=jdbcDF2.write
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
#Text=// Specifying create table column data types on write
#Text=jdbcDF.write
#Text=.option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
#Text=Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala" in the Spark repo.
#Text=// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
#Text=// Loading data from a JDBC source
#Text=Dataset<Row> jdbcDF = spark.read()
#Text=.format("jdbc")
#Text=.option("url", "jdbc:postgresql:dbserver")
#Text=.option("dbtable", "schema.tablename")
#Text=.option("user", "username")
#Text=.option("password", "password")
#Text=.load();
#Text=Properties connectionProperties = new Properties();
#Text=connectionProperties.put("user", "username");
#Text=connectionProperties.put("password", "password");
#Text=Dataset<Row> jdbcDF2 = spark.read()
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties);
#Text=// Saving data to a JDBC source
#Text=jdbcDF.write()
#Text=.format("jdbc")
#Text=.option("url", "jdbc:postgresql:dbserver")
#Text=.option("dbtable", "schema.tablename")
#Text=.option("user", "username")
#Text=.option("password", "password")
#Text=.save();
#Text=jdbcDF2.write()
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties);
#Text=// Specifying create table column data types on write
#Text=jdbcDF.write()
#Text=.option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties);
#Text=Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java" in the Spark repo.
#Text=# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
#Text=# Loading data from a JDBC source
#Text=jdbcDF = spark.read \\
#Text=.format("jdbc") \\
#Text=.option("url", "jdbc:postgresql:dbserver") \\
#Text=.option("dbtable", "schema.tablename") \\
#Text=.option("user", "username") \\
#Text=.option("password", "password") \\
#Text=.load()
#Text=jdbcDF2 = spark.read \\
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename",
#Text=properties={"user": "username", "password": "password"})
#Text=# Specifying dataframe column data types on read
#Text=jdbcDF3 = spark.read \\
#Text=.format("jdbc") \\
#Text=.option("url", "jdbc:postgresql:dbserver") \\
#Text=.option("dbtable", "schema.tablename") \\
#Text=.option("user", "username") \\
#Text=.option("password", "password") \\
#Text=.option("customSchema", "id DECIMAL(38, 0), name STRING") \\
#Text=.load()
#Text=# Saving data to a JDBC source
#Text=jdbcDF.write \\
#Text=.format("jdbc") \\
#Text=.option("url", "jdbc:postgresql:dbserver") \\
#Text=.option("dbtable", "schema.tablename") \\
#Text=.option("user", "username") \\
#Text=.option("password", "password") \\
#Text=.save()
#Text=jdbcDF2.write \\
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename",
#Text=properties={"user": "username", "password": "password"})
#Text=# Specifying create table column data types on write
#Text=jdbcDF.write \\
#Text=.option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)") \\
#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename",
#Text=properties={"user": "username", "password": "password"})
#Text=Find full example code at "examples/src/main/python/sql/datasource.py" in the Spark repo.
#Text=# Loading data from a JDBC source
#Text=df <- read.jdbc("jdbc:postgresql:dbserver", "schema.tablename", user = "username", password = "password")
#Text=# Saving data to a JDBC source
#Text=write.jdbc(df, "jdbc:postgresql:dbserver", "schema.tablename", user = "username", password = "password")
#Text=Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo.
66-1	8348-8357	Predicate	
66-2	8358-8367	push-down	
66-3	8368-8370	is	
66-4	8371-8378	usually	
66-5	8379-8385	turned	
66-6	8386-8389	off	
66-7	8390-8394	when	
66-8	8395-8398	the	
66-9	8399-8408	predicate	
66-10	8409-8418	filtering	
66-11	8419-8421	is	
66-12	8422-8431	performed	
66-13	8432-8438	faster	
66-14	8439-8441	by	
66-15	8442-8447	Spark	
66-16	8448-8452	than	
66-17	8453-8455	by	
66-18	8456-8459	the	
66-19	8460-8464	JDBC	
66-20	8465-8469	data	
66-21	8470-8476	source	
66-22	8476-8477	.	
66-23	8478-8479	/	
66-24	8479-8480	/	
66-25	8481-8485	Note	
66-26	8485-8486	:	
66-27	8487-8491	JDBC	
66-28	8492-8499	loading	
66-29	8500-8503	and	
66-30	8504-8510	saving	
66-31	8511-8514	can	
66-32	8515-8517	be	
66-33	8518-8526	achieved	
66-34	8527-8530	via	
66-35	8531-8537	either	
66-36	8538-8541	the	
66-37	8542-8546	load	
66-38	8546-8547	/	
66-39	8547-8551	save	
66-40	8552-8554	or	
66-41	8555-8559	jdbc	
66-42	8560-8567	methods	
66-43	8568-8569	/	
66-44	8569-8570	/	
66-45	8571-8578	Loading	
66-46	8579-8583	data	
66-47	8584-8588	from	
66-48	8589-8590	a	
66-49	8591-8595	JDBC	
66-50	8596-8602	source	
66-51	8603-8606	val	
66-52	8607-8613	jdbcDF	
66-53	8614-8615	=	
66-54	8616-8626	spark.read	
66-55	8627-8628	.	
66-56	8628-8634	format	
66-57	8634-8635	(	
66-58	8635-8636	"	
66-59	8636-8640	jdbc	
66-60	8640-8641	"	
66-61	8641-8642	)	
66-62	8643-8644	.	
66-63	8644-8650	option	
66-64	8650-8651	(	
66-65	8651-8652	"	
66-66	8652-8655	url	
66-67	8655-8656	"	
66-68	8656-8657	,	
66-69	8658-8659	"	
66-70	8659-8663	jdbc	
66-71	8663-8664	:	
66-72	8664-8674	postgresql	
66-73	8674-8675	:	
66-74	8675-8683	dbserver	
66-75	8683-8684	"	
66-76	8684-8685	)	
66-77	8686-8687	.	
66-78	8687-8693	option	
66-79	8693-8694	(	
66-80	8694-8695	"	
66-81	8695-8702	dbtable	
66-82	8702-8703	"	
66-83	8703-8704	,	
66-84	8705-8706	"	
66-85	8706-8722	schema.tablename	
66-86	8722-8723	"	
66-87	8723-8724	)	
66-88	8725-8726	.	
66-89	8726-8732	option	
66-90	8732-8733	(	
66-91	8733-8734	"	
66-92	8734-8738	user	
66-93	8738-8739	"	
66-94	8739-8740	,	
66-95	8741-8742	"	
66-96	8742-8750	username	
66-97	8750-8751	"	
66-98	8751-8752	)	
66-99	8753-8754	.	
66-100	8754-8760	option	
66-101	8760-8761	(	
66-102	8761-8762	"	
66-103	8762-8770	password	
66-104	8770-8771	"	
66-105	8771-8772	,	
66-106	8773-8774	"	
66-107	8774-8782	password	
66-108	8782-8783	"	
66-109	8783-8784	)	
66-110	8785-8786	.	
66-111	8786-8790	load	
66-112	8790-8791	(	
66-113	8791-8792	)	
66-114	8793-8796	val	
66-115	8797-8817	connectionProperties	
66-116	8818-8819	=	
66-117	8820-8823	new	
66-118	8824-8834	Properties	
66-119	8834-8835	(	
66-120	8835-8836	)	
66-121	8837-8861	connectionProperties.put	
66-122	8861-8862	(	
66-123	8862-8863	"	
66-124	8863-8867	user	
66-125	8867-8868	"	
66-126	8868-8869	,	
66-127	8870-8871	"	
66-128	8871-8879	username	
66-129	8879-8880	"	
66-130	8880-8881	)	
66-131	8882-8906	connectionProperties.put	
66-132	8906-8907	(	
66-133	8907-8908	"	
66-134	8908-8916	password	
66-135	8916-8917	"	
66-136	8917-8918	,	
66-137	8919-8920	"	
66-138	8920-8928	password	
66-139	8928-8929	"	
66-140	8929-8930	)	
66-141	8931-8934	val	
66-142	8935-8942	jdbcDF2	
66-143	8943-8944	=	
66-144	8945-8955	spark.read	
66-145	8956-8957	.	
66-146	8957-8961	jdbc	
66-147	8961-8962	(	
66-148	8962-8963	"	
66-149	8963-8967	jdbc	
66-150	8967-8968	:	
66-151	8968-8978	postgresql	
66-152	8978-8979	:	
66-153	8979-8987	dbserver	
66-154	8987-8988	"	
66-155	8988-8989	,	
66-156	8990-8991	"	
66-157	8991-9007	schema.tablename	
66-158	9007-9008	"	
66-159	9008-9009	,	
66-160	9010-9030	connectionProperties	
66-161	9030-9031	)	
66-162	9032-9033	/	
66-163	9033-9034	/	
66-164	9035-9045	Specifying	
66-165	9046-9049	the	
66-166	9050-9056	custom	
66-167	9057-9061	data	
66-168	9062-9067	types	
66-169	9068-9070	of	
66-170	9071-9074	the	
66-171	9075-9079	read	
66-172	9080-9086	schema	
66-173	9087-9111	connectionProperties.put	
66-174	9111-9112	(	
66-175	9112-9113	"	
66-176	9113-9125	customSchema	
66-177	9125-9126	"	
66-178	9126-9127	,	
66-179	9128-9129	"	
66-180	9129-9131	id	
66-181	9132-9139	DECIMAL	
66-182	9139-9140	(	
66-183	9140-9142	38	
66-184	9142-9143	,	
66-185	9144-9145	0	
66-186	9145-9146	)	
66-187	9146-9147	,	
66-188	9148-9152	name	
66-189	9153-9159	STRING	
66-190	9159-9160	"	
66-191	9160-9161	)	
66-192	9162-9165	val	
66-193	9166-9173	jdbcDF3	
66-194	9174-9175	=	
66-195	9176-9186	spark.read	
66-196	9187-9188	.	
66-197	9188-9192	jdbc	
66-198	9192-9193	(	
66-199	9193-9194	"	
66-200	9194-9198	jdbc	
66-201	9198-9199	:	
66-202	9199-9209	postgresql	
66-203	9209-9210	:	
66-204	9210-9218	dbserver	
66-205	9218-9219	"	
66-206	9219-9220	,	
66-207	9221-9222	"	
66-208	9222-9238	schema.tablename	
66-209	9238-9239	"	
66-210	9239-9240	,	
66-211	9241-9261	connectionProperties	
66-212	9261-9262	)	
66-213	9263-9264	/	
66-214	9264-9265	/	
66-215	9266-9272	Saving	
66-216	9273-9277	data	
66-217	9278-9280	to	
66-218	9281-9282	a	
66-219	9283-9287	JDBC	
66-220	9288-9294	source	
66-221	9295-9307	jdbcDF.write	
66-222	9308-9309	.	
66-223	9309-9315	format	
66-224	9315-9316	(	
66-225	9316-9317	"	
66-226	9317-9321	jdbc	
66-227	9321-9322	"	
66-228	9322-9323	)	
66-229	9324-9325	.	
66-230	9325-9331	option	
66-231	9331-9332	(	
66-232	9332-9333	"	
66-233	9333-9336	url	
66-234	9336-9337	"	
66-235	9337-9338	,	
66-236	9339-9340	"	
66-237	9340-9344	jdbc	
66-238	9344-9345	:	
66-239	9345-9355	postgresql	
66-240	9355-9356	:	
66-241	9356-9364	dbserver	
66-242	9364-9365	"	
66-243	9365-9366	)	
66-244	9367-9368	.	
66-245	9368-9374	option	
66-246	9374-9375	(	
66-247	9375-9376	"	
66-248	9376-9383	dbtable	
66-249	9383-9384	"	
66-250	9384-9385	,	
66-251	9386-9387	"	
66-252	9387-9403	schema.tablename	
66-253	9403-9404	"	
66-254	9404-9405	)	
66-255	9406-9407	.	
66-256	9407-9413	option	
66-257	9413-9414	(	
66-258	9414-9415	"	
66-259	9415-9419	user	
66-260	9419-9420	"	
66-261	9420-9421	,	
66-262	9422-9423	"	
66-263	9423-9431	username	
66-264	9431-9432	"	
66-265	9432-9433	)	
66-266	9434-9435	.	
66-267	9435-9441	option	
66-268	9441-9442	(	
66-269	9442-9443	"	
66-270	9443-9451	password	
66-271	9451-9452	"	
66-272	9452-9453	,	
66-273	9454-9455	"	
66-274	9455-9463	password	
66-275	9463-9464	"	
66-276	9464-9465	)	
66-277	9466-9467	.	
66-278	9467-9471	save	
66-279	9471-9472	(	
66-280	9472-9473	)	
66-281	9474-9481	jdbcDF2	
66-282	9481-9482	.	
66-283	9482-9487	write	
66-284	9488-9489	.	
66-285	9489-9493	jdbc	
66-286	9493-9494	(	
66-287	9494-9495	"	
66-288	9495-9499	jdbc	
66-289	9499-9500	:	
66-290	9500-9510	postgresql	
66-291	9510-9511	:	
66-292	9511-9519	dbserver	
66-293	9519-9520	"	
66-294	9520-9521	,	
66-295	9522-9523	"	
66-296	9523-9539	schema.tablename	
66-297	9539-9540	"	
66-298	9540-9541	,	
66-299	9542-9562	connectionProperties	
66-300	9562-9563	)	
66-301	9564-9565	/	
66-302	9565-9566	/	
66-303	9567-9577	Specifying	
66-304	9578-9584	create	
66-305	9585-9590	table	
66-306	9591-9597	column	
66-307	9598-9602	data	
66-308	9603-9608	types	
66-309	9609-9611	on	
66-310	9612-9617	write	
66-311	9618-9630	jdbcDF.write	
66-312	9631-9632	.	
66-313	9632-9638	option	
66-314	9638-9639	(	
66-315	9639-9640	"	
66-316	9640-9662	createTableColumnTypes	
66-317	9662-9663	"	
66-318	9663-9664	,	
66-319	9665-9666	"	
66-320	9666-9670	name	
66-321	9671-9675	CHAR	
66-322	9675-9676	(	
66-323	9676-9678	64	
66-324	9678-9679	)	
66-325	9679-9680	,	
66-326	9681-9689	comments	
66-327	9690-9697	VARCHAR	
66-328	9697-9698	(	
66-329	9698-9702	1024	
66-330	9702-9703	)	
66-331	9703-9704	"	
66-332	9704-9705	)	
66-333	9706-9707	.	
66-334	9707-9711	jdbc	
66-335	9711-9712	(	
66-336	9712-9713	"	
66-337	9713-9717	jdbc	
66-338	9717-9718	:	
66-339	9718-9728	postgresql	
66-340	9728-9729	:	
66-341	9729-9737	dbserver	
66-342	9737-9738	"	
66-343	9738-9739	,	
66-344	9740-9741	"	
66-345	9741-9757	schema.tablename	
66-346	9757-9758	"	
66-347	9758-9759	,	
66-348	9760-9780	connectionProperties	
66-349	9780-9781	)	
66-350	9782-9786	Find	
66-351	9787-9791	full	
66-352	9792-9799	example	
66-353	9800-9804	code	
66-354	9805-9807	at	
66-355	9808-9809	"	
66-356	9809-9817	examples	
66-357	9817-9818	/	
66-358	9818-9821	src	
66-359	9821-9822	/	
66-360	9822-9826	main	
66-361	9826-9827	/	
66-362	9827-9832	scala	
66-363	9832-9833	/	
66-364	9833-9836	org	
66-365	9836-9837	/	
66-366	9837-9843	apache	
66-367	9843-9844	/	
66-368	9844-9849	spark	
66-369	9849-9850	/	
66-370	9850-9858	examples	
66-371	9858-9859	/	
66-372	9859-9862	sql	
66-373	9862-9863	/	
66-374	9863-9889	SQLDataSourceExample.scala	
66-375	9889-9890	"	
66-376	9891-9893	in	
66-377	9894-9897	the	
66-378	9898-9903	Spark	
66-379	9904-9908	repo	
66-380	9908-9909	.	
66-381	9910-9911	/	
66-382	9911-9912	/	
66-383	9913-9917	Note	
66-384	9917-9918	:	
66-385	9919-9923	JDBC	
66-386	9924-9931	loading	
66-387	9932-9935	and	
66-388	9936-9942	saving	
66-389	9943-9946	can	
66-390	9947-9949	be	
66-391	9950-9958	achieved	
66-392	9959-9962	via	
66-393	9963-9969	either	
66-394	9970-9973	the	
66-395	9974-9978	load	
66-396	9978-9979	/	
66-397	9979-9983	save	
66-398	9984-9986	or	
66-399	9987-9991	jdbc	
66-400	9992-9999	methods	
66-401	10000-10001	/	
66-402	10001-10002	/	
66-403	10003-10010	Loading	
66-404	10011-10015	data	
66-405	10016-10020	from	
66-406	10021-10022	a	
66-407	10023-10027	JDBC	
66-408	10028-10034	source	
66-409	10035-10042	Dataset	
66-410	10042-10043	<	
66-411	10043-10046	Row	
66-412	10046-10047	>	
66-413	10048-10054	jdbcDF	
66-414	10055-10056	=	
66-415	10057-10067	spark.read	
66-416	10067-10068	(	
66-417	10068-10069	)	
66-418	10070-10071	.	
66-419	10071-10077	format	
66-420	10077-10078	(	
66-421	10078-10079	"	
66-422	10079-10083	jdbc	
66-423	10083-10084	"	
66-424	10084-10085	)	
66-425	10086-10087	.	
66-426	10087-10093	option	
66-427	10093-10094	(	
66-428	10094-10095	"	
66-429	10095-10098	url	
66-430	10098-10099	"	
66-431	10099-10100	,	
66-432	10101-10102	"	
66-433	10102-10106	jdbc	
66-434	10106-10107	:	
66-435	10107-10117	postgresql	
66-436	10117-10118	:	
66-437	10118-10126	dbserver	
66-438	10126-10127	"	
66-439	10127-10128	)	
66-440	10129-10130	.	
66-441	10130-10136	option	
66-442	10136-10137	(	
66-443	10137-10138	"	
66-444	10138-10145	dbtable	
66-445	10145-10146	"	
66-446	10146-10147	,	
66-447	10148-10149	"	
66-448	10149-10165	schema.tablename	
66-449	10165-10166	"	
66-450	10166-10167	)	
66-451	10168-10169	.	
66-452	10169-10175	option	
66-453	10175-10176	(	
66-454	10176-10177	"	
66-455	10177-10181	user	
66-456	10181-10182	"	
66-457	10182-10183	,	
66-458	10184-10185	"	
66-459	10185-10193	username	
66-460	10193-10194	"	
66-461	10194-10195	)	
66-462	10196-10197	.	
66-463	10197-10203	option	
66-464	10203-10204	(	
66-465	10204-10205	"	
66-466	10205-10213	password	
66-467	10213-10214	"	
66-468	10214-10215	,	
66-469	10216-10217	"	
66-470	10217-10225	password	
66-471	10225-10226	"	
66-472	10226-10227	)	
66-473	10228-10229	.	
66-474	10229-10233	load	
66-475	10233-10234	(	
66-476	10234-10235	)	
66-477	10235-10236	;	
66-478	10237-10247	Properties	
66-479	10248-10268	connectionProperties	
66-480	10269-10270	=	
66-481	10271-10274	new	
66-482	10275-10285	Properties	
66-483	10285-10286	(	
66-484	10286-10287	)	
66-485	10287-10288	;	
66-486	10289-10313	connectionProperties.put	
66-487	10313-10314	(	
66-488	10314-10315	"	
66-489	10315-10319	user	
66-490	10319-10320	"	
66-491	10320-10321	,	
66-492	10322-10323	"	
66-493	10323-10331	username	
66-494	10331-10332	"	
66-495	10332-10333	)	
66-496	10333-10334	;	
66-497	10335-10359	connectionProperties.put	
66-498	10359-10360	(	
66-499	10360-10361	"	
66-500	10361-10369	password	
66-501	10369-10370	"	
66-502	10370-10371	,	
66-503	10372-10373	"	
66-504	10373-10381	password	
66-505	10381-10382	"	
66-506	10382-10383	)	
66-507	10383-10384	;	
66-508	10385-10392	Dataset	
66-509	10392-10393	<	
66-510	10393-10396	Row	
66-511	10396-10397	>	
66-512	10398-10405	jdbcDF2	
66-513	10406-10407	=	
66-514	10408-10418	spark.read	
66-515	10418-10419	(	
66-516	10419-10420	)	
66-517	10421-10422	.	
66-518	10422-10426	jdbc	
66-519	10426-10427	(	
66-520	10427-10428	"	
66-521	10428-10432	jdbc	
66-522	10432-10433	:	
66-523	10433-10443	postgresql	
66-524	10443-10444	:	
66-525	10444-10452	dbserver	
66-526	10452-10453	"	
66-527	10453-10454	,	
66-528	10455-10456	"	
66-529	10456-10472	schema.tablename	
66-530	10472-10473	"	
66-531	10473-10474	,	
66-532	10475-10495	connectionProperties	
66-533	10495-10496	)	
66-534	10496-10497	;	
66-535	10498-10499	/	
66-536	10499-10500	/	
66-537	10501-10507	Saving	
66-538	10508-10512	data	
66-539	10513-10515	to	
66-540	10516-10517	a	
66-541	10518-10522	JDBC	
66-542	10523-10529	source	
66-543	10530-10542	jdbcDF.write	
66-544	10542-10543	(	
66-545	10543-10544	)	
66-546	10545-10546	.	
66-547	10546-10552	format	
66-548	10552-10553	(	
66-549	10553-10554	"	
66-550	10554-10558	jdbc	
66-551	10558-10559	"	
66-552	10559-10560	)	
66-553	10561-10562	.	
66-554	10562-10568	option	
66-555	10568-10569	(	
66-556	10569-10570	"	
66-557	10570-10573	url	
66-558	10573-10574	"	
66-559	10574-10575	,	
66-560	10576-10577	"	
66-561	10577-10581	jdbc	
66-562	10581-10582	:	
66-563	10582-10592	postgresql	
66-564	10592-10593	:	
66-565	10593-10601	dbserver	
66-566	10601-10602	"	
66-567	10602-10603	)	
66-568	10604-10605	.	
66-569	10605-10611	option	
66-570	10611-10612	(	
66-571	10612-10613	"	
66-572	10613-10620	dbtable	
66-573	10620-10621	"	
66-574	10621-10622	,	
66-575	10623-10624	"	
66-576	10624-10640	schema.tablename	
66-577	10640-10641	"	
66-578	10641-10642	)	
66-579	10643-10644	.	
66-580	10644-10650	option	
66-581	10650-10651	(	
66-582	10651-10652	"	
66-583	10652-10656	user	
66-584	10656-10657	"	
66-585	10657-10658	,	
66-586	10659-10660	"	
66-587	10660-10668	username	
66-588	10668-10669	"	
66-589	10669-10670	)	
66-590	10671-10672	.	
66-591	10672-10678	option	
66-592	10678-10679	(	
66-593	10679-10680	"	
66-594	10680-10688	password	
66-595	10688-10689	"	
66-596	10689-10690	,	
66-597	10691-10692	"	
66-598	10692-10700	password	
66-599	10700-10701	"	
66-600	10701-10702	)	
66-601	10703-10704	.	
66-602	10704-10708	save	
66-603	10708-10709	(	
66-604	10709-10710	)	
66-605	10710-10711	;	
66-606	10712-10719	jdbcDF2	
66-607	10719-10720	.	
66-608	10720-10725	write	
66-609	10725-10726	(	
66-610	10726-10727	)	
66-611	10728-10729	.	
66-612	10729-10733	jdbc	
66-613	10733-10734	(	
66-614	10734-10735	"	
66-615	10735-10739	jdbc	
66-616	10739-10740	:	
66-617	10740-10750	postgresql	
66-618	10750-10751	:	
66-619	10751-10759	dbserver	
66-620	10759-10760	"	
66-621	10760-10761	,	
66-622	10762-10763	"	
66-623	10763-10779	schema.tablename	
66-624	10779-10780	"	
66-625	10780-10781	,	
66-626	10782-10802	connectionProperties	
66-627	10802-10803	)	
66-628	10803-10804	;	
66-629	10805-10806	/	
66-630	10806-10807	/	
66-631	10808-10818	Specifying	
66-632	10819-10825	create	
66-633	10826-10831	table	
66-634	10832-10838	column	
66-635	10839-10843	data	
66-636	10844-10849	types	
66-637	10850-10852	on	
66-638	10853-10858	write	
66-639	10859-10871	jdbcDF.write	
66-640	10871-10872	(	
66-641	10872-10873	)	
66-642	10874-10875	.	
66-643	10875-10881	option	
66-644	10881-10882	(	
66-645	10882-10883	"	
66-646	10883-10905	createTableColumnTypes	
66-647	10905-10906	"	
66-648	10906-10907	,	
66-649	10908-10909	"	
66-650	10909-10913	name	
66-651	10914-10918	CHAR	
66-652	10918-10919	(	
66-653	10919-10921	64	
66-654	10921-10922	)	
66-655	10922-10923	,	
66-656	10924-10932	comments	
66-657	10933-10940	VARCHAR	
66-658	10940-10941	(	
66-659	10941-10945	1024	
66-660	10945-10946	)	
66-661	10946-10947	"	
66-662	10947-10948	)	
66-663	10949-10950	.	
66-664	10950-10954	jdbc	
66-665	10954-10955	(	
66-666	10955-10956	"	
66-667	10956-10960	jdbc	
66-668	10960-10961	:	
66-669	10961-10971	postgresql	
66-670	10971-10972	:	
66-671	10972-10980	dbserver	
66-672	10980-10981	"	
66-673	10981-10982	,	
66-674	10983-10984	"	
66-675	10984-11000	schema.tablename	
66-676	11000-11001	"	
66-677	11001-11002	,	
66-678	11003-11023	connectionProperties	
66-679	11023-11024	)	
66-680	11024-11025	;	
66-681	11026-11030	Find	
66-682	11031-11035	full	
66-683	11036-11043	example	
66-684	11044-11048	code	
66-685	11049-11051	at	
66-686	11052-11053	"	
66-687	11053-11061	examples	
66-688	11061-11062	/	
66-689	11062-11065	src	
66-690	11065-11066	/	
66-691	11066-11070	main	
66-692	11070-11071	/	
66-693	11071-11075	java	
66-694	11075-11076	/	
66-695	11076-11079	org	
66-696	11079-11080	/	
66-697	11080-11086	apache	
66-698	11086-11087	/	
66-699	11087-11092	spark	
66-700	11092-11093	/	
66-701	11093-11101	examples	
66-702	11101-11102	/	
66-703	11102-11105	sql	
66-704	11105-11106	/	
66-705	11106-11135	JavaSQLDataSourceExample.java	
66-706	11135-11136	"	
66-707	11137-11139	in	
66-708	11140-11143	the	
66-709	11144-11149	Spark	
66-710	11150-11154	repo	
66-711	11154-11155	.	
66-712	11156-11157	#	
66-713	11158-11162	Note	
66-714	11162-11163	:	
66-715	11164-11168	JDBC	
66-716	11169-11176	loading	
66-717	11177-11180	and	
66-718	11181-11187	saving	
66-719	11188-11191	can	
66-720	11192-11194	be	
66-721	11195-11203	achieved	
66-722	11204-11207	via	
66-723	11208-11214	either	
66-724	11215-11218	the	
66-725	11219-11223	load	
66-726	11223-11224	/	
66-727	11224-11228	save	
66-728	11229-11231	or	
66-729	11232-11236	jdbc	
66-730	11237-11244	methods	
66-731	11245-11246	#	
66-732	11247-11254	Loading	
66-733	11255-11259	data	
66-734	11260-11264	from	
66-735	11265-11266	a	
66-736	11267-11271	JDBC	
66-737	11272-11278	source	
66-738	11279-11285	jdbcDF	
66-739	11286-11287	=	
66-740	11288-11298	spark.read	
66-741	11299-11300	\	
66-742	11301-11302	.	
66-743	11302-11308	format	
66-744	11308-11309	(	
66-745	11309-11310	"	
66-746	11310-11314	jdbc	
66-747	11314-11315	"	
66-748	11315-11316	)	
66-749	11317-11318	\	
66-750	11319-11320	.	
66-751	11320-11326	option	
66-752	11326-11327	(	
66-753	11327-11328	"	
66-754	11328-11331	url	
66-755	11331-11332	"	
66-756	11332-11333	,	
66-757	11334-11335	"	
66-758	11335-11339	jdbc	
66-759	11339-11340	:	
66-760	11340-11350	postgresql	
66-761	11350-11351	:	
66-762	11351-11359	dbserver	
66-763	11359-11360	"	
66-764	11360-11361	)	
66-765	11362-11363	\	
66-766	11364-11365	.	
66-767	11365-11371	option	
66-768	11371-11372	(	
66-769	11372-11373	"	
66-770	11373-11380	dbtable	
66-771	11380-11381	"	
66-772	11381-11382	,	
66-773	11383-11384	"	
66-774	11384-11400	schema.tablename	
66-775	11400-11401	"	
66-776	11401-11402	)	
66-777	11403-11404	\	
66-778	11405-11406	.	
66-779	11406-11412	option	
66-780	11412-11413	(	
66-781	11413-11414	"	
66-782	11414-11418	user	
66-783	11418-11419	"	
66-784	11419-11420	,	
66-785	11421-11422	"	
66-786	11422-11430	username	
66-787	11430-11431	"	
66-788	11431-11432	)	
66-789	11433-11434	\	
66-790	11435-11436	.	
66-791	11436-11442	option	
66-792	11442-11443	(	
66-793	11443-11444	"	
66-794	11444-11452	password	
66-795	11452-11453	"	
66-796	11453-11454	,	
66-797	11455-11456	"	
66-798	11456-11464	password	
66-799	11464-11465	"	
66-800	11465-11466	)	
66-801	11467-11468	\	
66-802	11469-11470	.	
66-803	11470-11474	load	
66-804	11474-11475	(	
66-805	11475-11476	)	
66-806	11477-11484	jdbcDF2	
66-807	11485-11486	=	
66-808	11487-11497	spark.read	
66-809	11498-11499	\	
66-810	11500-11501	.	
66-811	11501-11505	jdbc	
66-812	11505-11506	(	
66-813	11506-11507	"	
66-814	11507-11511	jdbc	
66-815	11511-11512	:	
66-816	11512-11522	postgresql	
66-817	11522-11523	:	
66-818	11523-11531	dbserver	
66-819	11531-11532	"	
66-820	11532-11533	,	
66-821	11534-11535	"	
66-822	11535-11551	schema.tablename	
66-823	11551-11552	"	
66-824	11552-11553	,	
66-825	11554-11564	properties	
66-826	11564-11565	=	
66-827	11565-11566	{	
66-828	11566-11567	"	
66-829	11567-11571	user	
66-830	11571-11572	"	
66-831	11572-11573	:	
66-832	11574-11575	"	
66-833	11575-11583	username	
66-834	11583-11584	"	
66-835	11584-11585	,	
66-836	11586-11587	"	
66-837	11587-11595	password	
66-838	11595-11596	"	
66-839	11596-11597	:	
66-840	11598-11599	"	
66-841	11599-11607	password	
66-842	11607-11608	"	
66-843	11608-11609	}	
66-844	11609-11610	)	
66-845	11611-11612	#	
66-846	11613-11623	Specifying	
66-847	11624-11633	dataframe	
66-848	11634-11640	column	
66-849	11641-11645	data	
66-850	11646-11651	types	
66-851	11652-11654	on	
66-852	11655-11659	read	
66-853	11660-11667	jdbcDF3	
66-854	11668-11669	=	
66-855	11670-11680	spark.read	
66-856	11681-11682	\	
66-857	11683-11684	.	
66-858	11684-11690	format	
66-859	11690-11691	(	
66-860	11691-11692	"	
66-861	11692-11696	jdbc	
66-862	11696-11697	"	
66-863	11697-11698	)	
66-864	11699-11700	\	
66-865	11701-11702	.	
66-866	11702-11708	option	
66-867	11708-11709	(	
66-868	11709-11710	"	
66-869	11710-11713	url	
66-870	11713-11714	"	
66-871	11714-11715	,	
66-872	11716-11717	"	
66-873	11717-11721	jdbc	
66-874	11721-11722	:	
66-875	11722-11732	postgresql	
66-876	11732-11733	:	
66-877	11733-11741	dbserver	
66-878	11741-11742	"	
66-879	11742-11743	)	
66-880	11744-11745	\	
66-881	11746-11747	.	
66-882	11747-11753	option	
66-883	11753-11754	(	
66-884	11754-11755	"	
66-885	11755-11762	dbtable	
66-886	11762-11763	"	
66-887	11763-11764	,	
66-888	11765-11766	"	
66-889	11766-11782	schema.tablename	
66-890	11782-11783	"	
66-891	11783-11784	)	
66-892	11785-11786	\	
66-893	11787-11788	.	
66-894	11788-11794	option	
66-895	11794-11795	(	
66-896	11795-11796	"	
66-897	11796-11800	user	
66-898	11800-11801	"	
66-899	11801-11802	,	
66-900	11803-11804	"	
66-901	11804-11812	username	
66-902	11812-11813	"	
66-903	11813-11814	)	
66-904	11815-11816	\	
66-905	11817-11818	.	
66-906	11818-11824	option	
66-907	11824-11825	(	
66-908	11825-11826	"	
66-909	11826-11834	password	
66-910	11834-11835	"	
66-911	11835-11836	,	
66-912	11837-11838	"	
66-913	11838-11846	password	
66-914	11846-11847	"	
66-915	11847-11848	)	
66-916	11849-11850	\	
66-917	11851-11852	.	
66-918	11852-11858	option	
66-919	11858-11859	(	
66-920	11859-11860	"	
66-921	11860-11872	customSchema	
66-922	11872-11873	"	
66-923	11873-11874	,	
66-924	11875-11876	"	
66-925	11876-11878	id	
66-926	11879-11886	DECIMAL	
66-927	11886-11887	(	
66-928	11887-11889	38	
66-929	11889-11890	,	
66-930	11891-11892	0	
66-931	11892-11893	)	
66-932	11893-11894	,	
66-933	11895-11899	name	
66-934	11900-11906	STRING	
66-935	11906-11907	"	
66-936	11907-11908	)	
66-937	11909-11910	\	
66-938	11911-11912	.	
66-939	11912-11916	load	
66-940	11916-11917	(	
66-941	11917-11918	)	
66-942	11919-11920	#	
66-943	11921-11927	Saving	
66-944	11928-11932	data	
66-945	11933-11935	to	
66-946	11936-11937	a	
66-947	11938-11942	JDBC	
66-948	11943-11949	source	
66-949	11950-11962	jdbcDF.write	
66-950	11963-11964	\	
66-951	11965-11966	.	
66-952	11966-11972	format	
66-953	11972-11973	(	
66-954	11973-11974	"	
66-955	11974-11978	jdbc	
66-956	11978-11979	"	
66-957	11979-11980	)	
66-958	11981-11982	\	
66-959	11983-11984	.	
66-960	11984-11990	option	
66-961	11990-11991	(	
66-962	11991-11992	"	
66-963	11992-11995	url	
66-964	11995-11996	"	
66-965	11996-11997	,	
66-966	11998-11999	"	
66-967	11999-12003	jdbc	
66-968	12003-12004	:	
66-969	12004-12014	postgresql	
66-970	12014-12015	:	
66-971	12015-12023	dbserver	
66-972	12023-12024	"	
66-973	12024-12025	)	
66-974	12026-12027	\	
66-975	12028-12029	.	
66-976	12029-12035	option	
66-977	12035-12036	(	
66-978	12036-12037	"	
66-979	12037-12044	dbtable	
66-980	12044-12045	"	
66-981	12045-12046	,	
66-982	12047-12048	"	
66-983	12048-12064	schema.tablename	
66-984	12064-12065	"	
66-985	12065-12066	)	
66-986	12067-12068	\	
66-987	12069-12070	.	
66-988	12070-12076	option	
66-989	12076-12077	(	
66-990	12077-12078	"	
66-991	12078-12082	user	
66-992	12082-12083	"	
66-993	12083-12084	,	
66-994	12085-12086	"	
66-995	12086-12094	username	
66-996	12094-12095	"	
66-997	12095-12096	)	
66-998	12097-12098	\	
66-999	12099-12100	.	
66-1000	12100-12106	option	
66-1001	12106-12107	(	
66-1002	12107-12108	"	
66-1003	12108-12116	password	
66-1004	12116-12117	"	
66-1005	12117-12118	,	
66-1006	12119-12120	"	
66-1007	12120-12128	password	
66-1008	12128-12129	"	
66-1009	12129-12130	)	
66-1010	12131-12132	\	
66-1011	12133-12134	.	
66-1012	12134-12138	save	
66-1013	12138-12139	(	
66-1014	12139-12140	)	
66-1015	12141-12148	jdbcDF2	
66-1016	12148-12149	.	
66-1017	12149-12154	write	
66-1018	12155-12156	\	
66-1019	12157-12158	.	
66-1020	12158-12162	jdbc	
66-1021	12162-12163	(	
66-1022	12163-12164	"	
66-1023	12164-12168	jdbc	
66-1024	12168-12169	:	
66-1025	12169-12179	postgresql	
66-1026	12179-12180	:	
66-1027	12180-12188	dbserver	
66-1028	12188-12189	"	
66-1029	12189-12190	,	
66-1030	12191-12192	"	
66-1031	12192-12208	schema.tablename	
66-1032	12208-12209	"	
66-1033	12209-12210	,	
66-1034	12211-12221	properties	
66-1035	12221-12222	=	
66-1036	12222-12223	{	
66-1037	12223-12224	"	
66-1038	12224-12228	user	
66-1039	12228-12229	"	
66-1040	12229-12230	:	
66-1041	12231-12232	"	
66-1042	12232-12240	username	
66-1043	12240-12241	"	
66-1044	12241-12242	,	
66-1045	12243-12244	"	
66-1046	12244-12252	password	
66-1047	12252-12253	"	
66-1048	12253-12254	:	
66-1049	12255-12256	"	
66-1050	12256-12264	password	
66-1051	12264-12265	"	
66-1052	12265-12266	}	
66-1053	12266-12267	)	
66-1054	12268-12269	#	
66-1055	12270-12280	Specifying	
66-1056	12281-12287	create	
66-1057	12288-12293	table	
66-1058	12294-12300	column	
66-1059	12301-12305	data	
66-1060	12306-12311	types	
66-1061	12312-12314	on	
66-1062	12315-12320	write	
66-1063	12321-12333	jdbcDF.write	
66-1064	12334-12335	\	
66-1065	12336-12337	.	
66-1066	12337-12343	option	
66-1067	12343-12344	(	
66-1068	12344-12345	"	
66-1069	12345-12367	createTableColumnTypes	
66-1070	12367-12368	"	
66-1071	12368-12369	,	
66-1072	12370-12371	"	
66-1073	12371-12375	name	
66-1074	12376-12380	CHAR	
66-1075	12380-12381	(	
66-1076	12381-12383	64	
66-1077	12383-12384	)	
66-1078	12384-12385	,	
66-1079	12386-12394	comments	
66-1080	12395-12402	VARCHAR	
66-1081	12402-12403	(	
66-1082	12403-12407	1024	
66-1083	12407-12408	)	
66-1084	12408-12409	"	
66-1085	12409-12410	)	
66-1086	12411-12412	\	
66-1087	12413-12414	.	
66-1088	12414-12418	jdbc	
66-1089	12418-12419	(	
66-1090	12419-12420	"	
66-1091	12420-12424	jdbc	
66-1092	12424-12425	:	
66-1093	12425-12435	postgresql	
66-1094	12435-12436	:	
66-1095	12436-12444	dbserver	
66-1096	12444-12445	"	
66-1097	12445-12446	,	
66-1098	12447-12448	"	
66-1099	12448-12464	schema.tablename	
66-1100	12464-12465	"	
66-1101	12465-12466	,	
66-1102	12467-12477	properties	
66-1103	12477-12478	=	
66-1104	12478-12479	{	
66-1105	12479-12480	"	
66-1106	12480-12484	user	
66-1107	12484-12485	"	
66-1108	12485-12486	:	
66-1109	12487-12488	"	
66-1110	12488-12496	username	
66-1111	12496-12497	"	
66-1112	12497-12498	,	
66-1113	12499-12500	"	
66-1114	12500-12508	password	
66-1115	12508-12509	"	
66-1116	12509-12510	:	
66-1117	12511-12512	"	
66-1118	12512-12520	password	
66-1119	12520-12521	"	
66-1120	12521-12522	}	
66-1121	12522-12523	)	
66-1122	12524-12528	Find	
66-1123	12529-12533	full	
66-1124	12534-12541	example	
66-1125	12542-12546	code	
66-1126	12547-12549	at	
66-1127	12550-12551	"	
66-1128	12551-12559	examples	
66-1129	12559-12560	/	
66-1130	12560-12563	src	
66-1131	12563-12564	/	
66-1132	12564-12568	main	
66-1133	12568-12569	/	
66-1134	12569-12575	python	
66-1135	12575-12576	/	
66-1136	12576-12579	sql	
66-1137	12579-12580	/	
66-1138	12580-12593	datasource.py	
66-1139	12593-12594	"	
66-1140	12595-12597	in	
66-1141	12598-12601	the	
66-1142	12602-12607	Spark	
66-1143	12608-12612	repo	
66-1144	12612-12613	.	
66-1145	12614-12615	#	
66-1146	12616-12623	Loading	
66-1147	12624-12628	data	
66-1148	12629-12633	from	
66-1149	12634-12635	a	
66-1150	12636-12640	JDBC	
66-1151	12641-12647	source	
66-1152	12648-12650	df	
66-1153	12651-12652	<	
66-1154	12652-12653	-	
66-1155	12654-12663	read.jdbc	
66-1156	12663-12664	(	
66-1157	12664-12665	"	
66-1158	12665-12669	jdbc	
66-1159	12669-12670	:	
66-1160	12670-12680	postgresql	
66-1161	12680-12681	:	
66-1162	12681-12689	dbserver	
66-1163	12689-12690	"	
66-1164	12690-12691	,	
66-1165	12692-12693	"	
66-1166	12693-12709	schema.tablename	
66-1167	12709-12710	"	
66-1168	12710-12711	,	
66-1169	12712-12716	user	
66-1170	12717-12718	=	
66-1171	12719-12720	"	
66-1172	12720-12728	username	
66-1173	12728-12729	"	
66-1174	12729-12730	,	
66-1175	12731-12739	password	
66-1176	12740-12741	=	
66-1177	12742-12743	"	
66-1178	12743-12751	password	
66-1179	12751-12752	"	
66-1180	12752-12753	)	
66-1181	12754-12755	#	
66-1182	12756-12762	Saving	
66-1183	12763-12767	data	
66-1184	12768-12770	to	
66-1185	12771-12772	a	
66-1186	12773-12777	JDBC	
66-1187	12778-12784	source	
66-1188	12785-12795	write.jdbc	
66-1189	12795-12796	(	
66-1190	12796-12798	df	
66-1191	12798-12799	,	
66-1192	12800-12801	"	
66-1193	12801-12805	jdbc	
66-1194	12805-12806	:	
66-1195	12806-12816	postgresql	
66-1196	12816-12817	:	
66-1197	12817-12825	dbserver	
66-1198	12825-12826	"	
66-1199	12826-12827	,	
66-1200	12828-12829	"	
66-1201	12829-12845	schema.tablename	
66-1202	12845-12846	"	
66-1203	12846-12847	,	
66-1204	12848-12852	user	
66-1205	12853-12854	=	
66-1206	12855-12856	"	
66-1207	12856-12864	username	
66-1208	12864-12865	"	
66-1209	12865-12866	,	
66-1210	12867-12875	password	
66-1211	12876-12877	=	
66-1212	12878-12879	"	
66-1213	12879-12887	password	
66-1214	12887-12888	"	
66-1215	12888-12889	)	
66-1216	12890-12894	Find	
66-1217	12895-12899	full	
66-1218	12900-12907	example	
66-1219	12908-12912	code	
66-1220	12913-12915	at	
66-1221	12916-12917	"	
66-1222	12917-12925	examples	
66-1223	12925-12926	/	
66-1224	12926-12929	src	
66-1225	12929-12930	/	
66-1226	12930-12934	main	
66-1227	12934-12935	/	
66-1228	12935-12936	r	
66-1229	12936-12937	/	
66-1230	12937-12955	RSparkSQLExample.R	
66-1231	12955-12956	"	
66-1232	12957-12959	in	
66-1233	12960-12963	the	
66-1234	12964-12969	Spark	
66-1235	12970-12974	repo	
66-1236	12974-12975	.	

#Text=CREATE TEMPORARY VIEW jdbcTable
#Text=USING org.apache.spark.sql.jdbc
#Text=OPTIONS (
#Text=url "jdbc:postgresql:dbserver",
#Text=dbtable "schema.tablename",
#Text=user 'username',
#Text=password 'password'
#Text=INSERT INTO TABLE jdbcTable
#Text=SELECT * FROM resultTable
67-1	12976-12982	CREATE	
67-2	12983-12992	TEMPORARY	
67-3	12993-12997	VIEW	
67-4	12998-13007	jdbcTable	
67-5	13008-13013	USING	
67-6	13014-13039	org.apache.spark.sql.jdbc	
67-7	13040-13047	OPTIONS	
67-8	13048-13049	(	
67-9	13050-13053	url	
67-10	13054-13055	"	
67-11	13055-13059	jdbc	
67-12	13059-13060	:	
67-13	13060-13070	postgresql	
67-14	13070-13071	:	
67-15	13071-13079	dbserver	
67-16	13079-13080	"	
67-17	13080-13081	,	
67-18	13082-13089	dbtable	
67-19	13090-13091	"	
67-20	13091-13107	schema.tablename	
67-21	13107-13108	"	
67-22	13108-13109	,	
67-23	13110-13114	user	
67-24	13115-13116	'	
67-25	13116-13124	username	
67-26	13124-13125	'	
67-27	13125-13126	,	
67-28	13127-13135	password	
67-29	13136-13137	'	
67-30	13137-13145	password	
67-31	13145-13146	'	
67-32	13147-13153	INSERT	
67-33	13154-13158	INTO	
67-34	13159-13164	TABLE	
67-35	13165-13174	jdbcTable	
67-36	13175-13181	SELECT	
67-37	13182-13183	*	
67-38	13184-13188	FROM	
67-39	13189-13200	resultTable	
