#FORMAT=WebAnno TSV 3.3


#Text=Deep Dive into the New Features of Apache Spark 3.0 - Databricks SAIS 2020 Agenda Speakers Training Sponsors Special Events Women at Summit Financial Services Government and Education Healthcare and Life Sciences
1-1	0-4	Deep	
1-2	5-9	Dive	
1-3	10-14	into	
1-4	15-18	the	
1-5	19-22	New	
1-6	23-31	Features	
1-7	32-34	of	
1-8	35-41	Apache	
1-9	42-47	Spark	
1-10	48-51	3.0	
1-11	52-53	-	
1-12	54-64	Databricks	
1-13	65-69	SAIS	
1-14	70-74	2020	
1-15	75-81	Agenda	
1-16	82-90	Speakers	
1-17	91-99	Training	
1-18	100-108	Sponsors	
1-19	109-116	Special	
1-20	117-123	Events	
1-21	124-129	Women	
1-22	130-132	at	
1-23	133-139	Summit	
1-24	140-149	Financial	
1-25	150-158	Services	
1-26	159-169	Government	
1-27	170-173	and	
1-28	174-183	Education	
1-29	184-194	Healthcare	
1-30	195-198	and	
1-31	199-203	Life	
1-32	204-212	Sciences	

#Text=Media and Entertainment Retail and Consumer Goods Job Board FAQ WATCH KEYNOTES SAIS 2020 Agenda Speakers Training Sponsors Special Events Women at Summit Financial Services
2-1	213-218	Media	
2-2	219-222	and	
2-3	223-236	Entertainment	
2-4	237-243	Retail	
2-5	244-247	and	
2-6	248-256	Consumer	
2-7	257-262	Goods	
2-8	263-266	Job	
2-9	267-272	Board	
2-10	273-276	FAQ	
2-11	277-282	WATCH	
2-12	283-291	KEYNOTES	
2-13	292-296	SAIS	
2-14	297-301	2020	
2-15	302-308	Agenda	
2-16	309-317	Speakers	
2-17	318-326	Training	
2-18	327-335	Sponsors	
2-19	336-343	Special	
2-20	344-350	Events	
2-21	351-356	Women	
2-22	357-359	at	
2-23	360-366	Summit	
2-24	367-376	Financial	
2-25	377-385	Services	

#Text=Government and Education Healthcare and Life Sciences Media and Entertainment Retail and Consumer Goods Job Board FAQ WATCH KEYNOTES Deep Dive into the New Features of Apache Spark 3.0Download Slides
3-1	386-396	Government	
3-2	397-400	and	
3-3	401-410	Education	
3-4	411-421	Healthcare	
3-5	422-425	and	
3-6	426-430	Life	
3-7	431-439	Sciences	
3-8	440-445	Media	
3-9	446-449	and	
3-10	450-463	Entertainment	
3-11	464-470	Retail	
3-12	471-474	and	
3-13	475-483	Consumer	
3-14	484-489	Goods	
3-15	490-493	Job	
3-16	494-499	Board	
3-17	500-503	FAQ	
3-18	504-509	WATCH	
3-19	510-518	KEYNOTES	
3-20	519-523	Deep	
3-21	524-528	Dive	
3-22	529-533	into	
3-23	534-537	the	
3-24	538-541	New	
3-25	542-550	Features	
3-26	551-553	of	
3-27	554-560	Apache	
3-28	561-566	Spark	
3-29	567-578	3.0Download	
3-30	579-585	Slides	

#Text=Continuing with the objectives to make Spark faster, easier, and smarter, Apache Spark 3.0 extends its scope with more than 3000 resolved JIRAs. We will talk about the exciting new developments in the Spark 3.0 as well as some other major initiatives that are coming in the future. In this talk, we want to share with the community many of the more important changes with the examples and demos. The following features are covered: accelerator-aware scheduling, adaptive query execution, dynamic partition pruning, join hints, new query explain, better ANSI compliance, observable metrics, new UI for structured streaming, new UDAF and built-in functions, new unified interface for Pandas UDF, and various enhancements in the built-in data sources [e.g., parquet, ORC and JDBC].
4-1	586-596	Continuing	
4-2	597-601	with	
4-3	602-605	the	
4-4	606-616	objectives	
4-5	617-619	to	
4-6	620-624	make	
4-7	625-630	Spark	
4-8	631-637	faster	
4-9	637-638	,	
4-10	639-645	easier	
4-11	645-646	,	
4-12	647-650	and	
4-13	651-658	smarter	
4-14	658-659	,	
4-15	660-666	Apache	
4-16	667-672	Spark	
4-17	673-676	3.0	
4-18	677-684	extends	
4-19	685-688	its	
4-20	689-694	scope	
4-21	695-699	with	
4-22	700-704	more	
4-23	705-709	than	
4-24	710-714	3000	
4-25	715-723	resolved	
4-26	724-729	JIRAs	
4-27	729-730	.	
4-28	731-733	We	
4-29	734-738	will	
4-30	739-743	talk	
4-31	744-749	about	
4-32	750-753	the	
4-33	754-762	exciting	
4-34	763-766	new	
4-35	767-779	developments	
4-36	780-782	in	
4-37	783-786	the	
4-38	787-792	Spark	
4-39	793-796	3.0	
4-40	797-799	as	
4-41	800-804	well	
4-42	805-807	as	
4-43	808-812	some	
4-44	813-818	other	
4-45	819-824	major	
4-46	825-836	initiatives	
4-47	837-841	that	
4-48	842-845	are	
4-49	846-852	coming	
4-50	853-855	in	
4-51	856-859	the	
4-52	860-866	future	
4-53	866-867	.	
4-54	868-870	In	
4-55	871-875	this	
4-56	876-880	talk	
4-57	880-881	,	
4-58	882-884	we	
4-59	885-889	want	
4-60	890-892	to	
4-61	893-898	share	
4-62	899-903	with	
4-63	904-907	the	
4-64	908-917	community	
4-65	918-922	many	
4-66	923-925	of	
4-67	926-929	the	
4-68	930-934	more	
4-69	935-944	important	
4-70	945-952	changes	
4-71	953-957	with	
4-72	958-961	the	
4-73	962-970	examples	
4-74	971-974	and	
4-75	975-980	demos	
4-76	980-981	.	
4-77	982-985	The	
4-78	986-995	following	
4-79	996-1004	features	
4-80	1005-1008	are	
4-81	1009-1016	covered	
4-82	1016-1017	:	
4-83	1018-1035	accelerator-aware	
4-84	1036-1046	scheduling	
4-85	1046-1047	,	
4-86	1048-1056	adaptive	
4-87	1057-1062	query	
4-88	1063-1072	execution	
4-89	1072-1073	,	
4-90	1074-1081	dynamic	
4-91	1082-1091	partition	
4-92	1092-1099	pruning	
4-93	1099-1100	,	
4-94	1101-1105	join	
4-95	1106-1111	hints	
4-96	1111-1112	,	
4-97	1113-1116	new	
4-98	1117-1122	query	
4-99	1123-1130	explain	
4-100	1130-1131	,	
4-101	1132-1138	better	
4-102	1139-1143	ANSI	
4-103	1144-1154	compliance	
4-104	1154-1155	,	
4-105	1156-1166	observable	
4-106	1167-1174	metrics	
4-107	1174-1175	,	
4-108	1176-1179	new	
4-109	1180-1182	UI	
4-110	1183-1186	for	
4-111	1187-1197	structured	
4-112	1198-1207	streaming	
4-113	1207-1208	,	
4-114	1209-1212	new	
4-115	1213-1217	UDAF	
4-116	1218-1221	and	
4-117	1222-1230	built-in	
4-118	1231-1240	functions	
4-119	1240-1241	,	
4-120	1242-1245	new	
4-121	1246-1253	unified	
4-122	1254-1263	interface	
4-123	1264-1267	for	
4-124	1268-1274	Pandas	
4-125	1275-1278	UDF	
4-126	1278-1279	,	
4-127	1280-1283	and	
4-128	1284-1291	various	
4-129	1292-1304	enhancements	
4-130	1305-1307	in	
4-131	1308-1311	the	
4-132	1312-1320	built-in	
4-133	1321-1325	data	
4-134	1326-1333	sources	
4-135	1334-1335	[	
4-136	1335-1338	e.g	
4-137	1338-1339	.	
4-138	1339-1340	,	
4-139	1341-1348	parquet	
4-140	1348-1349	,	
4-141	1350-1353	ORC	
4-142	1354-1357	and	
4-143	1358-1362	JDBC	
4-144	1362-1363	]	
4-145	1363-1364	.	

#Text=Watch more Spark + AI sessions here Try Databricks for free Video Transcript About us and our Open Source contributions Hello, everyone. Today, Wenchen and I are glad to share with you the latest updates about the upcoming release, Spark 3.0. So I’m Xiao Li. Both Wenchen and I are
5-1	1365-1370	Watch	
5-2	1371-1375	more	
5-3	1376-1381	Spark	
5-4	1382-1383	+	
5-5	1384-1386	AI	
5-6	1387-1395	sessions	
5-7	1396-1400	here	
5-8	1401-1404	Try	
5-9	1405-1415	Databricks	
5-10	1416-1419	for	
5-11	1420-1424	free	
5-12	1425-1430	Video	
5-13	1431-1441	Transcript	
5-14	1442-1447	About	
5-15	1448-1450	us	
5-16	1451-1454	and	
5-17	1455-1458	our	
5-18	1459-1463	Open	
5-19	1464-1470	Source	
5-20	1471-1484	contributions	
5-21	1485-1490	Hello	
5-22	1490-1491	,	
5-23	1492-1500	everyone	
5-24	1500-1501	.	
5-25	1502-1507	Today	
5-26	1507-1508	,	
5-27	1509-1516	Wenchen	
5-28	1517-1520	and	
5-29	1521-1522	I	
5-30	1523-1526	are	
5-31	1527-1531	glad	
5-32	1532-1534	to	
5-33	1535-1540	share	
5-34	1541-1545	with	
5-35	1546-1549	you	
5-36	1550-1553	the	
5-37	1554-1560	latest	
5-38	1561-1568	updates	
5-39	1569-1574	about	
5-40	1575-1578	the	
5-41	1579-1587	upcoming	
5-42	1588-1595	release	
5-43	1595-1596	,	
5-44	1597-1602	Spark	
5-45	1603-1606	3.0	
5-46	1606-1607	.	
5-47	1608-1610	So	
5-48	1611-1612	I	
5-49	1612-1613	’	
5-50	1613-1614	m	
5-51	1615-1619	Xiao	
5-52	1620-1622	Li	
5-53	1622-1623	.	
5-54	1624-1628	Both	
5-55	1629-1636	Wenchen	
5-56	1637-1640	and	
5-57	1641-1642	I	
5-58	1643-1646	are	

#Text=working for Databricks. We focus on the open-source developments. Both of us are Spark committers and PMC members. About Databricks Databricks provides a unified data analytics platform to accelerate your data-driven animation. We are a global company with more than 5,000 customers across various industries, and we have more than 450 partners worldwide. And most of you might
6-1	1647-1654	working	
6-2	1655-1658	for	
6-3	1659-1669	Databricks	
6-4	1669-1670	.	
6-5	1671-1673	We	
6-6	1674-1679	focus	
6-7	1680-1682	on	
6-8	1683-1686	the	
6-9	1687-1698	open-source	
6-10	1699-1711	developments	
6-11	1711-1712	.	
6-12	1713-1717	Both	
6-13	1718-1720	of	
6-14	1721-1723	us	
6-15	1724-1727	are	
6-16	1728-1733	Spark	
6-17	1734-1744	committers	
6-18	1745-1748	and	
6-19	1749-1752	PMC	
6-20	1753-1760	members	
6-21	1760-1761	.	
6-22	1762-1767	About	
6-23	1768-1778	Databricks	
6-24	1779-1789	Databricks	
6-25	1790-1798	provides	
6-26	1799-1800	a	
6-27	1801-1808	unified	
6-28	1809-1813	data	
6-29	1814-1823	analytics	
6-30	1824-1832	platform	
6-31	1833-1835	to	
6-32	1836-1846	accelerate	
6-33	1847-1851	your	
6-34	1852-1863	data-driven	
6-35	1864-1873	animation	
6-36	1873-1874	.	
6-37	1875-1877	We	
6-38	1878-1881	are	
6-39	1882-1883	a	
6-40	1884-1890	global	
6-41	1891-1898	company	
6-42	1899-1903	with	
6-43	1904-1908	more	
6-44	1909-1913	than	
6-45	1914-1919	5,000	
6-46	1920-1929	customers	
6-47	1930-1936	across	
6-48	1937-1944	various	
6-49	1945-1955	industries	
6-50	1955-1956	,	
6-51	1957-1960	and	
6-52	1961-1963	we	
6-53	1964-1968	have	
6-54	1969-1973	more	
6-55	1974-1978	than	
6-56	1979-1982	450	
6-57	1983-1991	partners	
6-58	1992-2001	worldwide	
6-59	2001-2002	.	
6-60	2003-2006	And	
6-61	2007-2011	most	
6-62	2012-2014	of	
6-63	2015-2018	you	
6-64	2019-2024	might	

#Text=have heard of Databricks as original creator of Spark, Delta Lake, MLflow, and Koalas. These are open-source projects that are leading innovation in the fields of data and machine learnings. We continue to contribute and nurture this open-source community. Spark 3.0 Highlights In Spark 3.0, the whole community resolved more than 3,400 JIRAs. Spark SQL
7-1	2025-2029	have	
7-2	2030-2035	heard	
7-3	2036-2038	of	
7-4	2039-2049	Databricks	
7-5	2050-2052	as	
7-6	2053-2061	original	
7-7	2062-2069	creator	
7-8	2070-2072	of	
7-9	2073-2078	Spark	
7-10	2078-2079	,	
7-11	2080-2085	Delta	
7-12	2086-2090	Lake	
7-13	2090-2091	,	
7-14	2092-2098	MLflow	
7-15	2098-2099	,	
7-16	2100-2103	and	
7-17	2104-2110	Koalas	
7-18	2110-2111	.	
7-19	2112-2117	These	
7-20	2118-2121	are	
7-21	2122-2133	open-source	
7-22	2134-2142	projects	
7-23	2143-2147	that	
7-24	2148-2151	are	
7-25	2152-2159	leading	
7-26	2160-2170	innovation	
7-27	2171-2173	in	
7-28	2174-2177	the	
7-29	2178-2184	fields	
7-30	2185-2187	of	
7-31	2188-2192	data	
7-32	2193-2196	and	
7-33	2197-2204	machine	
7-34	2205-2214	learnings	
7-35	2214-2215	.	
7-36	2216-2218	We	
7-37	2219-2227	continue	
7-38	2228-2230	to	
7-39	2231-2241	contribute	
7-40	2242-2245	and	
7-41	2246-2253	nurture	
7-42	2254-2258	this	
7-43	2259-2270	open-source	
7-44	2271-2280	community	
7-45	2280-2281	.	
7-46	2282-2287	Spark	
7-47	2288-2291	3.0	
7-48	2292-2302	Highlights	
7-49	2303-2305	In	
7-50	2306-2311	Spark	
7-51	2312-2315	3.0	
7-52	2315-2316	,	
7-53	2317-2320	the	
7-54	2321-2326	whole	
7-55	2327-2336	community	
7-56	2337-2345	resolved	
7-57	2346-2350	more	
7-58	2351-2355	than	
7-59	2356-2361	3,400	
7-60	2362-2367	JIRAs	
7-61	2367-2368	.	
7-62	2369-2374	Spark	
7-63	2375-2378	SQL	

#Text=and the Core are the new core module, and all the other components are built on Spark SQL and the Core. Today, the pull requests for Spark SQL and the core constitute more than 60% of Spark 3.0. In the last few releases, the percentage keeps going up. Today, we will focus on the key features in both Spark SQL and the Core.
8-1	2379-2382	and	
8-2	2383-2386	the	
8-3	2387-2391	Core	
8-4	2392-2395	are	
8-5	2396-2399	the	
8-6	2400-2403	new	
8-7	2404-2408	core	
8-8	2409-2415	module	
8-9	2415-2416	,	
8-10	2417-2420	and	
8-11	2421-2424	all	
8-12	2425-2428	the	
8-13	2429-2434	other	
8-14	2435-2445	components	
8-15	2446-2449	are	
8-16	2450-2455	built	
8-17	2456-2458	on	
8-18	2459-2464	Spark	
8-19	2465-2468	SQL	
8-20	2469-2472	and	
8-21	2473-2476	the	
8-22	2477-2481	Core	
8-23	2481-2482	.	
8-24	2483-2488	Today	
8-25	2488-2489	,	
8-26	2490-2493	the	
8-27	2494-2498	pull	
8-28	2499-2507	requests	
8-29	2508-2511	for	
8-30	2512-2517	Spark	
8-31	2518-2521	SQL	
8-32	2522-2525	and	
8-33	2526-2529	the	
8-34	2530-2534	core	
8-35	2535-2545	constitute	
8-36	2546-2550	more	
8-37	2551-2555	than	
8-38	2556-2559	60%	
8-39	2560-2562	of	
8-40	2563-2568	Spark	
8-41	2569-2572	3.0	
8-42	2572-2573	.	
8-43	2574-2576	In	
8-44	2577-2580	the	
8-45	2581-2585	last	
8-46	2586-2589	few	
8-47	2590-2598	releases	
8-48	2598-2599	,	
8-49	2600-2603	the	
8-50	2604-2614	percentage	
8-51	2615-2620	keeps	
8-52	2621-2626	going	
8-53	2627-2629	up	
8-54	2629-2630	.	
8-55	2631-2636	Today	
8-56	2636-2637	,	
8-57	2638-2640	we	
8-58	2641-2645	will	
8-59	2646-2651	focus	
8-60	2652-2654	on	
8-61	2655-2658	the	
8-62	2659-2662	key	
8-63	2663-2671	features	
8-64	2672-2674	in	
8-65	2675-2679	both	
8-66	2680-2685	Spark	
8-67	2686-2689	SQL	
8-68	2690-2693	and	
8-69	2694-2697	the	
8-70	2698-2702	Core	
8-71	2702-2703	.	

#Text=This release delivered many new capabilities, performance gains, and extended compatibility for the Spark ecosystem. This is a combination of the tremendous contributions from the open-source community. It is impossible to discuss the new features within 16 minutes. We resolved more than 3,400 JIRAs. Even in
9-1	2704-2708	This	
9-2	2709-2716	release	
9-3	2717-2726	delivered	
9-4	2727-2731	many	
9-5	2732-2735	new	
9-6	2736-2748	capabilities	
9-7	2748-2749	,	
9-8	2750-2761	performance	
9-9	2762-2767	gains	
9-10	2767-2768	,	
9-11	2769-2772	and	
9-12	2773-2781	extended	
9-13	2782-2795	compatibility	
9-14	2796-2799	for	
9-15	2800-2803	the	
9-16	2804-2809	Spark	
9-17	2810-2819	ecosystem	
9-18	2819-2820	.	
9-19	2821-2825	This	
9-20	2826-2828	is	
9-21	2829-2830	a	
9-22	2831-2842	combination	
9-23	2843-2845	of	
9-24	2846-2849	the	
9-25	2850-2860	tremendous	
9-26	2861-2874	contributions	
9-27	2875-2879	from	
9-28	2880-2883	the	
9-29	2884-2895	open-source	
9-30	2896-2905	community	
9-31	2905-2906	.	
9-32	2907-2909	It	
9-33	2910-2912	is	
9-34	2913-2923	impossible	
9-35	2924-2926	to	
9-36	2927-2934	discuss	
9-37	2935-2938	the	
9-38	2939-2942	new	
9-39	2943-2951	features	
9-40	2952-2958	within	
9-41	2959-2961	16	
9-42	2962-2969	minutes	
9-43	2969-2970	.	
9-44	2971-2973	We	
9-45	2974-2982	resolved	
9-46	2983-2987	more	
9-47	2988-2992	than	
9-48	2993-2998	3,400	
9-49	2999-3004	JIRAs	
9-50	3004-3005	.	
9-51	3006-3010	Even	
9-52	3011-3013	in	

#Text=this light, I did my best, but I only can put 24 new Spark 3.0 features. Today, we would like to present some of them. First, let us talk about the performance-related features. Spark 3.0 Enhanced Performance High performance is one of the major advantages when people select Spark as their computation engine. This release keeps enhancing the performance
10-1	3014-3018	this	
10-2	3019-3024	light	
10-3	3024-3025	,	
10-4	3026-3027	I	
10-5	3028-3031	did	
10-6	3032-3034	my	
10-7	3035-3039	best	
10-8	3039-3040	,	
10-9	3041-3044	but	
10-10	3045-3046	I	
10-11	3047-3051	only	
10-12	3052-3055	can	
10-13	3056-3059	put	
10-14	3060-3062	24	
10-15	3063-3066	new	
10-16	3067-3072	Spark	
10-17	3073-3076	3.0	
10-18	3077-3085	features	
10-19	3085-3086	.	
10-20	3087-3092	Today	
10-21	3092-3093	,	
10-22	3094-3096	we	
10-23	3097-3102	would	
10-24	3103-3107	like	
10-25	3108-3110	to	
10-26	3111-3118	present	
10-27	3119-3123	some	
10-28	3124-3126	of	
10-29	3127-3131	them	
10-30	3131-3132	.	
10-31	3133-3138	First	
10-32	3138-3139	,	
10-33	3140-3143	let	
10-34	3144-3146	us	
10-35	3147-3151	talk	
10-36	3152-3157	about	
10-37	3158-3161	the	
10-38	3162-3181	performance-related	
10-39	3182-3190	features	
10-40	3190-3191	.	
10-41	3192-3197	Spark	
10-42	3198-3201	3.0	
10-43	3202-3210	Enhanced	
10-44	3211-3222	Performance	
10-45	3223-3227	High	
10-46	3228-3239	performance	
10-47	3240-3242	is	
10-48	3243-3246	one	
10-49	3247-3249	of	
10-50	3250-3253	the	
10-51	3254-3259	major	
10-52	3260-3270	advantages	
10-53	3271-3275	when	
10-54	3276-3282	people	
10-55	3283-3289	select	
10-56	3290-3295	Spark	
10-57	3296-3298	as	
10-58	3299-3304	their	
10-59	3305-3316	computation	
10-60	3317-3323	engine	
10-61	3323-3324	.	
10-62	3325-3329	This	
10-63	3330-3337	release	
10-64	3338-3343	keeps	
10-65	3344-3353	enhancing	
10-66	3354-3357	the	
10-67	3358-3369	performance	

#Text=for interactive, batch, streaming, and [inaudible] workloads. Here, I will first cover four of the performance features in SQL query compilers. Later, Wenchen will talk about the performance enhancement for building data sources. The four major features in query compilers include a new framework for adaptive query execution and a new runtime filtering for dynamic partition
11-1	3370-3373	for	
11-2	3374-3385	interactive	
11-3	3385-3386	,	
11-4	3387-3392	batch	
11-5	3392-3393	,	
11-6	3394-3403	streaming	
11-7	3403-3404	,	
11-8	3405-3408	and	
11-9	3409-3410	[	
11-10	3410-3419	inaudible	
11-11	3419-3420	]	
11-12	3421-3430	workloads	
11-13	3430-3431	.	
11-14	3432-3436	Here	
11-15	3436-3437	,	
11-16	3438-3439	I	
11-17	3440-3444	will	
11-18	3445-3450	first	
11-19	3451-3456	cover	
11-20	3457-3461	four	
11-21	3462-3464	of	
11-22	3465-3468	the	
11-23	3469-3480	performance	
11-24	3481-3489	features	
11-25	3490-3492	in	
11-26	3493-3496	SQL	
11-27	3497-3502	query	
11-28	3503-3512	compilers	
11-29	3512-3513	.	
11-30	3514-3519	Later	
11-31	3519-3520	,	
11-32	3521-3528	Wenchen	
11-33	3529-3533	will	
11-34	3534-3538	talk	
11-35	3539-3544	about	
11-36	3545-3548	the	
11-37	3549-3560	performance	
11-38	3561-3572	enhancement	
11-39	3573-3576	for	
11-40	3577-3585	building	
11-41	3586-3590	data	
11-42	3591-3598	sources	
11-43	3598-3599	.	
11-44	3600-3603	The	
11-45	3604-3608	four	
11-46	3609-3614	major	
11-47	3615-3623	features	
11-48	3624-3626	in	
11-49	3627-3632	query	
11-50	3633-3642	compilers	
11-51	3643-3650	include	
11-52	3651-3652	a	
11-53	3653-3656	new	
11-54	3657-3666	framework	
11-55	3667-3670	for	
11-56	3671-3679	adaptive	
11-57	3680-3685	query	
11-58	3686-3695	execution	
11-59	3696-3699	and	
11-60	3700-3701	a	
11-61	3702-3705	new	
11-62	3706-3713	runtime	
11-63	3714-3723	filtering	
11-64	3724-3727	for	
11-65	3728-3735	dynamic	
11-66	3736-3745	partition	

#Text=pruning. And also, we greatly reduce the overhead of our query compiler by more than a half, especially on the optimizer overhead and the SQL cache synchronization. Supporting a complete set of join hints is another useful features many people are waiting for. Adaptive query execution was available at the previous releases. However, the previous framework has a few
12-1	3746-3753	pruning	
12-2	3753-3754	.	
12-3	3755-3758	And	
12-4	3759-3763	also	
12-5	3763-3764	,	
12-6	3765-3767	we	
12-7	3768-3775	greatly	
12-8	3776-3782	reduce	
12-9	3783-3786	the	
12-10	3787-3795	overhead	
12-11	3796-3798	of	
12-12	3799-3802	our	
12-13	3803-3808	query	
12-14	3809-3817	compiler	
12-15	3818-3820	by	
12-16	3821-3825	more	
12-17	3826-3830	than	
12-18	3831-3832	a	
12-19	3833-3837	half	
12-20	3837-3838	,	
12-21	3839-3849	especially	
12-22	3850-3852	on	
12-23	3853-3856	the	
12-24	3857-3866	optimizer	
12-25	3867-3875	overhead	
12-26	3876-3879	and	
12-27	3880-3883	the	
12-28	3884-3887	SQL	
12-29	3888-3893	cache	
12-30	3894-3909	synchronization	
12-31	3909-3910	.	
12-32	3911-3921	Supporting	
12-33	3922-3923	a	
12-34	3924-3932	complete	
12-35	3933-3936	set	
12-36	3937-3939	of	
12-37	3940-3944	join	
12-38	3945-3950	hints	
12-39	3951-3953	is	
12-40	3954-3961	another	
12-41	3962-3968	useful	
12-42	3969-3977	features	
12-43	3978-3982	many	
12-44	3983-3989	people	
12-45	3990-3993	are	
12-46	3994-4001	waiting	
12-47	4002-4005	for	
12-48	4005-4006	.	
12-49	4007-4015	Adaptive	
12-50	4016-4021	query	
12-51	4022-4031	execution	
12-52	4032-4035	was	
12-53	4036-4045	available	
12-54	4046-4048	at	
12-55	4049-4052	the	
12-56	4053-4061	previous	
12-57	4062-4070	releases	
12-58	4070-4071	.	
12-59	4072-4079	However	
12-60	4079-4080	,	
12-61	4081-4084	the	
12-62	4085-4093	previous	
12-63	4094-4103	framework	
12-64	4104-4107	has	
12-65	4108-4109	a	
12-66	4110-4113	few	

#Text=major drawbacks. Very few companies are using it in the production systems. In this release, Databricks and the [inaudible] work together and redesigned the new framework and resolved all the known issues. Let us talk about what we did in this release. Spark Catalyst Optimizer Michael Armbrust.
13-1	4114-4119	major	
13-2	4120-4129	drawbacks	
13-3	4129-4130	.	
13-4	4131-4135	Very	
13-5	4136-4139	few	
13-6	4140-4149	companies	
13-7	4150-4153	are	
13-8	4154-4159	using	
13-9	4160-4162	it	
13-10	4163-4165	in	
13-11	4166-4169	the	
13-12	4170-4180	production	
13-13	4181-4188	systems	
13-14	4188-4189	.	
13-15	4190-4192	In	
13-16	4193-4197	this	
13-17	4198-4205	release	
13-18	4205-4206	,	
13-19	4207-4217	Databricks	
13-20	4218-4221	and	
13-21	4222-4225	the	
13-22	4226-4227	[	
13-23	4227-4236	inaudible	
13-24	4236-4237	]	
13-25	4238-4242	work	
13-26	4243-4251	together	
13-27	4252-4255	and	
13-28	4256-4266	redesigned	
13-29	4267-4270	the	
13-30	4271-4274	new	
13-31	4275-4284	framework	
13-32	4285-4288	and	
13-33	4289-4297	resolved	
13-34	4298-4301	all	
13-35	4302-4305	the	
13-36	4306-4311	known	
13-37	4312-4318	issues	
13-38	4318-4319	.	
13-39	4320-4323	Let	
13-40	4324-4326	us	
13-41	4327-4331	talk	
13-42	4332-4337	about	
13-43	4338-4342	what	
13-44	4343-4345	we	
13-45	4346-4349	did	
13-46	4350-4352	in	
13-47	4353-4357	this	
13-48	4358-4365	release	
13-49	4365-4366	.	
13-50	4367-4372	Spark	
13-51	4373-4381	Catalyst	
13-52	4382-4391	Optimizer	
13-53	4392-4399	Michael	
13-54	4400-4408	Armbrust	
13-55	4408-4409	.	

#Text=is the creator of Spark SQL and also, Catalyst Optimizer. In the initial release of Spark SQL, all the optimizer rules are heuristic-based. To generate good query plans, the query optimizer needs to understand the data characteristics. Then in Spark 2.x, we introduced a cost-based
14-1	4410-4412	is	
14-2	4413-4416	the	
14-3	4417-4424	creator	
14-4	4425-4427	of	
14-5	4428-4433	Spark	
14-6	4434-4437	SQL	
14-7	4438-4441	and	
14-8	4442-4446	also	
14-9	4446-4447	,	
14-10	4448-4456	Catalyst	
14-11	4457-4466	Optimizer	
14-12	4466-4467	.	
14-13	4468-4470	In	
14-14	4471-4474	the	
14-15	4475-4482	initial	
14-16	4483-4490	release	
14-17	4491-4493	of	
14-18	4494-4499	Spark	
14-19	4500-4503	SQL	
14-20	4503-4504	,	
14-21	4505-4508	all	
14-22	4509-4512	the	
14-23	4513-4522	optimizer	
14-24	4523-4528	rules	
14-25	4529-4532	are	
14-26	4533-4548	heuristic-based	
14-27	4548-4549	.	
14-28	4550-4552	To	
14-29	4553-4561	generate	
14-30	4562-4566	good	
14-31	4567-4572	query	
14-32	4573-4578	plans	
14-33	4578-4579	,	
14-34	4580-4583	the	
14-35	4584-4589	query	
14-36	4590-4599	optimizer	
14-37	4600-4605	needs	
14-38	4606-4608	to	
14-39	4609-4619	understand	
14-40	4620-4623	the	
14-41	4624-4628	data	
14-42	4629-4644	characteristics	
14-43	4644-4645	.	
14-44	4646-4650	Then	
14-45	4651-4653	in	
14-46	4654-4659	Spark	
14-47	4660-4661	2	
14-48	4661-4662	.	
14-49	4662-4663	x	
14-50	4663-4664	,	
14-51	4665-4667	we	
14-52	4668-4678	introduced	
14-53	4679-4680	a	
14-54	4681-4691	cost-based	

#Text=optimizer. However, in most cases, data statistics are commonly absent, especially when statistics collection is even more expensive than the data processing in the [search?]. Even if the statistics are available, the statistics are likely out of date. Based on the storage and the compute separation in Spark, the characteristics of data [rival?] is unpredictable. The costs are often misestimated due to the different deployment environment and the black box user-defined
15-1	4692-4701	optimizer	
15-2	4701-4702	.	
15-3	4703-4710	However	
15-4	4710-4711	,	
15-5	4712-4714	in	
15-6	4715-4719	most	
15-7	4720-4725	cases	
15-8	4725-4726	,	
15-9	4727-4731	data	
15-10	4732-4742	statistics	
15-11	4743-4746	are	
15-12	4747-4755	commonly	
15-13	4756-4762	absent	
15-14	4762-4763	,	
15-15	4764-4774	especially	
15-16	4775-4779	when	
15-17	4780-4790	statistics	
15-18	4791-4801	collection	
15-19	4802-4804	is	
15-20	4805-4809	even	
15-21	4810-4814	more	
15-22	4815-4824	expensive	
15-23	4825-4829	than	
15-24	4830-4833	the	
15-25	4834-4838	data	
15-26	4839-4849	processing	
15-27	4850-4852	in	
15-28	4853-4856	the	
15-29	4857-4858	[	
15-30	4858-4864	search	
15-31	4864-4865	?	
15-32	4865-4866	]	
15-33	4866-4867	.	
15-34	4868-4872	Even	
15-35	4873-4875	if	
15-36	4876-4879	the	
15-37	4880-4890	statistics	
15-38	4891-4894	are	
15-39	4895-4904	available	
15-40	4904-4905	,	
15-41	4906-4909	the	
15-42	4910-4920	statistics	
15-43	4921-4924	are	
15-44	4925-4931	likely	
15-45	4932-4935	out	
15-46	4936-4938	of	
15-47	4939-4943	date	
15-48	4943-4944	.	
15-49	4945-4950	Based	
15-50	4951-4953	on	
15-51	4954-4957	the	
15-52	4958-4965	storage	
15-53	4966-4969	and	
15-54	4970-4973	the	
15-55	4974-4981	compute	
15-56	4982-4992	separation	
15-57	4993-4995	in	
15-58	4996-5001	Spark	
15-59	5001-5002	,	
15-60	5003-5006	the	
15-61	5007-5022	characteristics	
15-62	5023-5025	of	
15-63	5026-5030	data	
15-64	5031-5032	[	
15-65	5032-5037	rival	
15-66	5037-5038	?	
15-67	5038-5039	]	
15-68	5040-5042	is	
15-69	5043-5056	unpredictable	
15-70	5056-5057	.	
15-71	5058-5061	The	
15-72	5062-5067	costs	
15-73	5068-5071	are	
15-74	5072-5077	often	
15-75	5078-5090	misestimated	
15-76	5091-5094	due	
15-77	5095-5097	to	
15-78	5098-5101	the	
15-79	5102-5111	different	
15-80	5112-5122	deployment	
15-81	5123-5134	environment	
15-82	5135-5138	and	
15-83	5139-5142	the	
15-84	5143-5148	black	
15-85	5149-5152	box	
15-86	5153-5165	user-defined	

#Text=functions. We are unable to estimate the cost for the UDF. Basically, in many cases, Spark optimizer is enabled to generate the best plan due to this limitation. Adaptive Query Execution For all these reasons, runtime adaptivity becomes more critical for Spark than the traditional systems. So this release introduced a new adaptive query execution framework called
16-1	5166-5175	functions	
16-2	5175-5176	.	
16-3	5177-5179	We	
16-4	5180-5183	are	
16-5	5184-5190	unable	
16-6	5191-5193	to	
16-7	5194-5202	estimate	
16-8	5203-5206	the	
16-9	5207-5211	cost	
16-10	5212-5215	for	
16-11	5216-5219	the	
16-12	5220-5223	UDF	
16-13	5223-5224	.	
16-14	5225-5234	Basically	
16-15	5234-5235	,	
16-16	5236-5238	in	
16-17	5239-5243	many	
16-18	5244-5249	cases	
16-19	5249-5250	,	
16-20	5251-5256	Spark	
16-21	5257-5266	optimizer	
16-22	5267-5269	is	
16-23	5270-5277	enabled	
16-24	5278-5280	to	
16-25	5281-5289	generate	
16-26	5290-5293	the	
16-27	5294-5298	best	
16-28	5299-5303	plan	
16-29	5304-5307	due	
16-30	5308-5310	to	
16-31	5311-5315	this	
16-32	5316-5326	limitation	
16-33	5326-5327	.	
16-34	5328-5336	Adaptive	
16-35	5337-5342	Query	
16-36	5343-5352	Execution	
16-37	5353-5356	For	
16-38	5357-5360	all	
16-39	5361-5366	these	
16-40	5367-5374	reasons	
16-41	5374-5375	,	
16-42	5376-5383	runtime	
16-43	5384-5394	adaptivity	
16-44	5395-5402	becomes	
16-45	5403-5407	more	
16-46	5408-5416	critical	
16-47	5417-5420	for	
16-48	5421-5426	Spark	
16-49	5427-5431	than	
16-50	5432-5435	the	
16-51	5436-5447	traditional	
16-52	5448-5455	systems	
16-53	5455-5456	.	
16-54	5457-5459	So	
16-55	5460-5464	this	
16-56	5465-5472	release	
16-57	5473-5483	introduced	
16-58	5484-5485	a	
16-59	5486-5489	new	
16-60	5490-5498	adaptive	
16-61	5499-5504	query	
16-62	5505-5514	execution	
16-63	5515-5524	framework	
16-64	5525-5531	called	

#Text=AQE. The basic idea of adaptive planning is simple. We optimize the execution plan using the existing rules of the optimizer and the planner after we collect more accurate statistics from the finished plans. The red line shows the new logics we added in this release. Instead of directly optimizing the execution plans, we send back the unfinished plan segments
17-1	5532-5535	AQE	
17-2	5535-5536	.	
17-3	5537-5540	The	
17-4	5541-5546	basic	
17-5	5547-5551	idea	
17-6	5552-5554	of	
17-7	5555-5563	adaptive	
17-8	5564-5572	planning	
17-9	5573-5575	is	
17-10	5576-5582	simple	
17-11	5582-5583	.	
17-12	5584-5586	We	
17-13	5587-5595	optimize	
17-14	5596-5599	the	
17-15	5600-5609	execution	
17-16	5610-5614	plan	
17-17	5615-5620	using	
17-18	5621-5624	the	
17-19	5625-5633	existing	
17-20	5634-5639	rules	
17-21	5640-5642	of	
17-22	5643-5646	the	
17-23	5647-5656	optimizer	
17-24	5657-5660	and	
17-25	5661-5664	the	
17-26	5665-5672	planner	
17-27	5673-5678	after	
17-28	5679-5681	we	
17-29	5682-5689	collect	
17-30	5690-5694	more	
17-31	5695-5703	accurate	
17-32	5704-5714	statistics	
17-33	5715-5719	from	
17-34	5720-5723	the	
17-35	5724-5732	finished	
17-36	5733-5738	plans	
17-37	5738-5739	.	
17-38	5740-5743	The	
17-39	5744-5747	red	
17-40	5748-5752	line	
17-41	5753-5758	shows	
17-42	5759-5762	the	
17-43	5763-5766	new	
17-44	5767-5773	logics	
17-45	5774-5776	we	
17-46	5777-5782	added	
17-47	5783-5785	in	
17-48	5786-5790	this	
17-49	5791-5798	release	
17-50	5798-5799	.	
17-51	5800-5807	Instead	
17-52	5808-5810	of	
17-53	5811-5819	directly	
17-54	5820-5830	optimizing	
17-55	5831-5834	the	
17-56	5835-5844	execution	
17-57	5845-5850	plans	
17-58	5850-5851	,	
17-59	5852-5854	we	
17-60	5855-5859	send	
17-61	5860-5864	back	
17-62	5865-5868	the	
17-63	5869-5879	unfinished	
17-64	5880-5884	plan	
17-65	5885-5893	segments	

#Text=and then use an existing optimizer and planner to optimize them at the end and build a new execution plan. This release includes three adaptive features. We can convert the soft merge join to broadcast hash join, based on the runtime statistics. We can shrink the number of reducers after over-partitioning. We can also handle the skew join at runtime.
18-1	5894-5897	and	
18-2	5898-5902	then	
18-3	5903-5906	use	
18-4	5907-5909	an	
18-5	5910-5918	existing	
18-6	5919-5928	optimizer	
18-7	5929-5932	and	
18-8	5933-5940	planner	
18-9	5941-5943	to	
18-10	5944-5952	optimize	
18-11	5953-5957	them	
18-12	5958-5960	at	
18-13	5961-5964	the	
18-14	5965-5968	end	
18-15	5969-5972	and	
18-16	5973-5978	build	
18-17	5979-5980	a	
18-18	5981-5984	new	
18-19	5985-5994	execution	
18-20	5995-5999	plan	
18-21	5999-6000	.	
18-22	6001-6005	This	
18-23	6006-6013	release	
18-24	6014-6022	includes	
18-25	6023-6028	three	
18-26	6029-6037	adaptive	
18-27	6038-6046	features	
18-28	6046-6047	.	
18-29	6048-6050	We	
18-30	6051-6054	can	
18-31	6055-6062	convert	
18-32	6063-6066	the	
18-33	6067-6071	soft	
18-34	6072-6077	merge	
18-35	6078-6082	join	
18-36	6083-6085	to	
18-37	6086-6095	broadcast	
18-38	6096-6100	hash	
18-39	6101-6105	join	
18-40	6105-6106	,	
18-41	6107-6112	based	
18-42	6113-6115	on	
18-43	6116-6119	the	
18-44	6120-6127	runtime	
18-45	6128-6138	statistics	
18-46	6138-6139	.	
18-47	6140-6142	We	
18-48	6143-6146	can	
18-49	6147-6153	shrink	
18-50	6154-6157	the	
18-51	6158-6164	number	
18-52	6165-6167	of	
18-53	6168-6176	reducers	
18-54	6177-6182	after	
18-55	6183-6200	over-partitioning	
18-56	6200-6201	.	
18-57	6202-6204	We	
18-58	6205-6208	can	
18-59	6209-6213	also	
18-60	6214-6220	handle	
18-61	6221-6224	the	
18-62	6225-6229	skew	
18-63	6230-6234	join	
18-64	6235-6237	at	
18-65	6238-6245	runtime	
18-66	6245-6246	.	

#Text=If you want to know more details, please read the blog post I posted here. Today, I will briefly explain them one by one. Maybe most of you already learn many performance tuning tips. For example, to make your join faster, you might guide your optimizer to choose a broadcast hash join instead of the sort merge join. You can
19-1	6247-6249	If	
19-2	6250-6253	you	
19-3	6254-6258	want	
19-4	6259-6261	to	
19-5	6262-6266	know	
19-6	6267-6271	more	
19-7	6272-6279	details	
19-8	6279-6280	,	
19-9	6281-6287	please	
19-10	6288-6292	read	
19-11	6293-6296	the	
19-12	6297-6301	blog	
19-13	6302-6306	post	
19-14	6307-6308	I	
19-15	6309-6315	posted	
19-16	6316-6320	here	
19-17	6320-6321	.	
19-18	6322-6327	Today	
19-19	6327-6328	,	
19-20	6329-6330	I	
19-21	6331-6335	will	
19-22	6336-6343	briefly	
19-23	6344-6351	explain	
19-24	6352-6356	them	
19-25	6357-6360	one	
19-26	6361-6363	by	
19-27	6364-6367	one	
19-28	6367-6368	.	
19-29	6369-6374	Maybe	
19-30	6375-6379	most	
19-31	6380-6382	of	
19-32	6383-6386	you	
19-33	6387-6394	already	
19-34	6395-6400	learn	
19-35	6401-6405	many	
19-36	6406-6417	performance	
19-37	6418-6424	tuning	
19-38	6425-6429	tips	
19-39	6429-6430	.	
19-40	6431-6434	For	
19-41	6435-6442	example	
19-42	6442-6443	,	
19-43	6444-6446	to	
19-44	6447-6451	make	
19-45	6452-6456	your	
19-46	6457-6461	join	
19-47	6462-6468	faster	
19-48	6468-6469	,	
19-49	6470-6473	you	
19-50	6474-6479	might	
19-51	6480-6485	guide	
19-52	6486-6490	your	
19-53	6491-6500	optimizer	
19-54	6501-6503	to	
19-55	6504-6510	choose	
19-56	6511-6512	a	
19-57	6513-6522	broadcast	
19-58	6523-6527	hash	
19-59	6528-6532	join	
19-60	6533-6540	instead	
19-61	6541-6543	of	
19-62	6544-6547	the	
19-63	6548-6552	sort	
19-64	6553-6558	merge	
19-65	6559-6563	join	
19-66	6563-6564	.	
19-67	6565-6568	You	
19-68	6569-6572	can	

#Text=increase the spark.sql.autobroadcastjointhreshold or use a broadcast join hint. However, it is hard to tune it. You might hit out of memory exceptions and even get worse performance. Even if it works now, it is hard to maintain over time because it is sensitive to your data workloads. You might be wondering why Spark is unable to make the wise choice by
20-1	6573-6581	increase	
20-2	6582-6585	the	
20-3	6586-6622	spark.sql.autobroadcastjointhreshold	
20-4	6623-6625	or	
20-5	6626-6629	use	
20-6	6630-6631	a	
20-7	6632-6641	broadcast	
20-8	6642-6646	join	
20-9	6647-6651	hint	
20-10	6651-6652	.	
20-11	6653-6660	However	
20-12	6660-6661	,	
20-13	6662-6664	it	
20-14	6665-6667	is	
20-15	6668-6672	hard	
20-16	6673-6675	to	
20-17	6676-6680	tune	
20-18	6681-6683	it	
20-19	6683-6684	.	
20-20	6685-6688	You	
20-21	6689-6694	might	
20-22	6695-6698	hit	
20-23	6699-6702	out	
20-24	6703-6705	of	
20-25	6706-6712	memory	
20-26	6713-6723	exceptions	
20-27	6724-6727	and	
20-28	6728-6732	even	
20-29	6733-6736	get	
20-30	6737-6742	worse	
20-31	6743-6754	performance	
20-32	6754-6755	.	
20-33	6756-6760	Even	
20-34	6761-6763	if	
20-35	6764-6766	it	
20-36	6767-6772	works	
20-37	6773-6776	now	
20-38	6776-6777	,	
20-39	6778-6780	it	
20-40	6781-6783	is	
20-41	6784-6788	hard	
20-42	6789-6791	to	
20-43	6792-6800	maintain	
20-44	6801-6805	over	
20-45	6806-6810	time	
20-46	6811-6818	because	
20-47	6819-6821	it	
20-48	6822-6824	is	
20-49	6825-6834	sensitive	
20-50	6835-6837	to	
20-51	6838-6842	your	
20-52	6843-6847	data	
20-53	6848-6857	workloads	
20-54	6857-6858	.	
20-55	6859-6862	You	
20-56	6863-6868	might	
20-57	6869-6871	be	
20-58	6872-6881	wondering	
20-59	6882-6885	why	
20-60	6886-6891	Spark	
20-61	6892-6894	is	
20-62	6895-6901	unable	
20-63	6902-6904	to	
20-64	6905-6909	make	
20-65	6910-6913	the	
20-66	6914-6918	wise	
20-67	6919-6925	choice	
20-68	6926-6928	by	

#Text=itself. I can easily list multiple reasons. the statistics might be missing or out of date. the file is compressed. the file format is column-based, so the file size does not represent the actual data volume. the filters could be compressed (the filters) might also contain the black box UDFs.
21-1	6929-6935	itself	
21-2	6935-6936	.	
21-3	6937-6938	I	
21-4	6939-6942	can	
21-5	6943-6949	easily	
21-6	6950-6954	list	
21-7	6955-6963	multiple	
21-8	6964-6971	reasons	
21-9	6971-6972	.	
21-10	6973-6976	the	
21-11	6977-6987	statistics	
21-12	6988-6993	might	
21-13	6994-6996	be	
21-14	6997-7004	missing	
21-15	7005-7007	or	
21-16	7008-7011	out	
21-17	7012-7014	of	
21-18	7015-7019	date	
21-19	7019-7020	.	
21-20	7021-7024	the	
21-21	7025-7029	file	
21-22	7030-7032	is	
21-23	7033-7043	compressed	
21-24	7043-7044	.	
21-25	7045-7048	the	
21-26	7049-7053	file	
21-27	7054-7060	format	
21-28	7061-7063	is	
21-29	7064-7076	column-based	
21-30	7076-7077	,	
21-31	7078-7080	so	
21-32	7081-7084	the	
21-33	7085-7089	file	
21-34	7090-7094	size	
21-35	7095-7099	does	
21-36	7100-7103	not	
21-37	7104-7113	represent	
21-38	7114-7117	the	
21-39	7118-7124	actual	
21-40	7125-7129	data	
21-41	7130-7136	volume	
21-42	7136-7137	.	
21-43	7138-7141	the	
21-44	7142-7149	filters	
21-45	7150-7155	could	
21-46	7156-7158	be	
21-47	7159-7169	compressed	
21-48	7170-7171	(	
21-49	7171-7174	the	
21-50	7175-7182	filters	
21-51	7182-7183	)	
21-52	7184-7189	might	
21-53	7190-7194	also	
21-54	7195-7202	contain	
21-55	7203-7206	the	
21-56	7207-7212	black	
21-57	7213-7216	box	
21-58	7217-7221	UDFs	
21-59	7221-7222	.	

#Text=The whole query fragments might be large, complex, and it is hard to estimate the actual data volume for Spark to make the best choice. Convert Sort Merge Join to Broadcast Hash Join So this is an example to show how AQE converts a sort merge join to
22-1	7223-7226	The	
22-2	7227-7232	whole	
22-3	7233-7238	query	
22-4	7239-7248	fragments	
22-5	7249-7254	might	
22-6	7255-7257	be	
22-7	7258-7263	large	
22-8	7263-7264	,	
22-9	7265-7272	complex	
22-10	7272-7273	,	
22-11	7274-7277	and	
22-12	7278-7280	it	
22-13	7281-7283	is	
22-14	7284-7288	hard	
22-15	7289-7291	to	
22-16	7292-7300	estimate	
22-17	7301-7304	the	
22-18	7305-7311	actual	
22-19	7312-7316	data	
22-20	7317-7323	volume	
22-21	7324-7327	for	
22-22	7328-7333	Spark	
22-23	7334-7336	to	
22-24	7337-7341	make	
22-25	7342-7345	the	
22-26	7346-7350	best	
22-27	7351-7357	choice	
22-28	7357-7358	.	
22-29	7359-7366	Convert	
22-30	7367-7371	Sort	
22-31	7372-7377	Merge	
22-32	7378-7382	Join	
22-33	7383-7385	to	
22-34	7386-7395	Broadcast	
22-35	7396-7400	Hash	
22-36	7401-7405	Join	
22-37	7406-7408	So	
22-38	7409-7413	this	
22-39	7414-7416	is	
22-40	7417-7419	an	
22-41	7420-7427	example	
22-42	7428-7430	to	
22-43	7431-7435	show	
22-44	7436-7439	how	
22-45	7440-7443	AQE	
22-46	7444-7452	converts	
22-47	7453-7454	a	
22-48	7455-7459	sort	
22-49	7460-7465	merge	
22-50	7466-7470	join	
22-51	7471-7473	to	

#Text=a broadcast hash join at runtime. First, execute the leave stages. Query the statistics from the shuffle operators which materialize the query fragments. You can see the actual size of stage two is much smaller than the estimated size reduced from 30 megabytes to 8 megabytes so we can optimize the remaining plan and change the join algorithm from sort merge
23-1	7474-7475	a	
23-2	7476-7485	broadcast	
23-3	7486-7490	hash	
23-4	7491-7495	join	
23-5	7496-7498	at	
23-6	7499-7506	runtime	
23-7	7506-7507	.	
23-8	7508-7513	First	
23-9	7513-7514	,	
23-10	7515-7522	execute	
23-11	7523-7526	the	
23-12	7527-7532	leave	
23-13	7533-7539	stages	
23-14	7539-7540	.	
23-15	7541-7546	Query	
23-16	7547-7550	the	
23-17	7551-7561	statistics	
23-18	7562-7566	from	
23-19	7567-7570	the	
23-20	7571-7578	shuffle	
23-21	7579-7588	operators	
23-22	7589-7594	which	
23-23	7595-7606	materialize	
23-24	7607-7610	the	
23-25	7611-7616	query	
23-26	7617-7626	fragments	
23-27	7626-7627	.	
23-28	7628-7631	You	
23-29	7632-7635	can	
23-30	7636-7639	see	
23-31	7640-7643	the	
23-32	7644-7650	actual	
23-33	7651-7655	size	
23-34	7656-7658	of	
23-35	7659-7664	stage	
23-36	7665-7668	two	
23-37	7669-7671	is	
23-38	7672-7676	much	
23-39	7677-7684	smaller	
23-40	7685-7689	than	
23-41	7690-7693	the	
23-42	7694-7703	estimated	
23-43	7704-7708	size	
23-44	7709-7716	reduced	
23-45	7717-7721	from	
23-46	7722-7724	30	
23-47	7725-7734	megabytes	
23-48	7735-7737	to	
23-49	7738-7739	8	
23-50	7740-7749	megabytes	
23-51	7750-7752	so	
23-52	7753-7755	we	
23-53	7756-7759	can	
23-54	7760-7768	optimize	
23-55	7769-7772	the	
23-56	7773-7782	remaining	
23-57	7783-7787	plan	
23-58	7788-7791	and	
23-59	7792-7798	change	
23-60	7799-7802	the	
23-61	7803-7807	join	
23-62	7808-7817	algorithm	
23-63	7818-7822	from	
23-64	7823-7827	sort	
23-65	7828-7833	merge	

#Text=join to broadcast hash join. Another popular performance tuning tip is to tune the configuration spark.sql.shuffle.partitions. The default value is a magic number, 200. Previously, the original default is 8. Later, it was increased to 200. I believe no one knows the reason
24-1	7834-7838	join	
24-2	7839-7841	to	
24-3	7842-7851	broadcast	
24-4	7852-7856	hash	
24-5	7857-7861	join	
24-6	7861-7862	.	
24-7	7863-7870	Another	
24-8	7871-7878	popular	
24-9	7879-7890	performance	
24-10	7891-7897	tuning	
24-11	7898-7901	tip	
24-12	7902-7904	is	
24-13	7905-7907	to	
24-14	7908-7912	tune	
24-15	7913-7916	the	
24-16	7917-7930	configuration	
24-17	7931-7959	spark.sql.shuffle.partitions	
24-18	7959-7960	.	
24-19	7961-7964	The	
24-20	7965-7972	default	
24-21	7973-7978	value	
24-22	7979-7981	is	
24-23	7982-7983	a	
24-24	7984-7989	magic	
24-25	7990-7996	number	
24-26	7996-7997	,	
24-27	7998-8001	200	
24-28	8001-8002	.	
24-29	8003-8013	Previously	
24-30	8013-8014	,	
24-31	8015-8018	the	
24-32	8019-8027	original	
24-33	8028-8035	default	
24-34	8036-8038	is	
24-35	8039-8040	8	
24-36	8040-8041	.	
24-37	8042-8047	Later	
24-38	8047-8048	,	
24-39	8049-8051	it	
24-40	8052-8055	was	
24-41	8056-8065	increased	
24-42	8066-8068	to	
24-43	8069-8072	200	
24-44	8072-8073	.	
24-45	8074-8075	I	
24-46	8076-8083	believe	
24-47	8084-8086	no	
24-48	8087-8090	one	
24-49	8091-8096	knows	
24-50	8097-8100	the	
24-51	8101-8107	reason	

#Text=why it become 200 instead of 50, 400, or 2,000. It is very hard to tune it, to be honest. Because it is a global configuration, it is almost impossible to decide the best value for every query’s fragment using a single configuration, especially when your query plan is huge and complex.
25-1	8108-8111	why	
25-2	8112-8114	it	
25-3	8115-8121	become	
25-4	8122-8125	200	
25-5	8126-8133	instead	
25-6	8134-8136	of	
25-7	8137-8139	50	
25-8	8139-8140	,	
25-9	8141-8144	400	
25-10	8144-8145	,	
25-11	8146-8148	or	
25-12	8149-8154	2,000	
25-13	8154-8155	.	
25-14	8156-8158	It	
25-15	8159-8161	is	
25-16	8162-8166	very	
25-17	8167-8171	hard	
25-18	8172-8174	to	
25-19	8175-8179	tune	
25-20	8180-8182	it	
25-21	8182-8183	,	
25-22	8184-8186	to	
25-23	8187-8189	be	
25-24	8190-8196	honest	
25-25	8196-8197	.	
25-26	8198-8205	Because	
25-27	8206-8208	it	
25-28	8209-8211	is	
25-29	8212-8213	a	
25-30	8214-8220	global	
25-31	8221-8234	configuration	
25-32	8234-8235	,	
25-33	8236-8238	it	
25-34	8239-8241	is	
25-35	8242-8248	almost	
25-36	8249-8259	impossible	
25-37	8260-8262	to	
25-38	8263-8269	decide	
25-39	8270-8273	the	
25-40	8274-8278	best	
25-41	8279-8284	value	
25-42	8285-8288	for	
25-43	8289-8294	every	
25-44	8295-8300	query	
25-45	8300-8301	’	
25-46	8301-8302	s	
25-47	8303-8311	fragment	
25-48	8312-8317	using	
25-49	8318-8319	a	
25-50	8320-8326	single	
25-51	8327-8340	configuration	
25-52	8340-8341	,	
25-53	8342-8352	especially	
25-54	8353-8357	when	
25-55	8358-8362	your	
25-56	8363-8368	query	
25-57	8369-8373	plan	
25-58	8374-8376	is	
25-59	8377-8381	huge	
25-60	8382-8385	and	
25-61	8386-8393	complex	
25-62	8393-8394	.	

#Text=If you set it to very small values, the partition will be huge, and the aggregation and the sort might need to spew the data to the disk. If the configuration values are too big, the partition will be small. But the number of partitions is big. It will cause inefficient IO and the performance bottleneck could be the task scheduler. Then
26-1	8395-8397	If	
26-2	8398-8401	you	
26-3	8402-8405	set	
26-4	8406-8408	it	
26-5	8409-8411	to	
26-6	8412-8416	very	
26-7	8417-8422	small	
26-8	8423-8429	values	
26-9	8429-8430	,	
26-10	8431-8434	the	
26-11	8435-8444	partition	
26-12	8445-8449	will	
26-13	8450-8452	be	
26-14	8453-8457	huge	
26-15	8457-8458	,	
26-16	8459-8462	and	
26-17	8463-8466	the	
26-18	8467-8478	aggregation	
26-19	8479-8482	and	
26-20	8483-8486	the	
26-21	8487-8491	sort	
26-22	8492-8497	might	
26-23	8498-8502	need	
26-24	8503-8505	to	
26-25	8506-8510	spew	
26-26	8511-8514	the	
26-27	8515-8519	data	
26-28	8520-8522	to	
26-29	8523-8526	the	
26-30	8527-8531	disk	
26-31	8531-8532	.	
26-32	8533-8535	If	
26-33	8536-8539	the	
26-34	8540-8553	configuration	
26-35	8554-8560	values	
26-36	8561-8564	are	
26-37	8565-8568	too	
26-38	8569-8572	big	
26-39	8572-8573	,	
26-40	8574-8577	the	
26-41	8578-8587	partition	
26-42	8588-8592	will	
26-43	8593-8595	be	
26-44	8596-8601	small	
26-45	8601-8602	.	
26-46	8603-8606	But	
26-47	8607-8610	the	
26-48	8611-8617	number	
26-49	8618-8620	of	
26-50	8621-8631	partitions	
26-51	8632-8634	is	
26-52	8635-8638	big	
26-53	8638-8639	.	
26-54	8640-8642	It	
26-55	8643-8647	will	
26-56	8648-8653	cause	
26-57	8654-8665	inefficient	
26-58	8666-8668	IO	
26-59	8669-8672	and	
26-60	8673-8676	the	
26-61	8677-8688	performance	
26-62	8689-8699	bottleneck	
26-63	8700-8705	could	
26-64	8706-8708	be	
26-65	8709-8712	the	
26-66	8713-8717	task	
26-67	8718-8727	scheduler	
26-68	8727-8728	.	
26-69	8729-8733	Then	

#Text=it will slow down everybody. Also, it is very hard to maintain over time. Dynamically Coalesce Shuffle Partitions Until you can solve it in a smart way, we can first increase our initial partition number to a big one. After we execute the leave query stage, we can know the actual size of each partition.
27-1	8734-8736	it	
27-2	8737-8741	will	
27-3	8742-8746	slow	
27-4	8747-8751	down	
27-5	8752-8761	everybody	
27-6	8761-8762	.	
27-7	8763-8767	Also	
27-8	8767-8768	,	
27-9	8769-8771	it	
27-10	8772-8774	is	
27-11	8775-8779	very	
27-12	8780-8784	hard	
27-13	8785-8787	to	
27-14	8788-8796	maintain	
27-15	8797-8801	over	
27-16	8802-8806	time	
27-17	8806-8807	.	
27-18	8808-8819	Dynamically	
27-19	8820-8828	Coalesce	
27-20	8829-8836	Shuffle	
27-21	8837-8847	Partitions	
27-22	8848-8853	Until	
27-23	8854-8857	you	
27-24	8858-8861	can	
27-25	8862-8867	solve	
27-26	8868-8870	it	
27-27	8871-8873	in	
27-28	8874-8875	a	
27-29	8876-8881	smart	
27-30	8882-8885	way	
27-31	8885-8886	,	
27-32	8887-8889	we	
27-33	8890-8893	can	
27-34	8894-8899	first	
27-35	8900-8908	increase	
27-36	8909-8912	our	
27-37	8913-8920	initial	
27-38	8921-8930	partition	
27-39	8931-8937	number	
27-40	8938-8940	to	
27-41	8941-8942	a	
27-42	8943-8946	big	
27-43	8947-8950	one	
27-44	8950-8951	.	
27-45	8952-8957	After	
27-46	8958-8960	we	
27-47	8961-8968	execute	
27-48	8969-8972	the	
27-49	8973-8978	leave	
27-50	8979-8984	query	
27-51	8985-8990	stage	
27-52	8990-8991	,	
27-53	8992-8994	we	
27-54	8995-8998	can	
27-55	8999-9003	know	
27-56	9004-9007	the	
27-57	9008-9014	actual	
27-58	9015-9019	size	
27-59	9020-9022	of	
27-60	9023-9027	each	
27-61	9028-9037	partition	
27-62	9037-9038	.	

#Text=Then we can automatically correlate the nearby partitions and automatically reduce the number of partitions to a smaller number. This example shows how we reduce the number of partitions from 50 to 5 at runtime. And we added the actual coalesce at runtime. Data Skew One more popular performance tuning tip is about data skew. Data skew is
28-1	9039-9043	Then	
28-2	9044-9046	we	
28-3	9047-9050	can	
28-4	9051-9064	automatically	
28-5	9065-9074	correlate	
28-6	9075-9078	the	
28-7	9079-9085	nearby	
28-8	9086-9096	partitions	
28-9	9097-9100	and	
28-10	9101-9114	automatically	
28-11	9115-9121	reduce	
28-12	9122-9125	the	
28-13	9126-9132	number	
28-14	9133-9135	of	
28-15	9136-9146	partitions	
28-16	9147-9149	to	
28-17	9150-9151	a	
28-18	9152-9159	smaller	
28-19	9160-9166	number	
28-20	9166-9167	.	
28-21	9168-9172	This	
28-22	9173-9180	example	
28-23	9181-9186	shows	
28-24	9187-9190	how	
28-25	9191-9193	we	
28-26	9194-9200	reduce	
28-27	9201-9204	the	
28-28	9205-9211	number	
28-29	9212-9214	of	
28-30	9215-9225	partitions	
28-31	9226-9230	from	
28-32	9231-9233	50	
28-33	9234-9236	to	
28-34	9237-9238	5	
28-35	9239-9241	at	
28-36	9242-9249	runtime	
28-37	9249-9250	.	
28-38	9251-9254	And	
28-39	9255-9257	we	
28-40	9258-9263	added	
28-41	9264-9267	the	
28-42	9268-9274	actual	
28-43	9275-9283	coalesce	
28-44	9284-9286	at	
28-45	9287-9294	runtime	
28-46	9294-9295	.	
28-47	9296-9300	Data	
28-48	9301-9305	Skew	
28-49	9306-9309	One	
28-50	9310-9314	more	
28-51	9315-9322	popular	
28-52	9323-9334	performance	
28-53	9335-9341	tuning	
28-54	9342-9345	tip	
28-55	9346-9348	is	
28-56	9349-9354	about	
28-57	9355-9359	data	
28-58	9360-9364	skew	
28-59	9364-9365	.	
28-60	9366-9370	Data	
28-61	9371-9375	skew	
28-62	9376-9378	is	

#Text=very annoying. You could see some long-running or frozen task and a lot of disks spinning and a very low resource authorization rate in most nodes and even out of memory. Our Spark community might tell you many different ways to solve such a typical performance problem. You can find the skew value and the right queries to handle the skew value separately.
29-1	9379-9383	very	
29-2	9384-9392	annoying	
29-3	9392-9393	.	
29-4	9394-9397	You	
29-5	9398-9403	could	
29-6	9404-9407	see	
29-7	9408-9412	some	
29-8	9413-9425	long-running	
29-9	9426-9428	or	
29-10	9429-9435	frozen	
29-11	9436-9440	task	
29-12	9441-9444	and	
29-13	9445-9446	a	
29-14	9447-9450	lot	
29-15	9451-9453	of	
29-16	9454-9459	disks	
29-17	9460-9468	spinning	
29-18	9469-9472	and	
29-19	9473-9474	a	
29-20	9475-9479	very	
29-21	9480-9483	low	
29-22	9484-9492	resource	
29-23	9493-9506	authorization	
29-24	9507-9511	rate	
29-25	9512-9514	in	
29-26	9515-9519	most	
29-27	9520-9525	nodes	
29-28	9526-9529	and	
29-29	9530-9534	even	
29-30	9535-9538	out	
29-31	9539-9541	of	
29-32	9542-9548	memory	
29-33	9548-9549	.	
29-34	9550-9553	Our	
29-35	9554-9559	Spark	
29-36	9560-9569	community	
29-37	9570-9575	might	
29-38	9576-9580	tell	
29-39	9581-9584	you	
29-40	9585-9589	many	
29-41	9590-9599	different	
29-42	9600-9604	ways	
29-43	9605-9607	to	
29-44	9608-9613	solve	
29-45	9614-9618	such	
29-46	9619-9620	a	
29-47	9621-9628	typical	
29-48	9629-9640	performance	
29-49	9641-9648	problem	
29-50	9648-9649	.	
29-51	9650-9653	You	
29-52	9654-9657	can	
29-53	9658-9662	find	
29-54	9663-9666	the	
29-55	9667-9671	skew	
29-56	9672-9677	value	
29-57	9678-9681	and	
29-58	9682-9685	the	
29-59	9686-9691	right	
29-60	9692-9699	queries	
29-61	9700-9702	to	
29-62	9703-9709	handle	
29-63	9710-9713	the	
29-64	9714-9718	skew	
29-65	9719-9724	value	
29-66	9725-9735	separately	
29-67	9735-9736	.	

#Text=And also, you can add the actual skew keys that can remove the data skew, either new columns or some existing columns. Anyway, you have to manually rewrite your queries, and this is annoying and sensitive to your workloads, too, which could be changed over time. This is an example without the skew optimization. Because of data skew,
30-1	9737-9740	And	
30-2	9741-9745	also	
30-3	9745-9746	,	
30-4	9747-9750	you	
30-5	9751-9754	can	
30-6	9755-9758	add	
30-7	9759-9762	the	
30-8	9763-9769	actual	
30-9	9770-9774	skew	
30-10	9775-9779	keys	
30-11	9780-9784	that	
30-12	9785-9788	can	
30-13	9789-9795	remove	
30-14	9796-9799	the	
30-15	9800-9804	data	
30-16	9805-9809	skew	
30-17	9809-9810	,	
30-18	9811-9817	either	
30-19	9818-9821	new	
30-20	9822-9829	columns	
30-21	9830-9832	or	
30-22	9833-9837	some	
30-23	9838-9846	existing	
30-24	9847-9854	columns	
30-25	9854-9855	.	
30-26	9856-9862	Anyway	
30-27	9862-9863	,	
30-28	9864-9867	you	
30-29	9868-9872	have	
30-30	9873-9875	to	
30-31	9876-9884	manually	
30-32	9885-9892	rewrite	
30-33	9893-9897	your	
30-34	9898-9905	queries	
30-35	9905-9906	,	
30-36	9907-9910	and	
30-37	9911-9915	this	
30-38	9916-9918	is	
30-39	9919-9927	annoying	
30-40	9928-9931	and	
30-41	9932-9941	sensitive	
30-42	9942-9944	to	
30-43	9945-9949	your	
30-44	9950-9959	workloads	
30-45	9959-9960	,	
30-46	9961-9964	too	
30-47	9964-9965	,	
30-48	9966-9971	which	
30-49	9972-9977	could	
30-50	9978-9980	be	
30-51	9981-9988	changed	
30-52	9989-9993	over	
30-53	9994-9998	time	
30-54	9998-9999	.	
30-55	10000-10004	This	
30-56	10005-10007	is	
30-57	10008-10010	an	
30-58	10011-10018	example	
30-59	10019-10026	without	
30-60	10027-10030	the	
30-61	10031-10035	skew	
30-62	10036-10048	optimization	
30-63	10048-10049	.	
30-64	10050-10057	Because	
30-65	10058-10060	of	
30-66	10061-10065	data	
30-67	10066-10070	skew	
30-68	10070-10071	,	

#Text=after the shuffle, the shuffle partition, A0, will be very large. If we do a join on these two tables, the whole performance bottleneck is to join the values for this specific partition, A0. For this partition, A0, the cost of shuffle, sort, and merge are much bigger than the other partitions. Everyone is waiting for the partition 0 to complete and slow down the execution
31-1	10072-10077	after	
31-2	10078-10081	the	
31-3	10082-10089	shuffle	
31-4	10089-10090	,	
31-5	10091-10094	the	
31-6	10095-10102	shuffle	
31-7	10103-10112	partition	
31-8	10112-10113	,	
31-9	10114-10116	A0	
31-10	10116-10117	,	
31-11	10118-10122	will	
31-12	10123-10125	be	
31-13	10126-10130	very	
31-14	10131-10136	large	
31-15	10136-10137	.	
31-16	10138-10140	If	
31-17	10141-10143	we	
31-18	10144-10146	do	
31-19	10147-10148	a	
31-20	10149-10153	join	
31-21	10154-10156	on	
31-22	10157-10162	these	
31-23	10163-10166	two	
31-24	10167-10173	tables	
31-25	10173-10174	,	
31-26	10175-10178	the	
31-27	10179-10184	whole	
31-28	10185-10196	performance	
31-29	10197-10207	bottleneck	
31-30	10208-10210	is	
31-31	10211-10213	to	
31-32	10214-10218	join	
31-33	10219-10222	the	
31-34	10223-10229	values	
31-35	10230-10233	for	
31-36	10234-10238	this	
31-37	10239-10247	specific	
31-38	10248-10257	partition	
31-39	10257-10258	,	
31-40	10259-10261	A0	
31-41	10261-10262	.	
31-42	10263-10266	For	
31-43	10267-10271	this	
31-44	10272-10281	partition	
31-45	10281-10282	,	
31-46	10283-10285	A0	
31-47	10285-10286	,	
31-48	10287-10290	the	
31-49	10291-10295	cost	
31-50	10296-10298	of	
31-51	10299-10306	shuffle	
31-52	10306-10307	,	
31-53	10308-10312	sort	
31-54	10312-10313	,	
31-55	10314-10317	and	
31-56	10318-10323	merge	
31-57	10324-10327	are	
31-58	10328-10332	much	
31-59	10333-10339	bigger	
31-60	10340-10344	than	
31-61	10345-10348	the	
31-62	10349-10354	other	
31-63	10355-10365	partitions	
31-64	10365-10366	.	
31-65	10367-10375	Everyone	
31-66	10376-10378	is	
31-67	10379-10386	waiting	
31-68	10387-10390	for	
31-69	10391-10394	the	
31-70	10395-10404	partition	
31-71	10405-10406	0	
31-72	10407-10409	to	
31-73	10410-10418	complete	
31-74	10419-10422	and	
31-75	10423-10427	slow	
31-76	10428-10432	down	
31-77	10433-10436	the	
31-78	10437-10446	execution	

#Text=of the whole query. Our adaptive query execution can handle it very well in the data skew case. After executing the leaf stages (stages one and stage two), we can optimize our queries with a skew shuffle reader. Basically, it will split the skew partitions into smaller
32-1	10447-10449	of	
32-2	10450-10453	the	
32-3	10454-10459	whole	
32-4	10460-10465	query	
32-5	10465-10466	.	
32-6	10467-10470	Our	
32-7	10471-10479	adaptive	
32-8	10480-10485	query	
32-9	10486-10495	execution	
32-10	10496-10499	can	
32-11	10500-10506	handle	
32-12	10507-10509	it	
32-13	10510-10514	very	
32-14	10515-10519	well	
32-15	10520-10522	in	
32-16	10523-10526	the	
32-17	10527-10531	data	
32-18	10532-10536	skew	
32-19	10537-10541	case	
32-20	10541-10542	.	
32-21	10543-10548	After	
32-22	10549-10558	executing	
32-23	10559-10562	the	
32-24	10563-10567	leaf	
32-25	10568-10574	stages	
32-26	10575-10576	(	
32-27	10576-10582	stages	
32-28	10583-10586	one	
32-29	10587-10590	and	
32-30	10591-10596	stage	
32-31	10597-10600	two	
32-32	10600-10601	)	
32-33	10601-10602	,	
32-34	10603-10605	we	
32-35	10606-10609	can	
32-36	10610-10618	optimize	
32-37	10619-10622	our	
32-38	10623-10630	queries	
32-39	10631-10635	with	
32-40	10636-10637	a	
32-41	10638-10642	skew	
32-42	10643-10650	shuffle	
32-43	10651-10657	reader	
32-44	10657-10658	.	
32-45	10659-10668	Basically	
32-46	10668-10669	,	
32-47	10670-10672	it	
32-48	10673-10677	will	
32-49	10678-10683	split	
32-50	10684-10687	the	
32-51	10688-10692	skew	
32-52	10693-10703	partitions	
32-53	10704-10708	into	
32-54	10709-10716	smaller	

#Text=subpartitions after we realize some shuffle partitions are too big. Let us use same example to show how to resolve it using adaptive query execution. After realizing partitions are too large, AQE will add a skew reader to automatically split table A’s partition part 0 to three segments: split 0, split 1, and split 2. Then
33-1	10717-10730	subpartitions	
33-2	10731-10736	after	
33-3	10737-10739	we	
33-4	10740-10747	realize	
33-5	10748-10752	some	
33-6	10753-10760	shuffle	
33-7	10761-10771	partitions	
33-8	10772-10775	are	
33-9	10776-10779	too	
33-10	10780-10783	big	
33-11	10783-10784	.	
33-12	10785-10788	Let	
33-13	10789-10791	us	
33-14	10792-10795	use	
33-15	10796-10800	same	
33-16	10801-10808	example	
33-17	10809-10811	to	
33-18	10812-10816	show	
33-19	10817-10820	how	
33-20	10821-10823	to	
33-21	10824-10831	resolve	
33-22	10832-10834	it	
33-23	10835-10840	using	
33-24	10841-10849	adaptive	
33-25	10850-10855	query	
33-26	10856-10865	execution	
33-27	10865-10866	.	
33-28	10867-10872	After	
33-29	10873-10882	realizing	
33-30	10883-10893	partitions	
33-31	10894-10897	are	
33-32	10898-10901	too	
33-33	10902-10907	large	
33-34	10907-10908	,	
33-35	10909-10912	AQE	
33-36	10913-10917	will	
33-37	10918-10921	add	
33-38	10922-10923	a	
33-39	10924-10928	skew	
33-40	10929-10935	reader	
33-41	10936-10938	to	
33-42	10939-10952	automatically	
33-43	10953-10958	split	
33-44	10959-10964	table	
33-45	10965-10966	A	
33-46	10966-10967	’	
33-47	10967-10968	s	
33-48	10969-10978	partition	
33-49	10979-10983	part	
33-50	10984-10985	0	
33-51	10986-10988	to	
33-52	10989-10994	three	
33-53	10995-11003	segments	
33-54	11003-11004	:	
33-55	11005-11010	split	
33-56	11011-11012	0	
33-57	11012-11013	,	
33-58	11014-11019	split	
33-59	11020-11021	1	
33-60	11021-11022	,	
33-61	11023-11026	and	
33-62	11027-11032	split	
33-63	11033-11034	2	
33-64	11034-11035	.	
33-65	11036-11040	Then	

#Text=it will also duplicate another side for table B. Then we will have three copies for table B’s part 0. After this step, we can parallelize the shuffle reading, sorting, merging for this split partition A0. We can avoid generating very big partition for the sort merge join. Overall, it will be much faster.
34-1	11041-11043	it	
34-2	11044-11048	will	
34-3	11049-11053	also	
34-4	11054-11063	duplicate	
34-5	11064-11071	another	
34-6	11072-11076	side	
34-7	11077-11080	for	
34-8	11081-11086	table	
34-9	11087-11088	B	
34-10	11088-11089	.	
34-11	11090-11094	Then	
34-12	11095-11097	we	
34-13	11098-11102	will	
34-14	11103-11107	have	
34-15	11108-11113	three	
34-16	11114-11120	copies	
34-17	11121-11124	for	
34-18	11125-11130	table	
34-19	11131-11132	B	
34-20	11132-11133	’	
34-21	11133-11134	s	
34-22	11135-11139	part	
34-23	11140-11141	0	
34-24	11141-11142	.	
34-25	11143-11148	After	
34-26	11149-11153	this	
34-27	11154-11158	step	
34-28	11158-11159	,	
34-29	11160-11162	we	
34-30	11163-11166	can	
34-31	11167-11178	parallelize	
34-32	11179-11182	the	
34-33	11183-11190	shuffle	
34-34	11191-11198	reading	
34-35	11198-11199	,	
34-36	11200-11207	sorting	
34-37	11207-11208	,	
34-38	11209-11216	merging	
34-39	11217-11220	for	
34-40	11221-11225	this	
34-41	11226-11231	split	
34-42	11232-11241	partition	
34-43	11242-11244	A0	
34-44	11244-11245	.	
34-45	11246-11248	We	
34-46	11249-11252	can	
34-47	11253-11258	avoid	
34-48	11259-11269	generating	
34-49	11270-11274	very	
34-50	11275-11278	big	
34-51	11279-11288	partition	
34-52	11289-11292	for	
34-53	11293-11296	the	
34-54	11297-11301	sort	
34-55	11302-11307	merge	
34-56	11308-11312	join	
34-57	11312-11313	.	
34-58	11314-11321	Overall	
34-59	11321-11322	,	
34-60	11323-11325	it	
34-61	11326-11330	will	
34-62	11331-11333	be	
34-63	11334-11338	much	
34-64	11339-11345	faster	
34-65	11345-11346	.	

#Text=Based on a terabyte of TPC-DS benchmark, without statistics, Spark 3.0 can make Q7 eight times faster and also achieve two times fast and speed up for Q5 and more than 1.1 speed up for another 26 queries. So this is just the beginning. In the future releases, we will continue to improve the compiler and introduce more new adaptive rules. Dynamic Partition Pruning
35-1	11347-11352	Based	
35-2	11353-11355	on	
35-3	11356-11357	a	
35-4	11358-11366	terabyte	
35-5	11367-11369	of	
35-6	11370-11376	TPC-DS	
35-7	11377-11386	benchmark	
35-8	11386-11387	,	
35-9	11388-11395	without	
35-10	11396-11406	statistics	
35-11	11406-11407	,	
35-12	11408-11413	Spark	
35-13	11414-11417	3.0	
35-14	11418-11421	can	
35-15	11422-11426	make	
35-16	11427-11429	Q7	
35-17	11430-11435	eight	
35-18	11436-11441	times	
35-19	11442-11448	faster	
35-20	11449-11452	and	
35-21	11453-11457	also	
35-22	11458-11465	achieve	
35-23	11466-11469	two	
35-24	11470-11475	times	
35-25	11476-11480	fast	
35-26	11481-11484	and	
35-27	11485-11490	speed	
35-28	11491-11493	up	
35-29	11494-11497	for	
35-30	11498-11500	Q5	
35-31	11501-11504	and	
35-32	11505-11509	more	
35-33	11510-11514	than	
35-34	11515-11518	1.1	
35-35	11519-11524	speed	
35-36	11525-11527	up	
35-37	11528-11531	for	
35-38	11532-11539	another	
35-39	11540-11542	26	
35-40	11543-11550	queries	
35-41	11550-11551	.	
35-42	11552-11554	So	
35-43	11555-11559	this	
35-44	11560-11562	is	
35-45	11563-11567	just	
35-46	11568-11571	the	
35-47	11572-11581	beginning	
35-48	11581-11582	.	
35-49	11583-11585	In	
35-50	11586-11589	the	
35-51	11590-11596	future	
35-52	11597-11605	releases	
35-53	11605-11606	,	
35-54	11607-11609	we	
35-55	11610-11614	will	
35-56	11615-11623	continue	
35-57	11624-11626	to	
35-58	11627-11634	improve	
35-59	11635-11638	the	
35-60	11639-11647	compiler	
35-61	11648-11651	and	
35-62	11652-11661	introduce	
35-63	11662-11666	more	
35-64	11667-11670	new	
35-65	11671-11679	adaptive	
35-66	11680-11685	rules	
35-67	11685-11686	.	
35-68	11687-11694	Dynamic	
35-69	11695-11704	Partition	
35-70	11705-11712	Pruning	

#Text=The second performance features I want to highlight is dynamic partition pruning. So this is another runtime optimization rule. Basically, dynamic partition pruning is to avoid partition scanning based on the queried results of the other query fragments. It is important for star schema queries. We can achieve a significant speed up in TPC-DS
36-1	11713-11716	The	
36-2	11717-11723	second	
36-3	11724-11735	performance	
36-4	11736-11744	features	
36-5	11745-11746	I	
36-6	11747-11751	want	
36-7	11752-11754	to	
36-8	11755-11764	highlight	
36-9	11765-11767	is	
36-10	11768-11775	dynamic	
36-11	11776-11785	partition	
36-12	11786-11793	pruning	
36-13	11793-11794	.	
36-14	11795-11797	So	
36-15	11798-11802	this	
36-16	11803-11805	is	
36-17	11806-11813	another	
36-18	11814-11821	runtime	
36-19	11822-11834	optimization	
36-20	11835-11839	rule	
36-21	11839-11840	.	
36-22	11841-11850	Basically	
36-23	11850-11851	,	
36-24	11852-11859	dynamic	
36-25	11860-11869	partition	
36-26	11870-11877	pruning	
36-27	11878-11880	is	
36-28	11881-11883	to	
36-29	11884-11889	avoid	
36-30	11890-11899	partition	
36-31	11900-11908	scanning	
36-32	11909-11914	based	
36-33	11915-11917	on	
36-34	11918-11921	the	
36-35	11922-11929	queried	
36-36	11930-11937	results	
36-37	11938-11940	of	
36-38	11941-11944	the	
36-39	11945-11950	other	
36-40	11951-11956	query	
36-41	11957-11966	fragments	
36-42	11966-11967	.	
36-43	11968-11970	It	
36-44	11971-11973	is	
36-45	11974-11983	important	
36-46	11984-11987	for	
36-47	11988-11992	star	
36-48	11993-11999	schema	
36-49	12000-12007	queries	
36-50	12007-12008	.	
36-51	12009-12011	We	
36-52	12012-12015	can	
36-53	12016-12023	achieve	
36-54	12024-12025	a	
36-55	12026-12037	significant	
36-56	12038-12043	speed	
36-57	12044-12046	up	
36-58	12047-12049	in	
36-59	12050-12056	TPC-DS	

#Text=queries. So this is a number, in a TPC-DS benchmark, 60 out of 102 queries show a significant speed up between 2 times and 18 times. It is to prune the partitions that joins read from the fact table T1 by identifying those partitions that result from filtering the
37-1	12057-12064	queries	
37-2	12064-12065	.	
37-3	12066-12068	So	
37-4	12069-12073	this	
37-5	12074-12076	is	
37-6	12077-12078	a	
37-7	12079-12085	number	
37-8	12085-12086	,	
37-9	12087-12089	in	
37-10	12090-12091	a	
37-11	12092-12098	TPC-DS	
37-12	12099-12108	benchmark	
37-13	12108-12109	,	
37-14	12110-12112	60	
37-15	12113-12116	out	
37-16	12117-12119	of	
37-17	12120-12123	102	
37-18	12124-12131	queries	
37-19	12132-12136	show	
37-20	12137-12138	a	
37-21	12139-12150	significant	
37-22	12151-12156	speed	
37-23	12157-12159	up	
37-24	12160-12167	between	
37-25	12168-12169	2	
37-26	12170-12175	times	
37-27	12176-12179	and	
37-28	12180-12182	18	
37-29	12183-12188	times	
37-30	12188-12189	.	
37-31	12190-12192	It	
37-32	12193-12195	is	
37-33	12196-12198	to	
37-34	12199-12204	prune	
37-35	12205-12208	the	
37-36	12209-12219	partitions	
37-37	12220-12224	that	
37-38	12225-12230	joins	
37-39	12231-12235	read	
37-40	12236-12240	from	
37-41	12241-12244	the	
37-42	12245-12249	fact	
37-43	12250-12255	table	
37-44	12256-12258	T1	
37-45	12259-12261	by	
37-46	12262-12273	identifying	
37-47	12274-12279	those	
37-48	12280-12290	partitions	
37-49	12291-12295	that	
37-50	12296-12302	result	
37-51	12303-12307	from	
37-52	12308-12317	filtering	
37-53	12318-12321	the	

#Text=dimension table, T2. Let us explain it step by step. First, we will do the filter push down in the left side. And on the right side, we can generate a new filter for the partition column PP because join P is a partition column. Then we get the query results of the left
38-1	12322-12331	dimension	
38-2	12332-12337	table	
38-3	12337-12338	,	
38-4	12339-12341	T2	
38-5	12341-12342	.	
38-6	12343-12346	Let	
38-7	12347-12349	us	
38-8	12350-12357	explain	
38-9	12358-12360	it	
38-10	12361-12365	step	
38-11	12366-12368	by	
38-12	12369-12373	step	
38-13	12373-12374	.	
38-14	12375-12380	First	
38-15	12380-12381	,	
38-16	12382-12384	we	
38-17	12385-12389	will	
38-18	12390-12392	do	
38-19	12393-12396	the	
38-20	12397-12403	filter	
38-21	12404-12408	push	
38-22	12409-12413	down	
38-23	12414-12416	in	
38-24	12417-12420	the	
38-25	12421-12425	left	
38-26	12426-12430	side	
38-27	12430-12431	.	
38-28	12432-12435	And	
38-29	12436-12438	on	
38-30	12439-12442	the	
38-31	12443-12448	right	
38-32	12449-12453	side	
38-33	12453-12454	,	
38-34	12455-12457	we	
38-35	12458-12461	can	
38-36	12462-12470	generate	
38-37	12471-12472	a	
38-38	12473-12476	new	
38-39	12477-12483	filter	
38-40	12484-12487	for	
38-41	12488-12491	the	
38-42	12492-12501	partition	
38-43	12502-12508	column	
38-44	12509-12511	PP	
38-45	12512-12519	because	
38-46	12520-12524	join	
38-47	12525-12526	P	
38-48	12527-12529	is	
38-49	12530-12531	a	
38-50	12532-12541	partition	
38-51	12542-12548	column	
38-52	12548-12549	.	
38-53	12550-12554	Then	
38-54	12555-12557	we	
38-55	12558-12561	get	
38-56	12562-12565	the	
38-57	12566-12571	query	
38-58	12572-12579	results	
38-59	12580-12582	of	
38-60	12583-12586	the	
38-61	12587-12591	left	

#Text=side. We can reuse our query results and generate the lists of constant values, EPP, and filter result. Now, we can push down the in filter in the right side. This will avoid scanning all the partitions of the huge fact table, T1. For this example, we can avoid scanning 90% of partitioning. With this dynamic partition pruning, we can achieve 33 times speed up.
39-1	12592-12596	side	
39-2	12596-12597	.	
39-3	12598-12600	We	
39-4	12601-12604	can	
39-5	12605-12610	reuse	
39-6	12611-12614	our	
39-7	12615-12620	query	
39-8	12621-12628	results	
39-9	12629-12632	and	
39-10	12633-12641	generate	
39-11	12642-12645	the	
39-12	12646-12651	lists	
39-13	12652-12654	of	
39-14	12655-12663	constant	
39-15	12664-12670	values	
39-16	12670-12671	,	
39-17	12672-12675	EPP	
39-18	12675-12676	,	
39-19	12677-12680	and	
39-20	12681-12687	filter	
39-21	12688-12694	result	
39-22	12694-12695	.	
39-23	12696-12699	Now	
39-24	12699-12700	,	
39-25	12701-12703	we	
39-26	12704-12707	can	
39-27	12708-12712	push	
39-28	12713-12717	down	
39-29	12718-12721	the	
39-30	12722-12724	in	
39-31	12725-12731	filter	
39-32	12732-12734	in	
39-33	12735-12738	the	
39-34	12739-12744	right	
39-35	12745-12749	side	
39-36	12749-12750	.	
39-37	12751-12755	This	
39-38	12756-12760	will	
39-39	12761-12766	avoid	
39-40	12767-12775	scanning	
39-41	12776-12779	all	
39-42	12780-12783	the	
39-43	12784-12794	partitions	
39-44	12795-12797	of	
39-45	12798-12801	the	
39-46	12802-12806	huge	
39-47	12807-12811	fact	
39-48	12812-12817	table	
39-49	12817-12818	,	
39-50	12819-12821	T1	
39-51	12821-12822	.	
39-52	12823-12826	For	
39-53	12827-12831	this	
39-54	12832-12839	example	
39-55	12839-12840	,	
39-56	12841-12843	we	
39-57	12844-12847	can	
39-58	12848-12853	avoid	
39-59	12854-12862	scanning	
39-60	12863-12866	90%	
39-61	12867-12869	of	
39-62	12870-12882	partitioning	
39-63	12882-12883	.	
39-64	12884-12888	With	
39-65	12889-12893	this	
39-66	12894-12901	dynamic	
39-67	12902-12911	partition	
39-68	12912-12919	pruning	
39-69	12919-12920	,	
39-70	12921-12923	we	
39-71	12924-12927	can	
39-72	12928-12935	achieve	
39-73	12936-12938	33	
39-74	12939-12944	times	
39-75	12945-12950	speed	
39-76	12951-12953	up	
39-77	12953-12954	.	

#Text=JOIN Optimizer Hints So the last performance feature is join hints. Join hints are very common optimizer hints. It can influence the optimizer to choose an expected join strategies. Previously, we already have a broadcast hash join. In this release, we also add the hints for the
40-1	12955-12959	JOIN	
40-2	12960-12969	Optimizer	
40-3	12970-12975	Hints	
40-4	12976-12978	So	
40-5	12979-12982	the	
40-6	12983-12987	last	
40-7	12988-12999	performance	
40-8	13000-13007	feature	
40-9	13008-13010	is	
40-10	13011-13015	join	
40-11	13016-13021	hints	
40-12	13021-13022	.	
40-13	13023-13027	Join	
40-14	13028-13033	hints	
40-15	13034-13037	are	
40-16	13038-13042	very	
40-17	13043-13049	common	
40-18	13050-13059	optimizer	
40-19	13060-13065	hints	
40-20	13065-13066	.	
40-21	13067-13069	It	
40-22	13070-13073	can	
40-23	13074-13083	influence	
40-24	13084-13087	the	
40-25	13088-13097	optimizer	
40-26	13098-13100	to	
40-27	13101-13107	choose	
40-28	13108-13110	an	
40-29	13111-13119	expected	
40-30	13120-13124	join	
40-31	13125-13135	strategies	
40-32	13135-13136	.	
40-33	13137-13147	Previously	
40-34	13147-13148	,	
40-35	13149-13151	we	
40-36	13152-13159	already	
40-37	13160-13164	have	
40-38	13165-13166	a	
40-39	13167-13176	broadcast	
40-40	13177-13181	hash	
40-41	13182-13186	join	
40-42	13186-13187	.	
40-43	13188-13190	In	
40-44	13191-13195	this	
40-45	13196-13203	release	
40-46	13203-13204	,	
40-47	13205-13207	we	
40-48	13208-13212	also	
40-49	13213-13216	add	
40-50	13217-13220	the	
40-51	13221-13226	hints	
40-52	13227-13230	for	
40-53	13231-13234	the	

#Text=other three join strategies: sort merge join, shuffle hash join, and the shuffle nested loop join. Please remember, this should be used very carefully. It is difficult to manage over time because it is sensitive to your workloads. If your workloads’ patterns are not stable, the hint could even make your query much slower. Here are examples how to
41-1	13235-13240	other	
41-2	13241-13246	three	
41-3	13247-13251	join	
41-4	13252-13262	strategies	
41-5	13262-13263	:	
41-6	13264-13268	sort	
41-7	13269-13274	merge	
41-8	13275-13279	join	
41-9	13279-13280	,	
41-10	13281-13288	shuffle	
41-11	13289-13293	hash	
41-12	13294-13298	join	
41-13	13298-13299	,	
41-14	13300-13303	and	
41-15	13304-13307	the	
41-16	13308-13315	shuffle	
41-17	13316-13322	nested	
41-18	13323-13327	loop	
41-19	13328-13332	join	
41-20	13332-13333	.	
41-21	13334-13340	Please	
41-22	13341-13349	remember	
41-23	13349-13350	,	
41-24	13351-13355	this	
41-25	13356-13362	should	
41-26	13363-13365	be	
41-27	13366-13370	used	
41-28	13371-13375	very	
41-29	13376-13385	carefully	
41-30	13385-13386	.	
41-31	13387-13389	It	
41-32	13390-13392	is	
41-33	13393-13402	difficult	
41-34	13403-13405	to	
41-35	13406-13412	manage	
41-36	13413-13417	over	
41-37	13418-13422	time	
41-38	13423-13430	because	
41-39	13431-13433	it	
41-40	13434-13436	is	
41-41	13437-13446	sensitive	
41-42	13447-13449	to	
41-43	13450-13454	your	
41-44	13455-13464	workloads	
41-45	13464-13465	.	
41-46	13466-13468	If	
41-47	13469-13473	your	
41-48	13474-13483	workloads	
41-49	13483-13484	’	
41-50	13485-13493	patterns	
41-51	13494-13497	are	
41-52	13498-13501	not	
41-53	13502-13508	stable	
41-54	13508-13509	,	
41-55	13510-13513	the	
41-56	13514-13518	hint	
41-57	13519-13524	could	
41-58	13525-13529	even	
41-59	13530-13534	make	
41-60	13535-13539	your	
41-61	13540-13545	query	
41-62	13546-13550	much	
41-63	13551-13557	slower	
41-64	13557-13558	.	
41-65	13559-13563	Here	
41-66	13564-13567	are	
41-67	13568-13576	examples	
41-68	13577-13580	how	
41-69	13581-13583	to	

#Text=use these hints in the SQL queries. You also can do the same thing in the DataFrame API. When we decide the join strategies, [our leads are different here?]. So a broadcast hash join requires one side to be small, no shuffle, no sort, so it performs very fast. For the shuffle hash join, it needs to shuffle the data but no sort is needed. So it can
42-1	13584-13587	use	
42-2	13588-13593	these	
42-3	13594-13599	hints	
42-4	13600-13602	in	
42-5	13603-13606	the	
42-6	13607-13610	SQL	
42-7	13611-13618	queries	
42-8	13618-13619	.	
42-9	13620-13623	You	
42-10	13624-13628	also	
42-11	13629-13632	can	
42-12	13633-13635	do	
42-13	13636-13639	the	
42-14	13640-13644	same	
42-15	13645-13650	thing	
42-16	13651-13653	in	
42-17	13654-13657	the	
42-18	13658-13667	DataFrame	
42-19	13668-13671	API	
42-20	13671-13672	.	
42-21	13673-13677	When	
42-22	13678-13680	we	
42-23	13681-13687	decide	
42-24	13688-13691	the	
42-25	13692-13696	join	
42-26	13697-13707	strategies	
42-27	13707-13708	,	
42-28	13709-13710	[	
42-29	13710-13713	our	
42-30	13714-13719	leads	
42-31	13720-13723	are	
42-32	13724-13733	different	
42-33	13734-13738	here	
42-34	13738-13739	?	
42-35	13739-13740	]	
42-36	13740-13741	.	
42-37	13742-13744	So	
42-38	13745-13746	a	
42-39	13747-13756	broadcast	
42-40	13757-13761	hash	
42-41	13762-13766	join	
42-42	13767-13775	requires	
42-43	13776-13779	one	
42-44	13780-13784	side	
42-45	13785-13787	to	
42-46	13788-13790	be	
42-47	13791-13796	small	
42-48	13796-13797	,	
42-49	13798-13800	no	
42-50	13801-13808	shuffle	
42-51	13808-13809	,	
42-52	13810-13812	no	
42-53	13813-13817	sort	
42-54	13817-13818	,	
42-55	13819-13821	so	
42-56	13822-13824	it	
42-57	13825-13833	performs	
42-58	13834-13838	very	
42-59	13839-13843	fast	
42-60	13843-13844	.	
42-61	13845-13848	For	
42-62	13849-13852	the	
42-63	13853-13860	shuffle	
42-64	13861-13865	hash	
42-65	13866-13870	join	
42-66	13870-13871	,	
42-67	13872-13874	it	
42-68	13875-13880	needs	
42-69	13881-13883	to	
42-70	13884-13891	shuffle	
42-71	13892-13895	the	
42-72	13896-13900	data	
42-73	13901-13904	but	
42-74	13905-13907	no	
42-75	13908-13912	sort	
42-76	13913-13915	is	
42-77	13916-13922	needed	
42-78	13922-13923	.	
42-79	13924-13926	So	
42-80	13927-13929	it	
42-81	13930-13933	can	

#Text=handle the large tables but will still hit out of memory if the data is skewed. Sort merge join is much more robust. It can handle any data size. It needs to shuffle and salt data slower in most cases when the table size is small compared with a broadcast hash join. And also, shuffle nested loop join, it doesn’t require the join keys, unlike the other three
43-1	13934-13940	handle	
43-2	13941-13944	the	
43-3	13945-13950	large	
43-4	13951-13957	tables	
43-5	13958-13961	but	
43-6	13962-13966	will	
43-7	13967-13972	still	
43-8	13973-13976	hit	
43-9	13977-13980	out	
43-10	13981-13983	of	
43-11	13984-13990	memory	
43-12	13991-13993	if	
43-13	13994-13997	the	
43-14	13998-14002	data	
43-15	14003-14005	is	
43-16	14006-14012	skewed	
43-17	14012-14013	.	
43-18	14014-14018	Sort	
43-19	14019-14024	merge	
43-20	14025-14029	join	
43-21	14030-14032	is	
43-22	14033-14037	much	
43-23	14038-14042	more	
43-24	14043-14049	robust	
43-25	14049-14050	.	
43-26	14051-14053	It	
43-27	14054-14057	can	
43-28	14058-14064	handle	
43-29	14065-14068	any	
43-30	14069-14073	data	
43-31	14074-14078	size	
43-32	14078-14079	.	
43-33	14080-14082	It	
43-34	14083-14088	needs	
43-35	14089-14091	to	
43-36	14092-14099	shuffle	
43-37	14100-14103	and	
43-38	14104-14108	salt	
43-39	14109-14113	data	
43-40	14114-14120	slower	
43-41	14121-14123	in	
43-42	14124-14128	most	
43-43	14129-14134	cases	
43-44	14135-14139	when	
43-45	14140-14143	the	
43-46	14144-14149	table	
43-47	14150-14154	size	
43-48	14155-14157	is	
43-49	14158-14163	small	
43-50	14164-14172	compared	
43-51	14173-14177	with	
43-52	14178-14179	a	
43-53	14180-14189	broadcast	
43-54	14190-14194	hash	
43-55	14195-14199	join	
43-56	14199-14200	.	
43-57	14201-14204	And	
43-58	14205-14209	also	
43-59	14209-14210	,	
43-60	14211-14218	shuffle	
43-61	14219-14225	nested	
43-62	14226-14230	loop	
43-63	14231-14235	join	
43-64	14235-14236	,	
43-65	14237-14239	it	
43-66	14240-14245	doesn	
43-67	14245-14246	’	
43-68	14246-14247	t	
43-69	14248-14255	require	
43-70	14256-14259	the	
43-71	14260-14264	join	
43-72	14265-14269	keys	
43-73	14269-14270	,	
43-74	14271-14277	unlike	
43-75	14278-14281	the	
43-76	14282-14287	other	
43-77	14288-14293	three	

#Text=join strategies. Richer APIs: new features and simplify development To enable new use cases and simplify the Spark application development, this release delivers a new capability and enhanced interesting features. Pandas UDF Let’s, first,
44-1	14294-14298	join	
44-2	14299-14309	strategies	
44-3	14309-14310	.	
44-4	14311-14317	Richer	
44-5	14318-14322	APIs	
44-6	14322-14323	:	
44-7	14324-14327	new	
44-8	14328-14336	features	
44-9	14337-14340	and	
44-10	14341-14349	simplify	
44-11	14350-14361	development	
44-12	14362-14364	To	
44-13	14365-14371	enable	
44-14	14372-14375	new	
44-15	14376-14379	use	
44-16	14380-14385	cases	
44-17	14386-14389	and	
44-18	14390-14398	simplify	
44-19	14399-14402	the	
44-20	14403-14408	Spark	
44-21	14409-14420	application	
44-22	14421-14432	development	
44-23	14432-14433	,	
44-24	14434-14438	this	
44-25	14439-14446	release	
44-26	14447-14455	delivers	
44-27	14456-14457	a	
44-28	14458-14461	new	
44-29	14462-14472	capability	
44-30	14473-14476	and	
44-31	14477-14485	enhanced	
44-32	14486-14497	interesting	
44-33	14498-14506	features	
44-34	14506-14507	.	
44-35	14508-14514	Pandas	
44-36	14515-14518	UDF	
44-37	14519-14522	Let	
44-38	14522-14523	’	
44-39	14523-14524	s	
44-40	14524-14525	,	
44-41	14526-14531	first	
44-42	14531-14532	,	

#Text=talk about Pandas UDF. This is a pretty popular performance features for the PySpark users. So let us talk about the history of UDF support in PySpark. In the first release of Python support, 2013, we already support Python lambda functions for RDD API. Then in 2014, users can register Python UDF for Spark SQL. Starting from Spark 2.0, Python UDF registration is
45-1	14533-14537	talk	
45-2	14538-14543	about	
45-3	14544-14550	Pandas	
45-4	14551-14554	UDF	
45-5	14554-14555	.	
45-6	14556-14560	This	
45-7	14561-14563	is	
45-8	14564-14565	a	
45-9	14566-14572	pretty	
45-10	14573-14580	popular	
45-11	14581-14592	performance	
45-12	14593-14601	features	
45-13	14602-14605	for	
45-14	14606-14609	the	
45-15	14610-14617	PySpark	
45-16	14618-14623	users	
45-17	14623-14624	.	
45-18	14625-14627	So	
45-19	14628-14631	let	
45-20	14632-14634	us	
45-21	14635-14639	talk	
45-22	14640-14645	about	
45-23	14646-14649	the	
45-24	14650-14657	history	
45-25	14658-14660	of	
45-26	14661-14664	UDF	
45-27	14665-14672	support	
45-28	14673-14675	in	
45-29	14676-14683	PySpark	
45-30	14683-14684	.	
45-31	14685-14687	In	
45-32	14688-14691	the	
45-33	14692-14697	first	
45-34	14698-14705	release	
45-35	14706-14708	of	
45-36	14709-14715	Python	
45-37	14716-14723	support	
45-38	14723-14724	,	
45-39	14725-14729	2013	
45-40	14729-14730	,	
45-41	14731-14733	we	
45-42	14734-14741	already	
45-43	14742-14749	support	
45-44	14750-14756	Python	
45-45	14757-14763	lambda	
45-46	14764-14773	functions	
45-47	14774-14777	for	
45-48	14778-14781	RDD	
45-49	14782-14785	API	
45-50	14785-14786	.	
45-51	14787-14791	Then	
45-52	14792-14794	in	
45-53	14795-14799	2014	
45-54	14799-14800	,	
45-55	14801-14806	users	
45-56	14807-14810	can	
45-57	14811-14819	register	
45-58	14820-14826	Python	
45-59	14827-14830	UDF	
45-60	14831-14834	for	
45-61	14835-14840	Spark	
45-62	14841-14844	SQL	
45-63	14844-14845	.	
45-64	14846-14854	Starting	
45-65	14855-14859	from	
45-66	14860-14865	Spark	
45-67	14866-14869	2.0	
45-68	14869-14870	,	
45-69	14871-14877	Python	
45-70	14878-14881	UDF	
45-71	14882-14894	registration	
45-72	14895-14897	is	

#Text=session-based. And then next year, users can register the use of Java UDF in Python API. In 2018, we introduced Pandas UDF. In this release, we redesigned the interface for Pandas UDF by using the Python tab hints and added more tabs for the Pandas UDFs. To adjust our compatibility with the old Pandas UDFs from Apache Spark 2.0
46-1	14898-14911	session-based	
46-2	14911-14912	.	
46-3	14913-14916	And	
46-4	14917-14921	then	
46-5	14922-14926	next	
46-6	14927-14931	year	
46-7	14931-14932	,	
46-8	14933-14938	users	
46-9	14939-14942	can	
46-10	14943-14951	register	
46-11	14952-14955	the	
46-12	14956-14959	use	
46-13	14960-14962	of	
46-14	14963-14967	Java	
46-15	14968-14971	UDF	
46-16	14972-14974	in	
46-17	14975-14981	Python	
46-18	14982-14985	API	
46-19	14985-14986	.	
46-20	14987-14989	In	
46-21	14990-14994	2018	
46-22	14994-14995	,	
46-23	14996-14998	we	
46-24	14999-15009	introduced	
46-25	15010-15016	Pandas	
46-26	15017-15020	UDF	
46-27	15020-15021	.	
46-28	15022-15024	In	
46-29	15025-15029	this	
46-30	15030-15037	release	
46-31	15037-15038	,	
46-32	15039-15041	we	
46-33	15042-15052	redesigned	
46-34	15053-15056	the	
46-35	15057-15066	interface	
46-36	15067-15070	for	
46-37	15071-15077	Pandas	
46-38	15078-15081	UDF	
46-39	15082-15084	by	
46-40	15085-15090	using	
46-41	15091-15094	the	
46-42	15095-15101	Python	
46-43	15102-15105	tab	
46-44	15106-15111	hints	
46-45	15112-15115	and	
46-46	15116-15121	added	
46-47	15122-15126	more	
46-48	15127-15131	tabs	
46-49	15132-15135	for	
46-50	15136-15139	the	
46-51	15140-15146	Pandas	
46-52	15147-15151	UDFs	
46-53	15151-15152	.	
46-54	15153-15155	To	
46-55	15156-15162	adjust	
46-56	15163-15166	our	
46-57	15167-15180	compatibility	
46-58	15181-15185	with	
46-59	15186-15189	the	
46-60	15190-15193	old	
46-61	15194-15200	Pandas	
46-62	15201-15205	UDFs	
46-63	15206-15210	from	
46-64	15211-15217	Apache	
46-65	15218-15223	Spark	
46-66	15224-15227	2.0	

#Text=with the Python 2.6 and above, Python [inaudible] such as pandas.Series, Pandas DataFrame, cube hole, and the iterator can be used to impress new Pandas UDF types. For example, in Spark 2.3, we have a Scala UDF. The input is a pandas.Series and its output is also pandas.Series. In Spark
47-1	15228-15232	with	
47-2	15233-15236	the	
47-3	15237-15243	Python	
47-4	15244-15247	2.6	
47-5	15248-15251	and	
47-6	15252-15257	above	
47-7	15257-15258	,	
47-8	15259-15265	Python	
47-9	15266-15267	[	
47-10	15267-15276	inaudible	
47-11	15276-15277	]	
47-12	15278-15282	such	
47-13	15283-15285	as	
47-14	15286-15299	pandas.Series	
47-15	15299-15300	,	
47-16	15301-15307	Pandas	
47-17	15308-15317	DataFrame	
47-18	15317-15318	,	
47-19	15319-15323	cube	
47-20	15324-15328	hole	
47-21	15328-15329	,	
47-22	15330-15333	and	
47-23	15334-15337	the	
47-24	15338-15346	iterator	
47-25	15347-15350	can	
47-26	15351-15353	be	
47-27	15354-15358	used	
47-28	15359-15361	to	
47-29	15362-15369	impress	
47-30	15370-15373	new	
47-31	15374-15380	Pandas	
47-32	15381-15384	UDF	
47-33	15385-15390	types	
47-34	15390-15391	.	
47-35	15392-15395	For	
47-36	15396-15403	example	
47-37	15403-15404	,	
47-38	15405-15407	in	
47-39	15408-15413	Spark	
47-40	15414-15417	2.3	
47-41	15417-15418	,	
47-42	15419-15421	we	
47-43	15422-15426	have	
47-44	15427-15428	a	
47-45	15429-15434	Scala	
47-46	15435-15438	UDF	
47-47	15438-15439	.	
47-48	15440-15443	The	
47-49	15444-15449	input	
47-50	15450-15452	is	
47-51	15453-15454	a	
47-52	15455-15468	pandas.Series	
47-53	15469-15472	and	
47-54	15473-15476	its	
47-55	15477-15483	output	
47-56	15484-15486	is	
47-57	15487-15491	also	
47-58	15492-15505	pandas.Series	
47-59	15505-15506	.	
47-60	15507-15509	In	
47-61	15510-15515	Spark	

#Text=2.0, we do not require users to remember any UDF types. You just need to specify the input and the output types. In Spark 2.3, we also have a Grouped Map Pandas UDF, so input is a Pandas DataFrame, and the output is also Pandas DataFrames. Old vs New Pandas UDF interface
48-1	15516-15519	2.0	
48-2	15519-15520	,	
48-3	15521-15523	we	
48-4	15524-15526	do	
48-5	15527-15530	not	
48-6	15531-15538	require	
48-7	15539-15544	users	
48-8	15545-15547	to	
48-9	15548-15556	remember	
48-10	15557-15560	any	
48-11	15561-15564	UDF	
48-12	15565-15570	types	
48-13	15570-15571	.	
48-14	15572-15575	You	
48-15	15576-15580	just	
48-16	15581-15585	need	
48-17	15586-15588	to	
48-18	15589-15596	specify	
48-19	15597-15600	the	
48-20	15601-15606	input	
48-21	15607-15610	and	
48-22	15611-15614	the	
48-23	15615-15621	output	
48-24	15622-15627	types	
48-25	15627-15628	.	
48-26	15629-15631	In	
48-27	15632-15637	Spark	
48-28	15638-15641	2.3	
48-29	15641-15642	,	
48-30	15643-15645	we	
48-31	15646-15650	also	
48-32	15651-15655	have	
48-33	15656-15657	a	
48-34	15658-15665	Grouped	
48-35	15666-15669	Map	
48-36	15670-15676	Pandas	
48-37	15677-15680	UDF	
48-38	15680-15681	,	
48-39	15682-15684	so	
48-40	15685-15690	input	
48-41	15691-15693	is	
48-42	15694-15695	a	
48-43	15696-15702	Pandas	
48-44	15703-15712	DataFrame	
48-45	15712-15713	,	
48-46	15714-15717	and	
48-47	15718-15721	the	
48-48	15722-15728	output	
48-49	15729-15731	is	
48-50	15732-15736	also	
48-51	15737-15743	Pandas	
48-52	15744-15754	DataFrames	
48-53	15754-15755	.	
48-54	15756-15759	Old	
48-55	15760-15762	vs	
48-56	15763-15766	New	
48-57	15767-15773	Pandas	
48-58	15774-15777	UDF	
48-59	15778-15787	interface	

#Text=This slide shows the difference between the old and the new interface. The same here. The new interface can also be used for the existing Grouped Aggregate Pandas UDFs. In addition, the old Pandas UDF was split into two API categories: Pandas UDFs and Pandas function APIs. You can treat Pandas UDFs in the same way that you use the other
49-1	15788-15792	This	
49-2	15793-15798	slide	
49-3	15799-15804	shows	
49-4	15805-15808	the	
49-5	15809-15819	difference	
49-6	15820-15827	between	
49-7	15828-15831	the	
49-8	15832-15835	old	
49-9	15836-15839	and	
49-10	15840-15843	the	
49-11	15844-15847	new	
49-12	15848-15857	interface	
49-13	15857-15858	.	
49-14	15859-15862	The	
49-15	15863-15867	same	
49-16	15868-15872	here	
49-17	15872-15873	.	
49-18	15874-15877	The	
49-19	15878-15881	new	
49-20	15882-15891	interface	
49-21	15892-15895	can	
49-22	15896-15900	also	
49-23	15901-15903	be	
49-24	15904-15908	used	
49-25	15909-15912	for	
49-26	15913-15916	the	
49-27	15917-15925	existing	
49-28	15926-15933	Grouped	
49-29	15934-15943	Aggregate	
49-30	15944-15950	Pandas	
49-31	15951-15955	UDFs	
49-32	15955-15956	.	
49-33	15957-15959	In	
49-34	15960-15968	addition	
49-35	15968-15969	,	
49-36	15970-15973	the	
49-37	15974-15977	old	
49-38	15978-15984	Pandas	
49-39	15985-15988	UDF	
49-40	15989-15992	was	
49-41	15993-15998	split	
49-42	15999-16003	into	
49-43	16004-16007	two	
49-44	16008-16011	API	
49-45	16012-16022	categories	
49-46	16022-16023	:	
49-47	16024-16030	Pandas	
49-48	16031-16035	UDFs	
49-49	16036-16039	and	
49-50	16040-16046	Pandas	
49-51	16047-16055	function	
49-52	16056-16060	APIs	
49-53	16060-16061	.	
49-54	16062-16065	You	
49-55	16066-16069	can	
49-56	16070-16075	treat	
49-57	16076-16082	Pandas	
49-58	16083-16087	UDFs	
49-59	16088-16090	in	
49-60	16091-16094	the	
49-61	16095-16099	same	
49-62	16100-16103	way	
49-63	16104-16108	that	
49-64	16109-16112	you	
49-65	16113-16116	use	
49-66	16117-16120	the	
49-67	16121-16126	other	

#Text=PySpark column instance. For example, here, calculate the values. You are calling the Pandas UDF calculate. We do support the new Pandas UDF types from iterators of series to iterator other series and from iterators of multiple series to iterator of series.
50-1	16127-16134	PySpark	
50-2	16135-16141	column	
50-3	16142-16150	instance	
50-4	16150-16151	.	
50-5	16152-16155	For	
50-6	16156-16163	example	
50-7	16163-16164	,	
50-8	16165-16169	here	
50-9	16169-16170	,	
50-10	16171-16180	calculate	
50-11	16181-16184	the	
50-12	16185-16191	values	
50-13	16191-16192	.	
50-14	16193-16196	You	
50-15	16197-16200	are	
50-16	16201-16208	calling	
50-17	16209-16212	the	
50-18	16213-16219	Pandas	
50-19	16220-16223	UDF	
50-20	16224-16233	calculate	
50-21	16233-16234	.	
50-22	16235-16237	We	
50-23	16238-16240	do	
50-24	16241-16248	support	
50-25	16249-16252	the	
50-26	16253-16256	new	
50-27	16257-16263	Pandas	
50-28	16264-16267	UDF	
50-29	16268-16273	types	
50-30	16274-16278	from	
50-31	16279-16288	iterators	
50-32	16289-16291	of	
50-33	16292-16298	series	
50-34	16299-16301	to	
50-35	16302-16310	iterator	
50-36	16311-16316	other	
50-37	16317-16323	series	
50-38	16324-16327	and	
50-39	16328-16332	from	
50-40	16333-16342	iterators	
50-41	16343-16345	of	
50-42	16346-16354	multiple	
50-43	16355-16361	series	
50-44	16362-16364	to	
50-45	16365-16373	iterator	
50-46	16374-16376	of	
50-47	16377-16383	series	
50-48	16383-16384	.	

#Text=So this is useful for [inaudible] state initialization of your Pandas UDFs and also useful for Pandas UDF parquet. However, you can now use Pandas function APIs with this column instance. Here are these two examples: map Pandas function API and the core group, the map Pandas UDF, the APIs. These APIs are newly added in these units.
51-1	16385-16387	So	
51-2	16388-16392	this	
51-3	16393-16395	is	
51-4	16396-16402	useful	
51-5	16403-16406	for	
51-6	16407-16408	[	
51-7	16408-16417	inaudible	
51-8	16417-16418	]	
51-9	16419-16424	state	
51-10	16425-16439	initialization	
51-11	16440-16442	of	
51-12	16443-16447	your	
51-13	16448-16454	Pandas	
51-14	16455-16459	UDFs	
51-15	16460-16463	and	
51-16	16464-16468	also	
51-17	16469-16475	useful	
51-18	16476-16479	for	
51-19	16480-16486	Pandas	
51-20	16487-16490	UDF	
51-21	16491-16498	parquet	
51-22	16498-16499	.	
51-23	16500-16507	However	
51-24	16507-16508	,	
51-25	16509-16512	you	
51-26	16513-16516	can	
51-27	16517-16520	now	
51-28	16521-16524	use	
51-29	16525-16531	Pandas	
51-30	16532-16540	function	
51-31	16541-16545	APIs	
51-32	16546-16550	with	
51-33	16551-16555	this	
51-34	16556-16562	column	
51-35	16563-16571	instance	
51-36	16571-16572	.	
51-37	16573-16577	Here	
51-38	16578-16581	are	
51-39	16582-16587	these	
51-40	16588-16591	two	
51-41	16592-16600	examples	
51-42	16600-16601	:	
51-43	16602-16605	map	
51-44	16606-16612	Pandas	
51-45	16613-16621	function	
51-46	16622-16625	API	
51-47	16626-16629	and	
51-48	16630-16633	the	
51-49	16634-16638	core	
51-50	16639-16644	group	
51-51	16644-16645	,	
51-52	16646-16649	the	
51-53	16650-16653	map	
51-54	16654-16660	Pandas	
51-55	16661-16664	UDF	
51-56	16664-16665	,	
51-57	16666-16669	the	
51-58	16670-16674	APIs	
51-59	16674-16675	.	
51-60	16676-16681	These	
51-61	16682-16686	APIs	
51-62	16687-16690	are	
51-63	16691-16696	newly	
51-64	16697-16702	added	
51-65	16703-16705	in	
51-66	16706-16711	these	
51-67	16712-16717	units	
51-68	16717-16718	.	

#Text=Back to Wenchen So next, Wenchen will go over the remaining features and provide a deep dive into accumulator with Scalar. Please welcome Wenchen. Thanks, Xiao, for the first half of the talk. Now, let me take over from here and introduce the remaining Spark 3.0 features.
52-1	16719-16723	Back	
52-2	16724-16726	to	
52-3	16727-16734	Wenchen	
52-4	16735-16737	So	
52-5	16738-16742	next	
52-6	16742-16743	,	
52-7	16744-16751	Wenchen	
52-8	16752-16756	will	
52-9	16757-16759	go	
52-10	16760-16764	over	
52-11	16765-16768	the	
52-12	16769-16778	remaining	
52-13	16779-16787	features	
52-14	16788-16791	and	
52-15	16792-16799	provide	
52-16	16800-16801	a	
52-17	16802-16806	deep	
52-18	16807-16811	dive	
52-19	16812-16816	into	
52-20	16817-16828	accumulator	
52-21	16829-16833	with	
52-22	16834-16840	Scalar	
52-23	16840-16841	.	
52-24	16842-16848	Please	
52-25	16849-16856	welcome	
52-26	16857-16864	Wenchen	
52-27	16864-16865	.	
52-28	16866-16872	Thanks	
52-29	16872-16873	,	
52-30	16874-16878	Xiao	
52-31	16878-16879	,	
52-32	16880-16883	for	
52-33	16884-16887	the	
52-34	16888-16893	first	
52-35	16894-16898	half	
52-36	16899-16901	of	
52-37	16902-16905	the	
52-38	16906-16910	talk	
52-39	16910-16911	.	
52-40	16912-16915	Now	
52-41	16915-16916	,	
52-42	16917-16920	let	
52-43	16921-16923	me	
52-44	16924-16928	take	
52-45	16929-16933	over	
52-46	16934-16938	from	
52-47	16939-16943	here	
52-48	16944-16947	and	
52-49	16948-16957	introduce	
52-50	16958-16961	the	
52-51	16962-16971	remaining	
52-52	16972-16977	Spark	
52-53	16978-16981	3.0	
52-54	16982-16990	features	
52-55	16990-16991	.	

#Text=Accelerator-aware Scheduling I will start with, straight away, our scheduler. In 2018 Spark Summit, we already announced the new project [inaudible]. As you’re now aware, our scheduler is part of this project. It can be widely used for executing
53-1	16992-17009	Accelerator-aware	
53-2	17010-17020	Scheduling	
53-3	17021-17022	I	
53-4	17023-17027	will	
53-5	17028-17033	start	
53-6	17034-17038	with	
53-7	17038-17039	,	
53-8	17040-17048	straight	
53-9	17049-17053	away	
53-10	17053-17054	,	
53-11	17055-17058	our	
53-12	17059-17068	scheduler	
53-13	17068-17069	.	
53-14	17070-17072	In	
53-15	17073-17077	2018	
53-16	17078-17083	Spark	
53-17	17084-17090	Summit	
53-18	17090-17091	,	
53-19	17092-17094	we	
53-20	17095-17102	already	
53-21	17103-17112	announced	
53-22	17113-17116	the	
53-23	17117-17120	new	
53-24	17121-17128	project	
53-25	17129-17130	[	
53-26	17130-17139	inaudible	
53-27	17139-17140	]	
53-28	17140-17141	.	
53-29	17142-17144	As	
53-30	17145-17148	you	
53-31	17148-17149	’	
53-32	17149-17151	re	
53-33	17152-17155	now	
53-34	17156-17161	aware	
53-35	17161-17162	,	
53-36	17163-17166	our	
53-37	17167-17176	scheduler	
53-38	17177-17179	is	
53-39	17180-17184	part	
53-40	17185-17187	of	
53-41	17188-17192	this	
53-42	17193-17200	project	
53-43	17200-17201	.	
53-44	17202-17204	It	
53-45	17205-17208	can	
53-46	17209-17211	be	
53-47	17212-17218	widely	
53-48	17219-17223	used	
53-49	17224-17227	for	
53-50	17228-17237	executing	

#Text=special workloads. In this release, we support standalone, YARN, and Kubernetes scheduler scheduler backend. So far, users need to specify the require resources using a [inaudible] configs. In the future, we will support the job, stage, and task levels. To further understand this feature, let’s look at the workflow. Ideally, the cost manager should be able to
54-1	17238-17245	special	
54-2	17246-17255	workloads	
54-3	17255-17256	.	
54-4	17257-17259	In	
54-5	17260-17264	this	
54-6	17265-17272	release	
54-7	17272-17273	,	
54-8	17274-17276	we	
54-9	17277-17284	support	
54-10	17285-17295	standalone	
54-11	17295-17296	,	
54-12	17297-17301	YARN	
54-13	17301-17302	,	
54-14	17303-17306	and	
54-15	17307-17317	Kubernetes	
54-16	17318-17327	scheduler	
54-17	17328-17337	scheduler	
54-18	17338-17345	backend	
54-19	17345-17346	.	
54-20	17347-17349	So	
54-21	17350-17353	far	
54-22	17353-17354	,	
54-23	17355-17360	users	
54-24	17361-17365	need	
54-25	17366-17368	to	
54-26	17369-17376	specify	
54-27	17377-17380	the	
54-28	17381-17388	require	
54-29	17389-17398	resources	
54-30	17399-17404	using	
54-31	17405-17406	a	
54-32	17407-17408	[	
54-33	17408-17417	inaudible	
54-34	17417-17418	]	
54-35	17419-17426	configs	
54-36	17426-17427	.	
54-37	17428-17430	In	
54-38	17431-17434	the	
54-39	17435-17441	future	
54-40	17441-17442	,	
54-41	17443-17445	we	
54-42	17446-17450	will	
54-43	17451-17458	support	
54-44	17459-17462	the	
54-45	17463-17466	job	
54-46	17466-17467	,	
54-47	17468-17473	stage	
54-48	17473-17474	,	
54-49	17475-17478	and	
54-50	17479-17483	task	
54-51	17484-17490	levels	
54-52	17490-17491	.	
54-53	17492-17494	To	
54-54	17495-17502	further	
54-55	17503-17513	understand	
54-56	17514-17518	this	
54-57	17519-17526	feature	
54-58	17526-17527	,	
54-59	17528-17531	let	
54-60	17531-17532	’	
54-61	17532-17533	s	
54-62	17534-17538	look	
54-63	17539-17541	at	
54-64	17542-17545	the	
54-65	17546-17554	workflow	
54-66	17554-17555	.	
54-67	17556-17563	Ideally	
54-68	17563-17564	,	
54-69	17565-17568	the	
54-70	17569-17573	cost	
54-71	17574-17581	manager	
54-72	17582-17588	should	
54-73	17589-17591	be	
54-74	17592-17596	able	
54-75	17597-17599	to	

#Text=automatically discover resources, like GPUs. When the user submits an application with resource request, Spark should pass the resources request to a cluster manager and then the cluster manager cooperates to allocate and launch executors with the required resources. After Spark job is submitted, Spark should schedule tasks on available executors, and the cluster
55-1	17600-17613	automatically	
55-2	17614-17622	discover	
55-3	17623-17632	resources	
55-4	17632-17633	,	
55-5	17634-17638	like	
55-6	17639-17643	GPUs	
55-7	17643-17644	.	
55-8	17645-17649	When	
55-9	17650-17653	the	
55-10	17654-17658	user	
55-11	17659-17666	submits	
55-12	17667-17669	an	
55-13	17670-17681	application	
55-14	17682-17686	with	
55-15	17687-17695	resource	
55-16	17696-17703	request	
55-17	17703-17704	,	
55-18	17705-17710	Spark	
55-19	17711-17717	should	
55-20	17718-17722	pass	
55-21	17723-17726	the	
55-22	17727-17736	resources	
55-23	17737-17744	request	
55-24	17745-17747	to	
55-25	17748-17749	a	
55-26	17750-17757	cluster	
55-27	17758-17765	manager	
55-28	17766-17769	and	
55-29	17770-17774	then	
55-30	17775-17778	the	
55-31	17779-17786	cluster	
55-32	17787-17794	manager	
55-33	17795-17805	cooperates	
55-34	17806-17808	to	
55-35	17809-17817	allocate	
55-36	17818-17821	and	
55-37	17822-17828	launch	
55-38	17829-17838	executors	
55-39	17839-17843	with	
55-40	17844-17847	the	
55-41	17848-17856	required	
55-42	17857-17866	resources	
55-43	17866-17867	.	
55-44	17868-17873	After	
55-45	17874-17879	Spark	
55-46	17880-17883	job	
55-47	17884-17886	is	
55-48	17887-17896	submitted	
55-49	17896-17897	,	
55-50	17898-17903	Spark	
55-51	17904-17910	should	
55-52	17911-17919	schedule	
55-53	17920-17925	tasks	
55-54	17926-17928	on	
55-55	17929-17938	available	
55-56	17939-17948	executors	
55-57	17948-17949	,	
55-58	17950-17953	and	
55-59	17954-17957	the	
55-60	17958-17965	cluster	

#Text=manager should track the results usage and perform dynamic resource allocation. For example, when there are too many pending tasks, the cluster manager should allocate more executors to run more tasks at the same time. When a task is running, the user shall be able to retrieve the assigned resources and use them in their code. In the meanwhile, cluster manager
56-1	17966-17973	manager	
56-2	17974-17980	should	
56-3	17981-17986	track	
56-4	17987-17990	the	
56-5	17991-17998	results	
56-6	17999-18004	usage	
56-7	18005-18008	and	
56-8	18009-18016	perform	
56-9	18017-18024	dynamic	
56-10	18025-18033	resource	
56-11	18034-18044	allocation	
56-12	18044-18045	.	
56-13	18046-18049	For	
56-14	18050-18057	example	
56-15	18057-18058	,	
56-16	18059-18063	when	
56-17	18064-18069	there	
56-18	18070-18073	are	
56-19	18074-18077	too	
56-20	18078-18082	many	
56-21	18083-18090	pending	
56-22	18091-18096	tasks	
56-23	18096-18097	,	
56-24	18098-18101	the	
56-25	18102-18109	cluster	
56-26	18110-18117	manager	
56-27	18118-18124	should	
56-28	18125-18133	allocate	
56-29	18134-18138	more	
56-30	18139-18148	executors	
56-31	18149-18151	to	
56-32	18152-18155	run	
56-33	18156-18160	more	
56-34	18161-18166	tasks	
56-35	18167-18169	at	
56-36	18170-18173	the	
56-37	18174-18178	same	
56-38	18179-18183	time	
56-39	18183-18184	.	
56-40	18185-18189	When	
56-41	18190-18191	a	
56-42	18192-18196	task	
56-43	18197-18199	is	
56-44	18200-18207	running	
56-45	18207-18208	,	
56-46	18209-18212	the	
56-47	18213-18217	user	
56-48	18218-18223	shall	
56-49	18224-18226	be	
56-50	18227-18231	able	
56-51	18232-18234	to	
56-52	18235-18243	retrieve	
56-53	18244-18247	the	
56-54	18248-18256	assigned	
56-55	18257-18266	resources	
56-56	18267-18270	and	
56-57	18271-18274	use	
56-58	18275-18279	them	
56-59	18280-18282	in	
56-60	18283-18288	their	
56-61	18289-18293	code	
56-62	18293-18294	.	
56-63	18295-18297	In	
56-64	18298-18301	the	
56-65	18302-18311	meanwhile	
56-66	18311-18312	,	
56-67	18313-18320	cluster	
56-68	18321-18328	manager	

#Text=shall monitor and recover failed executions. Now, let’s look at how can a cluster manager discover resources and how can users request resources. As an admin of the cluster, I can specify a script to auto discover executors. The discovery script can be specified separately on Java as executors.
57-1	18329-18334	shall	
57-2	18335-18342	monitor	
57-3	18343-18346	and	
57-4	18347-18354	recover	
57-5	18355-18361	failed	
57-6	18362-18372	executions	
57-7	18372-18373	.	
57-8	18374-18377	Now	
57-9	18377-18378	,	
57-10	18379-18382	let	
57-11	18382-18383	’	
57-12	18383-18384	s	
57-13	18385-18389	look	
57-14	18390-18392	at	
57-15	18393-18396	how	
57-16	18397-18400	can	
57-17	18401-18402	a	
57-18	18403-18410	cluster	
57-19	18411-18418	manager	
57-20	18419-18427	discover	
57-21	18428-18437	resources	
57-22	18438-18441	and	
57-23	18442-18445	how	
57-24	18446-18449	can	
57-25	18450-18455	users	
57-26	18456-18463	request	
57-27	18464-18473	resources	
57-28	18473-18474	.	
57-29	18475-18477	As	
57-30	18478-18480	an	
57-31	18481-18486	admin	
57-32	18487-18489	of	
57-33	18490-18493	the	
57-34	18494-18501	cluster	
57-35	18501-18502	,	
57-36	18503-18504	I	
57-37	18505-18508	can	
57-38	18509-18516	specify	
57-39	18517-18518	a	
57-40	18519-18525	script	
57-41	18526-18528	to	
57-42	18529-18533	auto	
57-43	18534-18542	discover	
57-44	18543-18552	executors	
57-45	18552-18553	.	
57-46	18554-18557	The	
57-47	18558-18567	discovery	
57-48	18568-18574	script	
57-49	18575-18578	can	
57-50	18579-18581	be	
57-51	18582-18591	specified	
57-52	18592-18602	separately	
57-53	18603-18605	on	
57-54	18606-18610	Java	
57-55	18611-18613	as	
57-56	18614-18623	executors	
57-57	18623-18624	.	

#Text=We also provided an example to auto discover Nvidia GPU resources. You can adjust this example script for other kinds of resources. Then as a user of Spark, I can request resources at the application level. I can use the config spark.executor.resource.{resourceName}.amount
58-1	18625-18627	We	
58-2	18628-18632	also	
58-3	18633-18641	provided	
58-4	18642-18644	an	
58-5	18645-18652	example	
58-6	18653-18655	to	
58-7	18656-18660	auto	
58-8	18661-18669	discover	
58-9	18670-18676	Nvidia	
58-10	18677-18680	GPU	
58-11	18681-18690	resources	
58-12	18690-18691	.	
58-13	18692-18695	You	
58-14	18696-18699	can	
58-15	18700-18706	adjust	
58-16	18707-18711	this	
58-17	18712-18719	example	
58-18	18720-18726	script	
58-19	18727-18730	for	
58-20	18731-18736	other	
58-21	18737-18742	kinds	
58-22	18743-18745	of	
58-23	18746-18755	resources	
58-24	18755-18756	.	
58-25	18757-18761	Then	
58-26	18762-18764	as	
58-27	18765-18766	a	
58-28	18767-18771	user	
58-29	18772-18774	of	
58-30	18775-18780	Spark	
58-31	18780-18781	,	
58-32	18782-18783	I	
58-33	18784-18787	can	
58-34	18788-18795	request	
58-35	18796-18805	resources	
58-36	18806-18808	at	
58-37	18809-18812	the	
58-38	18813-18824	application	
58-39	18825-18830	level	
58-40	18830-18831	.	
58-41	18832-18833	I	
58-42	18834-18837	can	
58-43	18838-18841	use	
58-44	18842-18845	the	
58-45	18846-18852	config	
58-46	18853-18876	spark.executor.resource	
58-47	18876-18877	.	
58-48	18877-18878	{	
58-49	18878-18890	resourceName	
58-50	18890-18891	}	
58-51	18891-18892	.	
58-52	18892-18898	amount	

#Text=and the corresponding config for Java to specify the executors amount on the Java and executors. Also, I can use the config spark.task.resource.{resourceName}.amount to specify the executors required by each task. As I mentioned earlier, we will support more time-proven labor later, like the job or stage labor. Please stay tuned.
59-1	18899-18902	and	
59-2	18903-18906	the	
59-3	18907-18920	corresponding	
59-4	18921-18927	config	
59-5	18928-18931	for	
59-6	18932-18936	Java	
59-7	18937-18939	to	
59-8	18940-18947	specify	
59-9	18948-18951	the	
59-10	18952-18961	executors	
59-11	18962-18968	amount	
59-12	18969-18971	on	
59-13	18972-18975	the	
59-14	18976-18980	Java	
59-15	18981-18984	and	
59-16	18985-18994	executors	
59-17	18994-18995	.	
59-18	18996-19000	Also	
59-19	19000-19001	,	
59-20	19002-19003	I	
59-21	19004-19007	can	
59-22	19008-19011	use	
59-23	19012-19015	the	
59-24	19016-19022	config	
59-25	19023-19042	spark.task.resource	
59-26	19042-19043	.	
59-27	19043-19044	{	
59-28	19044-19056	resourceName	
59-29	19056-19057	}	
59-30	19057-19058	.	
59-31	19058-19064	amount	
59-32	19065-19067	to	
59-33	19068-19075	specify	
59-34	19076-19079	the	
59-35	19080-19089	executors	
59-36	19090-19098	required	
59-37	19099-19101	by	
59-38	19102-19106	each	
59-39	19107-19111	task	
59-40	19111-19112	.	
59-41	19113-19115	As	
59-42	19116-19117	I	
59-43	19118-19127	mentioned	
59-44	19128-19135	earlier	
59-45	19135-19136	,	
59-46	19137-19139	we	
59-47	19140-19144	will	
59-48	19145-19152	support	
59-49	19153-19157	more	
59-50	19158-19169	time-proven	
59-51	19170-19175	labor	
59-52	19176-19181	later	
59-53	19181-19182	,	
59-54	19183-19187	like	
59-55	19188-19191	the	
59-56	19192-19195	job	
59-57	19196-19198	or	
59-58	19199-19204	stage	
59-59	19205-19210	labor	
59-60	19210-19211	.	
59-61	19212-19218	Please	
59-62	19219-19223	stay	
59-63	19224-19229	tuned	
59-64	19229-19230	.	

#Text=Retrieve Assigned Accelerators Next, we’ll see how you can leverage the assigned executors to actually execute your workloads, which is probably the most important part to the users. So as a user of Spark, I can retrieve the assigned executors from the task content. Here is an
60-1	19231-19239	Retrieve	
60-2	19240-19248	Assigned	
60-3	19249-19261	Accelerators	
60-4	19262-19266	Next	
60-5	19266-19267	,	
60-6	19268-19270	we	
60-7	19270-19271	’	
60-8	19271-19273	ll	
60-9	19274-19277	see	
60-10	19278-19281	how	
60-11	19282-19285	you	
60-12	19286-19289	can	
60-13	19290-19298	leverage	
60-14	19299-19302	the	
60-15	19303-19311	assigned	
60-16	19312-19321	executors	
60-17	19322-19324	to	
60-18	19325-19333	actually	
60-19	19334-19341	execute	
60-20	19342-19346	your	
60-21	19347-19356	workloads	
60-22	19356-19357	,	
60-23	19358-19363	which	
60-24	19364-19366	is	
60-25	19367-19375	probably	
60-26	19376-19379	the	
60-27	19380-19384	most	
60-28	19385-19394	important	
60-29	19395-19399	part	
60-30	19400-19402	to	
60-31	19403-19406	the	
60-32	19407-19412	users	
60-33	19412-19413	.	
60-34	19414-19416	So	
60-35	19417-19419	as	
60-36	19420-19421	a	
60-37	19422-19426	user	
60-38	19427-19429	of	
60-39	19430-19435	Spark	
60-40	19435-19436	,	
60-41	19437-19438	I	
60-42	19439-19442	can	
60-43	19443-19451	retrieve	
60-44	19452-19455	the	
60-45	19456-19464	assigned	
60-46	19465-19474	executors	
60-47	19475-19479	from	
60-48	19480-19483	the	
60-49	19484-19488	task	
60-50	19489-19496	content	
60-51	19496-19497	.	
60-52	19498-19502	Here	
60-53	19503-19505	is	
60-54	19506-19508	an	

#Text=example in PySpark. The contents of resources returns a map from the resource name to resource info. In the example, we request for GPUs, and we can take the GPU address from the resource map. Then we launch the TensorFlow to train my model within GPUs. Spark will take care
61-1	19509-19516	example	
61-2	19517-19519	in	
61-3	19520-19527	PySpark	
61-4	19527-19528	.	
61-5	19529-19532	The	
61-6	19533-19541	contents	
61-7	19542-19544	of	
61-8	19545-19554	resources	
61-9	19555-19562	returns	
61-10	19563-19564	a	
61-11	19565-19568	map	
61-12	19569-19573	from	
61-13	19574-19577	the	
61-14	19578-19586	resource	
61-15	19587-19591	name	
61-16	19592-19594	to	
61-17	19595-19603	resource	
61-18	19604-19608	info	
61-19	19608-19609	.	
61-20	19610-19612	In	
61-21	19613-19616	the	
61-22	19617-19624	example	
61-23	19624-19625	,	
61-24	19626-19628	we	
61-25	19629-19636	request	
61-26	19637-19640	for	
61-27	19641-19645	GPUs	
61-28	19645-19646	,	
61-29	19647-19650	and	
61-30	19651-19653	we	
61-31	19654-19657	can	
61-32	19658-19662	take	
61-33	19663-19666	the	
61-34	19667-19670	GPU	
61-35	19671-19678	address	
61-36	19679-19683	from	
61-37	19684-19687	the	
61-38	19688-19696	resource	
61-39	19697-19700	map	
61-40	19700-19701	.	
61-41	19702-19706	Then	
61-42	19707-19709	we	
61-43	19710-19716	launch	
61-44	19717-19720	the	
61-45	19721-19731	TensorFlow	
61-46	19732-19734	to	
61-47	19735-19740	train	
61-48	19741-19743	my	
61-49	19744-19749	model	
61-50	19750-19756	within	
61-51	19757-19761	GPUs	
61-52	19761-19762	.	
61-53	19763-19768	Spark	
61-54	19769-19773	will	
61-55	19774-19778	take	
61-56	19779-19783	care	

#Text=of the resource allocation and acceleration and also monitor the executors here for failure recovery, which makes my life much easier. Cluster Manager Support As I mentioned earlier, the executor aware of scheduling support has been added to standalone, YARN, and Kubernetes cost manager. You can check the Spark JIRA tickets to see more details. Unfortunately, the Mesos support is still
62-1	19784-19786	of	
62-2	19787-19790	the	
62-3	19791-19799	resource	
62-4	19800-19810	allocation	
62-5	19811-19814	and	
62-6	19815-19827	acceleration	
62-7	19828-19831	and	
62-8	19832-19836	also	
62-9	19837-19844	monitor	
62-10	19845-19848	the	
62-11	19849-19858	executors	
62-12	19859-19863	here	
62-13	19864-19867	for	
62-14	19868-19875	failure	
62-15	19876-19884	recovery	
62-16	19884-19885	,	
62-17	19886-19891	which	
62-18	19892-19897	makes	
62-19	19898-19900	my	
62-20	19901-19905	life	
62-21	19906-19910	much	
62-22	19911-19917	easier	
62-23	19917-19918	.	
62-24	19919-19926	Cluster	
62-25	19927-19934	Manager	
62-26	19935-19942	Support	
62-27	19943-19945	As	
62-28	19946-19947	I	
62-29	19948-19957	mentioned	
62-30	19958-19965	earlier	
62-31	19965-19966	,	
62-32	19967-19970	the	
62-33	19971-19979	executor	
62-34	19980-19985	aware	
62-35	19986-19988	of	
62-36	19989-19999	scheduling	
62-37	20000-20007	support	
62-38	20008-20011	has	
62-39	20012-20016	been	
62-40	20017-20022	added	
62-41	20023-20025	to	
62-42	20026-20036	standalone	
62-43	20036-20037	,	
62-44	20038-20042	YARN	
62-45	20042-20043	,	
62-46	20044-20047	and	
62-47	20048-20058	Kubernetes	
62-48	20059-20063	cost	
62-49	20064-20071	manager	
62-50	20071-20072	.	
62-51	20073-20076	You	
62-52	20077-20080	can	
62-53	20081-20086	check	
62-54	20087-20090	the	
62-55	20091-20096	Spark	
62-56	20097-20101	JIRA	
62-57	20102-20109	tickets	
62-58	20110-20112	to	
62-59	20113-20116	see	
62-60	20117-20121	more	
62-61	20122-20129	details	
62-62	20129-20130	.	
62-63	20131-20144	Unfortunately	
62-64	20144-20145	,	
62-65	20146-20149	the	
62-66	20150-20155	Mesos	
62-67	20156-20163	support	
62-68	20164-20166	is	
62-69	20167-20172	still	

#Text=not available. We’d really appreciate it if any Mesos expert has interest and is willing to help the Spark community to add the Mesos support. Please leave a comment in the [inaudible] if you want to work on it. Thanks in advance. Improved Spark Web UI for Accelerators
63-1	20173-20176	not	
63-2	20177-20186	available	
63-3	20186-20187	.	
63-4	20188-20190	We	
63-5	20190-20191	’	
63-6	20191-20192	d	
63-7	20193-20199	really	
63-8	20200-20210	appreciate	
63-9	20211-20213	it	
63-10	20214-20216	if	
63-11	20217-20220	any	
63-12	20221-20226	Mesos	
63-13	20227-20233	expert	
63-14	20234-20237	has	
63-15	20238-20246	interest	
63-16	20247-20250	and	
63-17	20251-20253	is	
63-18	20254-20261	willing	
63-19	20262-20264	to	
63-20	20265-20269	help	
63-21	20270-20273	the	
63-22	20274-20279	Spark	
63-23	20280-20289	community	
63-24	20290-20292	to	
63-25	20293-20296	add	
63-26	20297-20300	the	
63-27	20301-20306	Mesos	
63-28	20307-20314	support	
63-29	20314-20315	.	
63-30	20316-20322	Please	
63-31	20323-20328	leave	
63-32	20329-20330	a	
63-33	20331-20338	comment	
63-34	20339-20341	in	
63-35	20342-20345	the	
63-36	20346-20347	[	
63-37	20347-20356	inaudible	
63-38	20356-20357	]	
63-39	20358-20360	if	
63-40	20361-20364	you	
63-41	20365-20369	want	
63-42	20370-20372	to	
63-43	20373-20377	work	
63-44	20378-20380	on	
63-45	20381-20383	it	
63-46	20383-20384	.	
63-47	20385-20391	Thanks	
63-48	20392-20394	in	
63-49	20395-20402	advance	
63-50	20402-20403	.	
63-51	20404-20412	Improved	
63-52	20413-20418	Spark	
63-53	20419-20422	Web	
63-54	20423-20425	UI	
63-55	20426-20429	for	
63-56	20430-20442	Accelerators	

#Text=Last but not the least, we also improved the Spark Web UI to show all the discovery resources on the executor page. In this page, we can see that there are GPUs available on the executor one. You can check the Web UI to see how many executors are available in the cluster, so you can better schedule your jobs.
64-1	20443-20447	Last	
64-2	20448-20451	but	
64-3	20452-20455	not	
64-4	20456-20459	the	
64-5	20460-20465	least	
64-6	20465-20466	,	
64-7	20467-20469	we	
64-8	20470-20474	also	
64-9	20475-20483	improved	
64-10	20484-20487	the	
64-11	20488-20493	Spark	
64-12	20494-20497	Web	
64-13	20498-20500	UI	
64-14	20501-20503	to	
64-15	20504-20508	show	
64-16	20509-20512	all	
64-17	20513-20516	the	
64-18	20517-20526	discovery	
64-19	20527-20536	resources	
64-20	20537-20539	on	
64-21	20540-20543	the	
64-22	20544-20552	executor	
64-23	20553-20557	page	
64-24	20557-20558	.	
64-25	20559-20561	In	
64-26	20562-20566	this	
64-27	20567-20571	page	
64-28	20571-20572	,	
64-29	20573-20575	we	
64-30	20576-20579	can	
64-31	20580-20583	see	
64-32	20584-20588	that	
64-33	20589-20594	there	
64-34	20595-20598	are	
64-35	20599-20603	GPUs	
64-36	20604-20613	available	
64-37	20614-20616	on	
64-38	20617-20620	the	
64-39	20621-20629	executor	
64-40	20630-20633	one	
64-41	20633-20634	.	
64-42	20635-20638	You	
64-43	20639-20642	can	
64-44	20643-20648	check	
64-45	20649-20652	the	
64-46	20653-20656	Web	
64-47	20657-20659	UI	
64-48	20660-20662	to	
64-49	20663-20666	see	
64-50	20667-20670	how	
64-51	20671-20675	many	
64-52	20676-20685	executors	
64-53	20686-20689	are	
64-54	20690-20699	available	
64-55	20700-20702	in	
64-56	20703-20706	the	
64-57	20707-20714	cluster	
64-58	20714-20715	,	
64-59	20716-20718	so	
64-60	20719-20722	you	
64-61	20723-20726	can	
64-62	20727-20733	better	
64-63	20734-20742	schedule	
64-64	20743-20747	your	
64-65	20748-20752	jobs	
64-66	20752-20753	.	

#Text=32 New Built-in Functions In this release, we also introduced 32 new built-in functions and add high auto functions in the Scalar API. The Spark community pays a lot of attention to compatibility. We have investigated many other ecosystems, like the PostgreSQL, and implemented many commonly used functions in Spark.
65-1	20754-20756	32	
65-2	20757-20760	New	
65-3	20761-20769	Built-in	
65-4	20770-20779	Functions	
65-5	20780-20782	In	
65-6	20783-20787	this	
65-7	20788-20795	release	
65-8	20795-20796	,	
65-9	20797-20799	we	
65-10	20800-20804	also	
65-11	20805-20815	introduced	
65-12	20816-20818	32	
65-13	20819-20822	new	
65-14	20823-20831	built-in	
65-15	20832-20841	functions	
65-16	20842-20845	and	
65-17	20846-20849	add	
65-18	20850-20854	high	
65-19	20855-20859	auto	
65-20	20860-20869	functions	
65-21	20870-20872	in	
65-22	20873-20876	the	
65-23	20877-20883	Scalar	
65-24	20884-20887	API	
65-25	20887-20888	.	
65-26	20889-20892	The	
65-27	20893-20898	Spark	
65-28	20899-20908	community	
65-29	20909-20913	pays	
65-30	20914-20915	a	
65-31	20916-20919	lot	
65-32	20920-20922	of	
65-33	20923-20932	attention	
65-34	20933-20935	to	
65-35	20936-20949	compatibility	
65-36	20949-20950	.	
65-37	20951-20953	We	
65-38	20954-20958	have	
65-39	20959-20971	investigated	
65-40	20972-20976	many	
65-41	20977-20982	other	
65-42	20983-20993	ecosystems	
65-43	20993-20994	,	
65-44	20995-20999	like	
65-45	21000-21003	the	
65-46	21004-21014	PostgreSQL	
65-47	21014-21015	,	
65-48	21016-21019	and	
65-49	21020-21031	implemented	
65-50	21032-21036	many	
65-51	21037-21045	commonly	
65-52	21046-21050	used	
65-53	21051-21060	functions	
65-54	21061-21063	in	
65-55	21064-21069	Spark	
65-56	21069-21070	.	

#Text=Hopefully, these new built-in functions can make it faster to build your queries as you don’t need to waste time to learn a lot of UDFs. Due to the time limitations, I can’t go over all the functions here, so let me just introduce some map type functions as an example.
66-1	21071-21080	Hopefully	
66-2	21080-21081	,	
66-3	21082-21087	these	
66-4	21088-21091	new	
66-5	21092-21100	built-in	
66-6	21101-21110	functions	
66-7	21111-21114	can	
66-8	21115-21119	make	
66-9	21120-21122	it	
66-10	21123-21129	faster	
66-11	21130-21132	to	
66-12	21133-21138	build	
66-13	21139-21143	your	
66-14	21144-21151	queries	
66-15	21152-21154	as	
66-16	21155-21158	you	
66-17	21159-21162	don	
66-18	21162-21163	’	
66-19	21163-21164	t	
66-20	21165-21169	need	
66-21	21170-21172	to	
66-22	21173-21178	waste	
66-23	21179-21183	time	
66-24	21184-21186	to	
66-25	21187-21192	learn	
66-26	21193-21194	a	
66-27	21195-21198	lot	
66-28	21199-21201	of	
66-29	21202-21206	UDFs	
66-30	21206-21207	.	
66-31	21208-21211	Due	
66-32	21212-21214	to	
66-33	21215-21218	the	
66-34	21219-21223	time	
66-35	21224-21235	limitations	
66-36	21235-21236	,	
66-37	21237-21238	I	
66-38	21239-21242	can	
66-39	21242-21243	’	
66-40	21243-21244	t	
66-41	21245-21247	go	
66-42	21248-21252	over	
66-43	21253-21256	all	
66-44	21257-21260	the	
66-45	21261-21270	functions	
66-46	21271-21275	here	
66-47	21275-21276	,	
66-48	21277-21279	so	
66-49	21280-21283	let	
66-50	21284-21286	me	
66-51	21287-21291	just	
66-52	21292-21301	introduce	
66-53	21302-21306	some	
66-54	21307-21310	map	
66-55	21311-21315	type	
66-56	21316-21325	functions	
66-57	21326-21328	as	
66-58	21329-21331	an	
66-59	21332-21339	example	
66-60	21339-21340	.	

#Text=When you deal with map type values, it’s common to get the keys and values for the map as an array. There are two functions, map keys and map values can do this for you. The example is from the Databricks runtime notebook. Or you may want to do something more complicated, like creating a new map by transforming the original map
67-1	21341-21345	When	
67-2	21346-21349	you	
67-3	21350-21354	deal	
67-4	21355-21359	with	
67-5	21360-21363	map	
67-6	21364-21368	type	
67-7	21369-21375	values	
67-8	21375-21376	,	
67-9	21377-21379	it	
67-10	21379-21380	’	
67-11	21380-21381	s	
67-12	21382-21388	common	
67-13	21389-21391	to	
67-14	21392-21395	get	
67-15	21396-21399	the	
67-16	21400-21404	keys	
67-17	21405-21408	and	
67-18	21409-21415	values	
67-19	21416-21419	for	
67-20	21420-21423	the	
67-21	21424-21427	map	
67-22	21428-21430	as	
67-23	21431-21433	an	
67-24	21434-21439	array	
67-25	21439-21440	.	
67-26	21441-21446	There	
67-27	21447-21450	are	
67-28	21451-21454	two	
67-29	21455-21464	functions	
67-30	21464-21465	,	
67-31	21466-21469	map	
67-32	21470-21474	keys	
67-33	21475-21478	and	
67-34	21479-21482	map	
67-35	21483-21489	values	
67-36	21490-21493	can	
67-37	21494-21496	do	
67-38	21497-21501	this	
67-39	21502-21505	for	
67-40	21506-21509	you	
67-41	21509-21510	.	
67-42	21511-21514	The	
67-43	21515-21522	example	
67-44	21523-21525	is	
67-45	21526-21530	from	
67-46	21531-21534	the	
67-47	21535-21545	Databricks	
67-48	21546-21553	runtime	
67-49	21554-21562	notebook	
67-50	21562-21563	.	
67-51	21564-21566	Or	
67-52	21567-21570	you	
67-53	21571-21574	may	
67-54	21575-21579	want	
67-55	21580-21582	to	
67-56	21583-21585	do	
67-57	21586-21595	something	
67-58	21596-21600	more	
67-59	21601-21612	complicated	
67-60	21612-21613	,	
67-61	21614-21618	like	
67-62	21619-21627	creating	
67-63	21628-21629	a	
67-64	21630-21633	new	
67-65	21634-21637	map	
67-66	21638-21640	by	
67-67	21641-21653	transforming	
67-68	21654-21657	the	
67-69	21658-21666	original	
67-70	21667-21670	map	

#Text=where it’s a keys and a map values functions. So if there are two functions, transform keys and transform values can do this for you, and you just need to write a handler function to specify the transformation logic. As I mentioned earlier, the functions also have Scalar APIs rather than the SQL API. Here is an example about how to do the
68-1	21671-21676	where	
68-2	21677-21679	it	
68-3	21679-21680	’	
68-4	21680-21681	s	
68-5	21682-21683	a	
68-6	21684-21688	keys	
68-7	21689-21692	and	
68-8	21693-21694	a	
68-9	21695-21698	map	
68-10	21699-21705	values	
68-11	21706-21715	functions	
68-12	21715-21716	.	
68-13	21717-21719	So	
68-14	21720-21722	if	
68-15	21723-21728	there	
68-16	21729-21732	are	
68-17	21733-21736	two	
68-18	21737-21746	functions	
68-19	21746-21747	,	
68-20	21748-21757	transform	
68-21	21758-21762	keys	
68-22	21763-21766	and	
68-23	21767-21776	transform	
68-24	21777-21783	values	
68-25	21784-21787	can	
68-26	21788-21790	do	
68-27	21791-21795	this	
68-28	21796-21799	for	
68-29	21800-21803	you	
68-30	21803-21804	,	
68-31	21805-21808	and	
68-32	21809-21812	you	
68-33	21813-21817	just	
68-34	21818-21822	need	
68-35	21823-21825	to	
68-36	21826-21831	write	
68-37	21832-21833	a	
68-38	21834-21841	handler	
68-39	21842-21850	function	
68-40	21851-21853	to	
68-41	21854-21861	specify	
68-42	21862-21865	the	
68-43	21866-21880	transformation	
68-44	21881-21886	logic	
68-45	21886-21887	.	
68-46	21888-21890	As	
68-47	21891-21892	I	
68-48	21893-21902	mentioned	
68-49	21903-21910	earlier	
68-50	21910-21911	,	
68-51	21912-21915	the	
68-52	21916-21925	functions	
68-53	21926-21930	also	
68-54	21931-21935	have	
68-55	21936-21942	Scalar	
68-56	21943-21947	APIs	
68-57	21948-21954	rather	
68-58	21955-21959	than	
68-59	21960-21963	the	
68-60	21964-21967	SQL	
68-61	21968-21971	API	
68-62	21971-21972	.	
68-63	21973-21977	Here	
68-64	21978-21980	is	
68-65	21981-21983	an	
68-66	21984-21991	example	
68-67	21992-21997	about	
68-68	21998-22001	how	
68-69	22002-22004	to	
68-70	22005-22007	do	
68-71	22008-22011	the	

#Text=same thing, but it’s a Scalar API. You can just write a normal Scala function, which takes the [kernel?] objects as the input to have the same effect as the SQL API. Monitoring and Debuggability This release also includes many enhancements and makes the monitoring more comprehensive and stable. We can make it easier to close out and get back to your
69-1	22012-22016	same	
69-2	22017-22022	thing	
69-3	22022-22023	,	
69-4	22024-22027	but	
69-5	22028-22030	it	
69-6	22030-22031	’	
69-7	22031-22032	s	
69-8	22033-22034	a	
69-9	22035-22041	Scalar	
69-10	22042-22045	API	
69-11	22045-22046	.	
69-12	22047-22050	You	
69-13	22051-22054	can	
69-14	22055-22059	just	
69-15	22060-22065	write	
69-16	22066-22067	a	
69-17	22068-22074	normal	
69-18	22075-22080	Scala	
69-19	22081-22089	function	
69-20	22089-22090	,	
69-21	22091-22096	which	
69-22	22097-22102	takes	
69-23	22103-22106	the	
69-24	22107-22108	[	
69-25	22108-22114	kernel	
69-26	22114-22115	?	
69-27	22115-22116	]	
69-28	22117-22124	objects	
69-29	22125-22127	as	
69-30	22128-22131	the	
69-31	22132-22137	input	
69-32	22138-22140	to	
69-33	22141-22145	have	
69-34	22146-22149	the	
69-35	22150-22154	same	
69-36	22155-22161	effect	
69-37	22162-22164	as	
69-38	22165-22168	the	
69-39	22169-22172	SQL	
69-40	22173-22176	API	
69-41	22176-22177	.	
69-42	22178-22188	Monitoring	
69-43	22189-22192	and	
69-44	22193-22206	Debuggability	
69-45	22207-22211	This	
69-46	22212-22219	release	
69-47	22220-22224	also	
69-48	22225-22233	includes	
69-49	22234-22238	many	
69-50	22239-22251	enhancements	
69-51	22252-22255	and	
69-52	22256-22261	makes	
69-53	22262-22265	the	
69-54	22266-22276	monitoring	
69-55	22277-22281	more	
69-56	22282-22295	comprehensive	
69-57	22296-22299	and	
69-58	22300-22306	stable	
69-59	22306-22307	.	
69-60	22308-22310	We	
69-61	22311-22314	can	
69-62	22315-22319	make	
69-63	22320-22322	it	
69-64	22323-22329	easier	
69-65	22330-22332	to	
69-66	22333-22338	close	
69-67	22339-22342	out	
69-68	22343-22346	and	
69-69	22347-22350	get	
69-70	22351-22355	back	
69-71	22356-22358	to	
69-72	22359-22363	your	

#Text=Spark applications. Structured Streaming UI The first feature I will talk to you about is the new UI for the Spark streaming. Here, the drive to show it– Spark streaming was initially introduced in Spark 2.0. This release has the dedicated– it was Spark web UI for inspection of these streaming
70-1	22364-22369	Spark	
70-2	22370-22382	applications	
70-3	22382-22383	.	
70-4	22384-22394	Structured	
70-5	22395-22404	Streaming	
70-6	22405-22407	UI	
70-7	22408-22411	The	
70-8	22412-22417	first	
70-9	22418-22425	feature	
70-10	22426-22427	I	
70-11	22428-22432	will	
70-12	22433-22437	talk	
70-13	22438-22440	to	
70-14	22441-22444	you	
70-15	22445-22450	about	
70-16	22451-22453	is	
70-17	22454-22457	the	
70-18	22458-22461	new	
70-19	22462-22464	UI	
70-20	22465-22468	for	
70-21	22469-22472	the	
70-22	22473-22478	Spark	
70-23	22479-22488	streaming	
70-24	22488-22489	.	
70-25	22490-22494	Here	
70-26	22494-22495	,	
70-27	22496-22499	the	
70-28	22500-22505	drive	
70-29	22506-22508	to	
70-30	22509-22513	show	
70-31	22514-22516	it	
70-32	22516-22517	–	
70-33	22518-22523	Spark	
70-34	22524-22533	streaming	
70-35	22534-22537	was	
70-36	22538-22547	initially	
70-37	22548-22558	introduced	
70-38	22559-22561	in	
70-39	22562-22567	Spark	
70-40	22568-22571	2.0	
70-41	22571-22572	.	
70-42	22573-22577	This	
70-43	22578-22585	release	
70-44	22586-22589	has	
70-45	22590-22593	the	
70-46	22594-22603	dedicated	
70-47	22603-22604	–	
70-48	22605-22607	it	
70-49	22608-22611	was	
70-50	22612-22617	Spark	
70-51	22618-22621	web	
70-52	22622-22624	UI	
70-53	22625-22628	for	
70-54	22629-22639	inspection	
70-55	22640-22642	of	
70-56	22643-22648	these	
70-57	22649-22658	streaming	

#Text=jobs. This UI offers two sets of statistics: one, abbreviate information of [completed?] streaming queries and two, detailed statistics information about the streaming query including the input rate, processor rate, input loads, [inaudible], operation duration and others. More specifically, the input rate and processor rate means how many records per second the
71-1	22659-22663	jobs	
71-2	22663-22664	.	
71-3	22665-22669	This	
71-4	22670-22672	UI	
71-5	22673-22679	offers	
71-6	22680-22683	two	
71-7	22684-22688	sets	
71-8	22689-22691	of	
71-9	22692-22702	statistics	
71-10	22702-22703	:	
71-11	22704-22707	one	
71-12	22707-22708	,	
71-13	22709-22719	abbreviate	
71-14	22720-22731	information	
71-15	22732-22734	of	
71-16	22735-22736	[	
71-17	22736-22745	completed	
71-18	22745-22746	?	
71-19	22746-22747	]	
71-20	22748-22757	streaming	
71-21	22758-22765	queries	
71-22	22766-22769	and	
71-23	22770-22773	two	
71-24	22773-22774	,	
71-25	22775-22783	detailed	
71-26	22784-22794	statistics	
71-27	22795-22806	information	
71-28	22807-22812	about	
71-29	22813-22816	the	
71-30	22817-22826	streaming	
71-31	22827-22832	query	
71-32	22833-22842	including	
71-33	22843-22846	the	
71-34	22847-22852	input	
71-35	22853-22857	rate	
71-36	22857-22858	,	
71-37	22859-22868	processor	
71-38	22869-22873	rate	
71-39	22873-22874	,	
71-40	22875-22880	input	
71-41	22881-22886	loads	
71-42	22886-22887	,	
71-43	22888-22889	[	
71-44	22889-22898	inaudible	
71-45	22898-22899	]	
71-46	22899-22900	,	
71-47	22901-22910	operation	
71-48	22911-22919	duration	
71-49	22920-22923	and	
71-50	22924-22930	others	
71-51	22930-22931	.	
71-52	22932-22936	More	
71-53	22937-22949	specifically	
71-54	22949-22950	,	
71-55	22951-22954	the	
71-56	22955-22960	input	
71-57	22961-22965	rate	
71-58	22966-22969	and	
71-59	22970-22979	processor	
71-60	22980-22984	rate	
71-61	22985-22990	means	
71-62	22991-22994	how	
71-63	22995-22999	many	
71-64	23000-23007	records	
71-65	23008-23011	per	
71-66	23012-23018	second	
71-67	23019-23022	the	

#Text=streaming software produces and the Spark streaming engine processes. It can give you a sense about if the streaming engine is fast enough to process the continuous input data. Similarly, you can tell it from the past duration as well. If many batch takes more time than the micro-batch [inaudible], it means the engine is not fast enough to process your
72-1	23023-23032	streaming	
72-2	23033-23041	software	
72-3	23042-23050	produces	
72-4	23051-23054	and	
72-5	23055-23058	the	
72-6	23059-23064	Spark	
72-7	23065-23074	streaming	
72-8	23075-23081	engine	
72-9	23082-23091	processes	
72-10	23091-23092	.	
72-11	23093-23095	It	
72-12	23096-23099	can	
72-13	23100-23104	give	
72-14	23105-23108	you	
72-15	23109-23110	a	
72-16	23111-23116	sense	
72-17	23117-23122	about	
72-18	23123-23125	if	
72-19	23126-23129	the	
72-20	23130-23139	streaming	
72-21	23140-23146	engine	
72-22	23147-23149	is	
72-23	23150-23154	fast	
72-24	23155-23161	enough	
72-25	23162-23164	to	
72-26	23165-23172	process	
72-27	23173-23176	the	
72-28	23177-23187	continuous	
72-29	23188-23193	input	
72-30	23194-23198	data	
72-31	23198-23199	.	
72-32	23200-23209	Similarly	
72-33	23209-23210	,	
72-34	23211-23214	you	
72-35	23215-23218	can	
72-36	23219-23223	tell	
72-37	23224-23226	it	
72-38	23227-23231	from	
72-39	23232-23235	the	
72-40	23236-23240	past	
72-41	23241-23249	duration	
72-42	23250-23252	as	
72-43	23253-23257	well	
72-44	23257-23258	.	
72-45	23259-23261	If	
72-46	23262-23266	many	
72-47	23267-23272	batch	
72-48	23273-23278	takes	
72-49	23279-23283	more	
72-50	23284-23288	time	
72-51	23289-23293	than	
72-52	23294-23297	the	
72-53	23298-23309	micro-batch	
72-54	23310-23311	[	
72-55	23311-23320	inaudible	
72-56	23320-23321	]	
72-57	23321-23322	,	
72-58	23323-23325	it	
72-59	23326-23331	means	
72-60	23332-23335	the	
72-61	23336-23342	engine	
72-62	23343-23345	is	
72-63	23346-23349	not	
72-64	23350-23354	fast	
72-65	23355-23361	enough	
72-66	23362-23364	to	
72-67	23365-23372	process	
72-68	23373-23377	your	

#Text=data, and you may need to enable the [inaudible] feature to make the source produce the data slower. And so operating time is also a very useful matrix. It tells you the time spent on each operator so that you can know where is the bottleneck in your query. DDL and DML enhancements We also have many different enhancements in DDL and DML commands. Let
73-1	23378-23382	data	
73-2	23382-23383	,	
73-3	23384-23387	and	
73-4	23388-23391	you	
73-5	23392-23395	may	
73-6	23396-23400	need	
73-7	23401-23403	to	
73-8	23404-23410	enable	
73-9	23411-23414	the	
73-10	23415-23416	[	
73-11	23416-23425	inaudible	
73-12	23425-23426	]	
73-13	23427-23434	feature	
73-14	23435-23437	to	
73-15	23438-23442	make	
73-16	23443-23446	the	
73-17	23447-23453	source	
73-18	23454-23461	produce	
73-19	23462-23465	the	
73-20	23466-23470	data	
73-21	23471-23477	slower	
73-22	23477-23478	.	
73-23	23479-23482	And	
73-24	23483-23485	so	
73-25	23486-23495	operating	
73-26	23496-23500	time	
73-27	23501-23503	is	
73-28	23504-23508	also	
73-29	23509-23510	a	
73-30	23511-23515	very	
73-31	23516-23522	useful	
73-32	23523-23529	matrix	
73-33	23529-23530	.	
73-34	23531-23533	It	
73-35	23534-23539	tells	
73-36	23540-23543	you	
73-37	23544-23547	the	
73-38	23548-23552	time	
73-39	23553-23558	spent	
73-40	23559-23561	on	
73-41	23562-23566	each	
73-42	23567-23575	operator	
73-43	23576-23578	so	
73-44	23579-23583	that	
73-45	23584-23587	you	
73-46	23588-23591	can	
73-47	23592-23596	know	
73-48	23597-23602	where	
73-49	23603-23605	is	
73-50	23606-23609	the	
73-51	23610-23620	bottleneck	
73-52	23621-23623	in	
73-53	23624-23628	your	
73-54	23629-23634	query	
73-55	23634-23635	.	
73-56	23636-23639	DDL	
73-57	23640-23643	and	
73-58	23644-23647	DML	
73-59	23648-23660	enhancements	
73-60	23661-23663	We	
73-61	23664-23668	also	
73-62	23669-23673	have	
73-63	23674-23678	many	
73-64	23679-23688	different	
73-65	23689-23701	enhancements	
73-66	23702-23704	in	
73-67	23705-23708	DDL	
73-68	23709-23712	and	
73-69	23713-23716	DML	
73-70	23717-23725	commands	
73-71	23725-23726	.	
73-72	23727-23730	Let	

#Text=me talk about the improvements in the EXPLAIN command as an example. This is a typical output of the EXPLAIN command. You have many operators in the query plan tree and some operators have other additional information. Reading plans is critical for understanding and attuning queries. The existing solution looks [inaudible], and, as a stream of each operator, can be
74-1	23731-23733	me	
74-2	23734-23738	talk	
74-3	23739-23744	about	
74-4	23745-23748	the	
74-5	23749-23761	improvements	
74-6	23762-23764	in	
74-7	23765-23768	the	
74-8	23769-23776	EXPLAIN	
74-9	23777-23784	command	
74-10	23785-23787	as	
74-11	23788-23790	an	
74-12	23791-23798	example	
74-13	23798-23799	.	
74-14	23800-23804	This	
74-15	23805-23807	is	
74-16	23808-23809	a	
74-17	23810-23817	typical	
74-18	23818-23824	output	
74-19	23825-23827	of	
74-20	23828-23831	the	
74-21	23832-23839	EXPLAIN	
74-22	23840-23847	command	
74-23	23847-23848	.	
74-24	23849-23852	You	
74-25	23853-23857	have	
74-26	23858-23862	many	
74-27	23863-23872	operators	
74-28	23873-23875	in	
74-29	23876-23879	the	
74-30	23880-23885	query	
74-31	23886-23890	plan	
74-32	23891-23895	tree	
74-33	23896-23899	and	
74-34	23900-23904	some	
74-35	23905-23914	operators	
74-36	23915-23919	have	
74-37	23920-23925	other	
74-38	23926-23936	additional	
74-39	23937-23948	information	
74-40	23948-23949	.	
74-41	23950-23957	Reading	
74-42	23958-23963	plans	
74-43	23964-23966	is	
74-44	23967-23975	critical	
74-45	23976-23979	for	
74-46	23980-23993	understanding	
74-47	23994-23997	and	
74-48	23998-24006	attuning	
74-49	24007-24014	queries	
74-50	24014-24015	.	
74-51	24016-24019	The	
74-52	24020-24028	existing	
74-53	24029-24037	solution	
74-54	24038-24043	looks	
74-55	24044-24045	[	
74-56	24045-24054	inaudible	
74-57	24054-24055	]	
74-58	24055-24056	,	
74-59	24057-24060	and	
74-60	24060-24061	,	
74-61	24062-24064	as	
74-62	24065-24066	a	
74-63	24067-24073	stream	
74-64	24074-24076	of	
74-65	24077-24081	each	
74-66	24082-24090	operator	
74-67	24090-24091	,	
74-68	24092-24095	can	
74-69	24096-24098	be	

#Text=very wide or even truncated. And it becomes wider and wider each release as we add more and more information in the operator to help debugging. This release, we enhance the EXPLAIN command with a new formatted mode and also provided a capability to dump the plans to the files. You can see it becomes much easier to read and understand. So here is a very
75-1	24099-24103	very	
75-2	24104-24108	wide	
75-3	24109-24111	or	
75-4	24112-24116	even	
75-5	24117-24126	truncated	
75-6	24126-24127	.	
75-7	24128-24131	And	
75-8	24132-24134	it	
75-9	24135-24142	becomes	
75-10	24143-24148	wider	
75-11	24149-24152	and	
75-12	24153-24158	wider	
75-13	24159-24163	each	
75-14	24164-24171	release	
75-15	24172-24174	as	
75-16	24175-24177	we	
75-17	24178-24181	add	
75-18	24182-24186	more	
75-19	24187-24190	and	
75-20	24191-24195	more	
75-21	24196-24207	information	
75-22	24208-24210	in	
75-23	24211-24214	the	
75-24	24215-24223	operator	
75-25	24224-24226	to	
75-26	24227-24231	help	
75-27	24232-24241	debugging	
75-28	24241-24242	.	
75-29	24243-24247	This	
75-30	24248-24255	release	
75-31	24255-24256	,	
75-32	24257-24259	we	
75-33	24260-24267	enhance	
75-34	24268-24271	the	
75-35	24272-24279	EXPLAIN	
75-36	24280-24287	command	
75-37	24288-24292	with	
75-38	24293-24294	a	
75-39	24295-24298	new	
75-40	24299-24308	formatted	
75-41	24309-24313	mode	
75-42	24314-24317	and	
75-43	24318-24322	also	
75-44	24323-24331	provided	
75-45	24332-24333	a	
75-46	24334-24344	capability	
75-47	24345-24347	to	
75-48	24348-24352	dump	
75-49	24353-24356	the	
75-50	24357-24362	plans	
75-51	24363-24365	to	
75-52	24366-24369	the	
75-53	24370-24375	files	
75-54	24375-24376	.	
75-55	24377-24380	You	
75-56	24381-24384	can	
75-57	24385-24388	see	
75-58	24389-24391	it	
75-59	24392-24399	becomes	
75-60	24400-24404	much	
75-61	24405-24411	easier	
75-62	24412-24414	to	
75-63	24415-24419	read	
75-64	24420-24423	and	
75-65	24424-24434	understand	
75-66	24434-24435	.	
75-67	24436-24438	So	
75-68	24439-24443	here	
75-69	24444-24446	is	
75-70	24447-24448	a	
75-71	24449-24453	very	

#Text=simple plan tree at the beginning. Then follows a detailed section for each operator. This makes it very easy to get an overview of the query by looking at the plan tree. It also makes it very easy to see the details of each operator as the information is now stacked vertically. And in the end, there is a section to show all the subqueries. In the future
76-1	24454-24460	simple	
76-2	24461-24465	plan	
76-3	24466-24470	tree	
76-4	24471-24473	at	
76-5	24474-24477	the	
76-6	24478-24487	beginning	
76-7	24487-24488	.	
76-8	24489-24493	Then	
76-9	24494-24501	follows	
76-10	24502-24503	a	
76-11	24504-24512	detailed	
76-12	24513-24520	section	
76-13	24521-24524	for	
76-14	24525-24529	each	
76-15	24530-24538	operator	
76-16	24538-24539	.	
76-17	24540-24544	This	
76-18	24545-24550	makes	
76-19	24551-24553	it	
76-20	24554-24558	very	
76-21	24559-24563	easy	
76-22	24564-24566	to	
76-23	24567-24570	get	
76-24	24571-24573	an	
76-25	24574-24582	overview	
76-26	24583-24585	of	
76-27	24586-24589	the	
76-28	24590-24595	query	
76-29	24596-24598	by	
76-30	24599-24606	looking	
76-31	24607-24609	at	
76-32	24610-24613	the	
76-33	24614-24618	plan	
76-34	24619-24623	tree	
76-35	24623-24624	.	
76-36	24625-24627	It	
76-37	24628-24632	also	
76-38	24633-24638	makes	
76-39	24639-24641	it	
76-40	24642-24646	very	
76-41	24647-24651	easy	
76-42	24652-24654	to	
76-43	24655-24658	see	
76-44	24659-24662	the	
76-45	24663-24670	details	
76-46	24671-24673	of	
76-47	24674-24678	each	
76-48	24679-24687	operator	
76-49	24688-24690	as	
76-50	24691-24694	the	
76-51	24695-24706	information	
76-52	24707-24709	is	
76-53	24710-24713	now	
76-54	24714-24721	stacked	
76-55	24722-24732	vertically	
76-56	24732-24733	.	
76-57	24734-24737	And	
76-58	24738-24740	in	
76-59	24741-24744	the	
76-60	24745-24748	end	
76-61	24748-24749	,	
76-62	24750-24755	there	
76-63	24756-24758	is	
76-64	24759-24760	a	
76-65	24761-24768	section	
76-66	24769-24771	to	
76-67	24772-24776	show	
76-68	24777-24780	all	
76-69	24781-24784	the	
76-70	24785-24795	subqueries	
76-71	24795-24796	.	
76-72	24797-24799	In	
76-73	24800-24803	the	
76-74	24804-24810	future	

#Text=releases, we will add more and more useful information for each operator. This release, we also introduced a new API to define your own metrics to observe data quality. Data quality is very important to many applications. It’s usually easy to define metrics for data quality by some [other?] function, for example, but it’s also hard
77-1	24811-24819	releases	
77-2	24819-24820	,	
77-3	24821-24823	we	
77-4	24824-24828	will	
77-5	24829-24832	add	
77-6	24833-24837	more	
77-7	24838-24841	and	
77-8	24842-24846	more	
77-9	24847-24853	useful	
77-10	24854-24865	information	
77-11	24866-24869	for	
77-12	24870-24874	each	
77-13	24875-24883	operator	
77-14	24883-24884	.	
77-15	24885-24889	This	
77-16	24890-24897	release	
77-17	24897-24898	,	
77-18	24899-24901	we	
77-19	24902-24906	also	
77-20	24907-24917	introduced	
77-21	24918-24919	a	
77-22	24920-24923	new	
77-23	24924-24927	API	
77-24	24928-24930	to	
77-25	24931-24937	define	
77-26	24938-24942	your	
77-27	24943-24946	own	
77-28	24947-24954	metrics	
77-29	24955-24957	to	
77-30	24958-24965	observe	
77-31	24966-24970	data	
77-32	24971-24978	quality	
77-33	24978-24979	.	
77-34	24980-24984	Data	
77-35	24985-24992	quality	
77-36	24993-24995	is	
77-37	24996-25000	very	
77-38	25001-25010	important	
77-39	25011-25013	to	
77-40	25014-25018	many	
77-41	25019-25031	applications	
77-42	25031-25032	.	
77-43	25033-25035	It	
77-44	25035-25036	’	
77-45	25036-25037	s	
77-46	25038-25045	usually	
77-47	25046-25050	easy	
77-48	25051-25053	to	
77-49	25054-25060	define	
77-50	25061-25068	metrics	
77-51	25069-25072	for	
77-52	25073-25077	data	
77-53	25078-25085	quality	
77-54	25086-25088	by	
77-55	25089-25093	some	
77-56	25094-25095	[	
77-57	25095-25100	other	
77-58	25100-25101	?	
77-59	25101-25102	]	
77-60	25103-25111	function	
77-61	25111-25112	,	
77-62	25113-25116	for	
77-63	25117-25124	example	
77-64	25124-25125	,	
77-65	25126-25129	but	
77-66	25130-25132	it	
77-67	25132-25133	’	
77-68	25133-25134	s	
77-69	25135-25139	also	
77-70	25140-25144	hard	

#Text=to calculate the metrics, especially for streaming queries. For example, you want to keep monitoring the data quality of your streaming source. You can simply define the metrics as the percentage of the error records. Then you can do two things. Make it a habit. One, code observe method of the streaming error rate to define your metrics with a name and the start
78-1	25145-25147	to	
78-2	25148-25157	calculate	
78-3	25158-25161	the	
78-4	25162-25169	metrics	
78-5	25169-25170	,	
78-6	25171-25181	especially	
78-7	25182-25185	for	
78-8	25186-25195	streaming	
78-9	25196-25203	queries	
78-10	25203-25204	.	
78-11	25205-25208	For	
78-12	25209-25216	example	
78-13	25216-25217	,	
78-14	25218-25221	you	
78-15	25222-25226	want	
78-16	25227-25229	to	
78-17	25230-25234	keep	
78-18	25235-25245	monitoring	
78-19	25246-25249	the	
78-20	25250-25254	data	
78-21	25255-25262	quality	
78-22	25263-25265	of	
78-23	25266-25270	your	
78-24	25271-25280	streaming	
78-25	25281-25287	source	
78-26	25287-25288	.	
78-27	25289-25292	You	
78-28	25293-25296	can	
78-29	25297-25303	simply	
78-30	25304-25310	define	
78-31	25311-25314	the	
78-32	25315-25322	metrics	
78-33	25323-25325	as	
78-34	25326-25329	the	
78-35	25330-25340	percentage	
78-36	25341-25343	of	
78-37	25344-25347	the	
78-38	25348-25353	error	
78-39	25354-25361	records	
78-40	25361-25362	.	
78-41	25363-25367	Then	
78-42	25368-25371	you	
78-43	25372-25375	can	
78-44	25376-25378	do	
78-45	25379-25382	two	
78-46	25383-25389	things	
78-47	25389-25390	.	
78-48	25391-25395	Make	
78-49	25396-25398	it	
78-50	25399-25400	a	
78-51	25401-25406	habit	
78-52	25406-25407	.	
78-53	25408-25411	One	
78-54	25411-25412	,	
78-55	25413-25417	code	
78-56	25418-25425	observe	
78-57	25426-25432	method	
78-58	25433-25435	of	
78-59	25436-25439	the	
78-60	25440-25449	streaming	
78-61	25450-25455	error	
78-62	25456-25460	rate	
78-63	25461-25463	to	
78-64	25464-25470	define	
78-65	25471-25475	your	
78-66	25476-25483	metrics	
78-67	25484-25488	with	
78-68	25489-25490	a	
78-69	25491-25495	name	
78-70	25496-25499	and	
78-71	25500-25503	the	
78-72	25504-25509	start	

#Text=of stream. So this example, the name is data quality and the matrix, it just will count the error record and see how many percent of it in the total lookups. Two, you add a listener to watch the streaming process events, and in the case of your matrix, the name, do whatever you want to do, such as sending an email if there are more than 5% error data.
79-1	25510-25512	of	
79-2	25513-25519	stream	
79-3	25519-25520	.	
79-4	25521-25523	So	
79-5	25524-25528	this	
79-6	25529-25536	example	
79-7	25536-25537	,	
79-8	25538-25541	the	
79-9	25542-25546	name	
79-10	25547-25549	is	
79-11	25550-25554	data	
79-12	25555-25562	quality	
79-13	25563-25566	and	
79-14	25567-25570	the	
79-15	25571-25577	matrix	
79-16	25577-25578	,	
79-17	25579-25581	it	
79-18	25582-25586	just	
79-19	25587-25591	will	
79-20	25592-25597	count	
79-21	25598-25601	the	
79-22	25602-25607	error	
79-23	25608-25614	record	
79-24	25615-25618	and	
79-25	25619-25622	see	
79-26	25623-25626	how	
79-27	25627-25631	many	
79-28	25632-25639	percent	
79-29	25640-25642	of	
79-30	25643-25645	it	
79-31	25646-25648	in	
79-32	25649-25652	the	
79-33	25653-25658	total	
79-34	25659-25666	lookups	
79-35	25666-25667	.	
79-36	25668-25671	Two	
79-37	25671-25672	,	
79-38	25673-25676	you	
79-39	25677-25680	add	
79-40	25681-25682	a	
79-41	25683-25691	listener	
79-42	25692-25694	to	
79-43	25695-25700	watch	
79-44	25701-25704	the	
79-45	25705-25714	streaming	
79-46	25715-25722	process	
79-47	25723-25729	events	
79-48	25729-25730	,	
79-49	25731-25734	and	
79-50	25735-25737	in	
79-51	25738-25741	the	
79-52	25742-25746	case	
79-53	25747-25749	of	
79-54	25750-25754	your	
79-55	25755-25761	matrix	
79-56	25761-25762	,	
79-57	25763-25766	the	
79-58	25767-25771	name	
79-59	25771-25772	,	
79-60	25773-25775	do	
79-61	25776-25784	whatever	
79-62	25785-25788	you	
79-63	25789-25793	want	
79-64	25794-25796	to	
79-65	25797-25799	do	
79-66	25799-25800	,	
79-67	25801-25805	such	
79-68	25806-25808	as	
79-69	25809-25816	sending	
79-70	25817-25819	an	
79-71	25820-25825	email	
79-72	25826-25828	if	
79-73	25829-25834	there	
79-74	25835-25838	are	
79-75	25839-25843	more	
79-76	25844-25848	than	
79-77	25849-25851	5%	
79-78	25852-25857	error	
79-79	25858-25862	data	
79-80	25862-25863	.	

#Text=SQL Compatibility Now, let’s move to the next topic. SQL compatibility is also super critical for workloads mapped from the other database systems through Spark SQL. In this release, we introduced the ANSI store assignment policy for table insertion. We added runtime overall
80-1	25864-25867	SQL	
80-2	25868-25881	Compatibility	
80-3	25882-25885	Now	
80-4	25885-25886	,	
80-5	25887-25890	let	
80-6	25890-25891	’	
80-7	25891-25892	s	
80-8	25893-25897	move	
80-9	25898-25900	to	
80-10	25901-25904	the	
80-11	25905-25909	next	
80-12	25910-25915	topic	
80-13	25915-25916	.	
80-14	25917-25920	SQL	
80-15	25921-25934	compatibility	
80-16	25935-25937	is	
80-17	25938-25942	also	
80-18	25943-25948	super	
80-19	25949-25957	critical	
80-20	25958-25961	for	
80-21	25962-25971	workloads	
80-22	25972-25978	mapped	
80-23	25979-25983	from	
80-24	25984-25987	the	
80-25	25988-25993	other	
80-26	25994-26002	database	
80-27	26003-26010	systems	
80-28	26011-26018	through	
80-29	26019-26024	Spark	
80-30	26025-26028	SQL	
80-31	26028-26029	.	
80-32	26030-26032	In	
80-33	26033-26037	this	
80-34	26038-26045	release	
80-35	26045-26046	,	
80-36	26047-26049	we	
80-37	26050-26060	introduced	
80-38	26061-26064	the	
80-39	26065-26069	ANSI	
80-40	26070-26075	store	
80-41	26076-26086	assignment	
80-42	26087-26093	policy	
80-43	26094-26097	for	
80-44	26098-26103	table	
80-45	26104-26113	insertion	
80-46	26113-26114	.	
80-47	26115-26117	We	
80-48	26118-26123	added	
80-49	26124-26131	runtime	
80-50	26132-26139	overall	

#Text=checking with respect to ANSI results keywords into the parser. We also switched the calendar to the widely-used calendar which is the ISO and SQL standard. Let’s look at how the first two features can help you to enforce data quality. I say more about the assignment. It’s something like assigning a value to a variable in programming language. In the SQL
81-1	26140-26148	checking	
81-2	26149-26153	with	
81-3	26154-26161	respect	
81-4	26162-26164	to	
81-5	26165-26169	ANSI	
81-6	26170-26177	results	
81-7	26178-26186	keywords	
81-8	26187-26191	into	
81-9	26192-26195	the	
81-10	26196-26202	parser	
81-11	26202-26203	.	
81-12	26204-26206	We	
81-13	26207-26211	also	
81-14	26212-26220	switched	
81-15	26221-26224	the	
81-16	26225-26233	calendar	
81-17	26234-26236	to	
81-18	26237-26240	the	
81-19	26241-26252	widely-used	
81-20	26253-26261	calendar	
81-21	26262-26267	which	
81-22	26268-26270	is	
81-23	26271-26274	the	
81-24	26275-26278	ISO	
81-25	26279-26282	and	
81-26	26283-26286	SQL	
81-27	26287-26295	standard	
81-28	26295-26296	.	
81-29	26297-26300	Let	
81-30	26300-26301	’	
81-31	26301-26302	s	
81-32	26303-26307	look	
81-33	26308-26310	at	
81-34	26311-26314	how	
81-35	26315-26318	the	
81-36	26319-26324	first	
81-37	26325-26328	two	
81-38	26329-26337	features	
81-39	26338-26341	can	
81-40	26342-26346	help	
81-41	26347-26350	you	
81-42	26351-26353	to	
81-43	26354-26361	enforce	
81-44	26362-26366	data	
81-45	26367-26374	quality	
81-46	26374-26375	.	
81-47	26376-26377	I	
81-48	26378-26381	say	
81-49	26382-26386	more	
81-50	26387-26392	about	
81-51	26393-26396	the	
81-52	26397-26407	assignment	
81-53	26407-26408	.	
81-54	26409-26411	It	
81-55	26411-26412	’	
81-56	26412-26413	s	
81-57	26414-26423	something	
81-58	26424-26428	like	
81-59	26429-26438	assigning	
81-60	26439-26440	a	
81-61	26441-26446	value	
81-62	26447-26449	to	
81-63	26450-26451	a	
81-64	26452-26460	variable	
81-65	26461-26463	in	
81-66	26464-26475	programming	
81-67	26476-26484	language	
81-68	26484-26485	.	
81-69	26486-26488	In	
81-70	26489-26492	the	
81-71	26493-26496	SQL	

#Text=world, it is table insertion or upsert, which is kind of assigning values to a table column. Now, let’s see an example. Assume there is a table with two columns, I and J, which are type int and type string. If we write a int value to the string column, it’s totally okay. It’s totally
82-1	26497-26502	world	
82-2	26502-26503	,	
82-3	26504-26506	it	
82-4	26507-26509	is	
82-5	26510-26515	table	
82-6	26516-26525	insertion	
82-7	26526-26528	or	
82-8	26529-26535	upsert	
82-9	26535-26536	,	
82-10	26537-26542	which	
82-11	26543-26545	is	
82-12	26546-26550	kind	
82-13	26551-26553	of	
82-14	26554-26563	assigning	
82-15	26564-26570	values	
82-16	26571-26573	to	
82-17	26574-26575	a	
82-18	26576-26581	table	
82-19	26582-26588	column	
82-20	26588-26589	.	
82-21	26590-26593	Now	
82-22	26593-26594	,	
82-23	26595-26598	let	
82-24	26598-26599	’	
82-25	26599-26600	s	
82-26	26601-26604	see	
82-27	26605-26607	an	
82-28	26608-26615	example	
82-29	26615-26616	.	
82-30	26617-26623	Assume	
82-31	26624-26629	there	
82-32	26630-26632	is	
82-33	26633-26634	a	
82-34	26635-26640	table	
82-35	26641-26645	with	
82-36	26646-26649	two	
82-37	26650-26657	columns	
82-38	26657-26658	,	
82-39	26659-26660	I	
82-40	26661-26664	and	
82-41	26665-26666	J	
82-42	26666-26667	,	
82-43	26668-26673	which	
82-44	26674-26677	are	
82-45	26678-26682	type	
82-46	26683-26686	int	
82-47	26687-26690	and	
82-48	26691-26695	type	
82-49	26696-26702	string	
82-50	26702-26703	.	
82-51	26704-26706	If	
82-52	26707-26709	we	
82-53	26710-26715	write	
82-54	26716-26717	a	
82-55	26718-26721	int	
82-56	26722-26727	value	
82-57	26728-26730	to	
82-58	26731-26734	the	
82-59	26735-26741	string	
82-60	26742-26748	column	
82-61	26748-26749	,	
82-62	26750-26752	it	
82-63	26752-26753	’	
82-64	26753-26754	s	
82-65	26755-26762	totally	
82-66	26763-26767	okay	
82-67	26767-26768	.	
82-68	26769-26771	It	
82-69	26771-26772	’	
82-70	26772-26773	s	
82-71	26774-26781	totally	

#Text=safe. However, if we write a string value to the int column, it’s risky. The string value is very likely to not be in integer form, and Spark will fail and worry about it. If you do believe your string values are safe to be inserted into an int column, you can add a cast manually to bypass the type check in Spark.
83-1	26782-26786	safe	
83-2	26786-26787	.	
83-3	26788-26795	However	
83-4	26795-26796	,	
83-5	26797-26799	if	
83-6	26800-26802	we	
83-7	26803-26808	write	
83-8	26809-26810	a	
83-9	26811-26817	string	
83-10	26818-26823	value	
83-11	26824-26826	to	
83-12	26827-26830	the	
83-13	26831-26834	int	
83-14	26835-26841	column	
83-15	26841-26842	,	
83-16	26843-26845	it	
83-17	26845-26846	’	
83-18	26846-26847	s	
83-19	26848-26853	risky	
83-20	26853-26854	.	
83-21	26855-26858	The	
83-22	26859-26865	string	
83-23	26866-26871	value	
83-24	26872-26874	is	
83-25	26875-26879	very	
83-26	26880-26886	likely	
83-27	26887-26889	to	
83-28	26890-26893	not	
83-29	26894-26896	be	
83-30	26897-26899	in	
83-31	26900-26907	integer	
83-32	26908-26912	form	
83-33	26912-26913	,	
83-34	26914-26917	and	
83-35	26918-26923	Spark	
83-36	26924-26928	will	
83-37	26929-26933	fail	
83-38	26934-26937	and	
83-39	26938-26943	worry	
83-40	26944-26949	about	
83-41	26950-26952	it	
83-42	26952-26953	.	
83-43	26954-26956	If	
83-44	26957-26960	you	
83-45	26961-26963	do	
83-46	26964-26971	believe	
83-47	26972-26976	your	
83-48	26977-26983	string	
83-49	26984-26990	values	
83-50	26991-26994	are	
83-51	26995-26999	safe	
83-52	27000-27002	to	
83-53	27003-27005	be	
83-54	27006-27014	inserted	
83-55	27015-27019	into	
83-56	27020-27022	an	
83-57	27023-27026	int	
83-58	27027-27033	column	
83-59	27033-27034	,	
83-60	27035-27038	you	
83-61	27039-27042	can	
83-62	27043-27046	add	
83-63	27047-27048	a	
83-64	27049-27053	cast	
83-65	27054-27062	manually	
83-66	27063-27065	to	
83-67	27066-27072	bypass	
83-68	27073-27076	the	
83-69	27077-27081	type	
83-70	27082-27087	check	
83-71	27088-27090	in	
83-72	27091-27096	Spark	
83-73	27096-27097	.	

#Text=We can also write long type of values to the int column, and Spark will do the overflow check at runtime. If your input data is invalid, Spark will be show exception at runtime to tell you about it. In this example, the integer one is okay, but the larger value below can’t fit the integer
84-1	27098-27100	We	
84-2	27101-27104	can	
84-3	27105-27109	also	
84-4	27110-27115	write	
84-5	27116-27120	long	
84-6	27121-27125	type	
84-7	27126-27128	of	
84-8	27129-27135	values	
84-9	27136-27138	to	
84-10	27139-27142	the	
84-11	27143-27146	int	
84-12	27147-27153	column	
84-13	27153-27154	,	
84-14	27155-27158	and	
84-15	27159-27164	Spark	
84-16	27165-27169	will	
84-17	27170-27172	do	
84-18	27173-27176	the	
84-19	27177-27185	overflow	
84-20	27186-27191	check	
84-21	27192-27194	at	
84-22	27195-27202	runtime	
84-23	27202-27203	.	
84-24	27204-27206	If	
84-25	27207-27211	your	
84-26	27212-27217	input	
84-27	27218-27222	data	
84-28	27223-27225	is	
84-29	27226-27233	invalid	
84-30	27233-27234	,	
84-31	27235-27240	Spark	
84-32	27241-27245	will	
84-33	27246-27248	be	
84-34	27249-27253	show	
84-35	27254-27263	exception	
84-36	27264-27266	at	
84-37	27267-27274	runtime	
84-38	27275-27277	to	
84-39	27278-27282	tell	
84-40	27283-27286	you	
84-41	27287-27292	about	
84-42	27293-27295	it	
84-43	27295-27296	.	
84-44	27297-27299	In	
84-45	27300-27304	this	
84-46	27305-27312	example	
84-47	27312-27313	,	
84-48	27314-27317	the	
84-49	27318-27325	integer	
84-50	27326-27329	one	
84-51	27330-27332	is	
84-52	27333-27337	okay	
84-53	27337-27338	,	
84-54	27339-27342	but	
84-55	27343-27346	the	
84-56	27347-27353	larger	
84-57	27354-27359	value	
84-58	27360-27365	below	
84-59	27366-27369	can	
84-60	27369-27370	’	
84-61	27370-27371	t	
84-62	27372-27375	fit	
84-63	27376-27379	the	
84-64	27380-27387	integer	

#Text=type, and you’ll receive this error if you run this table insertion command which tells you about the overflow problem. Built-in Data Source Enhancements Also, this release enhances built-in data sources. For example, to populate data source, we can’t do nested column and filter pushdown. Also, we support
85-1	27388-27392	type	
85-2	27392-27393	,	
85-3	27394-27397	and	
85-4	27398-27401	you	
85-5	27401-27402	’	
85-6	27402-27404	ll	
85-7	27405-27412	receive	
85-8	27413-27417	this	
85-9	27418-27423	error	
85-10	27424-27426	if	
85-11	27427-27430	you	
85-12	27431-27434	run	
85-13	27435-27439	this	
85-14	27440-27445	table	
85-15	27446-27455	insertion	
85-16	27456-27463	command	
85-17	27464-27469	which	
85-18	27470-27475	tells	
85-19	27476-27479	you	
85-20	27480-27485	about	
85-21	27486-27489	the	
85-22	27490-27498	overflow	
85-23	27499-27506	problem	
85-24	27506-27507	.	
85-25	27508-27516	Built-in	
85-26	27517-27521	Data	
85-27	27522-27528	Source	
85-28	27529-27541	Enhancements	
85-29	27542-27546	Also	
85-30	27546-27547	,	
85-31	27548-27552	this	
85-32	27553-27560	release	
85-33	27561-27569	enhances	
85-34	27570-27578	built-in	
85-35	27579-27583	data	
85-36	27584-27591	sources	
85-37	27591-27592	.	
85-38	27593-27596	For	
85-39	27597-27604	example	
85-40	27604-27605	,	
85-41	27606-27608	to	
85-42	27609-27617	populate	
85-43	27618-27622	data	
85-44	27623-27629	source	
85-45	27629-27630	,	
85-46	27631-27633	we	
85-47	27634-27637	can	
85-48	27637-27638	’	
85-49	27638-27639	t	
85-50	27640-27642	do	
85-51	27643-27649	nested	
85-52	27650-27656	column	
85-53	27657-27660	and	
85-54	27661-27667	filter	
85-55	27668-27676	pushdown	
85-56	27676-27677	.	
85-57	27678-27682	Also	
85-58	27682-27683	,	
85-59	27684-27686	we	
85-60	27687-27694	support	

#Text=[inaudible] for CSV files. This release also introduced a new [inaudible] resource and also a new [inaudible] resource for testing and benchmarking. Let me further introduce the origin of nested columns in Parquet and ORC resource. The first one is a kind of [baloney?].
86-1	27695-27696	[	
86-2	27696-27705	inaudible	
86-3	27705-27706	]	
86-4	27707-27710	for	
86-5	27711-27714	CSV	
86-6	27715-27720	files	
86-7	27720-27721	.	
86-8	27722-27726	This	
86-9	27727-27734	release	
86-10	27735-27739	also	
86-11	27740-27750	introduced	
86-12	27751-27752	a	
86-13	27753-27756	new	
86-14	27757-27758	[	
86-15	27758-27767	inaudible	
86-16	27767-27768	]	
86-17	27769-27777	resource	
86-18	27778-27781	and	
86-19	27782-27786	also	
86-20	27787-27788	a	
86-21	27789-27792	new	
86-22	27793-27794	[	
86-23	27794-27803	inaudible	
86-24	27803-27804	]	
86-25	27805-27813	resource	
86-26	27814-27817	for	
86-27	27818-27825	testing	
86-28	27826-27829	and	
86-29	27830-27842	benchmarking	
86-30	27842-27843	.	
86-31	27844-27847	Let	
86-32	27848-27850	me	
86-33	27851-27858	further	
86-34	27859-27868	introduce	
86-35	27869-27872	the	
86-36	27873-27879	origin	
86-37	27880-27882	of	
86-38	27883-27889	nested	
86-39	27890-27897	columns	
86-40	27898-27900	in	
86-41	27901-27908	Parquet	
86-42	27909-27912	and	
86-43	27913-27916	ORC	
86-44	27917-27925	resource	
86-45	27925-27926	.	
86-46	27927-27930	The	
86-47	27931-27936	first	
86-48	27937-27940	one	
86-49	27941-27943	is	
86-50	27944-27945	a	
86-51	27946-27950	kind	
86-52	27951-27953	of	
86-53	27954-27955	[	
86-54	27955-27962	baloney	
86-55	27962-27963	?	
86-56	27963-27964	]	
86-57	27964-27965	.	

#Text=[inaudible] like [inaudible] and [ORC?], we can skip reading some [inaudible] in the blocks if they don’t contain the columns we need. This [technique?] can be applied to nested columns as well in Spark 2.0. To check if your query– in Spark 3.0. To check if your
87-1	27966-27967	[	
87-2	27967-27976	inaudible	
87-3	27976-27977	]	
87-4	27978-27982	like	
87-5	27983-27984	[	
87-6	27984-27993	inaudible	
87-7	27993-27994	]	
87-8	27995-27998	and	
87-9	27999-28000	[	
87-10	28000-28003	ORC	
87-11	28003-28004	?	
87-12	28004-28005	]	
87-13	28005-28006	,	
87-14	28007-28009	we	
87-15	28010-28013	can	
87-16	28014-28018	skip	
87-17	28019-28026	reading	
87-18	28027-28031	some	
87-19	28032-28033	[	
87-20	28033-28042	inaudible	
87-21	28042-28043	]	
87-22	28044-28046	in	
87-23	28047-28050	the	
87-24	28051-28057	blocks	
87-25	28058-28060	if	
87-26	28061-28065	they	
87-27	28066-28069	don	
87-28	28069-28070	’	
87-29	28070-28071	t	
87-30	28072-28079	contain	
87-31	28080-28083	the	
87-32	28084-28091	columns	
87-33	28092-28094	we	
87-34	28095-28099	need	
87-35	28099-28100	.	
87-36	28101-28105	This	
87-37	28106-28107	[	
87-38	28107-28116	technique	
87-39	28116-28117	?	
87-40	28117-28118	]	
87-41	28119-28122	can	
87-42	28123-28125	be	
87-43	28126-28133	applied	
87-44	28134-28136	to	
87-45	28137-28143	nested	
87-46	28144-28151	columns	
87-47	28152-28154	as	
87-48	28155-28159	well	
87-49	28160-28162	in	
87-50	28163-28168	Spark	
87-51	28169-28172	2.0	
87-52	28172-28173	.	
87-53	28174-28176	To	
87-54	28177-28182	check	
87-55	28183-28185	if	
87-56	28186-28190	your	
87-57	28191-28196	query	
87-58	28196-28197	–	
87-59	28198-28200	in	
87-60	28201-28206	Spark	
87-61	28207-28210	3.0	
87-62	28210-28211	.	
87-63	28212-28214	To	
87-64	28215-28220	check	
87-65	28221-28223	if	
87-66	28224-28228	your	

#Text=query can benefit from this [inaudible] or not, you can run the EXPLAIN command and see if the read schema over the file scan note strips the [inaudible] nested columns. In this example, only the nested column is inserted, so the read schema only contains A. [inaudible] is also a very popular technical [inaudible]. Similarly,
88-1	28229-28234	query	
88-2	28235-28238	can	
88-3	28239-28246	benefit	
88-4	28247-28251	from	
88-5	28252-28256	this	
88-6	28257-28258	[	
88-7	28258-28267	inaudible	
88-8	28267-28268	]	
88-9	28269-28271	or	
88-10	28272-28275	not	
88-11	28275-28276	,	
88-12	28277-28280	you	
88-13	28281-28284	can	
88-14	28285-28288	run	
88-15	28289-28292	the	
88-16	28293-28300	EXPLAIN	
88-17	28301-28308	command	
88-18	28309-28312	and	
88-19	28313-28316	see	
88-20	28317-28319	if	
88-21	28320-28323	the	
88-22	28324-28328	read	
88-23	28329-28335	schema	
88-24	28336-28340	over	
88-25	28341-28344	the	
88-26	28345-28349	file	
88-27	28350-28354	scan	
88-28	28355-28359	note	
88-29	28360-28366	strips	
88-30	28367-28370	the	
88-31	28371-28372	[	
88-32	28372-28381	inaudible	
88-33	28381-28382	]	
88-34	28383-28389	nested	
88-35	28390-28397	columns	
88-36	28397-28398	.	
88-37	28399-28401	In	
88-38	28402-28406	this	
88-39	28407-28414	example	
88-40	28414-28415	,	
88-41	28416-28420	only	
88-42	28421-28424	the	
88-43	28425-28431	nested	
88-44	28432-28438	column	
88-45	28439-28441	is	
88-46	28442-28450	inserted	
88-47	28450-28451	,	
88-48	28452-28454	so	
88-49	28455-28458	the	
88-50	28459-28463	read	
88-51	28464-28470	schema	
88-52	28471-28475	only	
88-53	28476-28484	contains	
88-54	28485-28486	A	
88-55	28486-28487	.	
88-56	28488-28489	[	
88-57	28489-28498	inaudible	
88-58	28498-28499	]	
88-59	28500-28502	is	
88-60	28503-28507	also	
88-61	28508-28509	a	
88-62	28510-28514	very	
88-63	28515-28522	popular	
88-64	28523-28532	technical	
88-65	28533-28534	[	
88-66	28534-28543	inaudible	
88-67	28543-28544	]	
88-68	28544-28545	.	
88-69	28546-28555	Similarly	
88-70	28555-28556	,	

#Text=you can also check the expand result and see if the [pushed?] filters over the file scan note contains the name of column filters. In this example, we do have a filter where it’s nested column A, and it does appear in the [pushed?] filter, which makes this version happen in this query. Catalog plugin API
89-1	28557-28560	you	
89-2	28561-28564	can	
89-3	28565-28569	also	
89-4	28570-28575	check	
89-5	28576-28579	the	
89-6	28580-28586	expand	
89-7	28587-28593	result	
89-8	28594-28597	and	
89-9	28598-28601	see	
89-10	28602-28604	if	
89-11	28605-28608	the	
89-12	28609-28610	[	
89-13	28610-28616	pushed	
89-14	28616-28617	?	
89-15	28617-28618	]	
89-16	28619-28626	filters	
89-17	28627-28631	over	
89-18	28632-28635	the	
89-19	28636-28640	file	
89-20	28641-28645	scan	
89-21	28646-28650	note	
89-22	28651-28659	contains	
89-23	28660-28663	the	
89-24	28664-28668	name	
89-25	28669-28671	of	
89-26	28672-28678	column	
89-27	28679-28686	filters	
89-28	28686-28687	.	
89-29	28688-28690	In	
89-30	28691-28695	this	
89-31	28696-28703	example	
89-32	28703-28704	,	
89-33	28705-28707	we	
89-34	28708-28710	do	
89-35	28711-28715	have	
89-36	28716-28717	a	
89-37	28718-28724	filter	
89-38	28725-28730	where	
89-39	28731-28733	it	
89-40	28733-28734	’	
89-41	28734-28735	s	
89-42	28736-28742	nested	
89-43	28743-28749	column	
89-44	28750-28751	A	
89-45	28751-28752	,	
89-46	28753-28756	and	
89-47	28757-28759	it	
89-48	28760-28764	does	
89-49	28765-28771	appear	
89-50	28772-28774	in	
89-51	28775-28778	the	
89-52	28779-28780	[	
89-53	28780-28786	pushed	
89-54	28786-28787	?	
89-55	28787-28788	]	
89-56	28789-28795	filter	
89-57	28795-28796	,	
89-58	28797-28802	which	
89-59	28803-28808	makes	
89-60	28809-28813	this	
89-61	28814-28821	version	
89-62	28822-28828	happen	
89-63	28829-28831	in	
89-64	28832-28836	this	
89-65	28837-28842	query	
89-66	28842-28843	.	
89-67	28844-28851	Catalog	
89-68	28852-28858	plugin	
89-69	28859-28862	API	

#Text=This release also expands other efforts on the extensibility and ecosystem like the v2 API enhancements, Java 11, Hadoop, and [inaudible] support. [inaudible] API. This release extends the [inaudible] to API by adding the Catalog plugin. The Catalog plug-in API allows users to reject their own [inaudible] and take over the [inaudible]
90-1	28863-28867	This	
90-2	28868-28875	release	
90-3	28876-28880	also	
90-4	28881-28888	expands	
90-5	28889-28894	other	
90-6	28895-28902	efforts	
90-7	28903-28905	on	
90-8	28906-28909	the	
90-9	28910-28923	extensibility	
90-10	28924-28927	and	
90-11	28928-28937	ecosystem	
90-12	28938-28942	like	
90-13	28943-28946	the	
90-14	28947-28949	v2	
90-15	28950-28953	API	
90-16	28954-28966	enhancements	
90-17	28966-28967	,	
90-18	28968-28972	Java	
90-19	28973-28975	11	
90-20	28975-28976	,	
90-21	28977-28983	Hadoop	
90-22	28983-28984	,	
90-23	28985-28988	and	
90-24	28989-28990	[	
90-25	28990-28999	inaudible	
90-26	28999-29000	]	
90-27	29001-29008	support	
90-28	29008-29009	.	
90-29	29010-29011	[	
90-30	29011-29020	inaudible	
90-31	29020-29021	]	
90-32	29022-29025	API	
90-33	29025-29026	.	
90-34	29027-29031	This	
90-35	29032-29039	release	
90-36	29040-29047	extends	
90-37	29048-29051	the	
90-38	29052-29053	[	
90-39	29053-29062	inaudible	
90-40	29062-29063	]	
90-41	29064-29066	to	
90-42	29067-29070	API	
90-43	29071-29073	by	
90-44	29074-29080	adding	
90-45	29081-29084	the	
90-46	29085-29092	Catalog	
90-47	29093-29099	plugin	
90-48	29099-29100	.	
90-49	29101-29104	The	
90-50	29105-29112	Catalog	
90-51	29113-29120	plug-in	
90-52	29121-29124	API	
90-53	29125-29131	allows	
90-54	29132-29137	users	
90-55	29138-29140	to	
90-56	29141-29147	reject	
90-57	29148-29153	their	
90-58	29154-29157	own	
90-59	29158-29159	[	
90-60	29159-29168	inaudible	
90-61	29168-29169	]	
90-62	29170-29173	and	
90-63	29174-29178	take	
90-64	29179-29183	over	
90-65	29184-29187	the	
90-66	29188-29189	[	
90-67	29189-29198	inaudible	
90-68	29198-29199	]	

#Text=data operations from Spark. This can give end users a more seamless experience to assist external tables. Now, end users [inaudible] reject [inaudible] and manipulate the tables [inaudible], where before, end users have to reject each table. For example, let’s say
91-1	29200-29204	data	
91-2	29205-29215	operations	
91-3	29216-29220	from	
91-4	29221-29226	Spark	
91-5	29226-29227	.	
91-6	29228-29232	This	
91-7	29233-29236	can	
91-8	29237-29241	give	
91-9	29242-29245	end	
91-10	29246-29251	users	
91-11	29252-29253	a	
91-12	29254-29258	more	
91-13	29259-29267	seamless	
91-14	29268-29278	experience	
91-15	29279-29281	to	
91-16	29282-29288	assist	
91-17	29289-29297	external	
91-18	29298-29304	tables	
91-19	29304-29305	.	
91-20	29306-29309	Now	
91-21	29309-29310	,	
91-22	29311-29314	end	
91-23	29315-29320	users	
91-24	29321-29322	[	
91-25	29322-29331	inaudible	
91-26	29331-29332	]	
91-27	29333-29339	reject	
91-28	29340-29341	[	
91-29	29341-29350	inaudible	
91-30	29350-29351	]	
91-31	29352-29355	and	
91-32	29356-29366	manipulate	
91-33	29367-29370	the	
91-34	29371-29377	tables	
91-35	29378-29379	[	
91-36	29379-29388	inaudible	
91-37	29388-29389	]	
91-38	29389-29390	,	
91-39	29391-29396	where	
91-40	29397-29403	before	
91-41	29403-29404	,	
91-42	29405-29408	end	
91-43	29409-29414	users	
91-44	29415-29419	have	
91-45	29420-29422	to	
91-46	29423-29429	reject	
91-47	29430-29434	each	
91-48	29435-29440	table	
91-49	29440-29441	.	
91-50	29442-29445	For	
91-51	29446-29453	example	
91-52	29453-29454	,	
91-53	29455-29458	let	
91-54	29458-29459	’	
91-55	29459-29460	s	
91-56	29461-29464	say	

#Text=you have rejected a MySQL connector [inaudible] named MySQL. You can use SELECT to get data from existing MySQL table. We can also INSERT into a MySQL table with Spark’s [inaudible]. You can also create an outer tables in MySQL with Spark, which was just not possible before,
92-1	29465-29468	you	
92-2	29469-29473	have	
92-3	29474-29482	rejected	
92-4	29483-29484	a	
92-5	29485-29490	MySQL	
92-6	29491-29500	connector	
92-7	29501-29502	[	
92-8	29502-29511	inaudible	
92-9	29511-29512	]	
92-10	29513-29518	named	
92-11	29519-29524	MySQL	
92-12	29524-29525	.	
92-13	29526-29529	You	
92-14	29530-29533	can	
92-15	29534-29537	use	
92-16	29538-29544	SELECT	
92-17	29545-29547	to	
92-18	29548-29551	get	
92-19	29552-29556	data	
92-20	29557-29561	from	
92-21	29562-29570	existing	
92-22	29571-29576	MySQL	
92-23	29577-29582	table	
92-24	29582-29583	.	
92-25	29584-29586	We	
92-26	29587-29590	can	
92-27	29591-29595	also	
92-28	29596-29602	INSERT	
92-29	29603-29607	into	
92-30	29608-29609	a	
92-31	29610-29615	MySQL	
92-32	29616-29621	table	
92-33	29622-29626	with	
92-34	29627-29632	Spark	
92-35	29632-29633	’	
92-36	29633-29634	s	
92-37	29635-29636	[	
92-38	29636-29645	inaudible	
92-39	29645-29646	]	
92-40	29646-29647	.	
92-41	29648-29651	You	
92-42	29652-29655	can	
92-43	29656-29660	also	
92-44	29661-29667	create	
92-45	29668-29670	an	
92-46	29671-29676	outer	
92-47	29677-29683	tables	
92-48	29684-29686	in	
92-49	29687-29692	MySQL	
92-50	29693-29697	with	
92-51	29698-29703	Spark	
92-52	29703-29704	,	
92-53	29705-29710	which	
92-54	29711-29714	was	
92-55	29715-29719	just	
92-56	29720-29723	not	
92-57	29724-29732	possible	
92-58	29733-29739	before	
92-59	29739-29740	,	

#Text=because before, we don’t have the Catalog plug-in. Now this example will be available in Spark 3.1 when we finish [inaudible]. When to use Data Source V2 API? Some people may have a question. Now Spark has both – it has a V1 and a V2
93-1	29741-29748	because	
93-2	29749-29755	before	
93-3	29755-29756	,	
93-4	29757-29759	we	
93-5	29760-29763	don	
93-6	29763-29764	’	
93-7	29764-29765	t	
93-8	29766-29770	have	
93-9	29771-29774	the	
93-10	29775-29782	Catalog	
93-11	29783-29790	plug-in	
93-12	29790-29791	.	
93-13	29792-29795	Now	
93-14	29796-29800	this	
93-15	29801-29808	example	
93-16	29809-29813	will	
93-17	29814-29816	be	
93-18	29817-29826	available	
93-19	29827-29829	in	
93-20	29830-29835	Spark	
93-21	29836-29839	3.1	
93-22	29840-29844	when	
93-23	29845-29847	we	
93-24	29848-29854	finish	
93-25	29855-29856	[	
93-26	29856-29865	inaudible	
93-27	29865-29866	]	
93-28	29866-29867	.	
93-29	29868-29872	When	
93-30	29873-29875	to	
93-31	29876-29879	use	
93-32	29880-29884	Data	
93-33	29885-29891	Source	
93-34	29892-29894	V2	
93-35	29895-29898	API	
93-36	29898-29899	?	
93-37	29900-29904	Some	
93-38	29905-29911	people	
93-39	29912-29915	may	
93-40	29916-29920	have	
93-41	29921-29922	a	
93-42	29923-29931	question	
93-43	29931-29932	.	
93-44	29933-29936	Now	
93-45	29937-29942	Spark	
93-46	29943-29946	has	
93-47	29947-29951	both	
93-48	29952-29953	–	
93-49	29954-29956	it	
93-50	29957-29960	has	
93-51	29961-29962	a	
93-52	29963-29965	V1	
93-53	29966-29969	and	
93-54	29970-29971	a	
93-55	29972-29974	V2	

#Text=APIs – which one should I use? In general, we want everyone to move to V2 [inaudible]. But the V2 API is not ready yet as we need more feedback to polish the API. Here are some tips about when to pick the V2 API. So if you want [inaudible], the catalogue function
94-1	29975-29979	APIs	
94-2	29980-29981	–	
94-3	29982-29987	which	
94-4	29988-29991	one	
94-5	29992-29998	should	
94-6	29999-30000	I	
94-7	30001-30004	use	
94-8	30004-30005	?	
94-9	30006-30008	In	
94-10	30009-30016	general	
94-11	30016-30017	,	
94-12	30018-30020	we	
94-13	30021-30025	want	
94-14	30026-30034	everyone	
94-15	30035-30037	to	
94-16	30038-30042	move	
94-17	30043-30045	to	
94-18	30046-30048	V2	
94-19	30049-30050	[	
94-20	30050-30059	inaudible	
94-21	30059-30060	]	
94-22	30060-30061	.	
94-23	30062-30065	But	
94-24	30066-30069	the	
94-25	30070-30072	V2	
94-26	30073-30076	API	
94-27	30077-30079	is	
94-28	30080-30083	not	
94-29	30084-30089	ready	
94-30	30090-30093	yet	
94-31	30094-30096	as	
94-32	30097-30099	we	
94-33	30100-30104	need	
94-34	30105-30109	more	
94-35	30110-30118	feedback	
94-36	30119-30121	to	
94-37	30122-30128	polish	
94-38	30129-30132	the	
94-39	30133-30136	API	
94-40	30136-30137	.	
94-41	30138-30142	Here	
94-42	30143-30146	are	
94-43	30147-30151	some	
94-44	30152-30156	tips	
94-45	30157-30162	about	
94-46	30163-30167	when	
94-47	30168-30170	to	
94-48	30171-30175	pick	
94-49	30176-30179	the	
94-50	30180-30182	V2	
94-51	30183-30186	API	
94-52	30186-30187	.	
94-53	30188-30190	So	
94-54	30191-30193	if	
94-55	30194-30197	you	
94-56	30198-30202	want	
94-57	30203-30204	[	
94-58	30204-30213	inaudible	
94-59	30213-30214	]	
94-60	30214-30215	,	
94-61	30216-30219	the	
94-62	30220-30229	catalogue	
94-63	30230-30238	function	

#Text=it is, so it has to be the V2 because V1 API doesn’t have this ability. If you want to support both versions streaming in your data source, then you should use V2 because in V1, the streaming and the [inaudible] are different APIs which makes it harder to reuse the code. And if you are sensitive to the scan performance, then you can try the V2
95-1	30239-30241	it	
95-2	30242-30244	is	
95-3	30244-30245	,	
95-4	30246-30248	so	
95-5	30249-30251	it	
95-6	30252-30255	has	
95-7	30256-30258	to	
95-8	30259-30261	be	
95-9	30262-30265	the	
95-10	30266-30268	V2	
95-11	30269-30276	because	
95-12	30277-30279	V1	
95-13	30280-30283	API	
95-14	30284-30289	doesn	
95-15	30289-30290	’	
95-16	30290-30291	t	
95-17	30292-30296	have	
95-18	30297-30301	this	
95-19	30302-30309	ability	
95-20	30309-30310	.	
95-21	30311-30313	If	
95-22	30314-30317	you	
95-23	30318-30322	want	
95-24	30323-30325	to	
95-25	30326-30333	support	
95-26	30334-30338	both	
95-27	30339-30347	versions	
95-28	30348-30357	streaming	
95-29	30358-30360	in	
95-30	30361-30365	your	
95-31	30366-30370	data	
95-32	30371-30377	source	
95-33	30377-30378	,	
95-34	30379-30383	then	
95-35	30384-30387	you	
95-36	30388-30394	should	
95-37	30395-30398	use	
95-38	30399-30401	V2	
95-39	30402-30409	because	
95-40	30410-30412	in	
95-41	30413-30415	V1	
95-42	30415-30416	,	
95-43	30417-30420	the	
95-44	30421-30430	streaming	
95-45	30431-30434	and	
95-46	30435-30438	the	
95-47	30439-30440	[	
95-48	30440-30449	inaudible	
95-49	30449-30450	]	
95-50	30451-30454	are	
95-51	30455-30464	different	
95-52	30465-30469	APIs	
95-53	30470-30475	which	
95-54	30476-30481	makes	
95-55	30482-30484	it	
95-56	30485-30491	harder	
95-57	30492-30494	to	
95-58	30495-30500	reuse	
95-59	30501-30504	the	
95-60	30505-30509	code	
95-61	30509-30510	.	
95-62	30511-30514	And	
95-63	30515-30517	if	
95-64	30518-30521	you	
95-65	30522-30525	are	
95-66	30526-30535	sensitive	
95-67	30536-30538	to	
95-68	30539-30542	the	
95-69	30543-30547	scan	
95-70	30548-30559	performance	
95-71	30559-30560	,	
95-72	30561-30565	then	
95-73	30566-30569	you	
95-74	30570-30573	can	
95-75	30574-30577	try	
95-76	30578-30581	the	
95-77	30582-30584	V2	

#Text=API because it allows you to report the data provisioning to [inaudible] in Spark, and also it allows you to implement [inaudible] reader for better performance. If you don’t care about this stuff and just want to [inaudible] source once and you change it, please use the V1 as V2 is not very stable.
96-1	30585-30588	API	
96-2	30589-30596	because	
96-3	30597-30599	it	
96-4	30600-30606	allows	
96-5	30607-30610	you	
96-6	30611-30613	to	
96-7	30614-30620	report	
96-8	30621-30624	the	
96-9	30625-30629	data	
96-10	30630-30642	provisioning	
96-11	30643-30645	to	
96-12	30646-30647	[	
96-13	30647-30656	inaudible	
96-14	30656-30657	]	
96-15	30658-30660	in	
96-16	30661-30666	Spark	
96-17	30666-30667	,	
96-18	30668-30671	and	
96-19	30672-30676	also	
96-20	30677-30679	it	
96-21	30680-30686	allows	
96-22	30687-30690	you	
96-23	30691-30693	to	
96-24	30694-30703	implement	
96-25	30704-30705	[	
96-26	30705-30714	inaudible	
96-27	30714-30715	]	
96-28	30716-30722	reader	
96-29	30723-30726	for	
96-30	30727-30733	better	
96-31	30734-30745	performance	
96-32	30745-30746	.	
96-33	30747-30749	If	
96-34	30750-30753	you	
96-35	30754-30757	don	
96-36	30757-30758	’	
96-37	30758-30759	t	
96-38	30760-30764	care	
96-39	30765-30770	about	
96-40	30771-30775	this	
96-41	30776-30781	stuff	
96-42	30782-30785	and	
96-43	30786-30790	just	
96-44	30791-30795	want	
96-45	30796-30798	to	
96-46	30799-30800	[	
96-47	30800-30809	inaudible	
96-48	30809-30810	]	
96-49	30811-30817	source	
96-50	30818-30822	once	
96-51	30823-30826	and	
96-52	30827-30830	you	
96-53	30831-30837	change	
96-54	30838-30840	it	
96-55	30840-30841	,	
96-56	30842-30848	please	
96-57	30849-30852	use	
96-58	30853-30856	the	
96-59	30857-30859	V1	
96-60	30860-30862	as	
96-61	30863-30865	V2	
96-62	30866-30868	is	
96-63	30869-30872	not	
96-64	30873-30877	very	
96-65	30878-30884	stable	
96-66	30884-30885	.	

#Text=Extensibility and Ecosystem The ecosystem also evolves very fast. In this release, Spark can be better integrated into the ecosystem by supporting the newer version of these common components like Java 11, Hadoop 3, Hadoop 3 [inaudible], and Hadoop 2.3 [inaudible].
97-1	30886-30899	Extensibility	
97-2	30900-30903	and	
97-3	30904-30913	Ecosystem	
97-4	30914-30917	The	
97-5	30918-30927	ecosystem	
97-6	30928-30932	also	
97-7	30933-30940	evolves	
97-8	30941-30945	very	
97-9	30946-30950	fast	
97-10	30950-30951	.	
97-11	30952-30954	In	
97-12	30955-30959	this	
97-13	30960-30967	release	
97-14	30967-30968	,	
97-15	30969-30974	Spark	
97-16	30975-30978	can	
97-17	30979-30981	be	
97-18	30982-30988	better	
97-19	30989-30999	integrated	
97-20	31000-31004	into	
97-21	31005-31008	the	
97-22	31009-31018	ecosystem	
97-23	31019-31021	by	
97-24	31022-31032	supporting	
97-25	31033-31036	the	
97-26	31037-31042	newer	
97-27	31043-31050	version	
97-28	31051-31053	of	
97-29	31054-31059	these	
97-30	31060-31066	common	
97-31	31067-31077	components	
97-32	31078-31082	like	
97-33	31083-31087	Java	
97-34	31088-31090	11	
97-35	31090-31091	,	
97-36	31092-31098	Hadoop	
97-37	31099-31100	3	
97-38	31100-31101	,	
97-39	31102-31108	Hadoop	
97-40	31109-31110	3	
97-41	31111-31112	[	
97-42	31112-31121	inaudible	
97-43	31121-31122	]	
97-44	31122-31123	,	
97-45	31124-31127	and	
97-46	31128-31134	Hadoop	
97-47	31135-31138	2.3	
97-48	31139-31140	[	
97-49	31140-31149	inaudible	
97-50	31149-31150	]	
97-51	31150-31151	.	

#Text=I want to mention some breaking changes here. Starting from this release, we’re only building Spark with Scala 2.12, so Scala 2.11 is no longer [inaudible]. And we deprecated Python 2 too because it is end of life. In the download image, we put a build of Spark with different
98-1	31152-31153	I	
98-2	31154-31158	want	
98-3	31159-31161	to	
98-4	31162-31169	mention	
98-5	31170-31174	some	
98-6	31175-31183	breaking	
98-7	31184-31191	changes	
98-8	31192-31196	here	
98-9	31196-31197	.	
98-10	31198-31206	Starting	
98-11	31207-31211	from	
98-12	31212-31216	this	
98-13	31217-31224	release	
98-14	31224-31225	,	
98-15	31226-31228	we	
98-16	31228-31229	’	
98-17	31229-31231	re	
98-18	31232-31236	only	
98-19	31237-31245	building	
98-20	31246-31251	Spark	
98-21	31252-31256	with	
98-22	31257-31262	Scala	
98-23	31263-31267	2.12	
98-24	31267-31268	,	
98-25	31269-31271	so	
98-26	31272-31277	Scala	
98-27	31278-31282	2.11	
98-28	31283-31285	is	
98-29	31286-31288	no	
98-30	31289-31295	longer	
98-31	31296-31297	[	
98-32	31297-31306	inaudible	
98-33	31306-31307	]	
98-34	31307-31308	.	
98-35	31309-31312	And	
98-36	31313-31315	we	
98-37	31316-31326	deprecated	
98-38	31327-31333	Python	
98-39	31334-31335	2	
98-40	31336-31339	too	
98-41	31340-31347	because	
98-42	31348-31350	it	
98-43	31351-31353	is	
98-44	31354-31357	end	
98-45	31358-31360	of	
98-46	31361-31365	life	
98-47	31365-31366	.	
98-48	31367-31369	In	
98-49	31370-31373	the	
98-50	31374-31382	download	
98-51	31383-31388	image	
98-52	31388-31389	,	
98-53	31390-31392	we	
98-54	31393-31396	put	
98-55	31397-31398	a	
98-56	31399-31404	build	
98-57	31405-31407	of	
98-58	31408-31413	Spark	
98-59	31414-31418	with	
98-60	31419-31428	different	

#Text=[inaudible] and Hadoop combinations. By default, it will be Hadoop 2.7 and it would have 2.3 [exclusion?]. There are another two [companies?] of previews available. One is Hadoop 2.7 and Hadoop 1.2 execution, which is for people who can’t upgrade their end forms. The other
99-1	31429-31430	[	
99-2	31430-31439	inaudible	
99-3	31439-31440	]	
99-4	31441-31444	and	
99-5	31445-31451	Hadoop	
99-6	31452-31464	combinations	
99-7	31464-31465	.	
99-8	31466-31468	By	
99-9	31469-31476	default	
99-10	31476-31477	,	
99-11	31478-31480	it	
99-12	31481-31485	will	
99-13	31486-31488	be	
99-14	31489-31495	Hadoop	
99-15	31496-31499	2.7	
99-16	31500-31503	and	
99-17	31504-31506	it	
99-18	31507-31512	would	
99-19	31513-31517	have	
99-20	31518-31521	2.3	
99-21	31522-31523	[	
99-22	31523-31532	exclusion	
99-23	31532-31533	?	
99-24	31533-31534	]	
99-25	31534-31535	.	
99-26	31536-31541	There	
99-27	31542-31545	are	
99-28	31546-31553	another	
99-29	31554-31557	two	
99-30	31558-31559	[	
99-31	31559-31568	companies	
99-32	31568-31569	?	
99-33	31569-31570	]	
99-34	31571-31573	of	
99-35	31574-31582	previews	
99-36	31583-31592	available	
99-37	31592-31593	.	
99-38	31594-31597	One	
99-39	31598-31600	is	
99-40	31601-31607	Hadoop	
99-41	31608-31611	2.7	
99-42	31612-31615	and	
99-43	31616-31622	Hadoop	
99-44	31623-31626	1.2	
99-45	31627-31636	execution	
99-46	31636-31637	,	
99-47	31638-31643	which	
99-48	31644-31646	is	
99-49	31647-31650	for	
99-50	31651-31657	people	
99-51	31658-31661	who	
99-52	31662-31665	can	
99-53	31665-31666	’	
99-54	31666-31667	t	
99-55	31668-31675	upgrade	
99-56	31676-31681	their	
99-57	31682-31685	end	
99-58	31686-31691	forms	
99-59	31691-31692	.	
99-60	31693-31696	The	
99-61	31697-31702	other	

#Text=is Hadoop 3.2 and Hadoop 2.3 execution, which is for people who want to try Hadoop 3. We also extend the support for different Hadoop and Hive versions from 0.12 to 3.1. Documentation Improvements Documentation improvements is the last existing news I want to share with everyone. How to read on a standard the web UI is a common question to many new Spark
100-1	31703-31705	is	
100-2	31706-31712	Hadoop	
100-3	31713-31716	3.2	
100-4	31717-31720	and	
100-5	31721-31727	Hadoop	
100-6	31728-31731	2.3	
100-7	31732-31741	execution	
100-8	31741-31742	,	
100-9	31743-31748	which	
100-10	31749-31751	is	
100-11	31752-31755	for	
100-12	31756-31762	people	
100-13	31763-31766	who	
100-14	31767-31771	want	
100-15	31772-31774	to	
100-16	31775-31778	try	
100-17	31779-31785	Hadoop	
100-18	31786-31787	3	
100-19	31787-31788	.	
100-20	31789-31791	We	
100-21	31792-31796	also	
100-22	31797-31803	extend	
100-23	31804-31807	the	
100-24	31808-31815	support	
100-25	31816-31819	for	
100-26	31820-31829	different	
100-27	31830-31836	Hadoop	
100-28	31837-31840	and	
100-29	31841-31845	Hive	
100-30	31846-31854	versions	
100-31	31855-31859	from	
100-32	31860-31864	0.12	
100-33	31865-31867	to	
100-34	31868-31871	3.1	
100-35	31871-31872	.	
100-36	31873-31886	Documentation	
100-37	31887-31899	Improvements	
100-38	31900-31913	Documentation	
100-39	31914-31926	improvements	
100-40	31927-31929	is	
100-41	31930-31933	the	
100-42	31934-31938	last	
100-43	31939-31947	existing	
100-44	31948-31952	news	
100-45	31953-31954	I	
100-46	31955-31959	want	
100-47	31960-31962	to	
100-48	31963-31968	share	
100-49	31969-31973	with	
100-50	31974-31982	everyone	
100-51	31982-31983	.	
100-52	31984-31987	How	
100-53	31988-31990	to	
100-54	31991-31995	read	
100-55	31996-31998	on	
100-56	31999-32000	a	
100-57	32001-32009	standard	
100-58	32010-32013	the	
100-59	32014-32017	web	
100-60	32018-32020	UI	
100-61	32021-32023	is	
100-62	32024-32025	a	
100-63	32026-32032	common	
100-64	32033-32041	question	
100-65	32042-32044	to	
100-66	32045-32049	many	
100-67	32050-32053	new	
100-68	32054-32059	Spark	

#Text=users. This is especially true for Spark SQL users and Spark streaming users. They are using the [inaudible]. They usually don’t know what it is, and what our jobs [inaudible]. Also, the [inaudible] are using many queries and matrix names, which are not very clear
101-1	32060-32065	users	
101-2	32065-32066	.	
101-3	32067-32071	This	
101-4	32072-32074	is	
101-5	32075-32085	especially	
101-6	32086-32090	true	
101-7	32091-32094	for	
101-8	32095-32100	Spark	
101-9	32101-32104	SQL	
101-10	32105-32110	users	
101-11	32111-32114	and	
101-12	32115-32120	Spark	
101-13	32121-32130	streaming	
101-14	32131-32136	users	
101-15	32136-32137	.	
101-16	32138-32142	They	
101-17	32143-32146	are	
101-18	32147-32152	using	
101-19	32153-32156	the	
101-20	32157-32158	[	
101-21	32158-32167	inaudible	
101-22	32167-32168	]	
101-23	32168-32169	.	
101-24	32170-32174	They	
101-25	32175-32182	usually	
101-26	32183-32186	don	
101-27	32186-32187	’	
101-28	32187-32188	t	
101-29	32189-32193	know	
101-30	32194-32198	what	
101-31	32199-32201	it	
101-32	32202-32204	is	
101-33	32204-32205	,	
101-34	32206-32209	and	
101-35	32210-32214	what	
101-36	32215-32218	our	
101-37	32219-32223	jobs	
101-38	32224-32225	[	
101-39	32225-32234	inaudible	
101-40	32234-32235	]	
101-41	32235-32236	.	
101-42	32237-32241	Also	
101-43	32241-32242	,	
101-44	32243-32246	the	
101-45	32247-32248	[	
101-46	32248-32257	inaudible	
101-47	32257-32258	]	
101-48	32259-32262	are	
101-49	32263-32268	using	
101-50	32269-32273	many	
101-51	32274-32281	queries	
101-52	32282-32285	and	
101-53	32286-32292	matrix	
101-54	32293-32298	names	
101-55	32298-32299	,	
101-56	32300-32305	which	
101-57	32306-32309	are	
101-58	32310-32313	not	
101-59	32314-32318	very	
101-60	32319-32324	clear	

#Text=to many users. Starting from this release, we add a new section for [inaudible] reading the web UI. It includes the [inaudible] job page and [inaudible] and also SQL streaming [inaudible]. This is just a start. We will continue to enhance it, then SQL reference.
102-1	32325-32327	to	
102-2	32328-32332	many	
102-3	32333-32338	users	
102-4	32338-32339	.	
102-5	32340-32348	Starting	
102-6	32349-32353	from	
102-7	32354-32358	this	
102-8	32359-32366	release	
102-9	32366-32367	,	
102-10	32368-32370	we	
102-11	32371-32374	add	
102-12	32375-32376	a	
102-13	32377-32380	new	
102-14	32381-32388	section	
102-15	32389-32392	for	
102-16	32393-32394	[	
102-17	32394-32403	inaudible	
102-18	32403-32404	]	
102-19	32405-32412	reading	
102-20	32413-32416	the	
102-21	32417-32420	web	
102-22	32421-32423	UI	
102-23	32423-32424	.	
102-24	32425-32427	It	
102-25	32428-32436	includes	
102-26	32437-32440	the	
102-27	32441-32442	[	
102-28	32442-32451	inaudible	
102-29	32451-32452	]	
102-30	32453-32456	job	
102-31	32457-32461	page	
102-32	32462-32465	and	
102-33	32466-32467	[	
102-34	32467-32476	inaudible	
102-35	32476-32477	]	
102-36	32478-32481	and	
102-37	32482-32486	also	
102-38	32487-32490	SQL	
102-39	32491-32500	streaming	
102-40	32501-32502	[	
102-41	32502-32511	inaudible	
102-42	32511-32512	]	
102-43	32512-32513	.	
102-44	32514-32518	This	
102-45	32519-32521	is	
102-46	32522-32526	just	
102-47	32527-32528	a	
102-48	32529-32534	start	
102-49	32534-32535	.	
102-50	32536-32538	We	
102-51	32539-32543	will	
102-52	32544-32552	continue	
102-53	32553-32555	to	
102-54	32556-32563	enhance	
102-55	32564-32566	it	
102-56	32566-32567	,	
102-57	32568-32572	then	
102-58	32573-32576	SQL	
102-59	32577-32586	reference	
102-60	32586-32587	.	

#Text=Finally, this release already has a SQL reference for Spark SQL. Spark SQL is the most popular and important component in Spark. However, we did not have our own SQL reference to define the SQL [semantic?] and detailed behaviors. Let me quickly go over the major chapters in SQL reference. So we have a page to explain the ANSI components
103-1	32588-32595	Finally	
103-2	32595-32596	,	
103-3	32597-32601	this	
103-4	32602-32609	release	
103-5	32610-32617	already	
103-6	32618-32621	has	
103-7	32622-32623	a	
103-8	32624-32627	SQL	
103-9	32628-32637	reference	
103-10	32638-32641	for	
103-11	32642-32647	Spark	
103-12	32648-32651	SQL	
103-13	32651-32652	.	
103-14	32653-32658	Spark	
103-15	32659-32662	SQL	
103-16	32663-32665	is	
103-17	32666-32669	the	
103-18	32670-32674	most	
103-19	32675-32682	popular	
103-20	32683-32686	and	
103-21	32687-32696	important	
103-22	32697-32706	component	
103-23	32707-32709	in	
103-24	32710-32715	Spark	
103-25	32715-32716	.	
103-26	32717-32724	However	
103-27	32724-32725	,	
103-28	32726-32728	we	
103-29	32729-32732	did	
103-30	32733-32736	not	
103-31	32737-32741	have	
103-32	32742-32745	our	
103-33	32746-32749	own	
103-34	32750-32753	SQL	
103-35	32754-32763	reference	
103-36	32764-32766	to	
103-37	32767-32773	define	
103-38	32774-32777	the	
103-39	32778-32781	SQL	
103-40	32782-32783	[	
103-41	32783-32791	semantic	
103-42	32791-32792	?	
103-43	32792-32793	]	
103-44	32794-32797	and	
103-45	32798-32806	detailed	
103-46	32807-32816	behaviors	
103-47	32816-32817	.	
103-48	32818-32821	Let	
103-49	32822-32824	me	
103-50	32825-32832	quickly	
103-51	32833-32835	go	
103-52	32836-32840	over	
103-53	32841-32844	the	
103-54	32845-32850	major	
103-55	32851-32859	chapters	
103-56	32860-32862	in	
103-57	32863-32866	SQL	
103-58	32867-32876	reference	
103-59	32876-32877	.	
103-60	32878-32880	So	
103-61	32881-32883	we	
103-62	32884-32888	have	
103-63	32889-32890	a	
103-64	32891-32895	page	
103-65	32896-32898	to	
103-66	32899-32906	explain	
103-67	32907-32910	the	
103-68	32911-32915	ANSI	
103-69	32916-32926	components	

#Text=of Spark. So as I mentioned before, we have SQL compatibility, but to avoid [correcting?] the [effecting?] queries, we make it optional. So you can only enable the ANSI compatibility by enabling this flag. You also have a page to explain the detailed semantic of each [inaudible], so you can know what it means and what’s the behavior of them. You also have a page to
104-1	32927-32929	of	
104-2	32930-32935	Spark	
104-3	32935-32936	.	
104-4	32937-32939	So	
104-5	32940-32942	as	
104-6	32943-32944	I	
104-7	32945-32954	mentioned	
104-8	32955-32961	before	
104-9	32961-32962	,	
104-10	32963-32965	we	
104-11	32966-32970	have	
104-12	32971-32974	SQL	
104-13	32975-32988	compatibility	
104-14	32988-32989	,	
104-15	32990-32993	but	
104-16	32994-32996	to	
104-17	32997-33002	avoid	
104-18	33003-33004	[	
104-19	33004-33014	correcting	
104-20	33014-33015	?	
104-21	33015-33016	]	
104-22	33017-33020	the	
104-23	33021-33022	[	
104-24	33022-33031	effecting	
104-25	33031-33032	?	
104-26	33032-33033	]	
104-27	33034-33041	queries	
104-28	33041-33042	,	
104-29	33043-33045	we	
104-30	33046-33050	make	
104-31	33051-33053	it	
104-32	33054-33062	optional	
104-33	33062-33063	.	
104-34	33064-33066	So	
104-35	33067-33070	you	
104-36	33071-33074	can	
104-37	33075-33079	only	
104-38	33080-33086	enable	
104-39	33087-33090	the	
104-40	33091-33095	ANSI	
104-41	33096-33109	compatibility	
104-42	33110-33112	by	
104-43	33113-33121	enabling	
104-44	33122-33126	this	
104-45	33127-33131	flag	
104-46	33131-33132	.	
104-47	33133-33136	You	
104-48	33137-33141	also	
104-49	33142-33146	have	
104-50	33147-33148	a	
104-51	33149-33153	page	
104-52	33154-33156	to	
104-53	33157-33164	explain	
104-54	33165-33168	the	
104-55	33169-33177	detailed	
104-56	33178-33186	semantic	
104-57	33187-33189	of	
104-58	33190-33194	each	
104-59	33195-33196	[	
104-60	33196-33205	inaudible	
104-61	33205-33206	]	
104-62	33206-33207	,	
104-63	33208-33210	so	
104-64	33211-33214	you	
104-65	33215-33218	can	
104-66	33219-33223	know	
104-67	33224-33228	what	
104-68	33229-33231	it	
104-69	33232-33237	means	
104-70	33238-33241	and	
104-71	33242-33246	what	
104-72	33246-33247	’	
104-73	33247-33248	s	
104-74	33249-33252	the	
104-75	33253-33261	behavior	
104-76	33262-33264	of	
104-77	33265-33269	them	
104-78	33269-33270	.	
104-79	33271-33274	You	
104-80	33275-33279	also	
104-81	33280-33284	have	
104-82	33285-33286	a	
104-83	33287-33291	page	
104-84	33292-33294	to	

#Text=explain the data and partner strings used for formatting and parsing functions [inaudible]. There’s also a page to give the document for each function in Spark. We also have a page to explain the syntax, how to define the table or function [inaudible]. Also, there’s a page to explain the syntax and the semantics of each [inaudible] in Spark SQL.
105-1	33295-33302	explain	
105-2	33303-33306	the	
105-3	33307-33311	data	
105-4	33312-33315	and	
105-5	33316-33323	partner	
105-6	33324-33331	strings	
105-7	33332-33336	used	
105-8	33337-33340	for	
105-9	33341-33351	formatting	
105-10	33352-33355	and	
105-11	33356-33363	parsing	
105-12	33364-33373	functions	
105-13	33374-33375	[	
105-14	33375-33384	inaudible	
105-15	33384-33385	]	
105-16	33385-33386	.	
105-17	33387-33392	There	
105-18	33392-33393	’	
105-19	33393-33394	s	
105-20	33395-33399	also	
105-21	33400-33401	a	
105-22	33402-33406	page	
105-23	33407-33409	to	
105-24	33410-33414	give	
105-25	33415-33418	the	
105-26	33419-33427	document	
105-27	33428-33431	for	
105-28	33432-33436	each	
105-29	33437-33445	function	
105-30	33446-33448	in	
105-31	33449-33454	Spark	
105-32	33454-33455	.	
105-33	33456-33458	We	
105-34	33459-33463	also	
105-35	33464-33468	have	
105-36	33469-33470	a	
105-37	33471-33475	page	
105-38	33476-33478	to	
105-39	33479-33486	explain	
105-40	33487-33490	the	
105-41	33491-33497	syntax	
105-42	33497-33498	,	
105-43	33499-33502	how	
105-44	33503-33505	to	
105-45	33506-33512	define	
105-46	33513-33516	the	
105-47	33517-33522	table	
105-48	33523-33525	or	
105-49	33526-33534	function	
105-50	33535-33536	[	
105-51	33536-33545	inaudible	
105-52	33545-33546	]	
105-53	33546-33547	.	
105-54	33548-33552	Also	
105-55	33552-33553	,	
105-56	33554-33559	there	
105-57	33559-33560	’	
105-58	33560-33561	s	
105-59	33562-33563	a	
105-60	33564-33568	page	
105-61	33569-33571	to	
105-62	33572-33579	explain	
105-63	33580-33583	the	
105-64	33584-33590	syntax	
105-65	33591-33594	and	
105-66	33595-33598	the	
105-67	33599-33608	semantics	
105-68	33609-33611	of	
105-69	33612-33616	each	
105-70	33617-33618	[	
105-71	33618-33627	inaudible	
105-72	33627-33628	]	
105-73	33629-33631	in	
105-74	33632-33637	Spark	
105-75	33638-33641	SQL	
105-76	33641-33642	.	

#Text=Also, there’s a page to explain the null semantic. The null is a very special value in Spark SQL and other ecosystems. So there must be a page to either explain what’s the meaning of null in the null queries. Also, we have a page to explain the syntax for all the commands, like DDL
106-1	33643-33647	Also	
106-2	33647-33648	,	
106-3	33649-33654	there	
106-4	33654-33655	’	
106-5	33655-33656	s	
106-6	33657-33658	a	
106-7	33659-33663	page	
106-8	33664-33666	to	
106-9	33667-33674	explain	
106-10	33675-33678	the	
106-11	33679-33683	null	
106-12	33684-33692	semantic	
106-13	33692-33693	.	
106-14	33694-33697	The	
106-15	33698-33702	null	
106-16	33703-33705	is	
106-17	33706-33707	a	
106-18	33708-33712	very	
106-19	33713-33720	special	
106-20	33721-33726	value	
106-21	33727-33729	in	
106-22	33730-33735	Spark	
106-23	33736-33739	SQL	
106-24	33740-33743	and	
106-25	33744-33749	other	
106-26	33750-33760	ecosystems	
106-27	33760-33761	.	
106-28	33762-33764	So	
106-29	33765-33770	there	
106-30	33771-33775	must	
106-31	33776-33778	be	
106-32	33779-33780	a	
106-33	33781-33785	page	
106-34	33786-33788	to	
106-35	33789-33795	either	
106-36	33796-33803	explain	
106-37	33804-33808	what	
106-38	33808-33809	’	
106-39	33809-33810	s	
106-40	33811-33814	the	
106-41	33815-33822	meaning	
106-42	33823-33825	of	
106-43	33826-33830	null	
106-44	33831-33833	in	
106-45	33834-33837	the	
106-46	33838-33842	null	
106-47	33843-33850	queries	
106-48	33850-33851	.	
106-49	33852-33856	Also	
106-50	33856-33857	,	
106-51	33858-33860	we	
106-52	33861-33865	have	
106-53	33866-33867	a	
106-54	33868-33872	page	
106-55	33873-33875	to	
106-56	33876-33883	explain	
106-57	33884-33887	the	
106-58	33888-33894	syntax	
106-59	33895-33898	for	
106-60	33899-33902	all	
106-61	33903-33906	the	
106-62	33907-33915	commands	
106-63	33915-33916	,	
106-64	33917-33921	like	
106-65	33922-33925	DDL	

#Text=and DML commands, and also insert is also included in the document. In fact, SELECT has so many features, so we want to have a page to explain all of them. Yeah, there are also a page for other special commands like SHOW TABLES. Finally, [inaudible]. This is another critical enhancements in Spark 3.0 document. In this release, all the components have [inaudible] guides. When you upgrade
107-1	33926-33929	and	
107-2	33930-33933	DML	
107-3	33934-33942	commands	
107-4	33942-33943	,	
107-5	33944-33947	and	
107-6	33948-33952	also	
107-7	33953-33959	insert	
107-8	33960-33962	is	
107-9	33963-33967	also	
107-10	33968-33976	included	
107-11	33977-33979	in	
107-12	33980-33983	the	
107-13	33984-33992	document	
107-14	33992-33993	.	
107-15	33994-33996	In	
107-16	33997-34001	fact	
107-17	34001-34002	,	
107-18	34003-34009	SELECT	
107-19	34010-34013	has	
107-20	34014-34016	so	
107-21	34017-34021	many	
107-22	34022-34030	features	
107-23	34030-34031	,	
107-24	34032-34034	so	
107-25	34035-34037	we	
107-26	34038-34042	want	
107-27	34043-34045	to	
107-28	34046-34050	have	
107-29	34051-34052	a	
107-30	34053-34057	page	
107-31	34058-34060	to	
107-32	34061-34068	explain	
107-33	34069-34072	all	
107-34	34073-34075	of	
107-35	34076-34080	them	
107-36	34080-34081	.	
107-37	34082-34086	Yeah	
107-38	34086-34087	,	
107-39	34088-34093	there	
107-40	34094-34097	are	
107-41	34098-34102	also	
107-42	34103-34104	a	
107-43	34105-34109	page	
107-44	34110-34113	for	
107-45	34114-34119	other	
107-46	34120-34127	special	
107-47	34128-34136	commands	
107-48	34137-34141	like	
107-49	34142-34146	SHOW	
107-50	34147-34153	TABLES	
107-51	34153-34154	.	
107-52	34155-34162	Finally	
107-53	34162-34163	,	
107-54	34164-34165	[	
107-55	34165-34174	inaudible	
107-56	34174-34175	]	
107-57	34175-34176	.	
107-58	34177-34181	This	
107-59	34182-34184	is	
107-60	34185-34192	another	
107-61	34193-34201	critical	
107-62	34202-34214	enhancements	
107-63	34215-34217	in	
107-64	34218-34223	Spark	
107-65	34224-34227	3.0	
107-66	34228-34236	document	
107-67	34236-34237	.	
107-68	34238-34240	In	
107-69	34241-34245	this	
107-70	34246-34253	release	
107-71	34253-34254	,	
107-72	34255-34258	all	
107-73	34259-34262	the	
107-74	34263-34273	components	
107-75	34274-34278	have	
107-76	34279-34280	[	
107-77	34280-34289	inaudible	
107-78	34289-34290	]	
107-79	34291-34297	guides	
107-80	34297-34298	.	
107-81	34299-34303	When	
107-82	34304-34307	you	
107-83	34308-34315	upgrade	

#Text=your Spark version, you can read them carefully, and, in fact, [inaudible]. You might be wondering why it is much longer than the previous version. It’s because we try to document all the important looking changes you want to hear. If you upgrade into some errors, that’s another wordy document or slightly confusing error message, please open a ticket, and we will try and fix it
108-1	34316-34320	your	
108-2	34321-34326	Spark	
108-3	34327-34334	version	
108-4	34334-34335	,	
108-5	34336-34339	you	
108-6	34340-34343	can	
108-7	34344-34348	read	
108-8	34349-34353	them	
108-9	34354-34363	carefully	
108-10	34363-34364	,	
108-11	34365-34368	and	
108-12	34368-34369	,	
108-13	34370-34372	in	
108-14	34373-34377	fact	
108-15	34377-34378	,	
108-16	34379-34380	[	
108-17	34380-34389	inaudible	
108-18	34389-34390	]	
108-19	34390-34391	.	
108-20	34392-34395	You	
108-21	34396-34401	might	
108-22	34402-34404	be	
108-23	34405-34414	wondering	
108-24	34415-34418	why	
108-25	34419-34421	it	
108-26	34422-34424	is	
108-27	34425-34429	much	
108-28	34430-34436	longer	
108-29	34437-34441	than	
108-30	34442-34445	the	
108-31	34446-34454	previous	
108-32	34455-34462	version	
108-33	34462-34463	.	
108-34	34464-34466	It	
108-35	34466-34467	’	
108-36	34467-34468	s	
108-37	34469-34476	because	
108-38	34477-34479	we	
108-39	34480-34483	try	
108-40	34484-34486	to	
108-41	34487-34495	document	
108-42	34496-34499	all	
108-43	34500-34503	the	
108-44	34504-34513	important	
108-45	34514-34521	looking	
108-46	34522-34529	changes	
108-47	34530-34533	you	
108-48	34534-34538	want	
108-49	34539-34541	to	
108-50	34542-34546	hear	
108-51	34546-34547	.	
108-52	34548-34550	If	
108-53	34551-34554	you	
108-54	34555-34562	upgrade	
108-55	34563-34567	into	
108-56	34568-34572	some	
108-57	34573-34579	errors	
108-58	34579-34580	,	
108-59	34581-34585	that	
108-60	34585-34586	’	
108-61	34586-34587	s	
108-62	34588-34595	another	
108-63	34596-34601	wordy	
108-64	34602-34610	document	
108-65	34611-34613	or	
108-66	34614-34622	slightly	
108-67	34623-34632	confusing	
108-68	34633-34638	error	
108-69	34639-34646	message	
108-70	34646-34647	,	
108-71	34648-34654	please	
108-72	34655-34659	open	
108-73	34660-34661	a	
108-74	34662-34668	ticket	
108-75	34668-34669	,	
108-76	34670-34673	and	
108-77	34674-34676	we	
108-78	34677-34681	will	
108-79	34682-34685	try	
108-80	34686-34689	and	
108-81	34690-34693	fix	
108-82	34694-34696	it	

#Text=in subsequent releases. Now, that Spark is almost 10 years old now. The Spark community is very serious about making change. And we try our best to avoid [inaudible] changing. If you upgrade to Spark 3.0 at this time, you may see explicit error messages about
109-1	34697-34699	in	
109-2	34700-34710	subsequent	
109-3	34711-34719	releases	
109-4	34719-34720	.	
109-5	34721-34724	Now	
109-6	34724-34725	,	
109-7	34726-34730	that	
109-8	34731-34736	Spark	
109-9	34737-34739	is	
109-10	34740-34746	almost	
109-11	34747-34749	10	
109-12	34750-34755	years	
109-13	34756-34759	old	
109-14	34760-34763	now	
109-15	34763-34764	.	
109-16	34765-34768	The	
109-17	34769-34774	Spark	
109-18	34775-34784	community	
109-19	34785-34787	is	
109-20	34788-34792	very	
109-21	34793-34800	serious	
109-22	34801-34806	about	
109-23	34807-34813	making	
109-24	34814-34820	change	
109-25	34820-34821	.	
109-26	34822-34825	And	
109-27	34826-34828	we	
109-28	34829-34832	try	
109-29	34833-34836	our	
109-30	34837-34841	best	
109-31	34842-34844	to	
109-32	34845-34850	avoid	
109-33	34851-34852	[	
109-34	34852-34861	inaudible	
109-35	34861-34862	]	
109-36	34863-34871	changing	
109-37	34871-34872	.	
109-38	34873-34875	If	
109-39	34876-34879	you	
109-40	34880-34887	upgrade	
109-41	34888-34890	to	
109-42	34891-34896	Spark	
109-43	34897-34900	3.0	
109-44	34901-34903	at	
109-45	34904-34908	this	
109-46	34909-34913	time	
109-47	34913-34914	,	
109-48	34915-34918	you	
109-49	34919-34922	may	
109-50	34923-34926	see	
109-51	34927-34935	explicit	
109-52	34936-34941	error	
109-53	34942-34950	messages	
109-54	34951-34956	about	

#Text=changing. So the error message also provides config names for you to either go back to existing behavior or go with the new behavior. this talk, we talked about many exciting features and improvements in Spark 3.0. Due to the lack of time, there are still many other nice features not being covered by this talk. Please download Spark 3.0 and try yourself. You can also try the
110-1	34957-34965	changing	
110-2	34965-34966	.	
110-3	34967-34969	So	
110-4	34970-34973	the	
110-5	34974-34979	error	
110-6	34980-34987	message	
110-7	34988-34992	also	
110-8	34993-35001	provides	
110-9	35002-35008	config	
110-10	35009-35014	names	
110-11	35015-35018	for	
110-12	35019-35022	you	
110-13	35023-35025	to	
110-14	35026-35032	either	
110-15	35033-35035	go	
110-16	35036-35040	back	
110-17	35041-35043	to	
110-18	35044-35052	existing	
110-19	35053-35061	behavior	
110-20	35062-35064	or	
110-21	35065-35067	go	
110-22	35068-35072	with	
110-23	35073-35076	the	
110-24	35077-35080	new	
110-25	35081-35089	behavior	
110-26	35089-35090	.	
110-27	35091-35095	this	
110-28	35096-35100	talk	
110-29	35100-35101	,	
110-30	35102-35104	we	
110-31	35105-35111	talked	
110-32	35112-35117	about	
110-33	35118-35122	many	
110-34	35123-35131	exciting	
110-35	35132-35140	features	
110-36	35141-35144	and	
110-37	35145-35157	improvements	
110-38	35158-35160	in	
110-39	35161-35166	Spark	
110-40	35167-35170	3.0	
110-41	35170-35171	.	
110-42	35172-35175	Due	
110-43	35176-35178	to	
110-44	35179-35182	the	
110-45	35183-35187	lack	
110-46	35188-35190	of	
110-47	35191-35195	time	
110-48	35195-35196	,	
110-49	35197-35202	there	
110-50	35203-35206	are	
110-51	35207-35212	still	
110-52	35213-35217	many	
110-53	35218-35223	other	
110-54	35224-35228	nice	
110-55	35229-35237	features	
110-56	35238-35241	not	
110-57	35242-35247	being	
110-58	35248-35255	covered	
110-59	35256-35258	by	
110-60	35259-35263	this	
110-61	35264-35268	talk	
110-62	35268-35269	.	
110-63	35270-35276	Please	
110-64	35277-35285	download	
110-65	35286-35291	Spark	
110-66	35292-35295	3.0	
110-67	35296-35299	and	
110-68	35300-35303	try	
110-69	35304-35312	yourself	
110-70	35312-35313	.	
110-71	35314-35317	You	
110-72	35318-35321	can	
110-73	35322-35326	also	
110-74	35327-35330	try	
110-75	35331-35334	the	

#Text=[inaudible] Databricks [inaudible] 10.0 beta. All the new features are already available. The Community Edition is for free. Without the contributions by the whole community, it is impossible to deliver such a successful release. It’s thanks to all the Spark committers all over the world. Thank you. Thank you, everyone. Watch more Spark + AI sessions here
111-1	35335-35336	[	
111-2	35336-35345	inaudible	
111-3	35345-35346	]	
111-4	35347-35357	Databricks	
111-5	35358-35359	[	
111-6	35359-35368	inaudible	
111-7	35368-35369	]	
111-8	35370-35374	10.0	
111-9	35375-35379	beta	
111-10	35379-35380	.	
111-11	35381-35384	All	
111-12	35385-35388	the	
111-13	35389-35392	new	
111-14	35393-35401	features	
111-15	35402-35405	are	
111-16	35406-35413	already	
111-17	35414-35423	available	
111-18	35423-35424	.	
111-19	35425-35428	The	
111-20	35429-35438	Community	
111-21	35439-35446	Edition	
111-22	35447-35449	is	
111-23	35450-35453	for	
111-24	35454-35458	free	
111-25	35458-35459	.	
111-26	35460-35467	Without	
111-27	35468-35471	the	
111-28	35472-35485	contributions	
111-29	35486-35488	by	
111-30	35489-35492	the	
111-31	35493-35498	whole	
111-32	35499-35508	community	
111-33	35508-35509	,	
111-34	35510-35512	it	
111-35	35513-35515	is	
111-36	35516-35526	impossible	
111-37	35527-35529	to	
111-38	35530-35537	deliver	
111-39	35538-35542	such	
111-40	35543-35544	a	
111-41	35545-35555	successful	
111-42	35556-35563	release	
111-43	35563-35564	.	
111-44	35565-35567	It	
111-45	35567-35568	’	
111-46	35568-35569	s	
111-47	35570-35576	thanks	
111-48	35577-35579	to	
111-49	35580-35583	all	
111-50	35584-35587	the	
111-51	35588-35593	Spark	
111-52	35594-35604	committers	
111-53	35605-35608	all	
111-54	35609-35613	over	
111-55	35614-35617	the	
111-56	35618-35623	world	
111-57	35623-35624	.	
111-58	35625-35630	Thank	
111-59	35631-35634	you	
111-60	35634-35635	.	
111-61	35636-35641	Thank	
111-62	35642-35645	you	
111-63	35645-35646	,	
111-64	35647-35655	everyone	
111-65	35655-35656	.	
111-66	35657-35662	Watch	
111-67	35663-35667	more	
111-68	35668-35673	Spark	
111-69	35674-35675	+	
111-70	35676-35678	AI	
111-71	35679-35687	sessions	
111-72	35688-35692	here	

#Text=Try Databricks for free « back About Xiao Li Databricks Xiao Li is an engineering manager, Apache Spark Committer and PMC member at Databricks. His main interests are on Spark SQL, data replication and data integration. Previously, he was an IBM master inventor and an expert on asynchronous database replication and consistency verification. He received his Ph.D. from University of Florida in 2011.
112-1	35693-35696	Try	
112-2	35697-35707	Databricks	
112-3	35708-35711	for	
112-4	35712-35716	free	
112-5	35717-35718	«	
112-6	35719-35723	back	
112-7	35724-35729	About	
112-8	35730-35734	Xiao	
112-9	35735-35737	Li	
112-10	35738-35748	Databricks	
112-11	35749-35753	Xiao	
112-12	35754-35756	Li	
112-13	35757-35759	is	
112-14	35760-35762	an	
112-15	35763-35774	engineering	
112-16	35775-35782	manager	
112-17	35782-35783	,	
112-18	35784-35790	Apache	
112-19	35791-35796	Spark	
112-20	35797-35806	Committer	
112-21	35807-35810	and	
112-22	35811-35814	PMC	
112-23	35815-35821	member	
112-24	35822-35824	at	
112-25	35825-35835	Databricks	
112-26	35835-35836	.	
112-27	35837-35840	His	
112-28	35841-35845	main	
112-29	35846-35855	interests	
112-30	35856-35859	are	
112-31	35860-35862	on	
112-32	35863-35868	Spark	
112-33	35869-35872	SQL	
112-34	35872-35873	,	
112-35	35874-35878	data	
112-36	35879-35890	replication	
112-37	35891-35894	and	
112-38	35895-35899	data	
112-39	35900-35911	integration	
112-40	35911-35912	.	
112-41	35913-35923	Previously	
112-42	35923-35924	,	
112-43	35925-35927	he	
112-44	35928-35931	was	
112-45	35932-35934	an	
112-46	35935-35938	IBM	
112-47	35939-35945	master	
112-48	35946-35954	inventor	
112-49	35955-35958	and	
112-50	35959-35961	an	
112-51	35962-35968	expert	
112-52	35969-35971	on	
112-53	35972-35984	asynchronous	
112-54	35985-35993	database	
112-55	35994-36005	replication	
112-56	36006-36009	and	
112-57	36010-36021	consistency	
112-58	36022-36034	verification	
112-59	36034-36035	.	
112-60	36036-36038	He	
112-61	36039-36047	received	
112-62	36048-36051	his	
112-63	36052-36056	Ph.D	
112-64	36056-36057	.	
112-65	36058-36062	from	
112-66	36063-36073	University	
112-67	36074-36076	of	
112-68	36077-36084	Florida	
112-69	36085-36087	in	
112-70	36088-36092	2011	
112-71	36092-36093	.	

#Text=About Wenchen Fan Databricks Wenchen Fan is a software engineer at Databricks, working on Spark Core and Spark SQL. He mainly focuses on the Apache Spark open source community, leading the discussion and reviews of many features/fixes in Spark. He is a Spark committer and a Spark PMC member.
113-1	36094-36099	About	
113-2	36100-36107	Wenchen	
113-3	36108-36111	Fan	
113-4	36112-36122	Databricks	
113-5	36123-36130	Wenchen	
113-6	36131-36134	Fan	
113-7	36135-36137	is	
113-8	36138-36139	a	
113-9	36140-36148	software	
113-10	36149-36157	engineer	
113-11	36158-36160	at	
113-12	36161-36171	Databricks	
113-13	36171-36172	,	
113-14	36173-36180	working	
113-15	36181-36183	on	
113-16	36184-36189	Spark	
113-17	36190-36194	Core	
113-18	36195-36198	and	
113-19	36199-36204	Spark	
113-20	36205-36208	SQL	
113-21	36208-36209	.	
113-22	36210-36212	He	
113-23	36213-36219	mainly	
113-24	36220-36227	focuses	
113-25	36228-36230	on	
113-26	36231-36234	the	
113-27	36235-36241	Apache	
113-28	36242-36247	Spark	
113-29	36248-36252	open	
113-30	36253-36259	source	
113-31	36260-36269	community	
113-32	36269-36270	,	
113-33	36271-36278	leading	
113-34	36279-36282	the	
113-35	36283-36293	discussion	
113-36	36294-36297	and	
113-37	36298-36305	reviews	
113-38	36306-36308	of	
113-39	36309-36313	many	
113-40	36314-36322	features	
113-41	36322-36323	/	
113-42	36323-36328	fixes	
113-43	36329-36331	in	
113-44	36332-36337	Spark	
113-45	36337-36338	.	
113-46	36339-36341	He	
113-47	36342-36344	is	
113-48	36345-36346	a	
113-49	36347-36352	Spark	
113-50	36353-36362	committer	
113-51	36363-36366	and	
113-52	36367-36368	a	
113-53	36369-36374	Spark	
113-54	36375-36378	PMC	
113-55	36379-36385	member	
113-56	36385-36386	.	

#Text=Video Archive Terms of Use Privacy Policy Event Policy Looking for a talk from a past event? Check the Video Archive Organized by Databricks If you have questions, or would like information on sponsoring a Spark + AI Summit, please contact organizers@spark-summit.org.
114-1	36387-36392	Video	
114-2	36393-36400	Archive	
114-3	36401-36406	Terms	
114-4	36407-36409	of	
114-5	36410-36413	Use	
114-6	36414-36421	Privacy	
114-7	36422-36428	Policy	
114-8	36429-36434	Event	
114-9	36435-36441	Policy	
114-10	36442-36449	Looking	
114-11	36450-36453	for	
114-12	36454-36455	a	
114-13	36456-36460	talk	
114-14	36461-36465	from	
114-15	36466-36467	a	
114-16	36468-36472	past	
114-17	36473-36478	event	
114-18	36478-36479	?	
114-19	36480-36485	Check	
114-20	36486-36489	the	
114-21	36490-36495	Video	
114-22	36496-36503	Archive	
114-23	36504-36513	Organized	
114-24	36514-36516	by	
114-25	36517-36527	Databricks	
114-26	36528-36530	If	
114-27	36531-36534	you	
114-28	36535-36539	have	
114-29	36540-36549	questions	
114-30	36549-36550	,	
114-31	36551-36553	or	
114-32	36554-36559	would	
114-33	36560-36564	like	
114-34	36565-36576	information	
114-35	36577-36579	on	
114-36	36580-36590	sponsoring	
114-37	36591-36592	a	
114-38	36593-36598	Spark	
114-39	36599-36600	+	
114-40	36601-36603	AI	
114-41	36604-36610	Summit	
114-42	36610-36611	,	
114-43	36612-36618	please	
114-44	36619-36626	contact	
114-45	36627-36637	organizers	
114-46	36637-36638	@	
114-47	36638-36654	spark-summit.org	
114-48	36654-36655	.	
