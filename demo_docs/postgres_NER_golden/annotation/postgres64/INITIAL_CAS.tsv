#FORMAT=WebAnno TSV 3.3


#Text=Apache Flink 1.12 Documentation: JDBC SQL Connector
#Text=v1.12
#Text=Home
#Text=Try Flink
#Text=Local Installation
#Text=Fraud Detection with the DataStream API
#Text=Real Time Reporting with the Table API
#Text=Flink Operations Playground
#Text=Learn Flink
#Text=Overview
#Text=Intro to the DataStream API
#Text=Data Pipelines & ETL
#Text=Streaming Analytics
#Text=Event-driven Applications
#Text=Fault Tolerance
#Text=Concepts
#Text=Overview
#Text=Stateful Stream Processing
#Text=Timely Stream Processing
#Text=Flink Architecture
#Text=Glossary
#Text=Application Development
#Text=DataStream API
#Text=Overview
#Text=Execution Mode (Batch/Streaming)
#Text=Event Time
#Text=Overview
#Text=Generating Watermarks
#Text=Builtin Watermark Generators
#Text=State & Fault Tolerance
#Text=Overview
#Text=Working with State
#Text=The Broadcast State Pattern
#Text=Checkpointing
#Text=Queryable State
#Text=State Backends
#Text=State Schema Evolution
#Text=Custom State Serialization
#Text=User-Defined Functions
#Text=Operators
#Text=Overview
#Text=Windows
#Text=Joining
#Text=Process Function
#Text=Async I/O
#Text=Data Sources
#Text=Side Outputs
#Text=Handling Application Parameters
#Text=Testing
#Text=Experimental Features
#Text=Scala API Extensions
#Text=Java Lambda Expressions
#Text=Project Configuration
#Text=DataSet API
#Text=Overview
#Text=Transformations
#Text=Iterations
#Text=Zipping Elements
#Text=Hadoop Compatibility
#Text=Local Execution
#Text=Cluster Execution
#Text=Batch Examples
#Text=Table API & SQL
#Text=Overview
#Text=Concepts & Common API
#Text=Streaming Concepts
#Text=Overview
#Text=Dynamic Tables
#Text=Time Attributes
#Text=Versioned Tables
#Text=Joins in Continuous Queries
#Text=Detecting Patterns
#Text=Query Configuration
#Text=Legacy Features
#Text=Data Types
#Text=Table API
#Text=SQL
#Text=Overview
#Text=Queries
#Text=CREATE Statements
#Text=DROP Statements
#Text=ALTER Statements
#Text=INSERT Statement
#Text=SQL Hints
#Text=DESCRIBE Statements
#Text=EXPLAIN Statements
#Text=USE Statements
#Text=SHOW Statements
#Text=Functions
#Text=Overview
#Text=System (Built-in) Functions
#Text=User-defined Functions
#Text=Modules
#Text=Catalogs
#Text=SQL Client
#Text=Configuration
#Text=Performance Tuning
#Text=Streaming Aggregation
#Text=User-defined Sources & Sinks
#Text=Python API
#Text=Overview
#Text=Installation
#Text=Table API Tutorial
#Text=DataStream API Tutorial
#Text=Table API User's Guide
#Text=Intro to the Python Table API
#Text=TableEnvironment
#Text=Operations
#Text=Data Types
#Text=System (Built-in) Functions
#Text=User Defined Functions
#Text=General User-defined Functions
#Text=Vectorized User-defined Functions
#Text=Conversions between PyFlink Table and Pandas DataFrame
#Text=Dependency Management
#Text=SQL
#Text=Catalogs
#Text=Metrics
#Text=Connectors
#Text=DataStream API User's Guide
#Text=Data Types
#Text=Operators
#Text=Dependency Management
#Text=Configuration
#Text=Environment Variables
#Text=FAQ
#Text=Data Types & Serialization
#Text=Overview
#Text=Custom Serializers
#Text=Managing Execution
#Text=Execution Configuration
#Text=Program Packaging
#Text=Parallel Execution
#Text=Execution Plans
#Text=Task Failure Recovery
#Text=API Migration Guides
#Text=Libraries
#Text=Event Processing (CEP)
#Text=State Processor API
#Text=Graphs: Gelly
#Text=Overview
#Text=Graph API
#Text=Iterative Graph Processing
#Text=Library Methods
#Text=Graph Algorithms
#Text=Graph Generators
#Text=Bipartite Graph
#Text=Connectors
#Text=DataStream Connectors
#Text=Overview
#Text=Fault Tolerance Guarantees
#Text=Kafka
#Text=Cassandra
#Text=Kinesis
#Text=Elasticsearch
#Text=File Sink
#Text=Streaming File Sink
#Text=RabbitMQ
#Text=NiFi
#Text=Google Cloud PubSub
#Text=Twitter
#Text=JDBC
#Text=Table & SQL Connectors
#Text=Overview
#Text=Formats
#Text=Overview
#Text=CSV
#Text=JSON
#Text=Confluent Avro
#Text=Avro
#Text=Debezium
#Text=Canal
#Text=Maxwell
#Text=Parquet
#Text=Orc
#Text=Raw
#Text=Kafka
#Text=Upsert Kafka
#Text=Kinesis
#Text=JDBC
#Text=Elasticsearch
#Text=FileSystem
#Text=HBase
#Text=DataGen
#Text=Print
#Text=BlackHole
#Text=Hive
#Text=Overview
#Text=Hive Catalog
#Text=Hive Dialect
#Text=Hive Read & Write
#Text=Hive Functions
#Text=Download
#Text=DataSet Connectors
#Text=Deployment
#Text=Overview
#Text=Resource Providers
#Text=Standalone
#Text=Overview
#Text=Docker
#Text=Kubernetes
#Text=Native Kubernetes
#Text=YARN
#Text=Mesos
#Text=Configuration
#Text=Memory Configuration
#Text=Set up Flink's Process Memory
#Text=Set up TaskManager Memory
#Text=Set up JobManager Memory
#Text=Memory tuning guide
#Text=Troubleshooting
#Text=Migration Guide
#Text=Command-Line Interface
#Text=File Systems
#Text=Overview
#Text=Common Configurations
#Text=Amazon S3
#Text=Aliyun OSS
#Text=Azure Blob Storage
#Text=Plugins
#Text=High Availability (HA)
#Text=Overview
#Text=ZooKeeper HA Services
#Text=Kubernetes HA Services
#Text=Metric Reporters
#Text=Security
#Text=SSL Setup
#Text=Kerberos
#Text=REPLs
#Text=Python REPL
#Text=Scala REPL
#Text=Advanced
#Text=External Resources
#Text=History Server
#Text=Logging
#Text=Operations
#Text=State & Fault Tolerance
#Text=Checkpoints
#Text=Savepoints
#Text=State Backends
#Text=Tuning Checkpoints and Large State
#Text=Metrics
#Text=REST API
#Text=Debugging
#Text=Debugging Windows & Event Time
#Text=Debugging Classloading
#Text=Application Profiling & Debugging
#Text=Monitoring
#Text=Monitoring Checkpointing
#Text=Monitoring Back Pressure
#Text=Upgrading Applications and Flink Versions
#Text=Production Readiness Checklist
#Text=Flink Development
#Text=Importing Flink into an IDE
#Text=Building Flink from Source
#Text=Internals
#Text=Jobs and Scheduling
#Text=Task Lifecycle
#Text=File Systems
#Text=Javadocs
#Text=Scaladocs
#Text=Pythondocs
#Text=Project Page
#Text=Pick Docs Version
#Text=v1.11
#Text=v1.10
#Text=v1.9
#Text=v1.8
#Text=v1.7
#Text=v1.6
#Text=v1.5
#Text=v1.4
#Text=v1.3
#Text=v1.2
#Text=v1.1
#Text=v1.0
#Text=中文版
#Text=Connectors
#Text=Table & SQL Connectors
#Text=JDBC
#Text=JDBC SQL Connector
#Text=Scan Source: Bounded
#Text=Lookup Source: Sync Mode
#Text=Sink: Batch
#Text=Sink: Streaming Append & Upsert Mode
#Text=Dependencies
#Text=How to create a JDBC table
#Text=Connector Options
#Text=Features
#Text=Key handling
#Text=Partitioned Scan
#Text=Lookup Cache
#Text=Idempotent Writes
#Text=Postgres Database as a Catalog
#Text=Data Type Mapping
#Text=The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver.
1-1	0-6	Apache	
1-2	7-12	Flink	
1-3	13-17	1.12	
1-4	18-31	Documentation	
1-5	31-32	:	
1-6	33-37	JDBC	
1-7	38-41	SQL	
1-8	42-51	Connector	
1-9	52-57	v1.12	
1-10	58-62	Home	
1-11	63-66	Try	
1-12	67-72	Flink	
1-13	73-78	Local	
1-14	79-91	Installation	
1-15	92-97	Fraud	
1-16	98-107	Detection	
1-17	108-112	with	
1-18	113-116	the	
1-19	117-127	DataStream	
1-20	128-131	API	
1-21	132-136	Real	
1-22	137-141	Time	
1-23	142-151	Reporting	
1-24	152-156	with	
1-25	157-160	the	
1-26	161-166	Table	
1-27	167-170	API	
1-28	171-176	Flink	
1-29	177-187	Operations	
1-30	188-198	Playground	
1-31	199-204	Learn	
1-32	205-210	Flink	
1-33	211-219	Overview	
1-34	220-225	Intro	
1-35	226-228	to	
1-36	229-232	the	
1-37	233-243	DataStream	
1-38	244-247	API	
1-39	248-252	Data	
1-40	253-262	Pipelines	
1-41	263-264	&	
1-42	265-268	ETL	
1-43	269-278	Streaming	
1-44	279-288	Analytics	
1-45	289-301	Event-driven	
1-46	302-314	Applications	
1-47	315-320	Fault	
1-48	321-330	Tolerance	
1-49	331-339	Concepts	
1-50	340-348	Overview	
1-51	349-357	Stateful	
1-52	358-364	Stream	
1-53	365-375	Processing	
1-54	376-382	Timely	
1-55	383-389	Stream	
1-56	390-400	Processing	
1-57	401-406	Flink	
1-58	407-419	Architecture	
1-59	420-428	Glossary	
1-60	429-440	Application	
1-61	441-452	Development	
1-62	453-463	DataStream	
1-63	464-467	API	
1-64	468-476	Overview	
1-65	477-486	Execution	
1-66	487-491	Mode	
1-67	492-493	(	
1-68	493-498	Batch	
1-69	498-499	/	
1-70	499-508	Streaming	
1-71	508-509	)	
1-72	510-515	Event	
1-73	516-520	Time	
1-74	521-529	Overview	
1-75	530-540	Generating	
1-76	541-551	Watermarks	
1-77	552-559	Builtin	
1-78	560-569	Watermark	
1-79	570-580	Generators	
1-80	581-586	State	
1-81	587-588	&	
1-82	589-594	Fault	
1-83	595-604	Tolerance	
1-84	605-613	Overview	
1-85	614-621	Working	
1-86	622-626	with	
1-87	627-632	State	
1-88	633-636	The	
1-89	637-646	Broadcast	
1-90	647-652	State	
1-91	653-660	Pattern	
1-92	661-674	Checkpointing	
1-93	675-684	Queryable	
1-94	685-690	State	
1-95	691-696	State	
1-96	697-705	Backends	
1-97	706-711	State	
1-98	712-718	Schema	
1-99	719-728	Evolution	
1-100	729-735	Custom	
1-101	736-741	State	
1-102	742-755	Serialization	
1-103	756-768	User-Defined	
1-104	769-778	Functions	
1-105	779-788	Operators	
1-106	789-797	Overview	
1-107	798-805	Windows	
1-108	806-813	Joining	
1-109	814-821	Process	
1-110	822-830	Function	
1-111	831-836	Async	
1-112	837-838	I	
1-113	838-839	/	
1-114	839-840	O	
1-115	841-845	Data	
1-116	846-853	Sources	
1-117	854-858	Side	
1-118	859-866	Outputs	
1-119	867-875	Handling	
1-120	876-887	Application	
1-121	888-898	Parameters	
1-122	899-906	Testing	
1-123	907-919	Experimental	
1-124	920-928	Features	
1-125	929-934	Scala	
1-126	935-938	API	
1-127	939-949	Extensions	
1-128	950-954	Java	
1-129	955-961	Lambda	
1-130	962-973	Expressions	
1-131	974-981	Project	
1-132	982-995	Configuration	
1-133	996-1003	DataSet	
1-134	1004-1007	API	
1-135	1008-1016	Overview	
1-136	1017-1032	Transformations	
1-137	1033-1043	Iterations	
1-138	1044-1051	Zipping	
1-139	1052-1060	Elements	
1-140	1061-1067	Hadoop	
1-141	1068-1081	Compatibility	
1-142	1082-1087	Local	
1-143	1088-1097	Execution	
1-144	1098-1105	Cluster	
1-145	1106-1115	Execution	
1-146	1116-1121	Batch	
1-147	1122-1130	Examples	
1-148	1131-1136	Table	
1-149	1137-1140	API	
1-150	1141-1142	&	
1-151	1143-1146	SQL	
1-152	1147-1155	Overview	
1-153	1156-1164	Concepts	
1-154	1165-1166	&	
1-155	1167-1173	Common	
1-156	1174-1177	API	
1-157	1178-1187	Streaming	
1-158	1188-1196	Concepts	
1-159	1197-1205	Overview	
1-160	1206-1213	Dynamic	
1-161	1214-1220	Tables	
1-162	1221-1225	Time	
1-163	1226-1236	Attributes	
1-164	1237-1246	Versioned	
1-165	1247-1253	Tables	
1-166	1254-1259	Joins	
1-167	1260-1262	in	
1-168	1263-1273	Continuous	
1-169	1274-1281	Queries	
1-170	1282-1291	Detecting	
1-171	1292-1300	Patterns	
1-172	1301-1306	Query	
1-173	1307-1320	Configuration	
1-174	1321-1327	Legacy	
1-175	1328-1336	Features	
1-176	1337-1341	Data	
1-177	1342-1347	Types	
1-178	1348-1353	Table	
1-179	1354-1357	API	
1-180	1358-1361	SQL	
1-181	1362-1370	Overview	
1-182	1371-1378	Queries	
1-183	1379-1385	CREATE	
1-184	1386-1396	Statements	
1-185	1397-1401	DROP	
1-186	1402-1412	Statements	
1-187	1413-1418	ALTER	
1-188	1419-1429	Statements	
1-189	1430-1436	INSERT	
1-190	1437-1446	Statement	
1-191	1447-1450	SQL	
1-192	1451-1456	Hints	
1-193	1457-1465	DESCRIBE	
1-194	1466-1476	Statements	
1-195	1477-1484	EXPLAIN	
1-196	1485-1495	Statements	
1-197	1496-1499	USE	
1-198	1500-1510	Statements	
1-199	1511-1515	SHOW	
1-200	1516-1526	Statements	
1-201	1527-1536	Functions	
1-202	1537-1545	Overview	
1-203	1546-1552	System	
1-204	1553-1554	(	
1-205	1554-1562	Built-in	
1-206	1562-1563	)	
1-207	1564-1573	Functions	
1-208	1574-1586	User-defined	
1-209	1587-1596	Functions	
1-210	1597-1604	Modules	
1-211	1605-1613	Catalogs	
1-212	1614-1617	SQL	
1-213	1618-1624	Client	
1-214	1625-1638	Configuration	
1-215	1639-1650	Performance	
1-216	1651-1657	Tuning	
1-217	1658-1667	Streaming	
1-218	1668-1679	Aggregation	
1-219	1680-1692	User-defined	
1-220	1693-1700	Sources	
1-221	1701-1702	&	
1-222	1703-1708	Sinks	
1-223	1709-1715	Python	
1-224	1716-1719	API	
1-225	1720-1728	Overview	
1-226	1729-1741	Installation	
1-227	1742-1747	Table	
1-228	1748-1751	API	
1-229	1752-1760	Tutorial	
1-230	1761-1771	DataStream	
1-231	1772-1775	API	
1-232	1776-1784	Tutorial	
1-233	1785-1790	Table	
1-234	1791-1794	API	
1-235	1795-1801	User's	
1-236	1802-1807	Guide	
1-237	1808-1813	Intro	
1-238	1814-1816	to	
1-239	1817-1820	the	
1-240	1821-1827	Python	
1-241	1828-1833	Table	
1-242	1834-1837	API	
1-243	1838-1854	TableEnvironment	
1-244	1855-1865	Operations	
1-245	1866-1870	Data	
1-246	1871-1876	Types	
1-247	1877-1883	System	
1-248	1884-1885	(	
1-249	1885-1893	Built-in	
1-250	1893-1894	)	
1-251	1895-1904	Functions	
1-252	1905-1909	User	
1-253	1910-1917	Defined	
1-254	1918-1927	Functions	
1-255	1928-1935	General	
1-256	1936-1948	User-defined	
1-257	1949-1958	Functions	
1-258	1959-1969	Vectorized	
1-259	1970-1982	User-defined	
1-260	1983-1992	Functions	
1-261	1993-2004	Conversions	
1-262	2005-2012	between	
1-263	2013-2020	PyFlink	
1-264	2021-2026	Table	
1-265	2027-2030	and	
1-266	2031-2037	Pandas	
1-267	2038-2047	DataFrame	
1-268	2048-2058	Dependency	
1-269	2059-2069	Management	
1-270	2070-2073	SQL	
1-271	2074-2082	Catalogs	
1-272	2083-2090	Metrics	
1-273	2091-2101	Connectors	
1-274	2102-2112	DataStream	
1-275	2113-2116	API	
1-276	2117-2123	User's	
1-277	2124-2129	Guide	
1-278	2130-2134	Data	
1-279	2135-2140	Types	
1-280	2141-2150	Operators	
1-281	2151-2161	Dependency	
1-282	2162-2172	Management	
1-283	2173-2186	Configuration	
1-284	2187-2198	Environment	
1-285	2199-2208	Variables	
1-286	2209-2212	FAQ	
1-287	2213-2217	Data	
1-288	2218-2223	Types	
1-289	2224-2225	&	
1-290	2226-2239	Serialization	
1-291	2240-2248	Overview	
1-292	2249-2255	Custom	
1-293	2256-2267	Serializers	
1-294	2268-2276	Managing	
1-295	2277-2286	Execution	
1-296	2287-2296	Execution	
1-297	2297-2310	Configuration	
1-298	2311-2318	Program	
1-299	2319-2328	Packaging	
1-300	2329-2337	Parallel	
1-301	2338-2347	Execution	
1-302	2348-2357	Execution	
1-303	2358-2363	Plans	
1-304	2364-2368	Task	
1-305	2369-2376	Failure	
1-306	2377-2385	Recovery	
1-307	2386-2389	API	
1-308	2390-2399	Migration	
1-309	2400-2406	Guides	
1-310	2407-2416	Libraries	
1-311	2417-2422	Event	
1-312	2423-2433	Processing	
1-313	2434-2435	(	
1-314	2435-2438	CEP	
1-315	2438-2439	)	
1-316	2440-2445	State	
1-317	2446-2455	Processor	
1-318	2456-2459	API	
1-319	2460-2466	Graphs	
1-320	2466-2467	:	
1-321	2468-2473	Gelly	
1-322	2474-2482	Overview	
1-323	2483-2488	Graph	
1-324	2489-2492	API	
1-325	2493-2502	Iterative	
1-326	2503-2508	Graph	
1-327	2509-2519	Processing	
1-328	2520-2527	Library	
1-329	2528-2535	Methods	
1-330	2536-2541	Graph	
1-331	2542-2552	Algorithms	
1-332	2553-2558	Graph	
1-333	2559-2569	Generators	
1-334	2570-2579	Bipartite	
1-335	2580-2585	Graph	
1-336	2586-2596	Connectors	
1-337	2597-2607	DataStream	
1-338	2608-2618	Connectors	
1-339	2619-2627	Overview	
1-340	2628-2633	Fault	
1-341	2634-2643	Tolerance	
1-342	2644-2654	Guarantees	
1-343	2655-2660	Kafka	
1-344	2661-2670	Cassandra	
1-345	2671-2678	Kinesis	
1-346	2679-2692	Elasticsearch	
1-347	2693-2697	File	
1-348	2698-2702	Sink	
1-349	2703-2712	Streaming	
1-350	2713-2717	File	
1-351	2718-2722	Sink	
1-352	2723-2731	RabbitMQ	
1-353	2732-2736	NiFi	
1-354	2737-2743	Google	
1-355	2744-2749	Cloud	
1-356	2750-2756	PubSub	
1-357	2757-2764	Twitter	
1-358	2765-2769	JDBC	
1-359	2770-2775	Table	
1-360	2776-2777	&	
1-361	2778-2781	SQL	
1-362	2782-2792	Connectors	
1-363	2793-2801	Overview	
1-364	2802-2809	Formats	
1-365	2810-2818	Overview	
1-366	2819-2822	CSV	
1-367	2823-2827	JSON	
1-368	2828-2837	Confluent	
1-369	2838-2842	Avro	
1-370	2843-2847	Avro	
1-371	2848-2856	Debezium	
1-372	2857-2862	Canal	
1-373	2863-2870	Maxwell	
1-374	2871-2878	Parquet	
1-375	2879-2882	Orc	
1-376	2883-2886	Raw	
1-377	2887-2892	Kafka	
1-378	2893-2899	Upsert	
1-379	2900-2905	Kafka	
1-380	2906-2913	Kinesis	
1-381	2914-2918	JDBC	
1-382	2919-2932	Elasticsearch	
1-383	2933-2943	FileSystem	
1-384	2944-2949	HBase	
1-385	2950-2957	DataGen	
1-386	2958-2963	Print	
1-387	2964-2973	BlackHole	
1-388	2974-2978	Hive	
1-389	2979-2987	Overview	
1-390	2988-2992	Hive	
1-391	2993-3000	Catalog	
1-392	3001-3005	Hive	
1-393	3006-3013	Dialect	
1-394	3014-3018	Hive	
1-395	3019-3023	Read	
1-396	3024-3025	&	
1-397	3026-3031	Write	
1-398	3032-3036	Hive	
1-399	3037-3046	Functions	
1-400	3047-3055	Download	
1-401	3056-3063	DataSet	
1-402	3064-3074	Connectors	
1-403	3075-3085	Deployment	
1-404	3086-3094	Overview	
1-405	3095-3103	Resource	
1-406	3104-3113	Providers	
1-407	3114-3124	Standalone	
1-408	3125-3133	Overview	
1-409	3134-3140	Docker	
1-410	3141-3151	Kubernetes	
1-411	3152-3158	Native	
1-412	3159-3169	Kubernetes	
1-413	3170-3174	YARN	
1-414	3175-3180	Mesos	
1-415	3181-3194	Configuration	
1-416	3195-3201	Memory	
1-417	3202-3215	Configuration	
1-418	3216-3219	Set	
1-419	3220-3222	up	
1-420	3223-3230	Flink's	
1-421	3231-3238	Process	
1-422	3239-3245	Memory	
1-423	3246-3249	Set	
1-424	3250-3252	up	
1-425	3253-3264	TaskManager	
1-426	3265-3271	Memory	
1-427	3272-3275	Set	
1-428	3276-3278	up	
1-429	3279-3289	JobManager	
1-430	3290-3296	Memory	
1-431	3297-3303	Memory	
1-432	3304-3310	tuning	
1-433	3311-3316	guide	
1-434	3317-3332	Troubleshooting	
1-435	3333-3342	Migration	
1-436	3343-3348	Guide	
1-437	3349-3361	Command-Line	
1-438	3362-3371	Interface	
1-439	3372-3376	File	
1-440	3377-3384	Systems	
1-441	3385-3393	Overview	
1-442	3394-3400	Common	
1-443	3401-3415	Configurations	
1-444	3416-3422	Amazon	
1-445	3423-3425	S3	
1-446	3426-3432	Aliyun	
1-447	3433-3436	OSS	
1-448	3437-3442	Azure	
1-449	3443-3447	Blob	
1-450	3448-3455	Storage	
1-451	3456-3463	Plugins	
1-452	3464-3468	High	
1-453	3469-3481	Availability	
1-454	3482-3483	(	
1-455	3483-3485	HA	
1-456	3485-3486	)	
1-457	3487-3495	Overview	
1-458	3496-3505	ZooKeeper	
1-459	3506-3508	HA	
1-460	3509-3517	Services	
1-461	3518-3528	Kubernetes	
1-462	3529-3531	HA	
1-463	3532-3540	Services	
1-464	3541-3547	Metric	
1-465	3548-3557	Reporters	
1-466	3558-3566	Security	
1-467	3567-3570	SSL	
1-468	3571-3576	Setup	
1-469	3577-3585	Kerberos	
1-470	3586-3591	REPLs	
1-471	3592-3598	Python	
1-472	3599-3603	REPL	
1-473	3604-3609	Scala	
1-474	3610-3614	REPL	
1-475	3615-3623	Advanced	
1-476	3624-3632	External	
1-477	3633-3642	Resources	
1-478	3643-3650	History	
1-479	3651-3657	Server	
1-480	3658-3665	Logging	
1-481	3666-3676	Operations	
1-482	3677-3682	State	
1-483	3683-3684	&	
1-484	3685-3690	Fault	
1-485	3691-3700	Tolerance	
1-486	3701-3712	Checkpoints	
1-487	3713-3723	Savepoints	
1-488	3724-3729	State	
1-489	3730-3738	Backends	
1-490	3739-3745	Tuning	
1-491	3746-3757	Checkpoints	
1-492	3758-3761	and	
1-493	3762-3767	Large	
1-494	3768-3773	State	
1-495	3774-3781	Metrics	
1-496	3782-3786	REST	
1-497	3787-3790	API	
1-498	3791-3800	Debugging	
1-499	3801-3810	Debugging	
1-500	3811-3818	Windows	
1-501	3819-3820	&	
1-502	3821-3826	Event	
1-503	3827-3831	Time	
1-504	3832-3841	Debugging	
1-505	3842-3854	Classloading	
1-506	3855-3866	Application	
1-507	3867-3876	Profiling	
1-508	3877-3878	&	
1-509	3879-3888	Debugging	
1-510	3889-3899	Monitoring	
1-511	3900-3910	Monitoring	
1-512	3911-3924	Checkpointing	
1-513	3925-3935	Monitoring	
1-514	3936-3940	Back	
1-515	3941-3949	Pressure	
1-516	3950-3959	Upgrading	
1-517	3960-3972	Applications	
1-518	3973-3976	and	
1-519	3977-3982	Flink	
1-520	3983-3991	Versions	
1-521	3992-4002	Production	
1-522	4003-4012	Readiness	
1-523	4013-4022	Checklist	
1-524	4023-4028	Flink	
1-525	4029-4040	Development	
1-526	4041-4050	Importing	
1-527	4051-4056	Flink	
1-528	4057-4061	into	
1-529	4062-4064	an	
1-530	4065-4068	IDE	
1-531	4069-4077	Building	
1-532	4078-4083	Flink	
1-533	4084-4088	from	
1-534	4089-4095	Source	
1-535	4096-4105	Internals	
1-536	4106-4110	Jobs	
1-537	4111-4114	and	
1-538	4115-4125	Scheduling	
1-539	4126-4130	Task	
1-540	4131-4140	Lifecycle	
1-541	4141-4145	File	
1-542	4146-4153	Systems	
1-543	4154-4162	Javadocs	
1-544	4163-4172	Scaladocs	
1-545	4173-4183	Pythondocs	
1-546	4184-4191	Project	
1-547	4192-4196	Page	
1-548	4197-4201	Pick	
1-549	4202-4206	Docs	
1-550	4207-4214	Version	
1-551	4215-4220	v1.11	
1-552	4221-4226	v1.10	
1-553	4227-4231	v1.9	
1-554	4232-4236	v1.8	
1-555	4237-4241	v1.7	
1-556	4242-4246	v1.6	
1-557	4247-4251	v1.5	
1-558	4252-4256	v1.4	
1-559	4257-4261	v1.3	
1-560	4262-4266	v1.2	
1-561	4267-4271	v1.1	
1-562	4272-4276	v1.0	
1-563	4277-4280	中文版	
1-564	4281-4291	Connectors	
1-565	4292-4297	Table	
1-566	4298-4299	&	
1-567	4300-4303	SQL	
1-568	4304-4314	Connectors	
1-569	4315-4319	JDBC	
1-570	4320-4324	JDBC	
1-571	4325-4328	SQL	
1-572	4329-4338	Connector	
1-573	4339-4343	Scan	
1-574	4344-4350	Source	
1-575	4350-4351	:	
1-576	4352-4359	Bounded	
1-577	4360-4366	Lookup	
1-578	4367-4373	Source	
1-579	4373-4374	:	
1-580	4375-4379	Sync	
1-581	4380-4384	Mode	
1-582	4385-4389	Sink	
1-583	4389-4390	:	
1-584	4391-4396	Batch	
1-585	4397-4401	Sink	
1-586	4401-4402	:	
1-587	4403-4412	Streaming	
1-588	4413-4419	Append	
1-589	4420-4421	&	
1-590	4422-4428	Upsert	
1-591	4429-4433	Mode	
1-592	4434-4446	Dependencies	
1-593	4447-4450	How	
1-594	4451-4453	to	
1-595	4454-4460	create	
1-596	4461-4462	a	
1-597	4463-4467	JDBC	
1-598	4468-4473	table	
1-599	4474-4483	Connector	
1-600	4484-4491	Options	
1-601	4492-4500	Features	
1-602	4501-4504	Key	
1-603	4505-4513	handling	
1-604	4514-4525	Partitioned	
1-605	4526-4530	Scan	
1-606	4531-4537	Lookup	
1-607	4538-4543	Cache	
1-608	4544-4554	Idempotent	
1-609	4555-4561	Writes	
1-610	4562-4570	Postgres	
1-611	4571-4579	Database	
1-612	4580-4582	as	
1-613	4583-4584	a	
1-614	4585-4592	Catalog	
1-615	4593-4597	Data	
1-616	4598-4602	Type	
1-617	4603-4610	Mapping	
1-618	4611-4614	The	
1-619	4615-4619	JDBC	
1-620	4620-4629	connector	
1-621	4630-4636	allows	
1-622	4637-4640	for	
1-623	4641-4648	reading	
1-624	4649-4653	data	
1-625	4654-4658	from	
1-626	4659-4662	and	
1-627	4663-4670	writing	
1-628	4671-4675	data	
1-629	4676-4680	into	
1-630	4681-4684	any	
1-631	4685-4695	relational	
1-632	4696-4705	databases	
1-633	4706-4710	with	
1-634	4711-4712	a	
1-635	4713-4717	JDBC	
1-636	4718-4724	driver	
1-637	4724-4725	.	

#Text=This document describes how to setup the JDBC connector to run SQL queries against relational databases.
2-1	4726-4730	This	
2-2	4731-4739	document	
2-3	4740-4749	describes	
2-4	4750-4753	how	
2-5	4754-4756	to	
2-6	4757-4762	setup	
2-7	4763-4766	the	
2-8	4767-4771	JDBC	
2-9	4772-4781	connector	
2-10	4782-4784	to	
2-11	4785-4788	run	
2-12	4789-4792	SQL	
2-13	4793-4800	queries	
2-14	4801-4808	against	
2-15	4809-4819	relational	
2-16	4820-4829	databases	
2-17	4829-4830	.	

#Text=The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn’t support to consume UPDATE/DELETE messages.
3-1	4831-4834	The	
3-2	4835-4839	JDBC	
3-3	4840-4844	sink	
3-4	4845-4852	operate	
3-5	4853-4855	in	
3-6	4856-4862	upsert	
3-7	4863-4867	mode	
3-8	4868-4871	for	
3-9	4872-4880	exchange	
3-10	4881-4887	UPDATE	
3-11	4887-4888	/	
3-12	4888-4894	DELETE	
3-13	4895-4903	messages	
3-14	4904-4908	with	
3-15	4909-4912	the	
3-16	4913-4921	external	
3-17	4922-4928	system	
3-18	4929-4931	if	
3-19	4932-4933	a	
3-20	4934-4941	primary	
3-21	4942-4945	key	
3-22	4946-4948	is	
3-23	4949-4956	defined	
3-24	4957-4959	on	
3-25	4960-4963	the	
3-26	4964-4967	DDL	
3-27	4967-4968	,	
3-28	4969-4978	otherwise	
3-29	4978-4979	,	
3-30	4980-4982	it	
3-31	4983-4991	operates	
3-32	4992-4994	in	
3-33	4995-5001	append	
3-34	5002-5006	mode	
3-35	5007-5010	and	
3-36	5011-5016	doesn	
3-37	5016-5017	’	
3-38	5017-5018	t	
3-39	5019-5026	support	
3-40	5027-5029	to	
3-41	5030-5037	consume	
3-42	5038-5044	UPDATE	
3-43	5044-5045	/	
3-44	5045-5051	DELETE	
3-45	5052-5060	messages	
3-46	5060-5061	.	

#Text=Dependencies
#Text=In order to use the JDBC connector the following
#Text=dependencies are required for both projects using a build automation tool (such as Maven or SBT)
#Text=and SQL Client with SQL JAR bundles.
4-1	5062-5074	Dependencies	
4-2	5075-5077	In	
4-3	5078-5083	order	
4-4	5084-5086	to	
4-5	5087-5090	use	
4-6	5091-5094	the	
4-7	5095-5099	JDBC	
4-8	5100-5109	connector	
4-9	5110-5113	the	
4-10	5114-5123	following	
4-11	5124-5136	dependencies	
4-12	5137-5140	are	
4-13	5141-5149	required	
4-14	5150-5153	for	
4-15	5154-5158	both	
4-16	5159-5167	projects	
4-17	5168-5173	using	
4-18	5174-5175	a	
4-19	5176-5181	build	
4-20	5182-5192	automation	
4-21	5193-5197	tool	
4-22	5198-5199	(	
4-23	5199-5203	such	
4-24	5204-5206	as	
4-25	5207-5212	Maven	
4-26	5213-5215	or	
4-27	5216-5219	SBT	
4-28	5219-5220	)	
4-29	5221-5224	and	
4-30	5225-5228	SQL	
4-31	5229-5235	Client	
4-32	5236-5240	with	
4-33	5241-5244	SQL	
4-34	5245-5248	JAR	
4-35	5249-5256	bundles	
4-36	5256-5257	.	

#Text=Maven dependency
#Text=SQL Client JAR
#Text=<dependency>
#Text=<groupId>org.apache.flink</groupId>
#Text=<artifactId>flink-connector-jdbc_2.11</artifactId>
#Text=<version>1.12.0</version>
#Text=</dependency>
#Text=Download
#Text=A driver dependency is also required to connect to a specified database.
5-1	5258-5263	Maven	
5-2	5264-5274	dependency	
5-3	5275-5278	SQL	
5-4	5279-5285	Client	
5-5	5286-5289	JAR	
5-6	5290-5291	<	
5-7	5291-5301	dependency	
5-8	5301-5302	>	
5-9	5303-5304	<	
5-10	5304-5311	groupId	
5-11	5311-5312	>	
5-12	5312-5328	org.apache.flink	
5-13	5328-5329	<	
5-14	5329-5330	/	
5-15	5330-5337	groupId	
5-16	5337-5338	>	
5-17	5339-5340	<	
5-18	5340-5350	artifactId	
5-19	5350-5351	>	
5-20	5351-5371	flink-connector-jdbc	
5-21	5371-5372	_	
5-22	5372-5376	2.11	
5-23	5376-5377	<	
5-24	5377-5378	/	
5-25	5378-5388	artifactId	
5-26	5388-5389	>	
5-27	5390-5391	<	
5-28	5391-5398	version	
5-29	5398-5399	>	
5-30	5399-5405	1.12.0	
5-31	5405-5406	<	
5-32	5406-5407	/	
5-33	5407-5414	version	
5-34	5414-5415	>	
5-35	5416-5417	<	
5-36	5417-5418	/	
5-37	5418-5428	dependency	
5-38	5428-5429	>	
5-39	5430-5438	Download	
5-40	5439-5440	A	
5-41	5441-5447	driver	
5-42	5448-5458	dependency	
5-43	5459-5461	is	
5-44	5462-5466	also	
5-45	5467-5475	required	
5-46	5476-5478	to	
5-47	5479-5486	connect	
5-48	5487-5489	to	
5-49	5490-5491	a	
5-50	5492-5501	specified	
5-51	5502-5510	database	
5-52	5510-5511	.	

#Text=Here are drivers currently supported:
#Text=Driver
#Text=Group Id
#Text=Artifact Id
#Text=JAR
#Text=MySQL
#Text=mysql
#Text=mysql-connector-java
#Text=Download
#Text=PostgreSQL
#Text=org.postgresql
#Text=postgresql
#Text=Download
#Text=Derby
#Text=org.apache.derby
#Text=derby
#Text=Download
#Text=JDBC connector and drivers are not currently part of Flink’s binary distribution.
6-1	5512-5516	Here	
6-2	5517-5520	are	
6-3	5521-5528	drivers	
6-4	5529-5538	currently	
6-5	5539-5548	supported	
6-6	5548-5549	:	
6-7	5550-5556	Driver	
6-8	5557-5562	Group	
6-9	5563-5565	Id	
6-10	5566-5574	Artifact	
6-11	5575-5577	Id	
6-12	5578-5581	JAR	
6-13	5582-5587	MySQL	
6-14	5588-5593	mysql	
6-15	5594-5614	mysql-connector-java	
6-16	5615-5623	Download	
6-17	5624-5634	PostgreSQL	
6-18	5635-5649	org.postgresql	
6-19	5650-5660	postgresql	
6-20	5661-5669	Download	
6-21	5670-5675	Derby	
6-22	5676-5692	org.apache.derby	
6-23	5693-5698	derby	
6-24	5699-5707	Download	
6-25	5708-5712	JDBC	
6-26	5713-5722	connector	
6-27	5723-5726	and	
6-28	5727-5734	drivers	
6-29	5735-5738	are	
6-30	5739-5742	not	
6-31	5743-5752	currently	
6-32	5753-5757	part	
6-33	5758-5760	of	
6-34	5761-5766	Flink	
6-35	5766-5767	’	
6-36	5767-5768	s	
6-37	5769-5775	binary	
6-38	5776-5788	distribution	
6-39	5788-5789	.	

#Text=See how to link with them for cluster execution here.
7-1	5790-5793	See	
7-2	5794-5797	how	
7-3	5798-5800	to	
7-4	5801-5805	link	
7-5	5806-5810	with	
7-6	5811-5815	them	
7-7	5816-5819	for	
7-8	5820-5827	cluster	
7-9	5828-5837	execution	
7-10	5838-5842	here	
7-11	5842-5843	.	

#Text=How to create a JDBC table
#Text=The JDBC table can be defined as following:
#Text=-- register a MySQL table 'users' in Flink SQL
#Text=CREATE TABLE MyUserTable (
#Text=id BIGINT,
#Text=name STRING,
#Text=age INT,
#Text=status BOOLEAN,
#Text=PRIMARY KEY (id) NOT ENFORCED
#Text=) WITH (
#Text='connector' = 'jdbc',
#Text='url' = 'jdbc:mysql://localhost:3306/mydatabase',
#Text='table-name' = 'users'
#Text=-- write data into the JDBC table from the other table "T"
#Text=INSERT INTO MyUserTable
#Text=SELECT id, name, age, status FROM T;
#Text=-- scan data from the JDBC table
#Text=SELECT id, name, age, status FROM MyUserTable;
#Text=-- temporal join the JDBC table as a dimension table
#Text=SELECT * FROM myTopic
#Text=LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime
#Text=ON myTopic.key = MyUserTable.id;
#Text=Connector Options
#Text=Option
#Text=Required
#Text=Default
#Text=Type
#Text=Description
#Text=connector
#Text=required
#Text=(none)
#Text=String
#Text=Specify what connector to use, here should be 'jdbc'.
#Text=url
#Text=required
#Text=(none)
#Text=String
#Text=The JDBC database url.
#Text=table-name
#Text=required
#Text=(none)
#Text=String
#Text=The name of JDBC table to connect.
#Text=driver
#Text=optional
#Text=(none)
#Text=String
#Text=The class name of the JDBC driver to use to connect to this URL, if not set, it will automatically be derived from the URL.
#Text=username
#Text=optional
#Text=(none)
#Text=String
#Text=The JDBC user name.
8-1	5844-5847	How	
8-2	5848-5850	to	
8-3	5851-5857	create	
8-4	5858-5859	a	
8-5	5860-5864	JDBC	
8-6	5865-5870	table	
8-7	5871-5874	The	
8-8	5875-5879	JDBC	
8-9	5880-5885	table	
8-10	5886-5889	can	
8-11	5890-5892	be	
8-12	5893-5900	defined	
8-13	5901-5903	as	
8-14	5904-5913	following	
8-15	5913-5914	:	
8-16	5915-5916	-	
8-17	5916-5917	-	
8-18	5918-5926	register	
8-19	5927-5928	a	
8-20	5929-5934	MySQL	
8-21	5935-5940	table	
8-22	5941-5942	'	
8-23	5942-5947	users	
8-24	5947-5948	'	
8-25	5949-5951	in	
8-26	5952-5957	Flink	
8-27	5958-5961	SQL	
8-28	5962-5968	CREATE	
8-29	5969-5974	TABLE	
8-30	5975-5986	MyUserTable	
8-31	5987-5988	(	
8-32	5989-5991	id	
8-33	5992-5998	BIGINT	
8-34	5998-5999	,	
8-35	6000-6004	name	
8-36	6005-6011	STRING	
8-37	6011-6012	,	
8-38	6013-6016	age	
8-39	6017-6020	INT	
8-40	6020-6021	,	
8-41	6022-6028	status	
8-42	6029-6036	BOOLEAN	
8-43	6036-6037	,	
8-44	6038-6045	PRIMARY	
8-45	6046-6049	KEY	
8-46	6050-6051	(	
8-47	6051-6053	id	
8-48	6053-6054	)	
8-49	6055-6058	NOT	
8-50	6059-6067	ENFORCED	
8-51	6068-6069	)	
8-52	6070-6074	WITH	
8-53	6075-6076	(	
8-54	6077-6078	'	
8-55	6078-6087	connector	
8-56	6087-6088	'	
8-57	6089-6090	=	
8-58	6091-6092	'	
8-59	6092-6096	jdbc	
8-60	6096-6097	'	
8-61	6097-6098	,	
8-62	6099-6100	'	
8-63	6100-6103	url	
8-64	6103-6104	'	
8-65	6105-6106	=	
8-66	6107-6108	'	
8-67	6108-6112	jdbc	
8-68	6112-6113	:	
8-69	6113-6118	mysql	
8-70	6118-6119	:	
8-71	6119-6120	/	
8-72	6120-6121	/	
8-73	6121-6130	localhost	
8-74	6130-6131	:	
8-75	6131-6135	3306	
8-76	6135-6136	/	
8-77	6136-6146	mydatabase	
8-78	6146-6147	'	
8-79	6147-6148	,	
8-80	6149-6150	'	
8-81	6150-6160	table-name	
8-82	6160-6161	'	
8-83	6162-6163	=	
8-84	6164-6165	'	
8-85	6165-6170	users	
8-86	6170-6171	'	
8-87	6172-6173	-	
8-88	6173-6174	-	
8-89	6175-6180	write	
8-90	6181-6185	data	
8-91	6186-6190	into	
8-92	6191-6194	the	
8-93	6195-6199	JDBC	
8-94	6200-6205	table	
8-95	6206-6210	from	
8-96	6211-6214	the	
8-97	6215-6220	other	
8-98	6221-6226	table	
8-99	6227-6228	"	
8-100	6228-6229	T	
8-101	6229-6230	"	
8-102	6231-6237	INSERT	
8-103	6238-6242	INTO	
8-104	6243-6254	MyUserTable	
8-105	6255-6261	SELECT	
8-106	6262-6264	id	
8-107	6264-6265	,	
8-108	6266-6270	name	
8-109	6270-6271	,	
8-110	6272-6275	age	
8-111	6275-6276	,	
8-112	6277-6283	status	
8-113	6284-6288	FROM	
8-114	6289-6290	T	
8-115	6290-6291	;	
8-116	6292-6293	-	
8-117	6293-6294	-	
8-118	6295-6299	scan	
8-119	6300-6304	data	
8-120	6305-6309	from	
8-121	6310-6313	the	
8-122	6314-6318	JDBC	
8-123	6319-6324	table	
8-124	6325-6331	SELECT	
8-125	6332-6334	id	
8-126	6334-6335	,	
8-127	6336-6340	name	
8-128	6340-6341	,	
8-129	6342-6345	age	
8-130	6345-6346	,	
8-131	6347-6353	status	
8-132	6354-6358	FROM	
8-133	6359-6370	MyUserTable	
8-134	6370-6371	;	
8-135	6372-6373	-	
8-136	6373-6374	-	
8-137	6375-6383	temporal	
8-138	6384-6388	join	
8-139	6389-6392	the	
8-140	6393-6397	JDBC	
8-141	6398-6403	table	
8-142	6404-6406	as	
8-143	6407-6408	a	
8-144	6409-6418	dimension	
8-145	6419-6424	table	
8-146	6425-6431	SELECT	
8-147	6432-6433	*	
8-148	6434-6438	FROM	
8-149	6439-6446	myTopic	
8-150	6447-6451	LEFT	
8-151	6452-6456	JOIN	
8-152	6457-6468	MyUserTable	
8-153	6469-6472	FOR	
8-154	6473-6484	SYSTEM_TIME	
8-155	6485-6487	AS	
8-156	6488-6490	OF	
8-157	6491-6507	myTopic.proctime	
8-158	6508-6510	ON	
8-159	6511-6522	myTopic.key	
8-160	6523-6524	=	
8-161	6525-6539	MyUserTable.id	
8-162	6539-6540	;	
8-163	6541-6550	Connector	
8-164	6551-6558	Options	
8-165	6559-6565	Option	
8-166	6566-6574	Required	
8-167	6575-6582	Default	
8-168	6583-6587	Type	
8-169	6588-6599	Description	
8-170	6600-6609	connector	
8-171	6610-6618	required	
8-172	6619-6620	(	
8-173	6620-6624	none	
8-174	6624-6625	)	
8-175	6626-6632	String	
8-176	6633-6640	Specify	
8-177	6641-6645	what	
8-178	6646-6655	connector	
8-179	6656-6658	to	
8-180	6659-6662	use	
8-181	6662-6663	,	
8-182	6664-6668	here	
8-183	6669-6675	should	
8-184	6676-6678	be	
8-185	6679-6680	'	
8-186	6680-6684	jdbc	
8-187	6684-6685	'	
8-188	6685-6686	.	
8-189	6687-6690	url	
8-190	6691-6699	required	
8-191	6700-6701	(	
8-192	6701-6705	none	
8-193	6705-6706	)	
8-194	6707-6713	String	
8-195	6714-6717	The	
8-196	6718-6722	JDBC	
8-197	6723-6731	database	
8-198	6732-6735	url	
8-199	6735-6736	.	
8-200	6737-6747	table-name	
8-201	6748-6756	required	
8-202	6757-6758	(	
8-203	6758-6762	none	
8-204	6762-6763	)	
8-205	6764-6770	String	
8-206	6771-6774	The	
8-207	6775-6779	name	
8-208	6780-6782	of	
8-209	6783-6787	JDBC	
8-210	6788-6793	table	
8-211	6794-6796	to	
8-212	6797-6804	connect	
8-213	6804-6805	.	
8-214	6806-6812	driver	
8-215	6813-6821	optional	
8-216	6822-6823	(	
8-217	6823-6827	none	
8-218	6827-6828	)	
8-219	6829-6835	String	
8-220	6836-6839	The	
8-221	6840-6845	class	
8-222	6846-6850	name	
8-223	6851-6853	of	
8-224	6854-6857	the	
8-225	6858-6862	JDBC	
8-226	6863-6869	driver	
8-227	6870-6872	to	
8-228	6873-6876	use	
8-229	6877-6879	to	
8-230	6880-6887	connect	
8-231	6888-6890	to	
8-232	6891-6895	this	
8-233	6896-6899	URL	
8-234	6899-6900	,	
8-235	6901-6903	if	
8-236	6904-6907	not	
8-237	6908-6911	set	
8-238	6911-6912	,	
8-239	6913-6915	it	
8-240	6916-6920	will	
8-241	6921-6934	automatically	
8-242	6935-6937	be	
8-243	6938-6945	derived	
8-244	6946-6950	from	
8-245	6951-6954	the	
8-246	6955-6958	URL	
8-247	6958-6959	.	
8-248	6960-6968	username	
8-249	6969-6977	optional	
8-250	6978-6979	(	
8-251	6979-6983	none	
8-252	6983-6984	)	
8-253	6985-6991	String	
8-254	6992-6995	The	
8-255	6996-7000	JDBC	
8-256	7001-7005	user	
8-257	7006-7010	name	
8-258	7010-7011	.	

#Text='username' and 'password' must both be specified if any of them is specified.
#Text=password
#Text=optional
#Text=(none)
#Text=String
#Text=The JDBC password.
#Text=scan.partition.column
#Text=optional
#Text=(none)
#Text=String
#Text=The column name used for partitioning the input.
9-1	7012-7013	'	
9-2	7013-7021	username	
9-3	7021-7022	'	
9-4	7023-7026	and	
9-5	7027-7028	'	
9-6	7028-7036	password	
9-7	7036-7037	'	
9-8	7038-7042	must	
9-9	7043-7047	both	
9-10	7048-7050	be	
9-11	7051-7060	specified	
9-12	7061-7063	if	
9-13	7064-7067	any	
9-14	7068-7070	of	
9-15	7071-7075	them	
9-16	7076-7078	is	
9-17	7079-7088	specified	
9-18	7088-7089	.	
9-19	7090-7098	password	
9-20	7099-7107	optional	
9-21	7108-7109	(	
9-22	7109-7113	none	
9-23	7113-7114	)	
9-24	7115-7121	String	
9-25	7122-7125	The	
9-26	7126-7130	JDBC	
9-27	7131-7139	password	
9-28	7139-7140	.	
9-29	7141-7162	scan.partition.column	
9-30	7163-7171	optional	
9-31	7172-7173	(	
9-32	7173-7177	none	
9-33	7177-7178	)	
9-34	7179-7185	String	
9-35	7186-7189	The	
9-36	7190-7196	column	
9-37	7197-7201	name	
9-38	7202-7206	used	
9-39	7207-7210	for	
9-40	7211-7223	partitioning	
9-41	7224-7227	the	
9-42	7228-7233	input	
9-43	7233-7234	.	

#Text=See the following Partitioned Scan section for more details.
#Text=scan.partition.num
#Text=optional
#Text=(none)
#Text=Integer
#Text=The number of partitions.
#Text=scan.partition.lower-bound
#Text=optional
#Text=(none)
#Text=Integer
#Text=The smallest value of the first partition.
#Text=scan.partition.upper-bound
#Text=optional
#Text=(none)
#Text=Integer
#Text=The largest value of the last partition.
#Text=scan.fetch-size
#Text=optional
#Text=Integer
#Text=The number of rows that should be fetched from the database when reading per round trip.
10-1	7235-7238	See	
10-2	7239-7242	the	
10-3	7243-7252	following	
10-4	7253-7264	Partitioned	
10-5	7265-7269	Scan	
10-6	7270-7277	section	
10-7	7278-7281	for	
10-8	7282-7286	more	
10-9	7287-7294	details	
10-10	7294-7295	.	
10-11	7296-7314	scan.partition.num	
10-12	7315-7323	optional	
10-13	7324-7325	(	
10-14	7325-7329	none	
10-15	7329-7330	)	
10-16	7331-7338	Integer	
10-17	7339-7342	The	
10-18	7343-7349	number	
10-19	7350-7352	of	
10-20	7353-7363	partitions	
10-21	7363-7364	.	
10-22	7365-7391	scan.partition.lower-bound	
10-23	7392-7400	optional	
10-24	7401-7402	(	
10-25	7402-7406	none	
10-26	7406-7407	)	
10-27	7408-7415	Integer	
10-28	7416-7419	The	
10-29	7420-7428	smallest	
10-30	7429-7434	value	
10-31	7435-7437	of	
10-32	7438-7441	the	
10-33	7442-7447	first	
10-34	7448-7457	partition	
10-35	7457-7458	.	
10-36	7459-7485	scan.partition.upper-bound	
10-37	7486-7494	optional	
10-38	7495-7496	(	
10-39	7496-7500	none	
10-40	7500-7501	)	
10-41	7502-7509	Integer	
10-42	7510-7513	The	
10-43	7514-7521	largest	
10-44	7522-7527	value	
10-45	7528-7530	of	
10-46	7531-7534	the	
10-47	7535-7539	last	
10-48	7540-7549	partition	
10-49	7549-7550	.	
10-50	7551-7566	scan.fetch-size	
10-51	7567-7575	optional	
10-52	7576-7583	Integer	
10-53	7584-7587	The	
10-54	7588-7594	number	
10-55	7595-7597	of	
10-56	7598-7602	rows	
10-57	7603-7607	that	
10-58	7608-7614	should	
10-59	7615-7617	be	
10-60	7618-7625	fetched	
10-61	7626-7630	from	
10-62	7631-7634	the	
10-63	7635-7643	database	
10-64	7644-7648	when	
10-65	7649-7656	reading	
10-66	7657-7660	per	
10-67	7661-7666	round	
10-68	7667-7671	trip	
10-69	7671-7672	.	

#Text=If the value specified is zero, then the hint is ignored.
#Text=scan.auto-commit
#Text=optional
#Text=true
#Text=Boolean
#Text=Sets the auto-commit flag on the JDBC driver,
#Text=which determines whether each statement is committed in a transaction automatically.
11-1	7673-7675	If	
11-2	7676-7679	the	
11-3	7680-7685	value	
11-4	7686-7695	specified	
11-5	7696-7698	is	
11-6	7699-7703	zero	
11-7	7703-7704	,	
11-8	7705-7709	then	
11-9	7710-7713	the	
11-10	7714-7718	hint	
11-11	7719-7721	is	
11-12	7722-7729	ignored	
11-13	7729-7730	.	
11-14	7731-7747	scan.auto-commit	
11-15	7748-7756	optional	
11-16	7757-7761	true	
11-17	7762-7769	Boolean	
11-18	7770-7774	Sets	
11-19	7775-7778	the	
11-20	7779-7790	auto-commit	
11-21	7791-7795	flag	
11-22	7796-7798	on	
11-23	7799-7802	the	
11-24	7803-7807	JDBC	
11-25	7808-7814	driver	
11-26	7814-7815	,	
11-27	7816-7821	which	
11-28	7822-7832	determines	
11-29	7833-7840	whether	
11-30	7841-7845	each	
11-31	7846-7855	statement	
11-32	7856-7858	is	
11-33	7859-7868	committed	
11-34	7869-7871	in	
11-35	7872-7873	a	
11-36	7874-7885	transaction	
11-37	7886-7899	automatically	
11-38	7899-7900	.	

#Text=Some JDBC drivers, specifically
#Text=Postgres, may require this to be set to false in order to stream results.
#Text=lookup.cache.max-rows
#Text=optional
#Text=(none)
#Text=Integer
#Text=The max number of rows of lookup cache, over this value, the oldest rows will be expired.
12-1	7901-7905	Some	
12-2	7906-7910	JDBC	
12-3	7911-7918	drivers	
12-4	7918-7919	,	
12-5	7920-7932	specifically	
12-6	7933-7941	Postgres	
12-7	7941-7942	,	
12-8	7943-7946	may	
12-9	7947-7954	require	
12-10	7955-7959	this	
12-11	7960-7962	to	
12-12	7963-7965	be	
12-13	7966-7969	set	
12-14	7970-7972	to	
12-15	7973-7978	false	
12-16	7979-7981	in	
12-17	7982-7987	order	
12-18	7988-7990	to	
12-19	7991-7997	stream	
12-20	7998-8005	results	
12-21	8005-8006	.	
12-22	8007-8028	lookup.cache.max-rows	
12-23	8029-8037	optional	
12-24	8038-8039	(	
12-25	8039-8043	none	
12-26	8043-8044	)	
12-27	8045-8052	Integer	
12-28	8053-8056	The	
12-29	8057-8060	max	
12-30	8061-8067	number	
12-31	8068-8070	of	
12-32	8071-8075	rows	
12-33	8076-8078	of	
12-34	8079-8085	lookup	
12-35	8086-8091	cache	
12-36	8091-8092	,	
12-37	8093-8097	over	
12-38	8098-8102	this	
12-39	8103-8108	value	
12-40	8108-8109	,	
12-41	8110-8113	the	
12-42	8114-8120	oldest	
12-43	8121-8125	rows	
12-44	8126-8130	will	
12-45	8131-8133	be	
12-46	8134-8141	expired	
12-47	8141-8142	.	

#Text=Lookup cache is disabled by default.
13-1	8143-8149	Lookup	
13-2	8150-8155	cache	
13-3	8156-8158	is	
13-4	8159-8167	disabled	
13-5	8168-8170	by	
13-6	8171-8178	default	
13-7	8178-8179	.	

#Text=See the following Lookup Cache section for more details.
#Text=lookup.cache.ttl
#Text=optional
#Text=(none)
#Text=Duration
#Text=The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired.
14-1	8180-8183	See	
14-2	8184-8187	the	
14-3	8188-8197	following	
14-4	8198-8204	Lookup	
14-5	8205-8210	Cache	
14-6	8211-8218	section	
14-7	8219-8222	for	
14-8	8223-8227	more	
14-9	8228-8235	details	
14-10	8235-8236	.	
14-11	8237-8253	lookup.cache.ttl	
14-12	8254-8262	optional	
14-13	8263-8264	(	
14-14	8264-8268	none	
14-15	8268-8269	)	
14-16	8270-8278	Duration	
14-17	8279-8282	The	
14-18	8283-8286	max	
14-19	8287-8291	time	
14-20	8292-8294	to	
14-21	8295-8299	live	
14-22	8300-8303	for	
14-23	8304-8308	each	
14-24	8309-8313	rows	
14-25	8314-8316	in	
14-26	8317-8323	lookup	
14-27	8324-8329	cache	
14-28	8329-8330	,	
14-29	8331-8335	over	
14-30	8336-8340	this	
14-31	8341-8345	time	
14-32	8345-8346	,	
14-33	8347-8350	the	
14-34	8351-8357	oldest	
14-35	8358-8362	rows	
14-36	8363-8367	will	
14-37	8368-8370	be	
14-38	8371-8378	expired	
14-39	8378-8379	.	

#Text=Lookup cache is disabled by default.
15-1	8380-8386	Lookup	
15-2	8387-8392	cache	
15-3	8393-8395	is	
15-4	8396-8404	disabled	
15-5	8405-8407	by	
15-6	8408-8415	default	
15-7	8415-8416	.	

#Text=See the following Lookup Cache section for more details.
#Text=lookup.max-retries
#Text=optional
#Text=Integer
#Text=The max retry times if lookup database failed.
#Text=sink.buffer-flush.max-rows
#Text=optional
#Text=100
#Text=Integer
#Text=The max size of buffered records before flush.
16-1	8417-8420	See	
16-2	8421-8424	the	
16-3	8425-8434	following	
16-4	8435-8441	Lookup	
16-5	8442-8447	Cache	
16-6	8448-8455	section	
16-7	8456-8459	for	
16-8	8460-8464	more	
16-9	8465-8472	details	
16-10	8472-8473	.	
16-11	8474-8492	lookup.max-retries	
16-12	8493-8501	optional	
16-13	8502-8509	Integer	
16-14	8510-8513	The	
16-15	8514-8517	max	
16-16	8518-8523	retry	
16-17	8524-8529	times	
16-18	8530-8532	if	
16-19	8533-8539	lookup	
16-20	8540-8548	database	
16-21	8549-8555	failed	
16-22	8555-8556	.	
16-23	8557-8583	sink.buffer-flush.max-rows	
16-24	8584-8592	optional	
16-25	8593-8596	100	
16-26	8597-8604	Integer	
16-27	8605-8608	The	
16-28	8609-8612	max	
16-29	8613-8617	size	
16-30	8618-8620	of	
16-31	8621-8629	buffered	
16-32	8630-8637	records	
16-33	8638-8644	before	
16-34	8645-8650	flush	
16-35	8650-8651	.	

#Text=Can be set to zero to disable it.
#Text=sink.buffer-flush.interval
#Text=optional
#Text=Duration
#Text=The flush interval mills, over this time, asynchronous threads will flush data.
17-1	8652-8655	Can	
17-2	8656-8658	be	
17-3	8659-8662	set	
17-4	8663-8665	to	
17-5	8666-8670	zero	
17-6	8671-8673	to	
17-7	8674-8681	disable	
17-8	8682-8684	it	
17-9	8684-8685	.	
17-10	8686-8712	sink.buffer-flush.interval	
17-11	8713-8721	optional	
17-12	8722-8730	Duration	
17-13	8731-8734	The	
17-14	8735-8740	flush	
17-15	8741-8749	interval	
17-16	8750-8755	mills	
17-17	8755-8756	,	
17-18	8757-8761	over	
17-19	8762-8766	this	
17-20	8767-8771	time	
17-21	8771-8772	,	
17-22	8773-8785	asynchronous	
17-23	8786-8793	threads	
17-24	8794-8798	will	
17-25	8799-8804	flush	
17-26	8805-8809	data	
17-27	8809-8810	.	

#Text=Can be set to '0' to disable it.
18-1	8811-8814	Can	
18-2	8815-8817	be	
18-3	8818-8821	set	
18-4	8822-8824	to	
18-5	8825-8826	'	
18-6	8826-8827	0	
18-7	8827-8828	'	
18-8	8829-8831	to	
18-9	8832-8839	disable	
18-10	8840-8842	it	
18-11	8842-8843	.	

#Text=Note, 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions.
#Text=sink.max-retries
#Text=optional
#Text=Integer
#Text=The max retry times if writing records to database failed.
19-1	8844-8848	Note	
19-2	8848-8849	,	
19-3	8850-8851	'	
19-4	8851-8877	sink.buffer-flush.max-rows	
19-5	8877-8878	'	
19-6	8879-8882	can	
19-7	8883-8885	be	
19-8	8886-8889	set	
19-9	8890-8892	to	
19-10	8893-8894	'	
19-11	8894-8895	0	
19-12	8895-8896	'	
19-13	8897-8901	with	
19-14	8902-8905	the	
19-15	8906-8911	flush	
19-16	8912-8920	interval	
19-17	8921-8924	set	
19-18	8925-8933	allowing	
19-19	8934-8937	for	
19-20	8938-8946	complete	
19-21	8947-8952	async	
19-22	8953-8963	processing	
19-23	8964-8966	of	
19-24	8967-8975	buffered	
19-25	8976-8983	actions	
19-26	8983-8984	.	
19-27	8985-9001	sink.max-retries	
19-28	9002-9010	optional	
19-29	9011-9018	Integer	
19-30	9019-9022	The	
19-31	9023-9026	max	
19-32	9027-9032	retry	
19-33	9033-9038	times	
19-34	9039-9041	if	
19-35	9042-9049	writing	
19-36	9050-9057	records	
19-37	9058-9060	to	
19-38	9061-9069	database	
19-39	9070-9076	failed	
19-40	9076-9077	.	

#Text=Features
#Text=Key handling
#Text=Flink uses the primary key that defined in DDL when writing data to external databases.
20-1	9078-9086	Features	
20-2	9087-9090	Key	
20-3	9091-9099	handling	
20-4	9100-9105	Flink	
20-5	9106-9110	uses	
20-6	9111-9114	the	
20-7	9115-9122	primary	
20-8	9123-9126	key	
20-9	9127-9131	that	
20-10	9132-9139	defined	
20-11	9140-9142	in	
20-12	9143-9146	DDL	
20-13	9147-9151	when	
20-14	9152-9159	writing	
20-15	9160-9164	data	
20-16	9165-9167	to	
20-17	9168-9176	external	
20-18	9177-9186	databases	
20-19	9186-9187	.	

#Text=The connector operate in upsert mode if the primary key was defined, otherwise, the connector operate in append mode.
21-1	9188-9191	The	
21-2	9192-9201	connector	
21-3	9202-9209	operate	
21-4	9210-9212	in	
21-5	9213-9219	upsert	
21-6	9220-9224	mode	
21-7	9225-9227	if	
21-8	9228-9231	the	
21-9	9232-9239	primary	
21-10	9240-9243	key	
21-11	9244-9247	was	
21-12	9248-9255	defined	
21-13	9255-9256	,	
21-14	9257-9266	otherwise	
21-15	9266-9267	,	
21-16	9268-9271	the	
21-17	9272-9281	connector	
21-18	9282-9289	operate	
21-19	9290-9292	in	
21-20	9293-9299	append	
21-21	9300-9304	mode	
21-22	9304-9305	.	

#Text=In upsert mode, Flink will insert a new row or update the existing row according to the primary key, Flink can ensure the idempotence in this way.
22-1	9306-9308	In	
22-2	9309-9315	upsert	
22-3	9316-9320	mode	
22-4	9320-9321	,	
22-5	9322-9327	Flink	
22-6	9328-9332	will	
22-7	9333-9339	insert	
22-8	9340-9341	a	
22-9	9342-9345	new	
22-10	9346-9349	row	
22-11	9350-9352	or	
22-12	9353-9359	update	
22-13	9360-9363	the	
22-14	9364-9372	existing	
22-15	9373-9376	row	
22-16	9377-9386	according	
22-17	9387-9389	to	
22-18	9390-9393	the	
22-19	9394-9401	primary	
22-20	9402-9405	key	
22-21	9405-9406	,	
22-22	9407-9412	Flink	
22-23	9413-9416	can	
22-24	9417-9423	ensure	
22-25	9424-9427	the	
22-26	9428-9439	idempotence	
22-27	9440-9442	in	
22-28	9443-9447	this	
22-29	9448-9451	way	
22-30	9451-9452	.	

#Text=To guarantee the output result is as expected, it’s recommended to define primary key for the table and make sure the primary key is one of the unique key sets or primary key of the underlying database table.
23-1	9453-9455	To	
23-2	9456-9465	guarantee	
23-3	9466-9469	the	
23-4	9470-9476	output	
23-5	9477-9483	result	
23-6	9484-9486	is	
23-7	9487-9489	as	
23-8	9490-9498	expected	
23-9	9498-9499	,	
23-10	9500-9502	it	
23-11	9502-9503	’	
23-12	9503-9504	s	
23-13	9505-9516	recommended	
23-14	9517-9519	to	
23-15	9520-9526	define	
23-16	9527-9534	primary	
23-17	9535-9538	key	
23-18	9539-9542	for	
23-19	9543-9546	the	
23-20	9547-9552	table	
23-21	9553-9556	and	
23-22	9557-9561	make	
23-23	9562-9566	sure	
23-24	9567-9570	the	
23-25	9571-9578	primary	
23-26	9579-9582	key	
23-27	9583-9585	is	
23-28	9586-9589	one	
23-29	9590-9592	of	
23-30	9593-9596	the	
23-31	9597-9603	unique	
23-32	9604-9607	key	
23-33	9608-9612	sets	
23-34	9613-9615	or	
23-35	9616-9623	primary	
23-36	9624-9627	key	
23-37	9628-9630	of	
23-38	9631-9634	the	
23-39	9635-9645	underlying	
23-40	9646-9654	database	
23-41	9655-9660	table	
23-42	9660-9661	.	

#Text=In append mode, Flink will interpret all records as INSERT messages, the INSERT operation may fail if a primary key or unique constraint violation happens in the underlying database.
24-1	9662-9664	In	
24-2	9665-9671	append	
24-3	9672-9676	mode	
24-4	9676-9677	,	
24-5	9678-9683	Flink	
24-6	9684-9688	will	
24-7	9689-9698	interpret	
24-8	9699-9702	all	
24-9	9703-9710	records	
24-10	9711-9713	as	
24-11	9714-9720	INSERT	
24-12	9721-9729	messages	
24-13	9729-9730	,	
24-14	9731-9734	the	
24-15	9735-9741	INSERT	
24-16	9742-9751	operation	
24-17	9752-9755	may	
24-18	9756-9760	fail	
24-19	9761-9763	if	
24-20	9764-9765	a	
24-21	9766-9773	primary	
24-22	9774-9777	key	
24-23	9778-9780	or	
24-24	9781-9787	unique	
24-25	9788-9798	constraint	
24-26	9799-9808	violation	
24-27	9809-9816	happens	
24-28	9817-9819	in	
24-29	9820-9823	the	
24-30	9824-9834	underlying	
24-31	9835-9843	database	
24-32	9843-9844	.	

#Text=See CREATE TABLE DDL for more details about PRIMARY KEY syntax.
25-1	9845-9848	See	
25-2	9849-9855	CREATE	
25-3	9856-9861	TABLE	
25-4	9862-9865	DDL	
25-5	9866-9869	for	
25-6	9870-9874	more	
25-7	9875-9882	details	
25-8	9883-9888	about	
25-9	9889-9896	PRIMARY	
25-10	9897-9900	KEY	
25-11	9901-9907	syntax	
25-12	9907-9908	.	

#Text=Partitioned Scan
#Text=To accelerate reading data in parallel Source task instances, Flink provides partitioned scan feature for JDBC table.
26-1	9909-9920	Partitioned	
26-2	9921-9925	Scan	
26-3	9926-9928	To	
26-4	9929-9939	accelerate	
26-5	9940-9947	reading	
26-6	9948-9952	data	
26-7	9953-9955	in	
26-8	9956-9964	parallel	
26-9	9965-9971	Source	
26-10	9972-9976	task	
26-11	9977-9986	instances	
26-12	9986-9987	,	
26-13	9988-9993	Flink	
26-14	9994-10002	provides	
26-15	10003-10014	partitioned	
26-16	10015-10019	scan	
26-17	10020-10027	feature	
26-18	10028-10031	for	
26-19	10032-10036	JDBC	
26-20	10037-10042	table	
26-21	10042-10043	.	

#Text=All the following scan partition options must all be specified if any of them is specified.
27-1	10044-10047	All	
27-2	10048-10051	the	
27-3	10052-10061	following	
27-4	10062-10066	scan	
27-5	10067-10076	partition	
27-6	10077-10084	options	
27-7	10085-10089	must	
27-8	10090-10093	all	
27-9	10094-10096	be	
27-10	10097-10106	specified	
27-11	10107-10109	if	
27-12	10110-10113	any	
27-13	10114-10116	of	
27-14	10117-10121	them	
27-15	10122-10124	is	
27-16	10125-10134	specified	
27-17	10134-10135	.	

#Text=They describe how to partition the table when reading in parallel from multiple tasks.
28-1	10136-10140	They	
28-2	10141-10149	describe	
28-3	10150-10153	how	
28-4	10154-10156	to	
28-5	10157-10166	partition	
28-6	10167-10170	the	
28-7	10171-10176	table	
28-8	10177-10181	when	
28-9	10182-10189	reading	
28-10	10190-10192	in	
28-11	10193-10201	parallel	
28-12	10202-10206	from	
28-13	10207-10215	multiple	
28-14	10216-10221	tasks	
28-15	10221-10222	.	

#Text=The scan.partition.column must be a numeric, date, or timestamp column from the table in question.
29-1	10223-10226	The	
29-2	10227-10248	scan.partition.column	
29-3	10249-10253	must	
29-4	10254-10256	be	
29-5	10257-10258	a	
29-6	10259-10266	numeric	
29-7	10266-10267	,	
29-8	10268-10272	date	
29-9	10272-10273	,	
29-10	10274-10276	or	
29-11	10277-10286	timestamp	
29-12	10287-10293	column	
29-13	10294-10298	from	
29-14	10299-10302	the	
29-15	10303-10308	table	
29-16	10309-10311	in	
29-17	10312-10320	question	
29-18	10320-10321	.	

#Text=Notice that scan.partition.lower-bound and scan.partition.upper-bound are just used to decide the partition stride, not for filtering the rows in table.
30-1	10322-10328	Notice	
30-2	10329-10333	that	
30-3	10334-10360	scan.partition.lower-bound	
30-4	10361-10364	and	
30-5	10365-10391	scan.partition.upper-bound	
30-6	10392-10395	are	
30-7	10396-10400	just	
30-8	10401-10405	used	
30-9	10406-10408	to	
30-10	10409-10415	decide	
30-11	10416-10419	the	
30-12	10420-10429	partition	
30-13	10430-10436	stride	
30-14	10436-10437	,	
30-15	10438-10441	not	
30-16	10442-10445	for	
30-17	10446-10455	filtering	
30-18	10456-10459	the	
30-19	10460-10464	rows	
30-20	10465-10467	in	
30-21	10468-10473	table	
30-22	10473-10474	.	

#Text=So all rows in the table will be partitioned and returned.
#Text=scan.partition.column: The column name used for partitioning the input.
#Text=scan.partition.num: The number of partitions.
#Text=scan.partition.lower-bound: The smallest value of the first partition.
#Text=scan.partition.upper-bound: The largest value of the last partition.
31-1	10475-10477	So	
31-2	10478-10481	all	
31-3	10482-10486	rows	
31-4	10487-10489	in	
31-5	10490-10493	the	
31-6	10494-10499	table	
31-7	10500-10504	will	
31-8	10505-10507	be	
31-9	10508-10519	partitioned	
31-10	10520-10523	and	
31-11	10524-10532	returned	
31-12	10532-10533	.	
31-13	10534-10555	scan.partition.column	
31-14	10555-10556	:	
31-15	10557-10560	The	
31-16	10561-10567	column	
31-17	10568-10572	name	
31-18	10573-10577	used	
31-19	10578-10581	for	
31-20	10582-10594	partitioning	
31-21	10595-10598	the	
31-22	10599-10604	input	
31-23	10604-10605	.	
31-24	10606-10624	scan.partition.num	
31-25	10624-10625	:	
31-26	10626-10629	The	
31-27	10630-10636	number	
31-28	10637-10639	of	
31-29	10640-10650	partitions	
31-30	10650-10651	.	
31-31	10652-10678	scan.partition.lower-bound	
31-32	10678-10679	:	
31-33	10680-10683	The	
31-34	10684-10692	smallest	
31-35	10693-10698	value	
31-36	10699-10701	of	
31-37	10702-10705	the	
31-38	10706-10711	first	
31-39	10712-10721	partition	
31-40	10721-10722	.	
31-41	10723-10749	scan.partition.upper-bound	
31-42	10749-10750	:	
31-43	10751-10754	The	
31-44	10755-10762	largest	
31-45	10763-10768	value	
31-46	10769-10771	of	
31-47	10772-10775	the	
31-48	10776-10780	last	
31-49	10781-10790	partition	
31-50	10790-10791	.	

#Text=Lookup Cache
#Text=JDBC connector can be used in temporal join as a lookup source (aka. dimension table).
32-1	10792-10798	Lookup	
32-2	10799-10804	Cache	
32-3	10805-10809	JDBC	
32-4	10810-10819	connector	
32-5	10820-10823	can	
32-6	10824-10826	be	
32-7	10827-10831	used	
32-8	10832-10834	in	
32-9	10835-10843	temporal	
32-10	10844-10848	join	
32-11	10849-10851	as	
32-12	10852-10853	a	
32-13	10854-10860	lookup	
32-14	10861-10867	source	
32-15	10868-10869	(	
32-16	10869-10872	aka	
32-17	10872-10873	.	
32-18	10874-10883	dimension	
32-19	10884-10889	table	
32-20	10889-10890	)	
32-21	10890-10891	.	

#Text=Currently, only sync lookup mode is supported.
33-1	10892-10901	Currently	
33-2	10901-10902	,	
33-3	10903-10907	only	
33-4	10908-10912	sync	
33-5	10913-10919	lookup	
33-6	10920-10924	mode	
33-7	10925-10927	is	
33-8	10928-10937	supported	
33-9	10937-10938	.	

#Text=By default, lookup cache is not enabled.
34-1	10939-10941	By	
34-2	10942-10949	default	
34-3	10949-10950	,	
34-4	10951-10957	lookup	
34-5	10958-10963	cache	
34-6	10964-10966	is	
34-7	10967-10970	not	
34-8	10971-10978	enabled	
34-9	10978-10979	.	

#Text=You can enable it by setting both lookup.cache.max-rows and lookup.cache.ttl.
35-1	10980-10983	You	
35-2	10984-10987	can	
35-3	10988-10994	enable	
35-4	10995-10997	it	
35-5	10998-11000	by	
35-6	11001-11008	setting	
35-7	11009-11013	both	
35-8	11014-11035	lookup.cache.max-rows	
35-9	11036-11039	and	
35-10	11040-11056	lookup.cache.ttl	
35-11	11056-11057	.	

#Text=The lookup cache is used to improve performance of temporal join the JDBC connector.
36-1	11058-11061	The	
36-2	11062-11068	lookup	
36-3	11069-11074	cache	
36-4	11075-11077	is	
36-5	11078-11082	used	
36-6	11083-11085	to	
36-7	11086-11093	improve	
36-8	11094-11105	performance	
36-9	11106-11108	of	
36-10	11109-11117	temporal	
36-11	11118-11122	join	
36-12	11123-11126	the	
36-13	11127-11131	JDBC	
36-14	11132-11141	connector	
36-15	11141-11142	.	

#Text=By default, lookup cache is not enabled, so all the requests are sent to external database.
37-1	11143-11145	By	
37-2	11146-11153	default	
37-3	11153-11154	,	
37-4	11155-11161	lookup	
37-5	11162-11167	cache	
37-6	11168-11170	is	
37-7	11171-11174	not	
37-8	11175-11182	enabled	
37-9	11182-11183	,	
37-10	11184-11186	so	
37-11	11187-11190	all	
37-12	11191-11194	the	
37-13	11195-11203	requests	
37-14	11204-11207	are	
37-15	11208-11212	sent	
37-16	11213-11215	to	
37-17	11216-11224	external	
37-18	11225-11233	database	
37-19	11233-11234	.	

#Text=When lookup cache is enabled, each process (i.e.
38-1	11235-11239	When	
38-2	11240-11246	lookup	
38-3	11247-11252	cache	
38-4	11253-11255	is	
38-5	11256-11263	enabled	
38-6	11263-11264	,	
38-7	11265-11269	each	
38-8	11270-11277	process	
38-9	11278-11279	(	
38-10	11279-11282	i.e	
38-11	11282-11283	.	

#Text=TaskManager) will hold a cache.
39-1	11284-11295	TaskManager	
39-2	11295-11296	)	
39-3	11297-11301	will	
39-4	11302-11306	hold	
39-5	11307-11308	a	
39-6	11309-11314	cache	
39-7	11314-11315	.	

#Text=Flink will lookup the cache first, and only send requests to external database when cache missing, and update cache with the rows returned.
40-1	11316-11321	Flink	
40-2	11322-11326	will	
40-3	11327-11333	lookup	
40-4	11334-11337	the	
40-5	11338-11343	cache	
40-6	11344-11349	first	
40-7	11349-11350	,	
40-8	11351-11354	and	
40-9	11355-11359	only	
40-10	11360-11364	send	
40-11	11365-11373	requests	
40-12	11374-11376	to	
40-13	11377-11385	external	
40-14	11386-11394	database	
40-15	11395-11399	when	
40-16	11400-11405	cache	
40-17	11406-11413	missing	
40-18	11413-11414	,	
40-19	11415-11418	and	
40-20	11419-11425	update	
40-21	11426-11431	cache	
40-22	11432-11436	with	
40-23	11437-11440	the	
40-24	11441-11445	rows	
40-25	11446-11454	returned	
40-26	11454-11455	.	

#Text=The oldest rows in cache will be expired when the cache hit to the max cached rows lookup.cache.max-rows or when the row exceeds the max time to live lookup.cache.ttl.
41-1	11456-11459	The	
41-2	11460-11466	oldest	
41-3	11467-11471	rows	
41-4	11472-11474	in	
41-5	11475-11480	cache	
41-6	11481-11485	will	
41-7	11486-11488	be	
41-8	11489-11496	expired	
41-9	11497-11501	when	
41-10	11502-11505	the	
41-11	11506-11511	cache	
41-12	11512-11515	hit	
41-13	11516-11518	to	
41-14	11519-11522	the	
41-15	11523-11526	max	
41-16	11527-11533	cached	
41-17	11534-11538	rows	
41-18	11539-11560	lookup.cache.max-rows	
41-19	11561-11563	or	
41-20	11564-11568	when	
41-21	11569-11572	the	
41-22	11573-11576	row	
41-23	11577-11584	exceeds	
41-24	11585-11588	the	
41-25	11589-11592	max	
41-26	11593-11597	time	
41-27	11598-11600	to	
41-28	11601-11605	live	
41-29	11606-11622	lookup.cache.ttl	
41-30	11622-11623	.	

#Text=The cached rows might not be the latest, users can tune lookup.cache.ttl to a smaller value to have a better fresh data, but this may increase the number of requests send to database.
42-1	11624-11627	The	
42-2	11628-11634	cached	
42-3	11635-11639	rows	
42-4	11640-11645	might	
42-5	11646-11649	not	
42-6	11650-11652	be	
42-7	11653-11656	the	
42-8	11657-11663	latest	
42-9	11663-11664	,	
42-10	11665-11670	users	
42-11	11671-11674	can	
42-12	11675-11679	tune	
42-13	11680-11696	lookup.cache.ttl	
42-14	11697-11699	to	
42-15	11700-11701	a	
42-16	11702-11709	smaller	
42-17	11710-11715	value	
42-18	11716-11718	to	
42-19	11719-11723	have	
42-20	11724-11725	a	
42-21	11726-11732	better	
42-22	11733-11738	fresh	
42-23	11739-11743	data	
42-24	11743-11744	,	
42-25	11745-11748	but	
42-26	11749-11753	this	
42-27	11754-11757	may	
42-28	11758-11766	increase	
42-29	11767-11770	the	
42-30	11771-11777	number	
42-31	11778-11780	of	
42-32	11781-11789	requests	
42-33	11790-11794	send	
42-34	11795-11797	to	
42-35	11798-11806	database	
42-36	11806-11807	.	

#Text=So this is a balance between throughput and correctness.
43-1	11808-11810	So	
43-2	11811-11815	this	
43-3	11816-11818	is	
43-4	11819-11820	a	
43-5	11821-11828	balance	
43-6	11829-11836	between	
43-7	11837-11847	throughput	
43-8	11848-11851	and	
43-9	11852-11863	correctness	
43-10	11863-11864	.	

#Text=Idempotent Writes
#Text=JDBC sink will use upsert semantics rather than plain INSERT statements if primary key is defined in DDL.
44-1	11865-11875	Idempotent	
44-2	11876-11882	Writes	
44-3	11883-11887	JDBC	
44-4	11888-11892	sink	
44-5	11893-11897	will	
44-6	11898-11901	use	
44-7	11902-11908	upsert	
44-8	11909-11918	semantics	
44-9	11919-11925	rather	
44-10	11926-11930	than	
44-11	11931-11936	plain	
44-12	11937-11943	INSERT	
44-13	11944-11954	statements	
44-14	11955-11957	if	
44-15	11958-11965	primary	
44-16	11966-11969	key	
44-17	11970-11972	is	
44-18	11973-11980	defined	
44-19	11981-11983	in	
44-20	11984-11987	DDL	
44-21	11987-11988	.	

#Text=Upsert semantics refer to atomically adding a new row or updating the existing row if there is a unique constraint violation in the underlying database, which provides idempotence.
45-1	11989-11995	Upsert	
45-2	11996-12005	semantics	
45-3	12006-12011	refer	
45-4	12012-12014	to	
45-5	12015-12025	atomically	
45-6	12026-12032	adding	
45-7	12033-12034	a	
45-8	12035-12038	new	
45-9	12039-12042	row	
45-10	12043-12045	or	
45-11	12046-12054	updating	
45-12	12055-12058	the	
45-13	12059-12067	existing	
45-14	12068-12071	row	
45-15	12072-12074	if	
45-16	12075-12080	there	
45-17	12081-12083	is	
45-18	12084-12085	a	
45-19	12086-12092	unique	
45-20	12093-12103	constraint	
45-21	12104-12113	violation	
45-22	12114-12116	in	
45-23	12117-12120	the	
45-24	12121-12131	underlying	
45-25	12132-12140	database	
45-26	12140-12141	,	
45-27	12142-12147	which	
45-28	12148-12156	provides	
45-29	12157-12168	idempotence	
45-30	12168-12169	.	

#Text=If there are failures, the Flink job will recover and re-process from last successful checkpoint, which can lead to re-processing messages during recovery.
46-1	12170-12172	If	
46-2	12173-12178	there	
46-3	12179-12182	are	
46-4	12183-12191	failures	
46-5	12191-12192	,	
46-6	12193-12196	the	
46-7	12197-12202	Flink	
46-8	12203-12206	job	
46-9	12207-12211	will	
46-10	12212-12219	recover	
46-11	12220-12223	and	
46-12	12224-12234	re-process	
46-13	12235-12239	from	
46-14	12240-12244	last	
46-15	12245-12255	successful	
46-16	12256-12266	checkpoint	
46-17	12266-12267	,	
46-18	12268-12273	which	
46-19	12274-12277	can	
46-20	12278-12282	lead	
46-21	12283-12285	to	
46-22	12286-12299	re-processing	
46-23	12300-12308	messages	
46-24	12309-12315	during	
46-25	12316-12324	recovery	
46-26	12324-12325	.	

#Text=The upsert mode is highly recommended as it helps avoid constraint violations or duplicate data if records need to be re-processed.
47-1	12326-12329	The	
47-2	12330-12336	upsert	
47-3	12337-12341	mode	
47-4	12342-12344	is	
47-5	12345-12351	highly	
47-6	12352-12363	recommended	
47-7	12364-12366	as	
47-8	12367-12369	it	
47-9	12370-12375	helps	
47-10	12376-12381	avoid	
47-11	12382-12392	constraint	
47-12	12393-12403	violations	
47-13	12404-12406	or	
47-14	12407-12416	duplicate	
47-15	12417-12421	data	
47-16	12422-12424	if	
47-17	12425-12432	records	
47-18	12433-12437	need	
47-19	12438-12440	to	
47-20	12441-12443	be	
47-21	12444-12456	re-processed	
47-22	12456-12457	.	

#Text=Aside from failure recovery, the source topic may also naturally contain multiple records over time with the same primary key, making upserts desirable.
48-1	12458-12463	Aside	
48-2	12464-12468	from	
48-3	12469-12476	failure	
48-4	12477-12485	recovery	
48-5	12485-12486	,	
48-6	12487-12490	the	
48-7	12491-12497	source	
48-8	12498-12503	topic	
48-9	12504-12507	may	
48-10	12508-12512	also	
48-11	12513-12522	naturally	
48-12	12523-12530	contain	
48-13	12531-12539	multiple	
48-14	12540-12547	records	
48-15	12548-12552	over	
48-16	12553-12557	time	
48-17	12558-12562	with	
48-18	12563-12566	the	
48-19	12567-12571	same	
48-20	12572-12579	primary	
48-21	12580-12583	key	
48-22	12583-12584	,	
48-23	12585-12591	making	
48-24	12592-12599	upserts	
48-25	12600-12609	desirable	
48-26	12609-12610	.	

#Text=As there is no standard syntax for upsert, the following table describes the database-specific DML that is used.
49-1	12611-12613	As	
49-2	12614-12619	there	
49-3	12620-12622	is	
49-4	12623-12625	no	
49-5	12626-12634	standard	
49-6	12635-12641	syntax	
49-7	12642-12645	for	
49-8	12646-12652	upsert	
49-9	12652-12653	,	
49-10	12654-12657	the	
49-11	12658-12667	following	
49-12	12668-12673	table	
49-13	12674-12683	describes	
49-14	12684-12687	the	
49-15	12688-12705	database-specific	
49-16	12706-12709	DML	
49-17	12710-12714	that	
49-18	12715-12717	is	
49-19	12718-12722	used	
49-20	12722-12723	.	

#Text=Database
#Text=Upsert Grammar
#Text=MySQL
#Text=INSERT ..
50-1	12724-12732	Database	
50-2	12733-12739	Upsert	
50-3	12740-12747	Grammar	
50-4	12748-12753	MySQL	
50-5	12754-12760	INSERT	
50-6	12761-12762	.	
50-7	12762-12763	.	

#Text=ON DUPLICATE KEY UPDATE ..
51-1	12764-12766	ON	
51-2	12767-12776	DUPLICATE	
51-3	12777-12780	KEY	
51-4	12781-12787	UPDATE	
51-5	12788-12789	.	
51-6	12789-12790	.	

#Text=PostgreSQL
#Text=INSERT ..
52-1	12791-12801	PostgreSQL	
52-2	12802-12808	INSERT	
52-3	12809-12810	.	
52-4	12810-12811	.	

#Text=ON CONFLICT ..
53-1	12812-12814	ON	
53-2	12815-12823	CONFLICT	
53-3	12824-12825	.	
53-4	12825-12826	.	

#Text=DO UPDATE SET ..
54-1	12827-12829	DO	
54-2	12830-12836	UPDATE	
54-3	12837-12840	SET	
54-4	12841-12842	.	
54-5	12842-12843	.	

#Text=Postgres Database as a Catalog
#Text=The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol.
55-1	12844-12852	Postgres	
55-2	12853-12861	Database	
55-3	12862-12864	as	
55-4	12865-12866	a	
55-5	12867-12874	Catalog	
55-6	12875-12878	The	
55-7	12879-12890	JdbcCatalog	
55-8	12891-12898	enables	
55-9	12899-12904	users	
55-10	12905-12907	to	
55-11	12908-12915	connect	
55-12	12916-12921	Flink	
55-13	12922-12924	to	
55-14	12925-12935	relational	
55-15	12936-12945	databases	
55-16	12946-12950	over	
55-17	12951-12955	JDBC	
55-18	12956-12964	protocol	
55-19	12964-12965	.	

#Text=Currently, PostgresCatalog is the only implementation of JDBC Catalog at the moment, PostgresCatalog only supports limited Catalog methods include:
#Text=// The supported methods by Postgres Catalog.
56-1	12966-12975	Currently	
56-2	12975-12976	,	
56-3	12977-12992	PostgresCatalog	
56-4	12993-12995	is	
56-5	12996-12999	the	
56-6	13000-13004	only	
56-7	13005-13019	implementation	
56-8	13020-13022	of	
56-9	13023-13027	JDBC	
56-10	13028-13035	Catalog	
56-11	13036-13038	at	
56-12	13039-13042	the	
56-13	13043-13049	moment	
56-14	13049-13050	,	
56-15	13051-13066	PostgresCatalog	
56-16	13067-13071	only	
56-17	13072-13080	supports	
56-18	13081-13088	limited	
56-19	13089-13096	Catalog	
56-20	13097-13104	methods	
56-21	13105-13112	include	
56-22	13112-13113	:	
56-23	13114-13115	/	
56-24	13115-13116	/	
56-25	13117-13120	The	
56-26	13121-13130	supported	
56-27	13131-13138	methods	
56-28	13139-13141	by	
56-29	13142-13150	Postgres	
56-30	13151-13158	Catalog	
56-31	13158-13159	.	

#Text=PostgresCatalog.databaseExists(String databaseName)
#Text=PostgresCatalog.listDatabases()
#Text=PostgresCatalog.getDatabase(String databaseName)
#Text=PostgresCatalog.listTables(String databaseName)
#Text=PostgresCatalog.getTable(ObjectPath tablePath)
#Text=PostgresCatalog.tableExists(ObjectPath tablePath)
#Text=Other Catalog methods is unsupported now.
57-1	13160-13190	PostgresCatalog.databaseExists	
57-2	13190-13191	(	
57-3	13191-13197	String	
57-4	13198-13210	databaseName	
57-5	13210-13211	)	
57-6	13212-13241	PostgresCatalog.listDatabases	
57-7	13241-13242	(	
57-8	13242-13243	)	
57-9	13244-13271	PostgresCatalog.getDatabase	
57-10	13271-13272	(	
57-11	13272-13278	String	
57-12	13279-13291	databaseName	
57-13	13291-13292	)	
57-14	13293-13319	PostgresCatalog.listTables	
57-15	13319-13320	(	
57-16	13320-13326	String	
57-17	13327-13339	databaseName	
57-18	13339-13340	)	
57-19	13341-13365	PostgresCatalog.getTable	
57-20	13365-13366	(	
57-21	13366-13376	ObjectPath	
57-22	13377-13386	tablePath	
57-23	13386-13387	)	
57-24	13388-13415	PostgresCatalog.tableExists	
57-25	13415-13416	(	
57-26	13416-13426	ObjectPath	
57-27	13427-13436	tablePath	
57-28	13436-13437	)	
57-29	13438-13443	Other	
57-30	13444-13451	Catalog	
57-31	13452-13459	methods	
57-32	13460-13462	is	
57-33	13463-13474	unsupported	
57-34	13475-13478	now	
57-35	13478-13479	.	

#Text=Usage of PostgresCatalog
#Text=Please refer to Dependencies section for how to setup a JDBC connector and Postgres driver.
58-1	13480-13485	Usage	
58-2	13486-13488	of	
58-3	13489-13504	PostgresCatalog	
58-4	13505-13511	Please	
58-5	13512-13517	refer	
58-6	13518-13520	to	
58-7	13521-13533	Dependencies	
58-8	13534-13541	section	
58-9	13542-13545	for	
58-10	13546-13549	how	
58-11	13550-13552	to	
58-12	13553-13558	setup	
58-13	13559-13560	a	
58-14	13561-13565	JDBC	
58-15	13566-13575	connector	
58-16	13576-13579	and	
58-17	13580-13588	Postgres	
58-18	13589-13595	driver	
58-19	13595-13596	.	

#Text=Postgres catalog supports the following options:
#Text=name: required, name of the catalog.
#Text=default-database: required, default database to connect to.
#Text=username: required, username of Postgres account.
#Text=password: required, password of the account.
#Text=base-url: required, should be of format "jdbc:postgresql://<ip>:<port>", and should not contain database name here.
59-1	13597-13605	Postgres	
59-2	13606-13613	catalog	
59-3	13614-13622	supports	
59-4	13623-13626	the	
59-5	13627-13636	following	
59-6	13637-13644	options	
59-7	13644-13645	:	
59-8	13646-13650	name	
59-9	13650-13651	:	
59-10	13652-13660	required	
59-11	13660-13661	,	
59-12	13662-13666	name	
59-13	13667-13669	of	
59-14	13670-13673	the	
59-15	13674-13681	catalog	
59-16	13681-13682	.	
59-17	13683-13699	default-database	
59-18	13699-13700	:	
59-19	13701-13709	required	
59-20	13709-13710	,	
59-21	13711-13718	default	
59-22	13719-13727	database	
59-23	13728-13730	to	
59-24	13731-13738	connect	
59-25	13739-13741	to	
59-26	13741-13742	.	
59-27	13743-13751	username	
59-28	13751-13752	:	
59-29	13753-13761	required	
59-30	13761-13762	,	
59-31	13763-13771	username	
59-32	13772-13774	of	
59-33	13775-13783	Postgres	
59-34	13784-13791	account	
59-35	13791-13792	.	
59-36	13793-13801	password	
59-37	13801-13802	:	
59-38	13803-13811	required	
59-39	13811-13812	,	
59-40	13813-13821	password	
59-41	13822-13824	of	
59-42	13825-13828	the	
59-43	13829-13836	account	
59-44	13836-13837	.	
59-45	13838-13846	base-url	
59-46	13846-13847	:	
59-47	13848-13856	required	
59-48	13856-13857	,	
59-49	13858-13864	should	
59-50	13865-13867	be	
59-51	13868-13870	of	
59-52	13871-13877	format	
59-53	13878-13879	"	
59-54	13879-13883	jdbc	
59-55	13883-13884	:	
59-56	13884-13894	postgresql	
59-57	13894-13895	:	
59-58	13895-13896	/	
59-59	13896-13897	/	
59-60	13897-13898	<	
59-61	13898-13900	ip	
59-62	13900-13901	>	
59-63	13901-13902	:	
59-64	13902-13903	<	
59-65	13903-13907	port	
59-66	13907-13908	>	
59-67	13908-13909	"	
59-68	13909-13910	,	
59-69	13911-13914	and	
59-70	13915-13921	should	
59-71	13922-13925	not	
59-72	13926-13933	contain	
59-73	13934-13942	database	
59-74	13943-13947	name	
59-75	13948-13952	here	
59-76	13952-13953	.	

#Text=CREATE CATALOG mypg WITH(
#Text='type' = 'jdbc',
#Text='default-database' = '...',
#Text='username' = '...',
#Text='password' = '...',
#Text='base-url' = '...'
60-1	13954-13960	CREATE	
60-2	13961-13968	CATALOG	
60-3	13969-13973	mypg	
60-4	13974-13978	WITH	
60-5	13978-13979	(	
60-6	13980-13981	'	
60-7	13981-13985	type	
60-8	13985-13986	'	
60-9	13987-13988	=	
60-10	13989-13990	'	
60-11	13990-13994	jdbc	
60-12	13994-13995	'	
60-13	13995-13996	,	
60-14	13997-13998	'	
60-15	13998-14014	default-database	
60-16	14014-14015	'	
60-17	14016-14017	=	
60-18	14018-14019	'	
60-19	14019-14020	.	
60-20	14020-14021	.	
60-21	14021-14022	.	
60-22	14022-14023	'	
60-23	14023-14024	,	
60-24	14025-14026	'	
60-25	14026-14034	username	
60-26	14034-14035	'	
60-27	14036-14037	=	
60-28	14038-14039	'	
60-29	14039-14040	.	
60-30	14040-14041	.	
60-31	14041-14042	.	
60-32	14042-14043	'	
60-33	14043-14044	,	
60-34	14045-14046	'	
60-35	14046-14054	password	
60-36	14054-14055	'	
60-37	14056-14057	=	
60-38	14058-14059	'	
60-39	14059-14060	.	
60-40	14060-14061	.	
60-41	14061-14062	.	
60-42	14062-14063	'	
60-43	14063-14064	,	
60-44	14065-14066	'	
60-45	14066-14074	base-url	
60-46	14074-14075	'	
60-47	14076-14077	=	
60-48	14078-14079	'	
60-49	14079-14080	.	
60-50	14080-14081	.	
60-51	14081-14082	.	
60-52	14082-14083	'	

#Text=USE CATALOG mypg;
#Text=EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build();
#Text=TableEnvironment tableEnv = TableEnvironment.create(settings);
#Text=String name
#Text== "mypg";
#Text=String defaultDatabase = "mydb";
#Text=String username
#Text== "
61-1	14084-14087	USE	
61-2	14088-14095	CATALOG	
61-3	14096-14100	mypg	
61-4	14100-14101	;	
61-5	14102-14121	EnvironmentSettings	
61-6	14122-14130	settings	
61-7	14131-14132	=	
61-8	14133-14164	EnvironmentSettings.newInstance	
61-9	14164-14165	(	
61-10	14165-14166	)	
61-11	14166-14167	.	
61-12	14167-14182	inStreamingMode	
61-13	14182-14183	(	
61-14	14183-14184	)	
61-15	14184-14185	.	
61-16	14185-14190	build	
61-17	14190-14191	(	
61-18	14191-14192	)	
61-19	14192-14193	;	
61-20	14194-14210	TableEnvironment	
61-21	14211-14219	tableEnv	
61-22	14220-14221	=	
61-23	14222-14245	TableEnvironment.create	
61-24	14245-14246	(	
61-25	14246-14254	settings	
61-26	14254-14255	)	
61-27	14255-14256	;	
61-28	14257-14263	String	
61-29	14264-14268	name	
61-30	14269-14270	=	
61-31	14271-14272	"	
61-32	14272-14276	mypg	
61-33	14276-14277	"	
61-34	14277-14278	;	
61-35	14279-14285	String	
61-36	14286-14301	defaultDatabase	
61-37	14302-14303	=	
61-38	14304-14305	"	
61-39	14305-14309	mydb	
61-40	14309-14310	"	
61-41	14310-14311	;	
61-42	14312-14318	String	
61-43	14319-14327	username	
61-44	14328-14329	=	
61-45	14330-14331	"	

#Text=.
62-1	14331-14332	.	

#Text=.
63-1	14332-14333	.	

#Text=.
64-1	14333-14334	.	

#Text=";
#Text=String password
#Text== "
65-1	14334-14335	"	
65-2	14335-14336	;	
65-3	14337-14343	String	
65-4	14344-14352	password	
65-5	14353-14354	=	
65-6	14355-14356	"	

#Text=.
66-1	14356-14357	.	

#Text=.
67-1	14357-14358	.	

#Text=.
68-1	14358-14359	.	

#Text=";
#Text=String baseUrl
#Text== "..."
69-1	14359-14360	"	
69-2	14360-14361	;	
69-3	14362-14368	String	
69-4	14369-14376	baseUrl	
69-5	14377-14378	=	
69-6	14379-14380	"	
69-7	14380-14381	.	
69-8	14381-14382	.	
69-9	14382-14383	.	
69-10	14383-14384	"	

#Text=JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl);
#Text=tableEnv.registerCatalog("mypg", catalog);
#Text=// set the JdbcCatalog as the current catalog of the session
#Text=tableEnv.useCatalog("mypg");
#Text=val settings = EnvironmentSettings.newInstance().inStreamingMode().build()
#Text=val tableEnv = TableEnvironment.create(settings)
#Text=val name
#Text== "mypg"
#Text=val defaultDatabase = "mydb"
#Text=val username
#Text== "..."
#Text=val password
#Text== "..."
#Text=val baseUrl
#Text== "..."
#Text=val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl)
#Text=tableEnv.registerCatalog("mypg", catalog)
#Text=// set the JdbcCatalog as the current catalog of the session
#Text=tableEnv.useCatalog("mypg")
#Text=from pyflink.table.catalog import JdbcCatalog
#Text=environment_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
#Text=t_env = StreamTableEnvironment.create(environment_settings=environment_settings)
#Text=name = "mypg"
#Text=default_database = "mydb"
#Text=username = "..."
#Text=password = "..."
#Text=base_url = "..."
#Text=catalog = JdbcCatalog(name, default_database, username, password, base_url)
#Text=t_env.register_catalog("mypg", catalog)
#Text=# set the JdbcCatalog as the current catalog of the session
#Text=t_env.use_catalog("mypg")
#Text=execution:
#Text=planner: blink
#Text=...
#Text=current-catalog: mypg
#Text=# set the JdbcCatalog as the current catalog of the session
#Text=current-database: mydb
#Text=catalogs:
#Text=- name: mypg
#Text=type: jdbc
#Text=default-database: mydb
#Text=username: ...
#Text=password: ...
#Text=base-url: ...
70-1	14385-14396	JdbcCatalog	
70-2	14397-14404	catalog	
70-3	14405-14406	=	
70-4	14407-14410	new	
70-5	14411-14422	JdbcCatalog	
70-6	14422-14423	(	
70-7	14423-14427	name	
70-8	14427-14428	,	
70-9	14429-14444	defaultDatabase	
70-10	14444-14445	,	
70-11	14446-14454	username	
70-12	14454-14455	,	
70-13	14456-14464	password	
70-14	14464-14465	,	
70-15	14466-14473	baseUrl	
70-16	14473-14474	)	
70-17	14474-14475	;	
70-18	14476-14500	tableEnv.registerCatalog	
70-19	14500-14501	(	
70-20	14501-14502	"	
70-21	14502-14506	mypg	
70-22	14506-14507	"	
70-23	14507-14508	,	
70-24	14509-14516	catalog	
70-25	14516-14517	)	
70-26	14517-14518	;	
70-27	14519-14520	/	
70-28	14520-14521	/	
70-29	14522-14525	set	
70-30	14526-14529	the	
70-31	14530-14541	JdbcCatalog	
70-32	14542-14544	as	
70-33	14545-14548	the	
70-34	14549-14556	current	
70-35	14557-14564	catalog	
70-36	14565-14567	of	
70-37	14568-14571	the	
70-38	14572-14579	session	
70-39	14580-14599	tableEnv.useCatalog	
70-40	14599-14600	(	
70-41	14600-14601	"	
70-42	14601-14605	mypg	
70-43	14605-14606	"	
70-44	14606-14607	)	
70-45	14607-14608	;	
70-46	14609-14612	val	
70-47	14613-14621	settings	
70-48	14622-14623	=	
70-49	14624-14655	EnvironmentSettings.newInstance	
70-50	14655-14656	(	
70-51	14656-14657	)	
70-52	14657-14658	.	
70-53	14658-14673	inStreamingMode	
70-54	14673-14674	(	
70-55	14674-14675	)	
70-56	14675-14676	.	
70-57	14676-14681	build	
70-58	14681-14682	(	
70-59	14682-14683	)	
70-60	14684-14687	val	
70-61	14688-14696	tableEnv	
70-62	14697-14698	=	
70-63	14699-14722	TableEnvironment.create	
70-64	14722-14723	(	
70-65	14723-14731	settings	
70-66	14731-14732	)	
70-67	14733-14736	val	
70-68	14737-14741	name	
70-69	14742-14743	=	
70-70	14744-14745	"	
70-71	14745-14749	mypg	
70-72	14749-14750	"	
70-73	14751-14754	val	
70-74	14755-14770	defaultDatabase	
70-75	14771-14772	=	
70-76	14773-14774	"	
70-77	14774-14778	mydb	
70-78	14778-14779	"	
70-79	14780-14783	val	
70-80	14784-14792	username	
70-81	14793-14794	=	
70-82	14795-14796	"	
70-83	14796-14797	.	
70-84	14797-14798	.	
70-85	14798-14799	.	
70-86	14799-14800	"	
70-87	14801-14804	val	
70-88	14805-14813	password	
70-89	14814-14815	=	
70-90	14816-14817	"	
70-91	14817-14818	.	
70-92	14818-14819	.	
70-93	14819-14820	.	
70-94	14820-14821	"	
70-95	14822-14825	val	
70-96	14826-14833	baseUrl	
70-97	14834-14835	=	
70-98	14836-14837	"	
70-99	14837-14838	.	
70-100	14838-14839	.	
70-101	14839-14840	.	
70-102	14840-14841	"	
70-103	14842-14845	val	
70-104	14846-14853	catalog	
70-105	14854-14855	=	
70-106	14856-14859	new	
70-107	14860-14871	JdbcCatalog	
70-108	14871-14872	(	
70-109	14872-14876	name	
70-110	14876-14877	,	
70-111	14878-14893	defaultDatabase	
70-112	14893-14894	,	
70-113	14895-14903	username	
70-114	14903-14904	,	
70-115	14905-14913	password	
70-116	14913-14914	,	
70-117	14915-14922	baseUrl	
70-118	14922-14923	)	
70-119	14924-14948	tableEnv.registerCatalog	
70-120	14948-14949	(	
70-121	14949-14950	"	
70-122	14950-14954	mypg	
70-123	14954-14955	"	
70-124	14955-14956	,	
70-125	14957-14964	catalog	
70-126	14964-14965	)	
70-127	14966-14967	/	
70-128	14967-14968	/	
70-129	14969-14972	set	
70-130	14973-14976	the	
70-131	14977-14988	JdbcCatalog	
70-132	14989-14991	as	
70-133	14992-14995	the	
70-134	14996-15003	current	
70-135	15004-15011	catalog	
70-136	15012-15014	of	
70-137	15015-15018	the	
70-138	15019-15026	session	
70-139	15027-15046	tableEnv.useCatalog	
70-140	15046-15047	(	
70-141	15047-15048	"	
70-142	15048-15052	mypg	
70-143	15052-15053	"	
70-144	15053-15054	)	
70-145	15055-15059	from	
70-146	15060-15081	pyflink.table.catalog	
70-147	15082-15088	import	
70-148	15089-15100	JdbcCatalog	
70-149	15101-15121	environment_settings	
70-150	15122-15123	=	
70-151	15124-15156	EnvironmentSettings.new_instance	
70-152	15156-15157	(	
70-153	15157-15158	)	
70-154	15158-15159	.	
70-155	15159-15176	in_streaming_mode	
70-156	15176-15177	(	
70-157	15177-15178	)	
70-158	15178-15179	.	
70-159	15179-15196	use_blink_planner	
70-160	15196-15197	(	
70-161	15197-15198	)	
70-162	15198-15199	.	
70-163	15199-15204	build	
70-164	15204-15205	(	
70-165	15205-15206	)	
70-166	15207-15212	t_env	
70-167	15213-15214	=	
70-168	15215-15244	StreamTableEnvironment.create	
70-169	15244-15245	(	
70-170	15245-15265	environment_settings	
70-171	15265-15266	=	
70-172	15266-15286	environment_settings	
70-173	15286-15287	)	
70-174	15288-15292	name	
70-175	15293-15294	=	
70-176	15295-15296	"	
70-177	15296-15300	mypg	
70-178	15300-15301	"	
70-179	15302-15318	default_database	
70-180	15319-15320	=	
70-181	15321-15322	"	
70-182	15322-15326	mydb	
70-183	15326-15327	"	
70-184	15328-15336	username	
70-185	15337-15338	=	
70-186	15339-15340	"	
70-187	15340-15341	.	
70-188	15341-15342	.	
70-189	15342-15343	.	
70-190	15343-15344	"	
70-191	15345-15353	password	
70-192	15354-15355	=	
70-193	15356-15357	"	
70-194	15357-15358	.	
70-195	15358-15359	.	
70-196	15359-15360	.	
70-197	15360-15361	"	
70-198	15362-15370	base_url	
70-199	15371-15372	=	
70-200	15373-15374	"	
70-201	15374-15375	.	
70-202	15375-15376	.	
70-203	15376-15377	.	
70-204	15377-15378	"	
70-205	15379-15386	catalog	
70-206	15387-15388	=	
70-207	15389-15400	JdbcCatalog	
70-208	15400-15401	(	
70-209	15401-15405	name	
70-210	15405-15406	,	
70-211	15407-15423	default_database	
70-212	15423-15424	,	
70-213	15425-15433	username	
70-214	15433-15434	,	
70-215	15435-15443	password	
70-216	15443-15444	,	
70-217	15445-15453	base_url	
70-218	15453-15454	)	
70-219	15455-15477	t_env.register_catalog	
70-220	15477-15478	(	
70-221	15478-15479	"	
70-222	15479-15483	mypg	
70-223	15483-15484	"	
70-224	15484-15485	,	
70-225	15486-15493	catalog	
70-226	15493-15494	)	
70-227	15495-15496	#	
70-228	15497-15500	set	
70-229	15501-15504	the	
70-230	15505-15516	JdbcCatalog	
70-231	15517-15519	as	
70-232	15520-15523	the	
70-233	15524-15531	current	
70-234	15532-15539	catalog	
70-235	15540-15542	of	
70-236	15543-15546	the	
70-237	15547-15554	session	
70-238	15555-15572	t_env.use_catalog	
70-239	15572-15573	(	
70-240	15573-15574	"	
70-241	15574-15578	mypg	
70-242	15578-15579	"	
70-243	15579-15580	)	
70-244	15581-15590	execution	
70-245	15590-15591	:	
70-246	15592-15599	planner	
70-247	15599-15600	:	
70-248	15601-15606	blink	
70-249	15607-15608	.	
70-250	15608-15609	.	
70-251	15609-15610	.	
70-252	15611-15626	current-catalog	
70-253	15626-15627	:	
70-254	15628-15632	mypg	
70-255	15633-15634	#	
70-256	15635-15638	set	
70-257	15639-15642	the	
70-258	15643-15654	JdbcCatalog	
70-259	15655-15657	as	
70-260	15658-15661	the	
70-261	15662-15669	current	
70-262	15670-15677	catalog	
70-263	15678-15680	of	
70-264	15681-15684	the	
70-265	15685-15692	session	
70-266	15693-15709	current-database	
70-267	15709-15710	:	
70-268	15711-15715	mydb	
70-269	15716-15724	catalogs	
70-270	15724-15725	:	
70-271	15726-15727	-	
70-272	15728-15732	name	
70-273	15732-15733	:	
70-274	15734-15738	mypg	
70-275	15739-15743	type	
70-276	15743-15744	:	
70-277	15745-15749	jdbc	
70-278	15750-15766	default-database	
70-279	15766-15767	:	
70-280	15768-15772	mydb	
70-281	15773-15781	username	
70-282	15781-15782	:	
70-283	15783-15784	.	
70-284	15784-15785	.	
70-285	15785-15786	.	
70-286	15787-15795	password	
70-287	15795-15796	:	
70-288	15797-15798	.	
70-289	15798-15799	.	
70-290	15799-15800	.	
70-291	15801-15809	base-url	
70-292	15809-15810	:	
70-293	15811-15812	.	
70-294	15812-15813	.	
70-295	15813-15814	.	

#Text=PostgresSQL Metaspace Mapping
#Text=PostgresSQL has an additional namespace as schema besides database.
71-1	15815-15826	PostgresSQL	
71-2	15827-15836	Metaspace	
71-3	15837-15844	Mapping	
71-4	15845-15856	PostgresSQL	
71-5	15857-15860	has	
71-6	15861-15863	an	
71-7	15864-15874	additional	
71-8	15875-15884	namespace	
71-9	15885-15887	as	
71-10	15888-15894	schema	
71-11	15895-15902	besides	
71-12	15903-15911	database	
71-13	15911-15912	.	

#Text=A Postgres instance can have multiple databases, each database can have multiple schemas with a default one named “public”, each schema can have multiple tables.
72-1	15913-15914	A	
72-2	15915-15923	Postgres	
72-3	15924-15932	instance	
72-4	15933-15936	can	
72-5	15937-15941	have	
72-6	15942-15950	multiple	
72-7	15951-15960	databases	
72-8	15960-15961	,	
72-9	15962-15966	each	
72-10	15967-15975	database	
72-11	15976-15979	can	
72-12	15980-15984	have	
72-13	15985-15993	multiple	
72-14	15994-16001	schemas	
72-15	16002-16006	with	
72-16	16007-16008	a	
72-17	16009-16016	default	
72-18	16017-16020	one	
72-19	16021-16026	named	
72-20	16027-16028	“	
72-21	16028-16034	public	
72-22	16034-16035	”	
72-23	16035-16036	,	
72-24	16037-16041	each	
72-25	16042-16048	schema	
72-26	16049-16052	can	
72-27	16053-16057	have	
72-28	16058-16066	multiple	
72-29	16067-16073	tables	
72-30	16073-16074	.	

#Text=In Flink, when querying tables registered by Postgres catalog, users can use either schema_name.table_name or just table_name.
73-1	16075-16077	In	
73-2	16078-16083	Flink	
73-3	16083-16084	,	
73-4	16085-16089	when	
73-5	16090-16098	querying	
73-6	16099-16105	tables	
73-7	16106-16116	registered	
73-8	16117-16119	by	
73-9	16120-16128	Postgres	
73-10	16129-16136	catalog	
73-11	16136-16137	,	
73-12	16138-16143	users	
73-13	16144-16147	can	
73-14	16148-16151	use	
73-15	16152-16158	either	
73-16	16159-16181	schema_name.table_name	
73-17	16182-16184	or	
73-18	16185-16189	just	
73-19	16190-16200	table_name	
73-20	16200-16201	.	

#Text=The schema_name is optional and defaults to “public”.
74-1	16202-16205	The	
74-2	16206-16217	schema_name	
74-3	16218-16220	is	
74-4	16221-16229	optional	
74-5	16230-16233	and	
74-6	16234-16242	defaults	
74-7	16243-16245	to	
74-8	16246-16247	“	
74-9	16247-16253	public	
74-10	16253-16254	”	
74-11	16254-16255	.	

#Text=Therefor the metaspace mapping between Flink Catalog and Postgres is as following:
#Text=Flink Catalog Metaspace Structure
#Text=Postgres Metaspace Structure
#Text=catalog name (defined in Flink only)
#Text=nan
#Text=database name
#Text=database name
#Text=table name
#Text=[schema_name.]table_name
#Text=The full path of Postgres table in Flink should be "<catalog>.
75-1	16256-16264	Therefor	
75-2	16265-16268	the	
75-3	16269-16278	metaspace	
75-4	16279-16286	mapping	
75-5	16287-16294	between	
75-6	16295-16300	Flink	
75-7	16301-16308	Catalog	
75-8	16309-16312	and	
75-9	16313-16321	Postgres	
75-10	16322-16324	is	
75-11	16325-16327	as	
75-12	16328-16337	following	
75-13	16337-16338	:	
75-14	16339-16344	Flink	
75-15	16345-16352	Catalog	
75-16	16353-16362	Metaspace	
75-17	16363-16372	Structure	
75-18	16373-16381	Postgres	
75-19	16382-16391	Metaspace	
75-20	16392-16401	Structure	
75-21	16402-16409	catalog	
75-22	16410-16414	name	
75-23	16415-16416	(	
75-24	16416-16423	defined	
75-25	16424-16426	in	
75-26	16427-16432	Flink	
75-27	16433-16437	only	
75-28	16437-16438	)	
75-29	16439-16442	nan	
75-30	16443-16451	database	
75-31	16452-16456	name	
75-32	16457-16465	database	
75-33	16466-16470	name	
75-34	16471-16476	table	
75-35	16477-16481	name	
75-36	16482-16483	[	
75-37	16483-16494	schema_name	
75-38	16494-16495	.	
75-39	16495-16496	]	
75-40	16496-16506	table_name	
75-41	16507-16510	The	
75-42	16511-16515	full	
75-43	16516-16520	path	
75-44	16521-16523	of	
75-45	16524-16532	Postgres	
75-46	16533-16538	table	
75-47	16539-16541	in	
75-48	16542-16547	Flink	
75-49	16548-16554	should	
75-50	16555-16557	be	
75-51	16558-16559	"	
75-52	16559-16560	<	
75-53	16560-16567	catalog	
75-54	16567-16568	>	
75-55	16568-16569	.	

#Text=<db>.
76-1	16569-16570	<	
76-2	16570-16572	db	
76-3	16572-16573	>	
76-4	16573-16574	.	

#Text=`<schema.table>`" if schema is specified, note the <schema.table> should be escaped.
77-1	16574-16575	`	
77-2	16575-16576	<	
77-3	16576-16588	schema.table	
77-4	16588-16589	>	
77-5	16589-16590	`	
77-6	16590-16591	"	
77-7	16592-16594	if	
77-8	16595-16601	schema	
77-9	16602-16604	is	
77-10	16605-16614	specified	
77-11	16614-16615	,	
77-12	16616-16620	note	
77-13	16621-16624	the	
77-14	16625-16626	<	
77-15	16626-16638	schema.table	
77-16	16638-16639	>	
77-17	16640-16646	should	
77-18	16647-16649	be	
77-19	16650-16657	escaped	
77-20	16657-16658	.	

#Text=Here are some examples to access Postgres tables:
#Text=-- scan table 'test_table' of 'public' schema (i.e. the default schema), the schema name can be omitted
#Text=SELECT * FROM mypg.mydb.test_table;
#Text=SELECT * FROM mydb.test_table;
#Text=SELECT * FROM test_table;
#Text=-- scan table 'test_table2' of 'custom_schema' schema,
#Text=-- the custom schema can not be omitted and must be escaped with table.
78-1	16659-16663	Here	
78-2	16664-16667	are	
78-3	16668-16672	some	
78-4	16673-16681	examples	
78-5	16682-16684	to	
78-6	16685-16691	access	
78-7	16692-16700	Postgres	
78-8	16701-16707	tables	
78-9	16707-16708	:	
78-10	16709-16710	-	
78-11	16710-16711	-	
78-12	16712-16716	scan	
78-13	16717-16722	table	
78-14	16723-16724	'	
78-15	16724-16734	test_table	
78-16	16734-16735	'	
78-17	16736-16738	of	
78-18	16739-16740	'	
78-19	16740-16746	public	
78-20	16746-16747	'	
78-21	16748-16754	schema	
78-22	16755-16756	(	
78-23	16756-16759	i.e	
78-24	16759-16760	.	
78-25	16761-16764	the	
78-26	16765-16772	default	
78-27	16773-16779	schema	
78-28	16779-16780	)	
78-29	16780-16781	,	
78-30	16782-16785	the	
78-31	16786-16792	schema	
78-32	16793-16797	name	
78-33	16798-16801	can	
78-34	16802-16804	be	
78-35	16805-16812	omitted	
78-36	16813-16819	SELECT	
78-37	16820-16821	*	
78-38	16822-16826	FROM	
78-39	16827-16847	mypg.mydb.test_table	
78-40	16847-16848	;	
78-41	16849-16855	SELECT	
78-42	16856-16857	*	
78-43	16858-16862	FROM	
78-44	16863-16878	mydb.test_table	
78-45	16878-16879	;	
78-46	16880-16886	SELECT	
78-47	16887-16888	*	
78-48	16889-16893	FROM	
78-49	16894-16904	test_table	
78-50	16904-16905	;	
78-51	16906-16907	-	
78-52	16907-16908	-	
78-53	16909-16913	scan	
78-54	16914-16919	table	
78-55	16920-16921	'	
78-56	16921-16932	test_table2	
78-57	16932-16933	'	
78-58	16934-16936	of	
78-59	16937-16938	'	
78-60	16938-16951	custom_schema	
78-61	16951-16952	'	
78-62	16953-16959	schema	
78-63	16959-16960	,	
78-64	16961-16962	-	
78-65	16962-16963	-	
78-66	16964-16967	the	
78-67	16968-16974	custom	
78-68	16975-16981	schema	
78-69	16982-16985	can	
78-70	16986-16989	not	
78-71	16990-16992	be	
78-72	16993-17000	omitted	
78-73	17001-17004	and	
78-74	17005-17009	must	
78-75	17010-17012	be	
78-76	17013-17020	escaped	
78-77	17021-17025	with	
78-78	17026-17031	table	
78-79	17031-17032	.	

#Text=SELECT * FROM mypg.mydb.
79-1	17033-17039	SELECT	
79-2	17040-17041	*	
79-3	17042-17046	FROM	
79-4	17047-17056	mypg.mydb	
79-5	17056-17057	.	

#Text=`custom_schema.test_table2`
#Text=SELECT * FROM mydb.
80-1	17057-17058	`	
80-2	17058-17083	custom_schema.test_table2	
80-3	17083-17084	`	
80-4	17085-17091	SELECT	
80-5	17092-17093	*	
80-6	17094-17098	FROM	
80-7	17099-17103	mydb	
80-8	17103-17104	.	

#Text=`custom_schema.test_table2`;
#Text=SELECT * FROM `custom_schema.test_table2`;
#Text=Data Type Mapping
#Text=Flink supports connect to several databases which uses dialect like MySQL, PostgresSQL, Derby.
81-1	17104-17105	`	
81-2	17105-17130	custom_schema.test_table2	
81-3	17130-17131	`	
81-4	17131-17132	;	
81-5	17133-17139	SELECT	
81-6	17140-17141	*	
81-7	17142-17146	FROM	
81-8	17147-17148	`	
81-9	17148-17173	custom_schema.test_table2	
81-10	17173-17174	`	
81-11	17174-17175	;	
81-12	17176-17180	Data	
81-13	17181-17185	Type	
81-14	17186-17193	Mapping	
81-15	17194-17199	Flink	
81-16	17200-17208	supports	
81-17	17209-17216	connect	
81-18	17217-17219	to	
81-19	17220-17227	several	
81-20	17228-17237	databases	
81-21	17238-17243	which	
81-22	17244-17248	uses	
81-23	17249-17256	dialect	
81-24	17257-17261	like	
81-25	17262-17267	MySQL	
81-26	17267-17268	,	
81-27	17269-17280	PostgresSQL	
81-28	17280-17281	,	
81-29	17282-17287	Derby	
81-30	17287-17288	.	

#Text=The Derby dialect usually used for testing purpose.
82-1	17289-17292	The	
82-2	17293-17298	Derby	
82-3	17299-17306	dialect	
82-4	17307-17314	usually	
82-5	17315-17319	used	
82-6	17320-17323	for	
82-7	17324-17331	testing	
82-8	17332-17339	purpose	
82-9	17339-17340	.	

#Text=The field data type mappings from relational databases data types to Flink SQL data types are listed in the following table, the mapping table can help define JDBC table in Flink easily.
83-1	17341-17344	The	
83-2	17345-17350	field	
83-3	17351-17355	data	
83-4	17356-17360	type	
83-5	17361-17369	mappings	
83-6	17370-17374	from	
83-7	17375-17385	relational	
83-8	17386-17395	databases	
83-9	17396-17400	data	
83-10	17401-17406	types	
83-11	17407-17409	to	
83-12	17410-17415	Flink	
83-13	17416-17419	SQL	
83-14	17420-17424	data	
83-15	17425-17430	types	
83-16	17431-17434	are	
83-17	17435-17441	listed	
83-18	17442-17444	in	
83-19	17445-17448	the	
83-20	17449-17458	following	
83-21	17459-17464	table	
83-22	17464-17465	,	
83-23	17466-17469	the	
83-24	17470-17477	mapping	
83-25	17478-17483	table	
83-26	17484-17487	can	
83-27	17488-17492	help	
83-28	17493-17499	define	
83-29	17500-17504	JDBC	
83-30	17505-17510	table	
83-31	17511-17513	in	
83-32	17514-17519	Flink	
83-33	17520-17526	easily	
83-34	17526-17527	.	

#Text=MySQL type
#Text=PostgreSQL type
#Text=Flink SQL type
#Text=TINYINT
#Text=TINYINT
#Text=SMALLINT
#Text=TINYINT UNSIGNED
#Text=SMALLINT
#Text=INT2
#Text=SMALLSERIAL
#Text=SERIAL2
#Text=SMALLINT
#Text=INT
#Text=MEDIUMINT
#Text=SMALLINT UNSIGNED
#Text=INTEGER
#Text=SERIAL
#Text=INT
#Text=BIGINT
#Text=INT UNSIGNED
#Text=BIGINT
#Text=BIGSERIAL
#Text=BIGINT
#Text=BIGINT UNSIGNED
#Text=DECIMAL(20, 0)
#Text=BIGINT
#Text=BIGINT
#Text=BIGINT
#Text=FLOAT
#Text=REAL
#Text=FLOAT4
#Text=FLOAT
#Text=DOUBLE
#Text=DOUBLE PRECISION
#Text=FLOAT8
#Text=DOUBLE PRECISION
#Text=DOUBLE
#Text=NUMERIC(p, s)
#Text=DECIMAL(p, s)
#Text=NUMERIC(p, s)
#Text=DECIMAL(p, s)
#Text=DECIMAL(p, s)
#Text=BOOLEAN
#Text=TINYINT(1)
#Text=BOOLEAN
#Text=BOOLEAN
#Text=DATE
#Text=DATE
#Text=DATE
#Text=TIME [(p)]
#Text=TIME [(p)] [WITHOUT TIMEZONE]
#Text=TIME [(p)] [WITHOUT TIMEZONE]
#Text=DATETIME [(p)]
#Text=TIMESTAMP [(p)] [WITHOUT TIMEZONE]
#Text=TIMESTAMP [(p)] [WITHOUT TIMEZONE]
#Text=CHAR(n)
#Text=VARCHAR(n)
#Text=TEXT
#Text=CHAR(n)
#Text=CHARACTER(n)
#Text=VARCHAR(n)
#Text=CHARACTER VARYING(n)
#Text=TEXT
#Text=STRING
#Text=BINARY
#Text=VARBINARY
#Text=BLOB
#Text=BYTEA
#Text=BYTES
#Text=ARRAY
#Text=ARRAY
#Text=Back to top
#Text=Want to contribute translation?
84-1	17528-17533	MySQL	
84-2	17534-17538	type	
84-3	17539-17549	PostgreSQL	
84-4	17550-17554	type	
84-5	17555-17560	Flink	
84-6	17561-17564	SQL	
84-7	17565-17569	type	
84-8	17570-17577	TINYINT	
84-9	17578-17585	TINYINT	
84-10	17586-17594	SMALLINT	
84-11	17595-17602	TINYINT	
84-12	17603-17611	UNSIGNED	
84-13	17612-17620	SMALLINT	
84-14	17621-17625	INT2	
84-15	17626-17637	SMALLSERIAL	
84-16	17638-17645	SERIAL2	
84-17	17646-17654	SMALLINT	
84-18	17655-17658	INT	
84-19	17659-17668	MEDIUMINT	
84-20	17669-17677	SMALLINT	
84-21	17678-17686	UNSIGNED	
84-22	17687-17694	INTEGER	
84-23	17695-17701	SERIAL	
84-24	17702-17705	INT	
84-25	17706-17712	BIGINT	
84-26	17713-17716	INT	
84-27	17717-17725	UNSIGNED	
84-28	17726-17732	BIGINT	
84-29	17733-17742	BIGSERIAL	
84-30	17743-17749	BIGINT	
84-31	17750-17756	BIGINT	
84-32	17757-17765	UNSIGNED	
84-33	17766-17773	DECIMAL	
84-34	17773-17774	(	
84-35	17774-17776	20	
84-36	17776-17777	,	
84-37	17778-17779	0	
84-38	17779-17780	)	
84-39	17781-17787	BIGINT	
84-40	17788-17794	BIGINT	
84-41	17795-17801	BIGINT	
84-42	17802-17807	FLOAT	
84-43	17808-17812	REAL	
84-44	17813-17819	FLOAT4	
84-45	17820-17825	FLOAT	
84-46	17826-17832	DOUBLE	
84-47	17833-17839	DOUBLE	
84-48	17840-17849	PRECISION	
84-49	17850-17856	FLOAT8	
84-50	17857-17863	DOUBLE	
84-51	17864-17873	PRECISION	
84-52	17874-17880	DOUBLE	
84-53	17881-17888	NUMERIC	
84-54	17888-17889	(	
84-55	17889-17890	p	
84-56	17890-17891	,	
84-57	17892-17893	s	
84-58	17893-17894	)	
84-59	17895-17902	DECIMAL	
84-60	17902-17903	(	
84-61	17903-17904	p	
84-62	17904-17905	,	
84-63	17906-17907	s	
84-64	17907-17908	)	
84-65	17909-17916	NUMERIC	
84-66	17916-17917	(	
84-67	17917-17918	p	
84-68	17918-17919	,	
84-69	17920-17921	s	
84-70	17921-17922	)	
84-71	17923-17930	DECIMAL	
84-72	17930-17931	(	
84-73	17931-17932	p	
84-74	17932-17933	,	
84-75	17934-17935	s	
84-76	17935-17936	)	
84-77	17937-17944	DECIMAL	
84-78	17944-17945	(	
84-79	17945-17946	p	
84-80	17946-17947	,	
84-81	17948-17949	s	
84-82	17949-17950	)	
84-83	17951-17958	BOOLEAN	
84-84	17959-17966	TINYINT	
84-85	17966-17967	(	
84-86	17967-17968	1	
84-87	17968-17969	)	
84-88	17970-17977	BOOLEAN	
84-89	17978-17985	BOOLEAN	
84-90	17986-17990	DATE	
84-91	17991-17995	DATE	
84-92	17996-18000	DATE	
84-93	18001-18005	TIME	
84-94	18006-18007	[	
84-95	18007-18008	(	
84-96	18008-18009	p	
84-97	18009-18010	)	
84-98	18010-18011	]	
84-99	18012-18016	TIME	
84-100	18017-18018	[	
84-101	18018-18019	(	
84-102	18019-18020	p	
84-103	18020-18021	)	
84-104	18021-18022	]	
84-105	18023-18024	[	
84-106	18024-18031	WITHOUT	
84-107	18032-18040	TIMEZONE	
84-108	18040-18041	]	
84-109	18042-18046	TIME	
84-110	18047-18048	[	
84-111	18048-18049	(	
84-112	18049-18050	p	
84-113	18050-18051	)	
84-114	18051-18052	]	
84-115	18053-18054	[	
84-116	18054-18061	WITHOUT	
84-117	18062-18070	TIMEZONE	
84-118	18070-18071	]	
84-119	18072-18080	DATETIME	
84-120	18081-18082	[	
84-121	18082-18083	(	
84-122	18083-18084	p	
84-123	18084-18085	)	
84-124	18085-18086	]	
84-125	18087-18096	TIMESTAMP	
84-126	18097-18098	[	
84-127	18098-18099	(	
84-128	18099-18100	p	
84-129	18100-18101	)	
84-130	18101-18102	]	
84-131	18103-18104	[	
84-132	18104-18111	WITHOUT	
84-133	18112-18120	TIMEZONE	
84-134	18120-18121	]	
84-135	18122-18131	TIMESTAMP	
84-136	18132-18133	[	
84-137	18133-18134	(	
84-138	18134-18135	p	
84-139	18135-18136	)	
84-140	18136-18137	]	
84-141	18138-18139	[	
84-142	18139-18146	WITHOUT	
84-143	18147-18155	TIMEZONE	
84-144	18155-18156	]	
84-145	18157-18161	CHAR	
84-146	18161-18162	(	
84-147	18162-18163	n	
84-148	18163-18164	)	
84-149	18165-18172	VARCHAR	
84-150	18172-18173	(	
84-151	18173-18174	n	
84-152	18174-18175	)	
84-153	18176-18180	TEXT	
84-154	18181-18185	CHAR	
84-155	18185-18186	(	
84-156	18186-18187	n	
84-157	18187-18188	)	
84-158	18189-18198	CHARACTER	
84-159	18198-18199	(	
84-160	18199-18200	n	
84-161	18200-18201	)	
84-162	18202-18209	VARCHAR	
84-163	18209-18210	(	
84-164	18210-18211	n	
84-165	18211-18212	)	
84-166	18213-18222	CHARACTER	
84-167	18223-18230	VARYING	
84-168	18230-18231	(	
84-169	18231-18232	n	
84-170	18232-18233	)	
84-171	18234-18238	TEXT	
84-172	18239-18245	STRING	
84-173	18246-18252	BINARY	
84-174	18253-18262	VARBINARY	
84-175	18263-18267	BLOB	
84-176	18268-18273	BYTEA	
84-177	18274-18279	BYTES	
84-178	18280-18285	ARRAY	
84-179	18286-18291	ARRAY	
84-180	18292-18296	Back	
84-181	18297-18299	to	
84-182	18300-18303	top	
84-183	18304-18308	Want	
84-184	18309-18311	to	
84-185	18312-18322	contribute	
84-186	18323-18334	translation	
84-187	18334-18335	?	
