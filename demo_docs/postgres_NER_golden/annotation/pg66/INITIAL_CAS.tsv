#FORMAT=WebAnno TSV 3.3


#Text=JDBC To Other Databases - Spark 2.4.7 Documentation 2.4.7 Overview Programming Guides Quick Start RDDs, Accumulators, Broadcasts Vars SQL, DataFrames, and Datasets
1-1	0-4	JDBC	
1-2	5-7	To	
1-3	8-13	Other	
1-4	14-23	Databases	
1-5	24-25	-	
1-6	26-31	Spark	
1-7	32-37	2.4.7	
1-8	38-51	Documentation	
1-9	52-57	2.4.7	
1-10	58-66	Overview	
1-11	67-78	Programming	
1-12	79-85	Guides	
1-13	86-91	Quick	
1-14	92-97	Start	
1-15	98-102	RDDs	
1-16	102-103	,	
1-17	104-116	Accumulators	
1-18	116-117	,	
1-19	118-128	Broadcasts	
1-20	129-133	Vars	
1-21	134-137	SQL	
1-22	137-138	,	
1-23	139-149	DataFrames	
1-24	149-150	,	
1-25	151-154	and	
1-26	155-163	Datasets	

#Text=Structured Streaming Spark Streaming (DStreams) MLlib (Machine Learning) GraphX (Graph Processing) SparkR (R on Spark) API Docs Scala Java Python
2-1	164-174	Structured	
2-2	175-184	Streaming	
2-3	185-190	Spark	
2-4	191-200	Streaming	
2-5	201-202	(	
2-6	202-210	DStreams	
2-7	210-211	)	
2-8	212-217	MLlib	
2-9	218-219	(	
2-10	219-226	Machine	
2-11	227-235	Learning	
2-12	235-236	)	
2-13	237-243	GraphX	
2-14	244-245	(	
2-15	245-250	Graph	
2-16	251-261	Processing	
2-17	261-262	)	
2-18	263-269	SparkR	
2-19	270-271	(	
2-20	271-272	R	
2-21	273-275	on	
2-22	276-281	Spark	
2-23	281-282	)	
2-24	283-286	API	
2-25	287-291	Docs	
2-26	292-297	Scala	
2-27	298-302	Java	
2-28	303-309	Python	

#Text=SQL, Built-in Functions Deploying Overview Submitting Applications Spark Standalone Mesos YARN Kubernetes More Configuration Monitoring Tuning Guide Job Scheduling
3-1	310-313	SQL	
3-2	313-314	,	
3-3	315-323	Built-in	
3-4	324-333	Functions	
3-5	334-343	Deploying	
3-6	344-352	Overview	
3-7	353-363	Submitting	
3-8	364-376	Applications	
3-9	377-382	Spark	
3-10	383-393	Standalone	
3-11	394-399	Mesos	
3-12	400-404	YARN	
3-13	405-415	Kubernetes	
3-14	416-420	More	
3-15	421-434	Configuration	
3-16	435-445	Monitoring	
3-17	446-452	Tuning	
3-18	453-458	Guide	
3-19	459-462	Job	
3-20	463-473	Scheduling	

#Text=Security Hardware Provisioning Building Spark Contributing to Spark Third Party Projects Spark SQL Guide Getting Started Data Sources Generic Load/Save Functions Parquet Files ORC Files
4-1	474-482	Security	
4-2	483-491	Hardware	
4-3	492-504	Provisioning	
4-4	505-513	Building	
4-5	514-519	Spark	
4-6	520-532	Contributing	
4-7	533-535	to	
4-8	536-541	Spark	
4-9	542-547	Third	
4-10	548-553	Party	
4-11	554-562	Projects	
4-12	563-568	Spark	
4-13	569-572	SQL	
4-14	573-578	Guide	
4-15	579-586	Getting	
4-16	587-594	Started	
4-17	595-599	Data	
4-18	600-607	Sources	
4-19	608-615	Generic	
4-20	616-620	Load	
4-21	620-621	/	
4-22	621-625	Save	
4-23	626-635	Functions	
4-24	636-643	Parquet	
4-25	644-649	Files	
4-26	650-653	ORC	
4-27	654-659	Files	

#Text=JSON Files Hive Tables JDBC To Other Databases Avro Files Troubleshooting Performance Tuning Distributed SQL Engine PySpark Usage Guide for Pandas with Apache Arrow Migration Guide Reference
5-1	660-664	JSON	
5-2	665-670	Files	
5-3	671-675	Hive	
5-4	676-682	Tables	
5-5	683-687	JDBC	
5-6	688-690	To	
5-7	691-696	Other	
5-8	697-706	Databases	
5-9	707-711	Avro	
5-10	712-717	Files	
5-11	718-733	Troubleshooting	
5-12	734-745	Performance	
5-13	746-752	Tuning	
5-14	753-764	Distributed	
5-15	765-768	SQL	
5-16	769-775	Engine	
5-17	776-783	PySpark	
5-18	784-789	Usage	
5-19	790-795	Guide	
5-20	796-799	for	
5-21	800-806	Pandas	
5-22	807-811	with	
5-23	812-818	Apache	
5-24	819-824	Arrow	
5-25	825-834	Migration	
5-26	835-840	Guide	
5-27	841-850	Reference	

#Text=JDBC To Other Databases Spark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using JdbcRDD. This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.
6-1	851-855	JDBC	
6-2	856-858	To	
6-3	859-864	Other	
6-4	865-874	Databases	
6-5	875-880	Spark	
6-6	881-884	SQL	
6-7	885-889	also	
6-8	890-898	includes	
6-9	899-900	a	
6-10	901-905	data	
6-11	906-912	source	
6-12	913-917	that	
6-13	918-921	can	
6-14	922-926	read	
6-15	927-931	data	
6-16	932-936	from	
6-17	937-942	other	
6-18	943-952	databases	
6-19	953-958	using	
6-20	959-963	JDBC	
6-21	963-964	.	
6-22	965-969	This	
6-23	970-983	functionality	
6-24	984-990	should	
6-25	991-993	be	
6-26	994-1003	preferred	
6-27	1004-1008	over	
6-28	1009-1014	using	
6-29	1015-1022	JdbcRDD	
6-30	1022-1023	.	
6-31	1024-1028	This	
6-32	1029-1031	is	
6-33	1032-1039	because	
6-34	1040-1043	the	
6-35	1044-1051	results	
6-36	1052-1055	are	
6-37	1056-1064	returned	
6-38	1065-1067	as	
6-39	1068-1069	a	
6-40	1070-1079	DataFrame	
6-41	1080-1083	and	
6-42	1084-1088	they	
6-43	1089-1092	can	
6-44	1093-1099	easily	
6-45	1100-1102	be	
6-46	1103-1112	processed	
6-47	1113-1115	in	
6-48	1116-1121	Spark	
6-49	1122-1125	SQL	
6-50	1126-1128	or	
6-51	1129-1135	joined	
6-52	1136-1140	with	
6-53	1141-1146	other	
6-54	1147-1151	data	
6-55	1152-1159	sources	
6-56	1159-1160	.	

#Text=The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL). To get started you will need to include the JDBC driver for your particular database on the
7-1	1161-1164	The	
7-2	1165-1169	JDBC	
7-3	1170-1174	data	
7-4	1175-1181	source	
7-5	1182-1184	is	
7-6	1185-1189	also	
7-7	1190-1196	easier	
7-8	1197-1199	to	
7-9	1200-1203	use	
7-10	1204-1208	from	
7-11	1209-1213	Java	
7-12	1214-1216	or	
7-13	1217-1223	Python	
7-14	1224-1226	as	
7-15	1227-1229	it	
7-16	1230-1234	does	
7-17	1235-1238	not	
7-18	1239-1246	require	
7-19	1247-1250	the	
7-20	1251-1255	user	
7-21	1256-1258	to	
7-22	1259-1266	provide	
7-23	1267-1268	a	
7-24	1269-1277	ClassTag	
7-25	1277-1278	.	
7-26	1279-1280	(	
7-27	1280-1284	Note	
7-28	1285-1289	that	
7-29	1290-1294	this	
7-30	1295-1297	is	
7-31	1298-1307	different	
7-32	1308-1312	than	
7-33	1313-1316	the	
7-34	1317-1322	Spark	
7-35	1323-1326	SQL	
7-36	1327-1331	JDBC	
7-37	1332-1338	server	
7-38	1338-1339	,	
7-39	1340-1345	which	
7-40	1346-1352	allows	
7-41	1353-1358	other	
7-42	1359-1371	applications	
7-43	1372-1374	to	
7-44	1375-1378	run	
7-45	1379-1386	queries	
7-46	1387-1392	using	
7-47	1393-1398	Spark	
7-48	1399-1402	SQL	
7-49	1402-1403	)	
7-50	1403-1404	.	
7-51	1405-1407	To	
7-52	1408-1411	get	
7-53	1412-1419	started	
7-54	1420-1423	you	
7-55	1424-1428	will	
7-56	1429-1433	need	
7-57	1434-1436	to	
7-58	1437-1444	include	
7-59	1445-1448	the	
7-60	1449-1453	JDBC	
7-61	1454-1460	driver	
7-62	1461-1464	for	
7-63	1465-1469	your	
7-64	1470-1480	particular	
7-65	1481-1489	database	
7-66	1490-1492	on	
7-67	1493-1496	the	

#Text=spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command: bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using
8-1	1497-1502	spark	
8-2	1503-1512	classpath	
8-3	1512-1513	.	
8-4	1514-1517	For	
8-5	1518-1525	example	
8-6	1525-1526	,	
8-7	1527-1529	to	
8-8	1530-1537	connect	
8-9	1538-1540	to	
8-10	1541-1549	postgres	
8-11	1550-1554	from	
8-12	1555-1558	the	
8-13	1559-1564	Spark	
8-14	1565-1570	Shell	
8-15	1571-1574	you	
8-16	1575-1580	would	
8-17	1581-1584	run	
8-18	1585-1588	the	
8-19	1589-1598	following	
8-20	1599-1606	command	
8-21	1606-1607	:	
8-22	1608-1611	bin	
8-23	1611-1612	/	
8-24	1612-1623	spark-shell	
8-25	1624-1625	-	
8-26	1625-1626	-	
8-27	1626-1643	driver-class-path	
8-28	1644-1654	postgresql	
8-29	1654-1655	-	
8-30	1655-1663	9.4.1207	
8-31	1663-1664	.	
8-32	1664-1667	jar	
8-33	1668-1669	-	
8-34	1669-1670	-	
8-35	1670-1674	jars	
8-36	1675-1685	postgresql	
8-37	1685-1686	-	
8-38	1686-1694	9.4.1207	
8-39	1694-1695	.	
8-40	1695-1698	jar	
8-41	1699-1705	Tables	
8-42	1706-1710	from	
8-43	1711-1714	the	
8-44	1715-1721	remote	
8-45	1722-1730	database	
8-46	1731-1734	can	
8-47	1735-1737	be	
8-48	1738-1744	loaded	
8-49	1745-1747	as	
8-50	1748-1749	a	
8-51	1750-1759	DataFrame	
8-52	1760-1762	or	
8-53	1763-1768	Spark	
8-54	1769-1772	SQL	
8-55	1773-1782	temporary	
8-56	1783-1787	view	
8-57	1788-1793	using	

#Text=the Data Sources API. Users can specify the JDBC connection properties in the data source options. user and password are normally provided as connection properties for logging into the data sources. In addition to the connection properties, Spark also supports the following case-insensitive options: Property NameMeaning url
9-1	1794-1797	the	
9-2	1798-1802	Data	
9-3	1803-1810	Sources	
9-4	1811-1814	API	
9-5	1814-1815	.	
9-6	1816-1821	Users	
9-7	1822-1825	can	
9-8	1826-1833	specify	
9-9	1834-1837	the	
9-10	1838-1842	JDBC	
9-11	1843-1853	connection	
9-12	1854-1864	properties	
9-13	1865-1867	in	
9-14	1868-1871	the	
9-15	1872-1876	data	
9-16	1877-1883	source	
9-17	1884-1891	options	
9-18	1891-1892	.	
9-19	1893-1897	user	
9-20	1898-1901	and	
9-21	1902-1910	password	
9-22	1911-1914	are	
9-23	1915-1923	normally	
9-24	1924-1932	provided	
9-25	1933-1935	as	
9-26	1936-1946	connection	
9-27	1947-1957	properties	
9-28	1958-1961	for	
9-29	1962-1969	logging	
9-30	1970-1974	into	
9-31	1975-1978	the	
9-32	1979-1983	data	
9-33	1984-1991	sources	
9-34	1991-1992	.	
9-35	1993-1995	In	
9-36	1996-2004	addition	
9-37	2005-2007	to	
9-38	2008-2011	the	
9-39	2012-2022	connection	
9-40	2023-2033	properties	
9-41	2033-2034	,	
9-42	2035-2040	Spark	
9-43	2041-2045	also	
9-44	2046-2054	supports	
9-45	2055-2058	the	
9-46	2059-2068	following	
9-47	2069-2085	case-insensitive	
9-48	2086-2093	options	
9-49	2093-2094	:	
9-50	2095-2103	Property	
9-51	2104-2115	NameMeaning	
9-52	2116-2119	url	

#Text=The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., jdbc:postgresql://localhost/test?user=fred&password=secret dbtable The JDBC table that should be read from or written into. Note that when using it in the read path anything that is valid in a FROM clause of a SQL query can be used.
10-1	2120-2123	The	
10-2	2124-2128	JDBC	
10-3	2129-2132	URL	
10-4	2133-2135	to	
10-5	2136-2143	connect	
10-6	2144-2146	to	
10-7	2146-2147	.	
10-8	2148-2151	The	
10-9	2152-2167	source-specific	
10-10	2168-2178	connection	
10-11	2179-2189	properties	
10-12	2190-2193	may	
10-13	2194-2196	be	
10-14	2197-2206	specified	
10-15	2207-2209	in	
10-16	2210-2213	the	
10-17	2214-2217	URL	
10-18	2217-2218	.	
10-19	2219-2222	e.g	
10-20	2222-2223	.	
10-21	2223-2224	,	
10-22	2225-2229	jdbc	
10-23	2229-2230	:	
10-24	2230-2240	postgresql	
10-25	2240-2241	:	
10-26	2241-2242	/	
10-27	2242-2243	/	
10-28	2243-2252	localhost	
10-29	2252-2253	/	
10-30	2253-2257	test	
10-31	2257-2258	?	
10-32	2258-2262	user	
10-33	2262-2263	=	
10-34	2263-2267	fred	
10-35	2267-2268	&	
10-36	2268-2276	password	
10-37	2276-2277	=	
10-38	2277-2283	secret	
10-39	2284-2291	dbtable	
10-40	2292-2295	The	
10-41	2296-2300	JDBC	
10-42	2301-2306	table	
10-43	2307-2311	that	
10-44	2312-2318	should	
10-45	2319-2321	be	
10-46	2322-2326	read	
10-47	2327-2331	from	
10-48	2332-2334	or	
10-49	2335-2342	written	
10-50	2343-2347	into	
10-51	2347-2348	.	
10-52	2349-2353	Note	
10-53	2354-2358	that	
10-54	2359-2363	when	
10-55	2364-2369	using	
10-56	2370-2372	it	
10-57	2373-2375	in	
10-58	2376-2379	the	
10-59	2380-2384	read	
10-60	2385-2389	path	
10-61	2390-2398	anything	
10-62	2399-2403	that	
10-63	2404-2406	is	
10-64	2407-2412	valid	
10-65	2413-2415	in	
10-66	2416-2417	a	
10-67	2418-2422	FROM	
10-68	2423-2429	clause	
10-69	2430-2432	of	
10-70	2433-2434	a	
10-71	2435-2438	SQL	
10-72	2439-2444	query	
10-73	2445-2448	can	
10-74	2449-2451	be	
10-75	2452-2456	used	
10-76	2456-2457	.	

#Text=For example, instead of a full table you could also use a subquery in parentheses. It is not allowed to specify `dbtable` and `query` options at the same time. query A query that will be used to read data into Spark. The specified query will be parenthesized and used
11-1	2458-2461	For	
11-2	2462-2469	example	
11-3	2469-2470	,	
11-4	2471-2478	instead	
11-5	2479-2481	of	
11-6	2482-2483	a	
11-7	2484-2488	full	
11-8	2489-2494	table	
11-9	2495-2498	you	
11-10	2499-2504	could	
11-11	2505-2509	also	
11-12	2510-2513	use	
11-13	2514-2515	a	
11-14	2516-2524	subquery	
11-15	2525-2527	in	
11-16	2528-2539	parentheses	
11-17	2539-2540	.	
11-18	2541-2543	It	
11-19	2544-2546	is	
11-20	2547-2550	not	
11-21	2551-2558	allowed	
11-22	2559-2561	to	
11-23	2562-2569	specify	
11-24	2570-2571	`	
11-25	2571-2578	dbtable	
11-26	2578-2579	`	
11-27	2580-2583	and	
11-28	2584-2585	`	
11-29	2585-2590	query	
11-30	2590-2591	`	
11-31	2592-2599	options	
11-32	2600-2602	at	
11-33	2603-2606	the	
11-34	2607-2611	same	
11-35	2612-2616	time	
11-36	2616-2617	.	
11-37	2618-2623	query	
11-38	2624-2625	A	
11-39	2626-2631	query	
11-40	2632-2636	that	
11-41	2637-2641	will	
11-42	2642-2644	be	
11-43	2645-2649	used	
11-44	2650-2652	to	
11-45	2653-2657	read	
11-46	2658-2662	data	
11-47	2663-2667	into	
11-48	2668-2673	Spark	
11-49	2673-2674	.	
11-50	2675-2678	The	
11-51	2679-2688	specified	
11-52	2689-2694	query	
11-53	2695-2699	will	
11-54	2700-2702	be	
11-55	2703-2716	parenthesized	
11-56	2717-2720	and	
11-57	2721-2725	used	

#Text=as a subquery in the FROM clause. Spark will also assign an alias to the subquery clause. As an example, spark will issue a query of the following form to the JDBC Source. SELECT <columns> FROM (<user_specified_query>) spark_gen_alias Below are couple of restrictions while using this option.
12-1	2726-2728	as	
12-2	2729-2730	a	
12-3	2731-2739	subquery	
12-4	2740-2742	in	
12-5	2743-2746	the	
12-6	2747-2751	FROM	
12-7	2752-2758	clause	
12-8	2758-2759	.	
12-9	2760-2765	Spark	
12-10	2766-2770	will	
12-11	2771-2775	also	
12-12	2776-2782	assign	
12-13	2783-2785	an	
12-14	2786-2791	alias	
12-15	2792-2794	to	
12-16	2795-2798	the	
12-17	2799-2807	subquery	
12-18	2808-2814	clause	
12-19	2814-2815	.	
12-20	2816-2818	As	
12-21	2819-2821	an	
12-22	2822-2829	example	
12-23	2829-2830	,	
12-24	2831-2836	spark	
12-25	2837-2841	will	
12-26	2842-2847	issue	
12-27	2848-2849	a	
12-28	2850-2855	query	
12-29	2856-2858	of	
12-30	2859-2862	the	
12-31	2863-2872	following	
12-32	2873-2877	form	
12-33	2878-2880	to	
12-34	2881-2884	the	
12-35	2885-2889	JDBC	
12-36	2890-2896	Source	
12-37	2896-2897	.	
12-38	2898-2904	SELECT	
12-39	2905-2906	<	
12-40	2906-2913	columns	
12-41	2913-2914	>	
12-42	2915-2919	FROM	
12-43	2920-2921	(	
12-44	2921-2922	<	
12-45	2922-2942	user_specified_query	
12-46	2942-2943	>	
12-47	2943-2944	)	
12-48	2945-2960	spark_gen_alias	
12-49	2961-2966	Below	
12-50	2967-2970	are	
12-51	2971-2977	couple	
12-52	2978-2980	of	
12-53	2981-2993	restrictions	
12-54	2994-2999	while	
12-55	3000-3005	using	
12-56	3006-3010	this	
12-57	3011-3017	option	
12-58	3017-3018	.	

#Text=It is not allowed to specify `dbtable` and `query` options at the same time. It is not allowed to specify `query` and `partitionColumn` options at the same time. When specifying `partitionColumn` option is required, the subquery can be specified using `dbtable` option instead and
13-1	3019-3021	It	
13-2	3022-3024	is	
13-3	3025-3028	not	
13-4	3029-3036	allowed	
13-5	3037-3039	to	
13-6	3040-3047	specify	
13-7	3048-3049	`	
13-8	3049-3056	dbtable	
13-9	3056-3057	`	
13-10	3058-3061	and	
13-11	3062-3063	`	
13-12	3063-3068	query	
13-13	3068-3069	`	
13-14	3070-3077	options	
13-15	3078-3080	at	
13-16	3081-3084	the	
13-17	3085-3089	same	
13-18	3090-3094	time	
13-19	3094-3095	.	
13-20	3096-3098	It	
13-21	3099-3101	is	
13-22	3102-3105	not	
13-23	3106-3113	allowed	
13-24	3114-3116	to	
13-25	3117-3124	specify	
13-26	3125-3126	`	
13-27	3126-3131	query	
13-28	3131-3132	`	
13-29	3133-3136	and	
13-30	3137-3138	`	
13-31	3138-3153	partitionColumn	
13-32	3153-3154	`	
13-33	3155-3162	options	
13-34	3163-3165	at	
13-35	3166-3169	the	
13-36	3170-3174	same	
13-37	3175-3179	time	
13-38	3179-3180	.	
13-39	3181-3185	When	
13-40	3186-3196	specifying	
13-41	3197-3198	`	
13-42	3198-3213	partitionColumn	
13-43	3213-3214	`	
13-44	3215-3221	option	
13-45	3222-3224	is	
13-46	3225-3233	required	
13-47	3233-3234	,	
13-48	3235-3238	the	
13-49	3239-3247	subquery	
13-50	3248-3251	can	
13-51	3252-3254	be	
13-52	3255-3264	specified	
13-53	3265-3270	using	
13-54	3271-3272	`	
13-55	3272-3279	dbtable	
13-56	3279-3280	`	
13-57	3281-3287	option	
13-58	3288-3295	instead	
13-59	3296-3299	and	

#Text=partition columns can be qualified using the subquery alias provided as part of `dbtable`. Example: spark.read.format("jdbc") .option("url", jdbcUrl) .option("query", "select c1, c2 from t1") .load()
14-1	3300-3309	partition	
14-2	3310-3317	columns	
14-3	3318-3321	can	
14-4	3322-3324	be	
14-5	3325-3334	qualified	
14-6	3335-3340	using	
14-7	3341-3344	the	
14-8	3345-3353	subquery	
14-9	3354-3359	alias	
14-10	3360-3368	provided	
14-11	3369-3371	as	
14-12	3372-3376	part	
14-13	3377-3379	of	
14-14	3380-3381	`	
14-15	3381-3388	dbtable	
14-16	3388-3389	`	
14-17	3389-3390	.	
14-18	3391-3398	Example	
14-19	3398-3399	:	
14-20	3400-3417	spark.read.format	
14-21	3417-3418	(	
14-22	3418-3419	"	
14-23	3419-3423	jdbc	
14-24	3423-3424	"	
14-25	3424-3425	)	
14-26	3426-3427	.	
14-27	3427-3433	option	
14-28	3433-3434	(	
14-29	3434-3435	"	
14-30	3435-3438	url	
14-31	3438-3439	"	
14-32	3439-3440	,	
14-33	3441-3448	jdbcUrl	
14-34	3448-3449	)	
14-35	3450-3451	.	
14-36	3451-3457	option	
14-37	3457-3458	(	
14-38	3458-3459	"	
14-39	3459-3464	query	
14-40	3464-3465	"	
14-41	3465-3466	,	
14-42	3467-3468	"	
14-43	3468-3474	select	
14-44	3475-3477	c1	
14-45	3477-3478	,	
14-46	3479-3481	c2	
14-47	3482-3486	from	
14-48	3487-3489	t1	
14-49	3489-3490	"	
14-50	3490-3491	)	
14-51	3492-3493	.	
14-52	3493-3497	load	
14-53	3497-3498	(	
14-54	3498-3499	)	

#Text=driver The class name of the JDBC driver to use to connect to this URL. partitionColumn, lowerBound, upperBound These options must all be specified if any of them is specified. In addition,
15-1	3500-3506	driver	
15-2	3507-3510	The	
15-3	3511-3516	class	
15-4	3517-3521	name	
15-5	3522-3524	of	
15-6	3525-3528	the	
15-7	3529-3533	JDBC	
15-8	3534-3540	driver	
15-9	3541-3543	to	
15-10	3544-3547	use	
15-11	3548-3550	to	
15-12	3551-3558	connect	
15-13	3559-3561	to	
15-14	3562-3566	this	
15-15	3567-3570	URL	
15-16	3570-3571	.	
15-17	3572-3587	partitionColumn	
15-18	3587-3588	,	
15-19	3589-3599	lowerBound	
15-20	3599-3600	,	
15-21	3601-3611	upperBound	
15-22	3612-3617	These	
15-23	3618-3625	options	
15-24	3626-3630	must	
15-25	3631-3634	all	
15-26	3635-3637	be	
15-27	3638-3647	specified	
15-28	3648-3650	if	
15-29	3651-3654	any	
15-30	3655-3657	of	
15-31	3658-3662	them	
15-32	3663-3665	is	
15-33	3666-3675	specified	
15-34	3675-3676	.	
15-35	3677-3679	In	
15-36	3680-3688	addition	
15-37	3688-3689	,	

#Text=numPartitions must be specified. They describe how to partition the table when reading in parallel from multiple workers. partitionColumn must be a numeric, date, or timestamp column from the table in question. Notice that lowerBound and upperBound are just used to decide the
16-1	3690-3703	numPartitions	
16-2	3704-3708	must	
16-3	3709-3711	be	
16-4	3712-3721	specified	
16-5	3721-3722	.	
16-6	3723-3727	They	
16-7	3728-3736	describe	
16-8	3737-3740	how	
16-9	3741-3743	to	
16-10	3744-3753	partition	
16-11	3754-3757	the	
16-12	3758-3763	table	
16-13	3764-3768	when	
16-14	3769-3776	reading	
16-15	3777-3779	in	
16-16	3780-3788	parallel	
16-17	3789-3793	from	
16-18	3794-3802	multiple	
16-19	3803-3810	workers	
16-20	3810-3811	.	
16-21	3812-3827	partitionColumn	
16-22	3828-3832	must	
16-23	3833-3835	be	
16-24	3836-3837	a	
16-25	3838-3845	numeric	
16-26	3845-3846	,	
16-27	3847-3851	date	
16-28	3851-3852	,	
16-29	3853-3855	or	
16-30	3856-3865	timestamp	
16-31	3866-3872	column	
16-32	3873-3877	from	
16-33	3878-3881	the	
16-34	3882-3887	table	
16-35	3888-3890	in	
16-36	3891-3899	question	
16-37	3899-3900	.	
16-38	3901-3907	Notice	
16-39	3908-3912	that	
16-40	3913-3923	lowerBound	
16-41	3924-3927	and	
16-42	3928-3938	upperBound	
16-43	3939-3942	are	
16-44	3943-3947	just	
16-45	3948-3952	used	
16-46	3953-3955	to	
16-47	3956-3962	decide	
16-48	3963-3966	the	

#Text=partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading. numPartitions The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections.
17-1	3967-3976	partition	
17-2	3977-3983	stride	
17-3	3983-3984	,	
17-4	3985-3988	not	
17-5	3989-3992	for	
17-6	3993-4002	filtering	
17-7	4003-4006	the	
17-8	4007-4011	rows	
17-9	4012-4014	in	
17-10	4015-4020	table	
17-11	4020-4021	.	
17-12	4022-4024	So	
17-13	4025-4028	all	
17-14	4029-4033	rows	
17-15	4034-4036	in	
17-16	4037-4040	the	
17-17	4041-4046	table	
17-18	4047-4051	will	
17-19	4052-4054	be	
17-20	4055-4066	partitioned	
17-21	4067-4070	and	
17-22	4071-4079	returned	
17-23	4079-4080	.	
17-24	4081-4085	This	
17-25	4086-4092	option	
17-26	4093-4100	applies	
17-27	4101-4105	only	
17-28	4106-4108	to	
17-29	4109-4116	reading	
17-30	4116-4117	.	
17-31	4118-4131	numPartitions	
17-32	4132-4135	The	
17-33	4136-4143	maximum	
17-34	4144-4150	number	
17-35	4151-4153	of	
17-36	4154-4164	partitions	
17-37	4165-4169	that	
17-38	4170-4173	can	
17-39	4174-4176	be	
17-40	4177-4181	used	
17-41	4182-4185	for	
17-42	4186-4197	parallelism	
17-43	4198-4200	in	
17-44	4201-4206	table	
17-45	4207-4214	reading	
17-46	4215-4218	and	
17-47	4219-4226	writing	
17-48	4226-4227	.	
17-49	4228-4232	This	
17-50	4233-4237	also	
17-51	4238-4248	determines	
17-52	4249-4252	the	
17-53	4253-4260	maximum	
17-54	4261-4267	number	
17-55	4268-4270	of	
17-56	4271-4281	concurrent	
17-57	4282-4286	JDBC	
17-58	4287-4298	connections	
17-59	4298-4299	.	

#Text=If the number of partitions to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before writing. queryTimeout The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on
18-1	4300-4302	If	
18-2	4303-4306	the	
18-3	4307-4313	number	
18-4	4314-4316	of	
18-5	4317-4327	partitions	
18-6	4328-4330	to	
18-7	4331-4336	write	
18-8	4337-4344	exceeds	
18-9	4345-4349	this	
18-10	4350-4355	limit	
18-11	4355-4356	,	
18-12	4357-4359	we	
18-13	4360-4368	decrease	
18-14	4369-4371	it	
18-15	4372-4374	to	
18-16	4375-4379	this	
18-17	4380-4385	limit	
18-18	4386-4388	by	
18-19	4389-4396	calling	
18-20	4397-4405	coalesce	
18-21	4405-4406	(	
18-22	4406-4419	numPartitions	
18-23	4419-4420	)	
18-24	4421-4427	before	
18-25	4428-4435	writing	
18-26	4435-4436	.	
18-27	4437-4449	queryTimeout	
18-28	4450-4453	The	
18-29	4454-4460	number	
18-30	4461-4463	of	
18-31	4464-4471	seconds	
18-32	4472-4475	the	
18-33	4476-4482	driver	
18-34	4483-4487	will	
18-35	4488-4492	wait	
18-36	4493-4496	for	
18-37	4497-4498	a	
18-38	4499-4508	Statement	
18-39	4509-4515	object	
18-40	4516-4518	to	
18-41	4519-4526	execute	
18-42	4527-4529	to	
18-43	4530-4533	the	
18-44	4534-4539	given	
18-45	4540-4546	number	
18-46	4547-4549	of	
18-47	4550-4557	seconds	
18-48	4557-4558	.	
18-49	4559-4563	Zero	
18-50	4564-4569	means	
18-51	4570-4575	there	
18-52	4576-4578	is	
18-53	4579-4581	no	
18-54	4582-4587	limit	
18-55	4587-4588	.	
18-56	4589-4591	In	
18-57	4592-4595	the	
18-58	4596-4601	write	
18-59	4602-4606	path	
18-60	4606-4607	,	
18-61	4608-4612	this	
18-62	4613-4619	option	
18-63	4620-4627	depends	
18-64	4628-4630	on	

#Text=how JDBC drivers implement the API setQueryTimeout, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to 0. fetchsize The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.
19-1	4631-4634	how	
19-2	4635-4639	JDBC	
19-3	4640-4647	drivers	
19-4	4648-4657	implement	
19-5	4658-4661	the	
19-6	4662-4665	API	
19-7	4666-4681	setQueryTimeout	
19-8	4681-4682	,	
19-9	4683-4686	e.g	
19-10	4686-4687	.	
19-11	4687-4688	,	
19-12	4689-4692	the	
19-13	4693-4695	h2	
19-14	4696-4700	JDBC	
19-15	4701-4707	driver	
19-16	4708-4714	checks	
19-17	4715-4718	the	
19-18	4719-4726	timeout	
19-19	4727-4729	of	
19-20	4730-4734	each	
19-21	4735-4740	query	
19-22	4741-4748	instead	
19-23	4749-4751	of	
19-24	4752-4754	an	
19-25	4755-4761	entire	
19-26	4762-4766	JDBC	
19-27	4767-4772	batch	
19-28	4772-4773	.	
19-29	4774-4776	It	
19-30	4777-4785	defaults	
19-31	4786-4788	to	
19-32	4789-4790	0	
19-33	4790-4791	.	
19-34	4792-4801	fetchsize	
19-35	4802-4805	The	
19-36	4806-4810	JDBC	
19-37	4811-4816	fetch	
19-38	4817-4821	size	
19-39	4821-4822	,	
19-40	4823-4828	which	
19-41	4829-4839	determines	
19-42	4840-4843	how	
19-43	4844-4848	many	
19-44	4849-4853	rows	
19-45	4854-4856	to	
19-46	4857-4862	fetch	
19-47	4863-4866	per	
19-48	4867-4872	round	
19-49	4873-4877	trip	
19-50	4877-4878	.	
19-51	4879-4883	This	
19-52	4884-4887	can	
19-53	4888-4892	help	
19-54	4893-4904	performance	
19-55	4905-4907	on	
19-56	4908-4912	JDBC	
19-57	4913-4920	drivers	
19-58	4921-4926	which	
19-59	4927-4934	default	
19-60	4935-4937	to	
19-61	4938-4941	low	
19-62	4942-4947	fetch	
19-63	4948-4952	size	
19-64	4953-4954	(	
19-65	4954-4956	eg	
19-66	4956-4957	.	
19-67	4958-4964	Oracle	
19-68	4965-4969	with	
19-69	4970-4972	10	
19-70	4973-4977	rows	
19-71	4977-4978	)	
19-72	4978-4979	.	
19-73	4980-4984	This	
19-74	4985-4991	option	
19-75	4992-4999	applies	
19-76	5000-5004	only	
19-77	5005-5007	to	
19-78	5008-5015	reading	
19-79	5015-5016	.	

#Text=batchsize The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to 1000. isolationLevel
20-1	5017-5026	batchsize	
20-2	5027-5030	The	
20-3	5031-5035	JDBC	
20-4	5036-5041	batch	
20-5	5042-5046	size	
20-6	5046-5047	,	
20-7	5048-5053	which	
20-8	5054-5064	determines	
20-9	5065-5068	how	
20-10	5069-5073	many	
20-11	5074-5078	rows	
20-12	5079-5081	to	
20-13	5082-5088	insert	
20-14	5089-5092	per	
20-15	5093-5098	round	
20-16	5099-5103	trip	
20-17	5103-5104	.	
20-18	5105-5109	This	
20-19	5110-5113	can	
20-20	5114-5118	help	
20-21	5119-5130	performance	
20-22	5131-5133	on	
20-23	5134-5138	JDBC	
20-24	5139-5146	drivers	
20-25	5146-5147	.	
20-26	5148-5152	This	
20-27	5153-5159	option	
20-28	5160-5167	applies	
20-29	5168-5172	only	
20-30	5173-5175	to	
20-31	5176-5183	writing	
20-32	5183-5184	.	
20-33	5185-5187	It	
20-34	5188-5196	defaults	
20-35	5197-5199	to	
20-36	5200-5204	1000	
20-37	5204-5205	.	
20-38	5206-5220	isolationLevel	

#Text=The transaction isolation level, which applies to current connection. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. This option applies only to writing. Please refer the documentation in java.sql.Connection. sessionInitStatement After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL/SQL block). Use this to implement session initialization code. Example: option("sessionInitStatement", """BEGIN execute immediate 'alter session set "_serial_direct_read"=true'; END;""")
21-1	5221-5224	The	
21-2	5225-5236	transaction	
21-3	5237-5246	isolation	
21-4	5247-5252	level	
21-5	5252-5253	,	
21-6	5254-5259	which	
21-7	5260-5267	applies	
21-8	5268-5270	to	
21-9	5271-5278	current	
21-10	5279-5289	connection	
21-11	5289-5290	.	
21-12	5291-5293	It	
21-13	5294-5297	can	
21-14	5298-5300	be	
21-15	5301-5304	one	
21-16	5305-5307	of	
21-17	5308-5312	NONE	
21-18	5312-5313	,	
21-19	5314-5328	READ_COMMITTED	
21-20	5328-5329	,	
21-21	5330-5346	READ_UNCOMMITTED	
21-22	5346-5347	,	
21-23	5348-5363	REPEATABLE_READ	
21-24	5363-5364	,	
21-25	5365-5367	or	
21-26	5368-5380	SERIALIZABLE	
21-27	5380-5381	,	
21-28	5382-5395	corresponding	
21-29	5396-5398	to	
21-30	5399-5407	standard	
21-31	5408-5419	transaction	
21-32	5420-5429	isolation	
21-33	5430-5436	levels	
21-34	5437-5444	defined	
21-35	5445-5447	by	
21-36	5448-5454	JDBC's	
21-37	5455-5465	Connection	
21-38	5466-5472	object	
21-39	5472-5473	,	
21-40	5474-5478	with	
21-41	5479-5486	default	
21-42	5487-5489	of	
21-43	5490-5506	READ_UNCOMMITTED	
21-44	5506-5507	.	
21-45	5508-5512	This	
21-46	5513-5519	option	
21-47	5520-5527	applies	
21-48	5528-5532	only	
21-49	5533-5535	to	
21-50	5536-5543	writing	
21-51	5543-5544	.	
21-52	5545-5551	Please	
21-53	5552-5557	refer	
21-54	5558-5561	the	
21-55	5562-5575	documentation	
21-56	5576-5578	in	
21-57	5579-5598	java.sql.Connection	
21-58	5598-5599	.	
21-59	5600-5620	sessionInitStatement	
21-60	5621-5626	After	
21-61	5627-5631	each	
21-62	5632-5640	database	
21-63	5641-5648	session	
21-64	5649-5651	is	
21-65	5652-5658	opened	
21-66	5659-5661	to	
21-67	5662-5665	the	
21-68	5666-5672	remote	
21-69	5673-5675	DB	
21-70	5676-5679	and	
21-71	5680-5686	before	
21-72	5687-5695	starting	
21-73	5696-5698	to	
21-74	5699-5703	read	
21-75	5704-5708	data	
21-76	5708-5709	,	
21-77	5710-5714	this	
21-78	5715-5721	option	
21-79	5722-5730	executes	
21-80	5731-5732	a	
21-81	5733-5739	custom	
21-82	5740-5743	SQL	
21-83	5744-5753	statement	
21-84	5754-5755	(	
21-85	5755-5757	or	
21-86	5758-5759	a	
21-87	5760-5762	PL	
21-88	5762-5763	/	
21-89	5763-5766	SQL	
21-90	5767-5772	block	
21-91	5772-5773	)	
21-92	5773-5774	.	
21-93	5775-5778	Use	
21-94	5779-5783	this	
21-95	5784-5786	to	
21-96	5787-5796	implement	
21-97	5797-5804	session	
21-98	5805-5819	initialization	
21-99	5820-5824	code	
21-100	5824-5825	.	
21-101	5826-5833	Example	
21-102	5833-5834	:	
21-103	5835-5841	option	
21-104	5841-5842	(	
21-105	5842-5843	"	
21-106	5843-5863	sessionInitStatement	
21-107	5863-5864	"	
21-108	5864-5865	,	
21-109	5866-5867	"	
21-110	5867-5868	"	
21-111	5868-5869	"	
21-112	5869-5874	BEGIN	
21-113	5875-5882	execute	
21-114	5883-5892	immediate	
21-115	5893-5894	'	
21-116	5894-5899	alter	
21-117	5900-5907	session	
21-118	5908-5911	set	
21-119	5912-5913	"	
21-120	5913-5914	_	
21-121	5914-5932	serial_direct_read	
21-122	5932-5933	"	
21-123	5933-5934	=	
21-124	5934-5938	true	
21-125	5938-5939	'	
21-126	5939-5940	;	
21-127	5941-5944	END	
21-128	5944-5945	;	
21-129	5945-5946	"	
21-130	5946-5947	"	
21-131	5947-5948	"	
21-132	5948-5949	)	

#Text=truncate This is a JDBC writer related option. When SaveMode.Overwrite is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to false. This option applies only to writing.
22-1	5950-5958	truncate	
22-2	5959-5963	This	
22-3	5964-5966	is	
22-4	5967-5968	a	
22-5	5969-5973	JDBC	
22-6	5974-5980	writer	
22-7	5981-5988	related	
22-8	5989-5995	option	
22-9	5995-5996	.	
22-10	5997-6001	When	
22-11	6002-6020	SaveMode.Overwrite	
22-12	6021-6023	is	
22-13	6024-6031	enabled	
22-14	6031-6032	,	
22-15	6033-6037	this	
22-16	6038-6044	option	
22-17	6045-6051	causes	
22-18	6052-6057	Spark	
22-19	6058-6060	to	
22-20	6061-6069	truncate	
22-21	6070-6072	an	
22-22	6073-6081	existing	
22-23	6082-6087	table	
22-24	6088-6095	instead	
22-25	6096-6098	of	
22-26	6099-6107	dropping	
22-27	6108-6111	and	
22-28	6112-6122	recreating	
22-29	6123-6125	it	
22-30	6125-6126	.	
22-31	6127-6131	This	
22-32	6132-6135	can	
22-33	6136-6138	be	
22-34	6139-6143	more	
22-35	6144-6153	efficient	
22-36	6153-6154	,	
22-37	6155-6158	and	
22-38	6159-6167	prevents	
22-39	6168-6171	the	
22-40	6172-6177	table	
22-41	6178-6186	metadata	
22-42	6187-6188	(	
22-43	6188-6191	e.g	
22-44	6191-6192	.	
22-45	6192-6193	,	
22-46	6194-6201	indices	
22-47	6201-6202	)	
22-48	6203-6207	from	
22-49	6208-6213	being	
22-50	6214-6221	removed	
22-51	6221-6222	.	
22-52	6223-6230	However	
22-53	6230-6231	,	
22-54	6232-6234	it	
22-55	6235-6239	will	
22-56	6240-6243	not	
22-57	6244-6248	work	
22-58	6249-6251	in	
22-59	6252-6256	some	
22-60	6257-6262	cases	
22-61	6262-6263	,	
22-62	6264-6268	such	
22-63	6269-6271	as	
22-64	6272-6276	when	
22-65	6277-6280	the	
22-66	6281-6284	new	
22-67	6285-6289	data	
22-68	6290-6293	has	
22-69	6294-6295	a	
22-70	6296-6305	different	
22-71	6306-6312	schema	
22-72	6312-6313	.	
22-73	6314-6316	It	
22-74	6317-6325	defaults	
22-75	6326-6328	to	
22-76	6329-6334	false	
22-77	6334-6335	.	
22-78	6336-6340	This	
22-79	6341-6347	option	
22-80	6348-6355	applies	
22-81	6356-6360	only	
22-82	6361-6363	to	
22-83	6364-6371	writing	
22-84	6371-6372	.	

#Text=cascadeTruncate
23-1	6373-6388	cascadeTruncate	

#Text=This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a TRUNCATE TABLE t CASCADE (in the case of PostgreSQL a TRUNCATE TABLE ONLY t CASCADE is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care. This option applies only to writing. It defaults to the default cascading truncate behaviour of the JDBC database in question, specified in the isCascadeTruncate in each JDBCDialect. createTableOptions This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., CREATE TABLE t (name string) ENGINE=InnoDB.). This option applies only to writing.
24-1	6389-6393	This	
24-2	6394-6396	is	
24-3	6397-6398	a	
24-4	6399-6403	JDBC	
24-5	6404-6410	writer	
24-6	6411-6418	related	
24-7	6419-6425	option	
24-8	6425-6426	.	
24-9	6427-6429	If	
24-10	6430-6437	enabled	
24-11	6438-6441	and	
24-12	6442-6451	supported	
24-13	6452-6454	by	
24-14	6455-6458	the	
24-15	6459-6463	JDBC	
24-16	6464-6472	database	
24-17	6473-6474	(	
24-18	6474-6484	PostgreSQL	
24-19	6485-6488	and	
24-20	6489-6495	Oracle	
24-21	6496-6498	at	
24-22	6499-6502	the	
24-23	6503-6509	moment	
24-24	6509-6510	)	
24-25	6510-6511	,	
24-26	6512-6516	this	
24-27	6517-6524	options	
24-28	6525-6531	allows	
24-29	6532-6541	execution	
24-30	6542-6544	of	
24-31	6545-6546	a	
24-32	6547-6555	TRUNCATE	
24-33	6556-6561	TABLE	
24-34	6562-6563	t	
24-35	6564-6571	CASCADE	
24-36	6572-6573	(	
24-37	6573-6575	in	
24-38	6576-6579	the	
24-39	6580-6584	case	
24-40	6585-6587	of	
24-41	6588-6598	PostgreSQL	
24-42	6599-6600	a	
24-43	6601-6609	TRUNCATE	
24-44	6610-6615	TABLE	
24-45	6616-6620	ONLY	
24-46	6621-6622	t	
24-47	6623-6630	CASCADE	
24-48	6631-6633	is	
24-49	6634-6642	executed	
24-50	6643-6645	to	
24-51	6646-6653	prevent	
24-52	6654-6667	inadvertently	
24-53	6668-6678	truncating	
24-54	6679-6689	descendant	
24-55	6690-6696	tables	
24-56	6696-6697	)	
24-57	6697-6698	.	
24-58	6699-6703	This	
24-59	6704-6708	will	
24-60	6709-6715	affect	
24-61	6716-6721	other	
24-62	6722-6728	tables	
24-63	6728-6729	,	
24-64	6730-6733	and	
24-65	6734-6738	thus	
24-66	6739-6745	should	
24-67	6746-6748	be	
24-68	6749-6753	used	
24-69	6754-6758	with	
24-70	6759-6763	care	
24-71	6763-6764	.	
24-72	6765-6769	This	
24-73	6770-6776	option	
24-74	6777-6784	applies	
24-75	6785-6789	only	
24-76	6790-6792	to	
24-77	6793-6800	writing	
24-78	6800-6801	.	
24-79	6802-6804	It	
24-80	6805-6813	defaults	
24-81	6814-6816	to	
24-82	6817-6820	the	
24-83	6821-6828	default	
24-84	6829-6838	cascading	
24-85	6839-6847	truncate	
24-86	6848-6857	behaviour	
24-87	6858-6860	of	
24-88	6861-6864	the	
24-89	6865-6869	JDBC	
24-90	6870-6878	database	
24-91	6879-6881	in	
24-92	6882-6890	question	
24-93	6890-6891	,	
24-94	6892-6901	specified	
24-95	6902-6904	in	
24-96	6905-6908	the	
24-97	6909-6926	isCascadeTruncate	
24-98	6927-6929	in	
24-99	6930-6934	each	
24-100	6935-6946	JDBCDialect	
24-101	6946-6947	.	
24-102	6948-6966	createTableOptions	
24-103	6967-6971	This	
24-104	6972-6974	is	
24-105	6975-6976	a	
24-106	6977-6981	JDBC	
24-107	6982-6988	writer	
24-108	6989-6996	related	
24-109	6997-7003	option	
24-110	7003-7004	.	
24-111	7005-7007	If	
24-112	7008-7017	specified	
24-113	7017-7018	,	
24-114	7019-7023	this	
24-115	7024-7030	option	
24-116	7031-7037	allows	
24-117	7038-7045	setting	
24-118	7046-7048	of	
24-119	7049-7066	database-specific	
24-120	7067-7072	table	
24-121	7073-7076	and	
24-122	7077-7086	partition	
24-123	7087-7094	options	
24-124	7095-7099	when	
24-125	7100-7108	creating	
24-126	7109-7110	a	
24-127	7111-7116	table	
24-128	7117-7118	(	
24-129	7118-7121	e.g	
24-130	7121-7122	.	
24-131	7122-7123	,	
24-132	7124-7130	CREATE	
24-133	7131-7136	TABLE	
24-134	7137-7138	t	
24-135	7139-7140	(	
24-136	7140-7144	name	
24-137	7145-7151	string	
24-138	7151-7152	)	
24-139	7153-7159	ENGINE	
24-140	7159-7160	=	
24-141	7160-7166	InnoDB	
24-142	7166-7167	.	
24-143	7167-7168	)	
24-144	7168-7169	.	
24-145	7170-7174	This	
24-146	7175-7181	option	
24-147	7182-7189	applies	
24-148	7190-7194	only	
24-149	7195-7197	to	
24-150	7198-7205	writing	
24-151	7205-7206	.	

#Text=createTableColumnTypes The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: "name CHAR(64), comments VARCHAR(1024)"). The specified types should be valid spark sql data types. This option applies only to writing.
25-1	7207-7229	createTableColumnTypes	
25-2	7230-7233	The	
25-3	7234-7242	database	
25-4	7243-7249	column	
25-5	7250-7254	data	
25-6	7255-7260	types	
25-7	7261-7263	to	
25-8	7264-7267	use	
25-9	7268-7275	instead	
25-10	7276-7278	of	
25-11	7279-7282	the	
25-12	7283-7291	defaults	
25-13	7291-7292	,	
25-14	7293-7297	when	
25-15	7298-7306	creating	
25-16	7307-7310	the	
25-17	7311-7316	table	
25-18	7316-7317	.	
25-19	7318-7322	Data	
25-20	7323-7327	type	
25-21	7328-7339	information	
25-22	7340-7346	should	
25-23	7347-7349	be	
25-24	7350-7359	specified	
25-25	7360-7362	in	
25-26	7363-7366	the	
25-27	7367-7371	same	
25-28	7372-7378	format	
25-29	7379-7381	as	
25-30	7382-7388	CREATE	
25-31	7389-7394	TABLE	
25-32	7395-7402	columns	
25-33	7403-7409	syntax	
25-34	7410-7411	(	
25-35	7411-7414	e.g	
25-36	7414-7415	:	
25-37	7416-7417	"	
25-38	7417-7421	name	
25-39	7422-7426	CHAR	
25-40	7426-7427	(	
25-41	7427-7429	64	
25-42	7429-7430	)	
25-43	7430-7431	,	
25-44	7432-7440	comments	
25-45	7441-7448	VARCHAR	
25-46	7448-7449	(	
25-47	7449-7453	1024	
25-48	7453-7454	)	
25-49	7454-7455	"	
25-50	7455-7456	)	
25-51	7456-7457	.	
25-52	7458-7461	The	
25-53	7462-7471	specified	
25-54	7472-7477	types	
25-55	7478-7484	should	
25-56	7485-7487	be	
25-57	7488-7493	valid	
25-58	7494-7499	spark	
25-59	7500-7503	sql	
25-60	7504-7508	data	
25-61	7509-7514	types	
25-62	7514-7515	.	
25-63	7516-7520	This	
25-64	7521-7527	option	
25-65	7528-7535	applies	
25-66	7536-7540	only	
25-67	7541-7543	to	
25-68	7544-7551	writing	
25-69	7551-7552	.	

#Text=customSchema The custom schema to use for reading data from JDBC connectors. For example, "id DECIMAL(38, 0), name STRING". You can also specify partial fields, and the others use the default type mapping. For example, "id DECIMAL(38, 0)". The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. This option applies only to reading.
26-1	7553-7565	customSchema	
26-2	7566-7569	The	
26-3	7570-7576	custom	
26-4	7577-7583	schema	
26-5	7584-7586	to	
26-6	7587-7590	use	
26-7	7591-7594	for	
26-8	7595-7602	reading	
26-9	7603-7607	data	
26-10	7608-7612	from	
26-11	7613-7617	JDBC	
26-12	7618-7628	connectors	
26-13	7628-7629	.	
26-14	7630-7633	For	
26-15	7634-7641	example	
26-16	7641-7642	,	
26-17	7643-7644	"	
26-18	7644-7646	id	
26-19	7647-7654	DECIMAL	
26-20	7654-7655	(	
26-21	7655-7657	38	
26-22	7657-7658	,	
26-23	7659-7660	0	
26-24	7660-7661	)	
26-25	7661-7662	,	
26-26	7663-7667	name	
26-27	7668-7674	STRING	
26-28	7674-7675	"	
26-29	7675-7676	.	
26-30	7677-7680	You	
26-31	7681-7684	can	
26-32	7685-7689	also	
26-33	7690-7697	specify	
26-34	7698-7705	partial	
26-35	7706-7712	fields	
26-36	7712-7713	,	
26-37	7714-7717	and	
26-38	7718-7721	the	
26-39	7722-7728	others	
26-40	7729-7732	use	
26-41	7733-7736	the	
26-42	7737-7744	default	
26-43	7745-7749	type	
26-44	7750-7757	mapping	
26-45	7757-7758	.	
26-46	7759-7762	For	
26-47	7763-7770	example	
26-48	7770-7771	,	
26-49	7772-7773	"	
26-50	7773-7775	id	
26-51	7776-7783	DECIMAL	
26-52	7783-7784	(	
26-53	7784-7786	38	
26-54	7786-7787	,	
26-55	7788-7789	0	
26-56	7789-7790	)	
26-57	7790-7791	"	
26-58	7791-7792	.	
26-59	7793-7796	The	
26-60	7797-7803	column	
26-61	7804-7809	names	
26-62	7810-7816	should	
26-63	7817-7819	be	
26-64	7820-7829	identical	
26-65	7830-7832	to	
26-66	7833-7836	the	
26-67	7837-7850	corresponding	
26-68	7851-7857	column	
26-69	7858-7863	names	
26-70	7864-7866	of	
26-71	7867-7871	JDBC	
26-72	7872-7877	table	
26-73	7877-7878	.	
26-74	7879-7884	Users	
26-75	7885-7888	can	
26-76	7889-7896	specify	
26-77	7897-7900	the	
26-78	7901-7914	corresponding	
26-79	7915-7919	data	
26-80	7920-7925	types	
26-81	7926-7928	of	
26-82	7929-7934	Spark	
26-83	7935-7938	SQL	
26-84	7939-7946	instead	
26-85	7947-7949	of	
26-86	7950-7955	using	
26-87	7956-7959	the	
26-88	7960-7968	defaults	
26-89	7968-7969	.	
26-90	7970-7974	This	
26-91	7975-7981	option	
26-92	7982-7989	applies	
26-93	7990-7994	only	
26-94	7995-7997	to	
26-95	7998-8005	reading	
26-96	8005-8006	.	

#Text=pushDownPredicate The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.
27-1	8007-8024	pushDownPredicate	
27-2	8025-8028	The	
27-3	8029-8035	option	
27-4	8036-8038	to	
27-5	8039-8045	enable	
27-6	8046-8048	or	
27-7	8049-8056	disable	
27-8	8057-8066	predicate	
27-9	8067-8076	push-down	
27-10	8077-8081	into	
27-11	8082-8085	the	
27-12	8086-8090	JDBC	
27-13	8091-8095	data	
27-14	8096-8102	source	
27-15	8102-8103	.	
27-16	8104-8107	The	
27-17	8108-8115	default	
27-18	8116-8121	value	
27-19	8122-8124	is	
27-20	8125-8129	true	
27-21	8129-8130	,	
27-22	8131-8133	in	
27-23	8134-8139	which	
27-24	8140-8144	case	
27-25	8145-8150	Spark	
27-26	8151-8155	will	
27-27	8156-8160	push	
27-28	8161-8165	down	
27-29	8166-8173	filters	
27-30	8174-8176	to	
27-31	8177-8180	the	
27-32	8181-8185	JDBC	
27-33	8186-8190	data	
27-34	8191-8197	source	
27-35	8198-8200	as	
27-36	8201-8205	much	
27-37	8206-8208	as	
27-38	8209-8217	possible	
27-39	8217-8218	.	
27-40	8219-8228	Otherwise	
27-41	8228-8229	,	
27-42	8230-8232	if	
27-43	8233-8236	set	
27-44	8237-8239	to	
27-45	8240-8245	false	
27-46	8245-8246	,	
27-47	8247-8249	no	
27-48	8250-8256	filter	
27-49	8257-8261	will	
27-50	8262-8264	be	
27-51	8265-8271	pushed	
27-52	8272-8276	down	
27-53	8277-8279	to	
27-54	8280-8283	the	
27-55	8284-8288	JDBC	
27-56	8289-8293	data	
27-57	8294-8300	source	
27-58	8301-8304	and	
27-59	8305-8309	thus	
27-60	8310-8313	all	
27-61	8314-8321	filters	
27-62	8322-8326	will	
27-63	8327-8329	be	
27-64	8330-8337	handled	
27-65	8338-8340	by	
27-66	8341-8346	Spark	
27-67	8346-8347	.	
27-68	8348-8357	Predicate	
27-69	8358-8367	push-down	
27-70	8368-8370	is	
27-71	8371-8378	usually	
27-72	8379-8385	turned	
27-73	8386-8389	off	
27-74	8390-8394	when	
27-75	8395-8398	the	
27-76	8399-8408	predicate	
27-77	8409-8418	filtering	
27-78	8419-8421	is	
27-79	8422-8431	performed	
27-80	8432-8438	faster	
27-81	8439-8441	by	
27-82	8442-8447	Spark	
27-83	8448-8452	than	
27-84	8453-8455	by	
27-85	8456-8459	the	
27-86	8460-8464	JDBC	
27-87	8465-8469	data	
27-88	8470-8476	source	
27-89	8476-8477	.	

#Text=// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods // Loading data from a JDBC source val jdbcDF = spark.read .format("jdbc") .option("url", "jdbc:postgresql:dbserver")
28-1	8478-8479	/	
28-2	8479-8480	/	
28-3	8481-8485	Note	
28-4	8485-8486	:	
28-5	8487-8491	JDBC	
28-6	8492-8499	loading	
28-7	8500-8503	and	
28-8	8504-8510	saving	
28-9	8511-8514	can	
28-10	8515-8517	be	
28-11	8518-8526	achieved	
28-12	8527-8530	via	
28-13	8531-8537	either	
28-14	8538-8541	the	
28-15	8542-8546	load	
28-16	8546-8547	/	
28-17	8547-8551	save	
28-18	8552-8554	or	
28-19	8555-8559	jdbc	
28-20	8560-8567	methods	
28-21	8568-8569	/	
28-22	8569-8570	/	
28-23	8571-8578	Loading	
28-24	8579-8583	data	
28-25	8584-8588	from	
28-26	8589-8590	a	
28-27	8591-8595	JDBC	
28-28	8596-8602	source	
28-29	8603-8606	val	
28-30	8607-8613	jdbcDF	
28-31	8614-8615	=	
28-32	8616-8626	spark.read	
28-33	8627-8628	.	
28-34	8628-8634	format	
28-35	8634-8635	(	
28-36	8635-8636	"	
28-37	8636-8640	jdbc	
28-38	8640-8641	"	
28-39	8641-8642	)	
28-40	8643-8644	.	
28-41	8644-8650	option	
28-42	8650-8651	(	
28-43	8651-8652	"	
28-44	8652-8655	url	
28-45	8655-8656	"	
28-46	8656-8657	,	
28-47	8658-8659	"	
28-48	8659-8663	jdbc	
28-49	8663-8664	:	
28-50	8664-8674	postgresql	
28-51	8674-8675	:	
28-52	8675-8683	dbserver	
28-53	8683-8684	"	
28-54	8684-8685	)	

#Text=.option("dbtable", "schema.tablename") .option("user", "username") .option("password", "password") .load() val connectionProperties = new Properties() connectionProperties.put("user", "username")
29-1	8686-8687	.	
29-2	8687-8693	option	
29-3	8693-8694	(	
29-4	8694-8695	"	
29-5	8695-8702	dbtable	
29-6	8702-8703	"	
29-7	8703-8704	,	
29-8	8705-8706	"	
29-9	8706-8722	schema.tablename	
29-10	8722-8723	"	
29-11	8723-8724	)	
29-12	8725-8726	.	
29-13	8726-8732	option	
29-14	8732-8733	(	
29-15	8733-8734	"	
29-16	8734-8738	user	
29-17	8738-8739	"	
29-18	8739-8740	,	
29-19	8741-8742	"	
29-20	8742-8750	username	
29-21	8750-8751	"	
29-22	8751-8752	)	
29-23	8753-8754	.	
29-24	8754-8760	option	
29-25	8760-8761	(	
29-26	8761-8762	"	
29-27	8762-8770	password	
29-28	8770-8771	"	
29-29	8771-8772	,	
29-30	8773-8774	"	
29-31	8774-8782	password	
29-32	8782-8783	"	
29-33	8783-8784	)	
29-34	8785-8786	.	
29-35	8786-8790	load	
29-36	8790-8791	(	
29-37	8791-8792	)	
29-38	8793-8796	val	
29-39	8797-8817	connectionProperties	
29-40	8818-8819	=	
29-41	8820-8823	new	
29-42	8824-8834	Properties	
29-43	8834-8835	(	
29-44	8835-8836	)	
29-45	8837-8861	connectionProperties.put	
29-46	8861-8862	(	
29-47	8862-8863	"	
29-48	8863-8867	user	
29-49	8867-8868	"	
29-50	8868-8869	,	
29-51	8870-8871	"	
29-52	8871-8879	username	
29-53	8879-8880	"	
29-54	8880-8881	)	

#Text=connectionProperties.put("password", "password") val jdbcDF2 = spark.read .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties) // Specifying the custom data types of the read schema
30-1	8882-8906	connectionProperties.put	
30-2	8906-8907	(	
30-3	8907-8908	"	
30-4	8908-8916	password	
30-5	8916-8917	"	
30-6	8917-8918	,	
30-7	8919-8920	"	
30-8	8920-8928	password	
30-9	8928-8929	"	
30-10	8929-8930	)	
30-11	8931-8934	val	
30-12	8935-8942	jdbcDF2	
30-13	8943-8944	=	
30-14	8945-8955	spark.read	
30-15	8956-8957	.	
30-16	8957-8961	jdbc	
30-17	8961-8962	(	
30-18	8962-8963	"	
30-19	8963-8967	jdbc	
30-20	8967-8968	:	
30-21	8968-8978	postgresql	
30-22	8978-8979	:	
30-23	8979-8987	dbserver	
30-24	8987-8988	"	
30-25	8988-8989	,	
30-26	8990-8991	"	
30-27	8991-9007	schema.tablename	
30-28	9007-9008	"	
30-29	9008-9009	,	
30-30	9010-9030	connectionProperties	
30-31	9030-9031	)	
30-32	9032-9033	/	
30-33	9033-9034	/	
30-34	9035-9045	Specifying	
30-35	9046-9049	the	
30-36	9050-9056	custom	
30-37	9057-9061	data	
30-38	9062-9067	types	
30-39	9068-9070	of	
30-40	9071-9074	the	
30-41	9075-9079	read	
30-42	9080-9086	schema	

#Text=connectionProperties.put("customSchema", "id DECIMAL(38, 0), name STRING") val jdbcDF3 = spark.read .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties) // Saving data to a JDBC source
31-1	9087-9111	connectionProperties.put	
31-2	9111-9112	(	
31-3	9112-9113	"	
31-4	9113-9125	customSchema	
31-5	9125-9126	"	
31-6	9126-9127	,	
31-7	9128-9129	"	
31-8	9129-9131	id	
31-9	9132-9139	DECIMAL	
31-10	9139-9140	(	
31-11	9140-9142	38	
31-12	9142-9143	,	
31-13	9144-9145	0	
31-14	9145-9146	)	
31-15	9146-9147	,	
31-16	9148-9152	name	
31-17	9153-9159	STRING	
31-18	9159-9160	"	
31-19	9160-9161	)	
31-20	9162-9165	val	
31-21	9166-9173	jdbcDF3	
31-22	9174-9175	=	
31-23	9176-9186	spark.read	
31-24	9187-9188	.	
31-25	9188-9192	jdbc	
31-26	9192-9193	(	
31-27	9193-9194	"	
31-28	9194-9198	jdbc	
31-29	9198-9199	:	
31-30	9199-9209	postgresql	
31-31	9209-9210	:	
31-32	9210-9218	dbserver	
31-33	9218-9219	"	
31-34	9219-9220	,	
31-35	9221-9222	"	
31-36	9222-9238	schema.tablename	
31-37	9238-9239	"	
31-38	9239-9240	,	
31-39	9241-9261	connectionProperties	
31-40	9261-9262	)	
31-41	9263-9264	/	
31-42	9264-9265	/	
31-43	9266-9272	Saving	
31-44	9273-9277	data	
31-45	9278-9280	to	
31-46	9281-9282	a	
31-47	9283-9287	JDBC	
31-48	9288-9294	source	

#Text=jdbcDF.write .format("jdbc") .option("url", "jdbc:postgresql:dbserver") .option("dbtable", "schema.tablename")
32-1	9295-9307	jdbcDF.write	
32-2	9308-9309	.	
32-3	9309-9315	format	
32-4	9315-9316	(	
32-5	9316-9317	"	
32-6	9317-9321	jdbc	
32-7	9321-9322	"	
32-8	9322-9323	)	
32-9	9324-9325	.	
32-10	9325-9331	option	
32-11	9331-9332	(	
32-12	9332-9333	"	
32-13	9333-9336	url	
32-14	9336-9337	"	
32-15	9337-9338	,	
32-16	9339-9340	"	
32-17	9340-9344	jdbc	
32-18	9344-9345	:	
32-19	9345-9355	postgresql	
32-20	9355-9356	:	
32-21	9356-9364	dbserver	
32-22	9364-9365	"	
32-23	9365-9366	)	
32-24	9367-9368	.	
32-25	9368-9374	option	
32-26	9374-9375	(	
32-27	9375-9376	"	
32-28	9376-9383	dbtable	
32-29	9383-9384	"	
32-30	9384-9385	,	
32-31	9386-9387	"	
32-32	9387-9403	schema.tablename	
32-33	9403-9404	"	
32-34	9404-9405	)	

#Text=.option("user", "username") .option("password", "password") .save() jdbcDF2.write .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
33-1	9406-9407	.	
33-2	9407-9413	option	
33-3	9413-9414	(	
33-4	9414-9415	"	
33-5	9415-9419	user	
33-6	9419-9420	"	
33-7	9420-9421	,	
33-8	9422-9423	"	
33-9	9423-9431	username	
33-10	9431-9432	"	
33-11	9432-9433	)	
33-12	9434-9435	.	
33-13	9435-9441	option	
33-14	9441-9442	(	
33-15	9442-9443	"	
33-16	9443-9451	password	
33-17	9451-9452	"	
33-18	9452-9453	,	
33-19	9454-9455	"	
33-20	9455-9463	password	
33-21	9463-9464	"	
33-22	9464-9465	)	
33-23	9466-9467	.	
33-24	9467-9471	save	
33-25	9471-9472	(	
33-26	9472-9473	)	
33-27	9474-9481	jdbcDF2	
33-28	9481-9482	.	
33-29	9482-9487	write	
33-30	9488-9489	.	
33-31	9489-9493	jdbc	
33-32	9493-9494	(	
33-33	9494-9495	"	
33-34	9495-9499	jdbc	
33-35	9499-9500	:	
33-36	9500-9510	postgresql	
33-37	9510-9511	:	
33-38	9511-9519	dbserver	
33-39	9519-9520	"	
33-40	9520-9521	,	
33-41	9522-9523	"	
33-42	9523-9539	schema.tablename	
33-43	9539-9540	"	
33-44	9540-9541	,	
33-45	9542-9562	connectionProperties	
33-46	9562-9563	)	

#Text=// Specifying create table column data types on write jdbcDF.write .option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
34-1	9564-9565	/	
34-2	9565-9566	/	
34-3	9567-9577	Specifying	
34-4	9578-9584	create	
34-5	9585-9590	table	
34-6	9591-9597	column	
34-7	9598-9602	data	
34-8	9603-9608	types	
34-9	9609-9611	on	
34-10	9612-9617	write	
34-11	9618-9630	jdbcDF.write	
34-12	9631-9632	.	
34-13	9632-9638	option	
34-14	9638-9639	(	
34-15	9639-9640	"	
34-16	9640-9662	createTableColumnTypes	
34-17	9662-9663	"	
34-18	9663-9664	,	
34-19	9665-9666	"	
34-20	9666-9670	name	
34-21	9671-9675	CHAR	
34-22	9675-9676	(	
34-23	9676-9678	64	
34-24	9678-9679	)	
34-25	9679-9680	,	
34-26	9681-9689	comments	
34-27	9690-9697	VARCHAR	
34-28	9697-9698	(	
34-29	9698-9702	1024	
34-30	9702-9703	)	
34-31	9703-9704	"	
34-32	9704-9705	)	

#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties) Find full example code at "examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala" in the Spark repo.
35-1	9706-9707	.	
35-2	9707-9711	jdbc	
35-3	9711-9712	(	
35-4	9712-9713	"	
35-5	9713-9717	jdbc	
35-6	9717-9718	:	
35-7	9718-9728	postgresql	
35-8	9728-9729	:	
35-9	9729-9737	dbserver	
35-10	9737-9738	"	
35-11	9738-9739	,	
35-12	9740-9741	"	
35-13	9741-9757	schema.tablename	
35-14	9757-9758	"	
35-15	9758-9759	,	
35-16	9760-9780	connectionProperties	
35-17	9780-9781	)	
35-18	9782-9786	Find	
35-19	9787-9791	full	
35-20	9792-9799	example	
35-21	9800-9804	code	
35-22	9805-9807	at	
35-23	9808-9809	"	
35-24	9809-9817	examples	
35-25	9817-9818	/	
35-26	9818-9821	src	
35-27	9821-9822	/	
35-28	9822-9826	main	
35-29	9826-9827	/	
35-30	9827-9832	scala	
35-31	9832-9833	/	
35-32	9833-9836	org	
35-33	9836-9837	/	
35-34	9837-9843	apache	
35-35	9843-9844	/	
35-36	9844-9849	spark	
35-37	9849-9850	/	
35-38	9850-9858	examples	
35-39	9858-9859	/	
35-40	9859-9862	sql	
35-41	9862-9863	/	
35-42	9863-9889	SQLDataSourceExample.scala	
35-43	9889-9890	"	
35-44	9891-9893	in	
35-45	9894-9897	the	
35-46	9898-9903	Spark	
35-47	9904-9908	repo	
35-48	9908-9909	.	

#Text=// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods // Loading data from a JDBC source Dataset<Row> jdbcDF = spark.read() .format("jdbc") .option("url", "jdbc:postgresql:dbserver")
36-1	9910-9911	/	
36-2	9911-9912	/	
36-3	9913-9917	Note	
36-4	9917-9918	:	
36-5	9919-9923	JDBC	
36-6	9924-9931	loading	
36-7	9932-9935	and	
36-8	9936-9942	saving	
36-9	9943-9946	can	
36-10	9947-9949	be	
36-11	9950-9958	achieved	
36-12	9959-9962	via	
36-13	9963-9969	either	
36-14	9970-9973	the	
36-15	9974-9978	load	
36-16	9978-9979	/	
36-17	9979-9983	save	
36-18	9984-9986	or	
36-19	9987-9991	jdbc	
36-20	9992-9999	methods	
36-21	10000-10001	/	
36-22	10001-10002	/	
36-23	10003-10010	Loading	
36-24	10011-10015	data	
36-25	10016-10020	from	
36-26	10021-10022	a	
36-27	10023-10027	JDBC	
36-28	10028-10034	source	
36-29	10035-10042	Dataset	
36-30	10042-10043	<	
36-31	10043-10046	Row	
36-32	10046-10047	>	
36-33	10048-10054	jdbcDF	
36-34	10055-10056	=	
36-35	10057-10067	spark.read	
36-36	10067-10068	(	
36-37	10068-10069	)	
36-38	10070-10071	.	
36-39	10071-10077	format	
36-40	10077-10078	(	
36-41	10078-10079	"	
36-42	10079-10083	jdbc	
36-43	10083-10084	"	
36-44	10084-10085	)	
36-45	10086-10087	.	
36-46	10087-10093	option	
36-47	10093-10094	(	
36-48	10094-10095	"	
36-49	10095-10098	url	
36-50	10098-10099	"	
36-51	10099-10100	,	
36-52	10101-10102	"	
36-53	10102-10106	jdbc	
36-54	10106-10107	:	
36-55	10107-10117	postgresql	
36-56	10117-10118	:	
36-57	10118-10126	dbserver	
36-58	10126-10127	"	
36-59	10127-10128	)	

#Text=.option("dbtable", "schema.tablename") .option("user", "username") .option("password", "password") .load(); Properties connectionProperties = new Properties(); connectionProperties.put("user", "username");
37-1	10129-10130	.	
37-2	10130-10136	option	
37-3	10136-10137	(	
37-4	10137-10138	"	
37-5	10138-10145	dbtable	
37-6	10145-10146	"	
37-7	10146-10147	,	
37-8	10148-10149	"	
37-9	10149-10165	schema.tablename	
37-10	10165-10166	"	
37-11	10166-10167	)	
37-12	10168-10169	.	
37-13	10169-10175	option	
37-14	10175-10176	(	
37-15	10176-10177	"	
37-16	10177-10181	user	
37-17	10181-10182	"	
37-18	10182-10183	,	
37-19	10184-10185	"	
37-20	10185-10193	username	
37-21	10193-10194	"	
37-22	10194-10195	)	
37-23	10196-10197	.	
37-24	10197-10203	option	
37-25	10203-10204	(	
37-26	10204-10205	"	
37-27	10205-10213	password	
37-28	10213-10214	"	
37-29	10214-10215	,	
37-30	10216-10217	"	
37-31	10217-10225	password	
37-32	10225-10226	"	
37-33	10226-10227	)	
37-34	10228-10229	.	
37-35	10229-10233	load	
37-36	10233-10234	(	
37-37	10234-10235	)	
37-38	10235-10236	;	
37-39	10237-10247	Properties	
37-40	10248-10268	connectionProperties	
37-41	10269-10270	=	
37-42	10271-10274	new	
37-43	10275-10285	Properties	
37-44	10285-10286	(	
37-45	10286-10287	)	
37-46	10287-10288	;	
37-47	10289-10313	connectionProperties.put	
37-48	10313-10314	(	
37-49	10314-10315	"	
37-50	10315-10319	user	
37-51	10319-10320	"	
37-52	10320-10321	,	
37-53	10322-10323	"	
37-54	10323-10331	username	
37-55	10331-10332	"	
37-56	10332-10333	)	
37-57	10333-10334	;	

#Text=connectionProperties.put("password", "password"); Dataset<Row> jdbcDF2 = spark.read() .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties);
38-1	10335-10359	connectionProperties.put	
38-2	10359-10360	(	
38-3	10360-10361	"	
38-4	10361-10369	password	
38-5	10369-10370	"	
38-6	10370-10371	,	
38-7	10372-10373	"	
38-8	10373-10381	password	
38-9	10381-10382	"	
38-10	10382-10383	)	
38-11	10383-10384	;	
38-12	10385-10392	Dataset	
38-13	10392-10393	<	
38-14	10393-10396	Row	
38-15	10396-10397	>	
38-16	10398-10405	jdbcDF2	
38-17	10406-10407	=	
38-18	10408-10418	spark.read	
38-19	10418-10419	(	
38-20	10419-10420	)	
38-21	10421-10422	.	
38-22	10422-10426	jdbc	
38-23	10426-10427	(	
38-24	10427-10428	"	
38-25	10428-10432	jdbc	
38-26	10432-10433	:	
38-27	10433-10443	postgresql	
38-28	10443-10444	:	
38-29	10444-10452	dbserver	
38-30	10452-10453	"	
38-31	10453-10454	,	
38-32	10455-10456	"	
38-33	10456-10472	schema.tablename	
38-34	10472-10473	"	
38-35	10473-10474	,	
38-36	10475-10495	connectionProperties	
38-37	10495-10496	)	
38-38	10496-10497	;	

#Text=// Saving data to a JDBC source jdbcDF.write() .format("jdbc") .option("url", "jdbc:postgresql:dbserver") .option("dbtable", "schema.tablename")
39-1	10498-10499	/	
39-2	10499-10500	/	
39-3	10501-10507	Saving	
39-4	10508-10512	data	
39-5	10513-10515	to	
39-6	10516-10517	a	
39-7	10518-10522	JDBC	
39-8	10523-10529	source	
39-9	10530-10542	jdbcDF.write	
39-10	10542-10543	(	
39-11	10543-10544	)	
39-12	10545-10546	.	
39-13	10546-10552	format	
39-14	10552-10553	(	
39-15	10553-10554	"	
39-16	10554-10558	jdbc	
39-17	10558-10559	"	
39-18	10559-10560	)	
39-19	10561-10562	.	
39-20	10562-10568	option	
39-21	10568-10569	(	
39-22	10569-10570	"	
39-23	10570-10573	url	
39-24	10573-10574	"	
39-25	10574-10575	,	
39-26	10576-10577	"	
39-27	10577-10581	jdbc	
39-28	10581-10582	:	
39-29	10582-10592	postgresql	
39-30	10592-10593	:	
39-31	10593-10601	dbserver	
39-32	10601-10602	"	
39-33	10602-10603	)	
39-34	10604-10605	.	
39-35	10605-10611	option	
39-36	10611-10612	(	
39-37	10612-10613	"	
39-38	10613-10620	dbtable	
39-39	10620-10621	"	
39-40	10621-10622	,	
39-41	10623-10624	"	
39-42	10624-10640	schema.tablename	
39-43	10640-10641	"	
39-44	10641-10642	)	

#Text=.option("user", "username") .option("password", "password") .save(); jdbcDF2.write() .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties);
40-1	10643-10644	.	
40-2	10644-10650	option	
40-3	10650-10651	(	
40-4	10651-10652	"	
40-5	10652-10656	user	
40-6	10656-10657	"	
40-7	10657-10658	,	
40-8	10659-10660	"	
40-9	10660-10668	username	
40-10	10668-10669	"	
40-11	10669-10670	)	
40-12	10671-10672	.	
40-13	10672-10678	option	
40-14	10678-10679	(	
40-15	10679-10680	"	
40-16	10680-10688	password	
40-17	10688-10689	"	
40-18	10689-10690	,	
40-19	10691-10692	"	
40-20	10692-10700	password	
40-21	10700-10701	"	
40-22	10701-10702	)	
40-23	10703-10704	.	
40-24	10704-10708	save	
40-25	10708-10709	(	
40-26	10709-10710	)	
40-27	10710-10711	;	
40-28	10712-10719	jdbcDF2	
40-29	10719-10720	.	
40-30	10720-10725	write	
40-31	10725-10726	(	
40-32	10726-10727	)	
40-33	10728-10729	.	
40-34	10729-10733	jdbc	
40-35	10733-10734	(	
40-36	10734-10735	"	
40-37	10735-10739	jdbc	
40-38	10739-10740	:	
40-39	10740-10750	postgresql	
40-40	10750-10751	:	
40-41	10751-10759	dbserver	
40-42	10759-10760	"	
40-43	10760-10761	,	
40-44	10762-10763	"	
40-45	10763-10779	schema.tablename	
40-46	10779-10780	"	
40-47	10780-10781	,	
40-48	10782-10802	connectionProperties	
40-49	10802-10803	)	
40-50	10803-10804	;	

#Text=// Specifying create table column data types on write jdbcDF.write() .option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)")
41-1	10805-10806	/	
41-2	10806-10807	/	
41-3	10808-10818	Specifying	
41-4	10819-10825	create	
41-5	10826-10831	table	
41-6	10832-10838	column	
41-7	10839-10843	data	
41-8	10844-10849	types	
41-9	10850-10852	on	
41-10	10853-10858	write	
41-11	10859-10871	jdbcDF.write	
41-12	10871-10872	(	
41-13	10872-10873	)	
41-14	10874-10875	.	
41-15	10875-10881	option	
41-16	10881-10882	(	
41-17	10882-10883	"	
41-18	10883-10905	createTableColumnTypes	
41-19	10905-10906	"	
41-20	10906-10907	,	
41-21	10908-10909	"	
41-22	10909-10913	name	
41-23	10914-10918	CHAR	
41-24	10918-10919	(	
41-25	10919-10921	64	
41-26	10921-10922	)	
41-27	10922-10923	,	
41-28	10924-10932	comments	
41-29	10933-10940	VARCHAR	
41-30	10940-10941	(	
41-31	10941-10945	1024	
41-32	10945-10946	)	
41-33	10946-10947	"	
41-34	10947-10948	)	

#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties); Find full example code at "examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java" in the Spark repo.
42-1	10949-10950	.	
42-2	10950-10954	jdbc	
42-3	10954-10955	(	
42-4	10955-10956	"	
42-5	10956-10960	jdbc	
42-6	10960-10961	:	
42-7	10961-10971	postgresql	
42-8	10971-10972	:	
42-9	10972-10980	dbserver	
42-10	10980-10981	"	
42-11	10981-10982	,	
42-12	10983-10984	"	
42-13	10984-11000	schema.tablename	
42-14	11000-11001	"	
42-15	11001-11002	,	
42-16	11003-11023	connectionProperties	
42-17	11023-11024	)	
42-18	11024-11025	;	
42-19	11026-11030	Find	
42-20	11031-11035	full	
42-21	11036-11043	example	
42-22	11044-11048	code	
42-23	11049-11051	at	
42-24	11052-11053	"	
42-25	11053-11061	examples	
42-26	11061-11062	/	
42-27	11062-11065	src	
42-28	11065-11066	/	
42-29	11066-11070	main	
42-30	11070-11071	/	
42-31	11071-11075	java	
42-32	11075-11076	/	
42-33	11076-11079	org	
42-34	11079-11080	/	
42-35	11080-11086	apache	
42-36	11086-11087	/	
42-37	11087-11092	spark	
42-38	11092-11093	/	
42-39	11093-11101	examples	
42-40	11101-11102	/	
42-41	11102-11105	sql	
42-42	11105-11106	/	
42-43	11106-11135	JavaSQLDataSourceExample.java	
42-44	11135-11136	"	
42-45	11137-11139	in	
42-46	11140-11143	the	
42-47	11144-11149	Spark	
42-48	11150-11154	repo	
42-49	11154-11155	.	

#Text=# Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods # Loading data from a JDBC source jdbcDF = spark.read \\ .format("jdbc") \\ .option("url", "jdbc:postgresql:dbserver") \\
43-1	11156-11157	#	
43-2	11158-11162	Note	
43-3	11162-11163	:	
43-4	11164-11168	JDBC	
43-5	11169-11176	loading	
43-6	11177-11180	and	
43-7	11181-11187	saving	
43-8	11188-11191	can	
43-9	11192-11194	be	
43-10	11195-11203	achieved	
43-11	11204-11207	via	
43-12	11208-11214	either	
43-13	11215-11218	the	
43-14	11219-11223	load	
43-15	11223-11224	/	
43-16	11224-11228	save	
43-17	11229-11231	or	
43-18	11232-11236	jdbc	
43-19	11237-11244	methods	
43-20	11245-11246	#	
43-21	11247-11254	Loading	
43-22	11255-11259	data	
43-23	11260-11264	from	
43-24	11265-11266	a	
43-25	11267-11271	JDBC	
43-26	11272-11278	source	
43-27	11279-11285	jdbcDF	
43-28	11286-11287	=	
43-29	11288-11298	spark.read	
43-30	11299-11300	\	
43-31	11301-11302	.	
43-32	11302-11308	format	
43-33	11308-11309	(	
43-34	11309-11310	"	
43-35	11310-11314	jdbc	
43-36	11314-11315	"	
43-37	11315-11316	)	
43-38	11317-11318	\	
43-39	11319-11320	.	
43-40	11320-11326	option	
43-41	11326-11327	(	
43-42	11327-11328	"	
43-43	11328-11331	url	
43-44	11331-11332	"	
43-45	11332-11333	,	
43-46	11334-11335	"	
43-47	11335-11339	jdbc	
43-48	11339-11340	:	
43-49	11340-11350	postgresql	
43-50	11350-11351	:	
43-51	11351-11359	dbserver	
43-52	11359-11360	"	
43-53	11360-11361	)	
43-54	11362-11363	\	

#Text=.option("dbtable", "schema.tablename") \\ .option("user", "username") \\ .option("password", "password") \\ .load() jdbcDF2 = spark.read \\
44-1	11364-11365	.	
44-2	11365-11371	option	
44-3	11371-11372	(	
44-4	11372-11373	"	
44-5	11373-11380	dbtable	
44-6	11380-11381	"	
44-7	11381-11382	,	
44-8	11383-11384	"	
44-9	11384-11400	schema.tablename	
44-10	11400-11401	"	
44-11	11401-11402	)	
44-12	11403-11404	\	
44-13	11405-11406	.	
44-14	11406-11412	option	
44-15	11412-11413	(	
44-16	11413-11414	"	
44-17	11414-11418	user	
44-18	11418-11419	"	
44-19	11419-11420	,	
44-20	11421-11422	"	
44-21	11422-11430	username	
44-22	11430-11431	"	
44-23	11431-11432	)	
44-24	11433-11434	\	
44-25	11435-11436	.	
44-26	11436-11442	option	
44-27	11442-11443	(	
44-28	11443-11444	"	
44-29	11444-11452	password	
44-30	11452-11453	"	
44-31	11453-11454	,	
44-32	11455-11456	"	
44-33	11456-11464	password	
44-34	11464-11465	"	
44-35	11465-11466	)	
44-36	11467-11468	\	
44-37	11469-11470	.	
44-38	11470-11474	load	
44-39	11474-11475	(	
44-40	11475-11476	)	
44-41	11477-11484	jdbcDF2	
44-42	11485-11486	=	
44-43	11487-11497	spark.read	
44-44	11498-11499	\	

#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", properties={"user": "username", "password": "password"}) # Specifying dataframe column data types on read jdbcDF3 = spark.read \\ .format("jdbc") \\
45-1	11500-11501	.	
45-2	11501-11505	jdbc	
45-3	11505-11506	(	
45-4	11506-11507	"	
45-5	11507-11511	jdbc	
45-6	11511-11512	:	
45-7	11512-11522	postgresql	
45-8	11522-11523	:	
45-9	11523-11531	dbserver	
45-10	11531-11532	"	
45-11	11532-11533	,	
45-12	11534-11535	"	
45-13	11535-11551	schema.tablename	
45-14	11551-11552	"	
45-15	11552-11553	,	
45-16	11554-11564	properties	
45-17	11564-11565	=	
45-18	11565-11566	{	
45-19	11566-11567	"	
45-20	11567-11571	user	
45-21	11571-11572	"	
45-22	11572-11573	:	
45-23	11574-11575	"	
45-24	11575-11583	username	
45-25	11583-11584	"	
45-26	11584-11585	,	
45-27	11586-11587	"	
45-28	11587-11595	password	
45-29	11595-11596	"	
45-30	11596-11597	:	
45-31	11598-11599	"	
45-32	11599-11607	password	
45-33	11607-11608	"	
45-34	11608-11609	}	
45-35	11609-11610	)	
45-36	11611-11612	#	
45-37	11613-11623	Specifying	
45-38	11624-11633	dataframe	
45-39	11634-11640	column	
45-40	11641-11645	data	
45-41	11646-11651	types	
45-42	11652-11654	on	
45-43	11655-11659	read	
45-44	11660-11667	jdbcDF3	
45-45	11668-11669	=	
45-46	11670-11680	spark.read	
45-47	11681-11682	\	
45-48	11683-11684	.	
45-49	11684-11690	format	
45-50	11690-11691	(	
45-51	11691-11692	"	
45-52	11692-11696	jdbc	
45-53	11696-11697	"	
45-54	11697-11698	)	
45-55	11699-11700	\	

#Text=.option("url", "jdbc:postgresql:dbserver") \\ .option("dbtable", "schema.tablename") \\ .option("user", "username") \\ .option("password", "password") \\
46-1	11701-11702	.	
46-2	11702-11708	option	
46-3	11708-11709	(	
46-4	11709-11710	"	
46-5	11710-11713	url	
46-6	11713-11714	"	
46-7	11714-11715	,	
46-8	11716-11717	"	
46-9	11717-11721	jdbc	
46-10	11721-11722	:	
46-11	11722-11732	postgresql	
46-12	11732-11733	:	
46-13	11733-11741	dbserver	
46-14	11741-11742	"	
46-15	11742-11743	)	
46-16	11744-11745	\	
46-17	11746-11747	.	
46-18	11747-11753	option	
46-19	11753-11754	(	
46-20	11754-11755	"	
46-21	11755-11762	dbtable	
46-22	11762-11763	"	
46-23	11763-11764	,	
46-24	11765-11766	"	
46-25	11766-11782	schema.tablename	
46-26	11782-11783	"	
46-27	11783-11784	)	
46-28	11785-11786	\	
46-29	11787-11788	.	
46-30	11788-11794	option	
46-31	11794-11795	(	
46-32	11795-11796	"	
46-33	11796-11800	user	
46-34	11800-11801	"	
46-35	11801-11802	,	
46-36	11803-11804	"	
46-37	11804-11812	username	
46-38	11812-11813	"	
46-39	11813-11814	)	
46-40	11815-11816	\	
46-41	11817-11818	.	
46-42	11818-11824	option	
46-43	11824-11825	(	
46-44	11825-11826	"	
46-45	11826-11834	password	
46-46	11834-11835	"	
46-47	11835-11836	,	
46-48	11837-11838	"	
46-49	11838-11846	password	
46-50	11846-11847	"	
46-51	11847-11848	)	
46-52	11849-11850	\	

#Text=.option("customSchema", "id DECIMAL(38, 0), name STRING") \\ .load() # Saving data to a JDBC source jdbcDF.write \\ .format("jdbc") \\ .option("url", "jdbc:postgresql:dbserver") \\
47-1	11851-11852	.	
47-2	11852-11858	option	
47-3	11858-11859	(	
47-4	11859-11860	"	
47-5	11860-11872	customSchema	
47-6	11872-11873	"	
47-7	11873-11874	,	
47-8	11875-11876	"	
47-9	11876-11878	id	
47-10	11879-11886	DECIMAL	
47-11	11886-11887	(	
47-12	11887-11889	38	
47-13	11889-11890	,	
47-14	11891-11892	0	
47-15	11892-11893	)	
47-16	11893-11894	,	
47-17	11895-11899	name	
47-18	11900-11906	STRING	
47-19	11906-11907	"	
47-20	11907-11908	)	
47-21	11909-11910	\	
47-22	11911-11912	.	
47-23	11912-11916	load	
47-24	11916-11917	(	
47-25	11917-11918	)	
47-26	11919-11920	#	
47-27	11921-11927	Saving	
47-28	11928-11932	data	
47-29	11933-11935	to	
47-30	11936-11937	a	
47-31	11938-11942	JDBC	
47-32	11943-11949	source	
47-33	11950-11962	jdbcDF.write	
47-34	11963-11964	\	
47-35	11965-11966	.	
47-36	11966-11972	format	
47-37	11972-11973	(	
47-38	11973-11974	"	
47-39	11974-11978	jdbc	
47-40	11978-11979	"	
47-41	11979-11980	)	
47-42	11981-11982	\	
47-43	11983-11984	.	
47-44	11984-11990	option	
47-45	11990-11991	(	
47-46	11991-11992	"	
47-47	11992-11995	url	
47-48	11995-11996	"	
47-49	11996-11997	,	
47-50	11998-11999	"	
47-51	11999-12003	jdbc	
47-52	12003-12004	:	
47-53	12004-12014	postgresql	
47-54	12014-12015	:	
47-55	12015-12023	dbserver	
47-56	12023-12024	"	
47-57	12024-12025	)	
47-58	12026-12027	\	

#Text=.option("dbtable", "schema.tablename") \\ .option("user", "username") \\ .option("password", "password") \\ .save() jdbcDF2.write \\ .jdbc("jdbc:postgresql:dbserver", "schema.tablename",
48-1	12028-12029	.	
48-2	12029-12035	option	
48-3	12035-12036	(	
48-4	12036-12037	"	
48-5	12037-12044	dbtable	
48-6	12044-12045	"	
48-7	12045-12046	,	
48-8	12047-12048	"	
48-9	12048-12064	schema.tablename	
48-10	12064-12065	"	
48-11	12065-12066	)	
48-12	12067-12068	\	
48-13	12069-12070	.	
48-14	12070-12076	option	
48-15	12076-12077	(	
48-16	12077-12078	"	
48-17	12078-12082	user	
48-18	12082-12083	"	
48-19	12083-12084	,	
48-20	12085-12086	"	
48-21	12086-12094	username	
48-22	12094-12095	"	
48-23	12095-12096	)	
48-24	12097-12098	\	
48-25	12099-12100	.	
48-26	12100-12106	option	
48-27	12106-12107	(	
48-28	12107-12108	"	
48-29	12108-12116	password	
48-30	12116-12117	"	
48-31	12117-12118	,	
48-32	12119-12120	"	
48-33	12120-12128	password	
48-34	12128-12129	"	
48-35	12129-12130	)	
48-36	12131-12132	\	
48-37	12133-12134	.	
48-38	12134-12138	save	
48-39	12138-12139	(	
48-40	12139-12140	)	
48-41	12141-12148	jdbcDF2	
48-42	12148-12149	.	
48-43	12149-12154	write	
48-44	12155-12156	\	
48-45	12157-12158	.	
48-46	12158-12162	jdbc	
48-47	12162-12163	(	
48-48	12163-12164	"	
48-49	12164-12168	jdbc	
48-50	12168-12169	:	
48-51	12169-12179	postgresql	
48-52	12179-12180	:	
48-53	12180-12188	dbserver	
48-54	12188-12189	"	
48-55	12189-12190	,	
48-56	12191-12192	"	
48-57	12192-12208	schema.tablename	
48-58	12208-12209	"	
48-59	12209-12210	,	

#Text=properties={"user": "username", "password": "password"}) # Specifying create table column data types on write jdbcDF.write \\ .option("createTableColumnTypes", "name CHAR(64), comments VARCHAR(1024)") \\
49-1	12211-12221	properties	
49-2	12221-12222	=	
49-3	12222-12223	{	
49-4	12223-12224	"	
49-5	12224-12228	user	
49-6	12228-12229	"	
49-7	12229-12230	:	
49-8	12231-12232	"	
49-9	12232-12240	username	
49-10	12240-12241	"	
49-11	12241-12242	,	
49-12	12243-12244	"	
49-13	12244-12252	password	
49-14	12252-12253	"	
49-15	12253-12254	:	
49-16	12255-12256	"	
49-17	12256-12264	password	
49-18	12264-12265	"	
49-19	12265-12266	}	
49-20	12266-12267	)	
49-21	12268-12269	#	
49-22	12270-12280	Specifying	
49-23	12281-12287	create	
49-24	12288-12293	table	
49-25	12294-12300	column	
49-26	12301-12305	data	
49-27	12306-12311	types	
49-28	12312-12314	on	
49-29	12315-12320	write	
49-30	12321-12333	jdbcDF.write	
49-31	12334-12335	\	
49-32	12336-12337	.	
49-33	12337-12343	option	
49-34	12343-12344	(	
49-35	12344-12345	"	
49-36	12345-12367	createTableColumnTypes	
49-37	12367-12368	"	
49-38	12368-12369	,	
49-39	12370-12371	"	
49-40	12371-12375	name	
49-41	12376-12380	CHAR	
49-42	12380-12381	(	
49-43	12381-12383	64	
49-44	12383-12384	)	
49-45	12384-12385	,	
49-46	12386-12394	comments	
49-47	12395-12402	VARCHAR	
49-48	12402-12403	(	
49-49	12403-12407	1024	
49-50	12407-12408	)	
49-51	12408-12409	"	
49-52	12409-12410	)	
49-53	12411-12412	\	

#Text=.jdbc("jdbc:postgresql:dbserver", "schema.tablename", properties={"user": "username", "password": "password"}) Find full example code at "examples/src/main/python/sql/datasource.py" in the Spark repo.
50-1	12413-12414	.	
50-2	12414-12418	jdbc	
50-3	12418-12419	(	
50-4	12419-12420	"	
50-5	12420-12424	jdbc	
50-6	12424-12425	:	
50-7	12425-12435	postgresql	
50-8	12435-12436	:	
50-9	12436-12444	dbserver	
50-10	12444-12445	"	
50-11	12445-12446	,	
50-12	12447-12448	"	
50-13	12448-12464	schema.tablename	
50-14	12464-12465	"	
50-15	12465-12466	,	
50-16	12467-12477	properties	
50-17	12477-12478	=	
50-18	12478-12479	{	
50-19	12479-12480	"	
50-20	12480-12484	user	
50-21	12484-12485	"	
50-22	12485-12486	:	
50-23	12487-12488	"	
50-24	12488-12496	username	
50-25	12496-12497	"	
50-26	12497-12498	,	
50-27	12499-12500	"	
50-28	12500-12508	password	
50-29	12508-12509	"	
50-30	12509-12510	:	
50-31	12511-12512	"	
50-32	12512-12520	password	
50-33	12520-12521	"	
50-34	12521-12522	}	
50-35	12522-12523	)	
50-36	12524-12528	Find	
50-37	12529-12533	full	
50-38	12534-12541	example	
50-39	12542-12546	code	
50-40	12547-12549	at	
50-41	12550-12551	"	
50-42	12551-12559	examples	
50-43	12559-12560	/	
50-44	12560-12563	src	
50-45	12563-12564	/	
50-46	12564-12568	main	
50-47	12568-12569	/	
50-48	12569-12575	python	
50-49	12575-12576	/	
50-50	12576-12579	sql	
50-51	12579-12580	/	
50-52	12580-12593	datasource.py	
50-53	12593-12594	"	
50-54	12595-12597	in	
50-55	12598-12601	the	
50-56	12602-12607	Spark	
50-57	12608-12612	repo	
50-58	12612-12613	.	

#Text=# Loading data from a JDBC source df <- read.jdbc("jdbc:postgresql:dbserver", "schema.tablename", user = "username", password = "password") # Saving data to a JDBC source
51-1	12614-12615	#	
51-2	12616-12623	Loading	
51-3	12624-12628	data	
51-4	12629-12633	from	
51-5	12634-12635	a	
51-6	12636-12640	JDBC	
51-7	12641-12647	source	
51-8	12648-12650	df	
51-9	12651-12652	<	
51-10	12652-12653	-	
51-11	12654-12663	read.jdbc	
51-12	12663-12664	(	
51-13	12664-12665	"	
51-14	12665-12669	jdbc	
51-15	12669-12670	:	
51-16	12670-12680	postgresql	
51-17	12680-12681	:	
51-18	12681-12689	dbserver	
51-19	12689-12690	"	
51-20	12690-12691	,	
51-21	12692-12693	"	
51-22	12693-12709	schema.tablename	
51-23	12709-12710	"	
51-24	12710-12711	,	
51-25	12712-12716	user	
51-26	12717-12718	=	
51-27	12719-12720	"	
51-28	12720-12728	username	
51-29	12728-12729	"	
51-30	12729-12730	,	
51-31	12731-12739	password	
51-32	12740-12741	=	
51-33	12742-12743	"	
51-34	12743-12751	password	
51-35	12751-12752	"	
51-36	12752-12753	)	
51-37	12754-12755	#	
51-38	12756-12762	Saving	
51-39	12763-12767	data	
51-40	12768-12770	to	
51-41	12771-12772	a	
51-42	12773-12777	JDBC	
51-43	12778-12784	source	

#Text=write.jdbc(df, "jdbc:postgresql:dbserver", "schema.tablename", user = "username", password = "password") Find full example code at "examples/src/main/r/RSparkSQLExample.R" in the Spark repo. CREATE TEMPORARY VIEW jdbcTable USING org.apache.spark.sql.jdbc
52-1	12785-12795	write.jdbc	
52-2	12795-12796	(	
52-3	12796-12798	df	
52-4	12798-12799	,	
52-5	12800-12801	"	
52-6	12801-12805	jdbc	
52-7	12805-12806	:	
52-8	12806-12816	postgresql	
52-9	12816-12817	:	
52-10	12817-12825	dbserver	
52-11	12825-12826	"	
52-12	12826-12827	,	
52-13	12828-12829	"	
52-14	12829-12845	schema.tablename	
52-15	12845-12846	"	
52-16	12846-12847	,	
52-17	12848-12852	user	
52-18	12853-12854	=	
52-19	12855-12856	"	
52-20	12856-12864	username	
52-21	12864-12865	"	
52-22	12865-12866	,	
52-23	12867-12875	password	
52-24	12876-12877	=	
52-25	12878-12879	"	
52-26	12879-12887	password	
52-27	12887-12888	"	
52-28	12888-12889	)	
52-29	12890-12894	Find	
52-30	12895-12899	full	
52-31	12900-12907	example	
52-32	12908-12912	code	
52-33	12913-12915	at	
52-34	12916-12917	"	
52-35	12917-12925	examples	
52-36	12925-12926	/	
52-37	12926-12929	src	
52-38	12929-12930	/	
52-39	12930-12934	main	
52-40	12934-12935	/	
52-41	12935-12936	r	
52-42	12936-12937	/	
52-43	12937-12955	RSparkSQLExample.R	
52-44	12955-12956	"	
52-45	12957-12959	in	
52-46	12960-12963	the	
52-47	12964-12969	Spark	
52-48	12970-12974	repo	
52-49	12974-12975	.	
52-50	12976-12982	CREATE	
52-51	12983-12992	TEMPORARY	
52-52	12993-12997	VIEW	
52-53	12998-13007	jdbcTable	
52-54	13008-13013	USING	
52-55	13014-13039	org.apache.spark.sql.jdbc	

#Text=OPTIONS ( url "jdbc:postgresql:dbserver", dbtable "schema.tablename", user 'username', password 'password' INSERT INTO TABLE jdbcTable
53-1	13040-13047	OPTIONS	
53-2	13048-13049	(	
53-3	13050-13053	url	
53-4	13054-13055	"	
53-5	13055-13059	jdbc	
53-6	13059-13060	:	
53-7	13060-13070	postgresql	
53-8	13070-13071	:	
53-9	13071-13079	dbserver	
53-10	13079-13080	"	
53-11	13080-13081	,	
53-12	13082-13089	dbtable	
53-13	13090-13091	"	
53-14	13091-13107	schema.tablename	
53-15	13107-13108	"	
53-16	13108-13109	,	
53-17	13110-13114	user	
53-18	13115-13116	'	
53-19	13116-13124	username	
53-20	13124-13125	'	
53-21	13125-13126	,	
53-22	13127-13135	password	
53-23	13136-13137	'	
53-24	13137-13145	password	
53-25	13145-13146	'	
53-26	13147-13153	INSERT	
53-27	13154-13158	INTO	
53-28	13159-13164	TABLE	
53-29	13165-13174	jdbcTable	
