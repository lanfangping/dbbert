#FORMAT=WebAnno TSV 3.3


#Text=Spark tips. DataFrame API - Blog | luminousmen Spark tips. DataFrame API Last updated Thu May 07 2020
1-1	0-5	Spark	
1-2	6-10	tips	
1-3	10-11	.	
1-4	12-21	DataFrame	
1-5	22-25	API	
1-6	26-27	-	
1-7	28-32	Blog	
1-8	33-34	|	
1-9	35-46	luminousmen	
1-10	47-52	Spark	
1-11	53-57	tips	
1-12	57-58	.	
1-13	59-68	DataFrame	
1-14	69-72	API	
1-15	73-77	Last	
1-16	78-85	updated	
1-17	86-89	Thu	
1-18	90-93	May	
1-19	94-96	07	
1-20	97-101	2020	

#Text=There are many different tools in the world, each of which solves a range of problems. Many of them are judged by how well and correct they solve this or that problem, but there are tools that you just like, you want to use them. They are properly designed and fit well in your hand, you do not need to dig into the documentation and understand how to do this or that simple action. About one of these tools for me I will be writing this series of posts. I will describe the optimization methods and tips that help me solve certain technical problems and achieve high efficiency using Apache Spark. This is my updated collection. Many of the optimizations that I will describe will not affect the JVM languages ​​so much, but without these methods, many Python applications may simply not work.
2-1	102-107	There	
2-2	108-111	are	
2-3	112-116	many	
2-4	117-126	different	
2-5	127-132	tools	
2-6	133-135	in	
2-7	136-139	the	
2-8	140-145	world	
2-9	145-146	,	
2-10	147-151	each	
2-11	152-154	of	
2-12	155-160	which	
2-13	161-167	solves	
2-14	168-169	a	
2-15	170-175	range	
2-16	176-178	of	
2-17	179-187	problems	
2-18	187-188	.	
2-19	189-193	Many	
2-20	194-196	of	
2-21	197-201	them	
2-22	202-205	are	
2-23	206-212	judged	
2-24	213-215	by	
2-25	216-219	how	
2-26	220-224	well	
2-27	225-228	and	
2-28	229-236	correct	
2-29	237-241	they	
2-30	242-247	solve	
2-31	248-252	this	
2-32	253-255	or	
2-33	256-260	that	
2-34	261-268	problem	
2-35	268-269	,	
2-36	270-273	but	
2-37	274-279	there	
2-38	280-283	are	
2-39	284-289	tools	
2-40	290-294	that	
2-41	295-298	you	
2-42	299-303	just	
2-43	304-308	like	
2-44	308-309	,	
2-45	310-313	you	
2-46	314-318	want	
2-47	319-321	to	
2-48	322-325	use	
2-49	326-330	them	
2-50	330-331	.	
2-51	332-336	They	
2-52	337-340	are	
2-53	341-349	properly	
2-54	350-358	designed	
2-55	359-362	and	
2-56	363-366	fit	
2-57	367-371	well	
2-58	372-374	in	
2-59	375-379	your	
2-60	380-384	hand	
2-61	384-385	,	
2-62	386-389	you	
2-63	390-392	do	
2-64	393-396	not	
2-65	397-401	need	
2-66	402-404	to	
2-67	405-408	dig	
2-68	409-413	into	
2-69	414-417	the	
2-70	418-431	documentation	
2-71	432-435	and	
2-72	436-446	understand	
2-73	447-450	how	
2-74	451-453	to	
2-75	454-456	do	
2-76	457-461	this	
2-77	462-464	or	
2-78	465-469	that	
2-79	470-476	simple	
2-80	477-483	action	
2-81	483-484	.	
2-82	485-490	About	
2-83	491-494	one	
2-84	495-497	of	
2-85	498-503	these	
2-86	504-509	tools	
2-87	510-513	for	
2-88	514-516	me	
2-89	517-518	I	
2-90	519-523	will	
2-91	524-526	be	
2-92	527-534	writing	
2-93	535-539	this	
2-94	540-546	series	
2-95	547-549	of	
2-96	550-555	posts	
2-97	555-556	.	
2-98	557-558	I	
2-99	559-563	will	
2-100	564-572	describe	
2-101	573-576	the	
2-102	577-589	optimization	
2-103	590-597	methods	
2-104	598-601	and	
2-105	602-606	tips	
2-106	607-611	that	
2-107	612-616	help	
2-108	617-619	me	
2-109	620-625	solve	
2-110	626-633	certain	
2-111	634-643	technical	
2-112	644-652	problems	
2-113	653-656	and	
2-114	657-664	achieve	
2-115	665-669	high	
2-116	670-680	efficiency	
2-117	681-686	using	
2-118	687-693	Apache	
2-119	694-699	Spark	
2-120	699-700	.	
2-121	701-705	This	
2-122	706-708	is	
2-123	709-711	my	
2-124	712-719	updated	
2-125	720-730	collection	
2-126	730-731	.	
2-127	732-736	Many	
2-128	737-739	of	
2-129	740-743	the	
2-130	744-757	optimizations	
2-131	758-762	that	
2-132	763-764	I	
2-133	765-769	will	
2-134	770-778	describe	
2-135	779-783	will	
2-136	784-787	not	
2-137	788-794	affect	
2-138	795-798	the	
2-139	799-802	JVM	
2-140	803-812	languages	
2-141	813-815	​​	
2-142	815-817	so	
2-143	818-822	much	
2-144	822-823	,	
2-145	824-827	but	
2-146	828-835	without	
2-147	836-841	these	
2-148	842-849	methods	
2-149	849-850	,	
2-150	851-855	many	
2-151	856-862	Python	
2-152	863-875	applications	
2-153	876-879	may	
2-154	880-886	simply	
2-155	887-890	not	
2-156	891-895	work	
2-157	895-896	.	

#Text=Whole series: Spark tips. DataFrame API Spark Tips. Don't collect data on driver The 5-minute guide to using bucketing in Pyspark Spark Tips. Partition Tuning Use DataFrame API
3-1	897-902	Whole	
3-2	903-909	series	
3-3	909-910	:	
3-4	911-916	Spark	
3-5	917-921	tips	
3-6	921-922	.	
3-7	923-932	DataFrame	
3-8	933-936	API	
3-9	937-942	Spark	
3-10	943-947	Tips	
3-11	947-948	.	
3-12	949-954	Don't	
3-13	955-962	collect	
3-14	963-967	data	
3-15	968-970	on	
3-16	971-977	driver	
3-17	978-981	The	
3-18	982-983	5	
3-19	983-984	-	
3-20	984-990	minute	
3-21	991-996	guide	
3-22	997-999	to	
3-23	1000-1005	using	
3-24	1006-1015	bucketing	
3-25	1016-1018	in	
3-26	1019-1026	Pyspark	
3-27	1027-1032	Spark	
3-28	1033-1037	Tips	
3-29	1037-1038	.	
3-30	1039-1048	Partition	
3-31	1049-1055	Tuning	
3-32	1056-1059	Use	
3-33	1060-1069	DataFrame	
3-34	1070-1073	API	

#Text=We know that RDD is a fault-tolerant collection of elements that can be processed in parallel. But RDDs actually kind of black box of data — we know that it holds some data but we do not know the type of the data or any other properties of the data. Hence it's data cannot be optimized as well as the operations on it. Spark 1.3 introduced a new abstraction — a DataFrame, in Spark 1.6 the Project Tungsten was introduced, an initiative which seeks to improve the performance and scalability of Spark. DataFrame data is organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.
4-1	1074-1076	We	
4-2	1077-1081	know	
4-3	1082-1086	that	
4-4	1087-1090	RDD	
4-5	1091-1093	is	
4-6	1094-1095	a	
4-7	1096-1110	fault-tolerant	
4-8	1111-1121	collection	
4-9	1122-1124	of	
4-10	1125-1133	elements	
4-11	1134-1138	that	
4-12	1139-1142	can	
4-13	1143-1145	be	
4-14	1146-1155	processed	
4-15	1156-1158	in	
4-16	1159-1167	parallel	
4-17	1167-1168	.	
4-18	1169-1172	But	
4-19	1173-1177	RDDs	
4-20	1178-1186	actually	
4-21	1187-1191	kind	
4-22	1192-1194	of	
4-23	1195-1200	black	
4-24	1201-1204	box	
4-25	1205-1207	of	
4-26	1208-1212	data	
4-27	1213-1214	—	
4-28	1215-1217	we	
4-29	1218-1222	know	
4-30	1223-1227	that	
4-31	1228-1230	it	
4-32	1231-1236	holds	
4-33	1237-1241	some	
4-34	1242-1246	data	
4-35	1247-1250	but	
4-36	1251-1253	we	
4-37	1254-1256	do	
4-38	1257-1260	not	
4-39	1261-1265	know	
4-40	1266-1269	the	
4-41	1270-1274	type	
4-42	1275-1277	of	
4-43	1278-1281	the	
4-44	1282-1286	data	
4-45	1287-1289	or	
4-46	1290-1293	any	
4-47	1294-1299	other	
4-48	1300-1310	properties	
4-49	1311-1313	of	
4-50	1314-1317	the	
4-51	1318-1322	data	
4-52	1322-1323	.	
4-53	1324-1329	Hence	
4-54	1330-1334	it's	
4-55	1335-1339	data	
4-56	1340-1346	cannot	
4-57	1347-1349	be	
4-58	1350-1359	optimized	
4-59	1360-1362	as	
4-60	1363-1367	well	
4-61	1368-1370	as	
4-62	1371-1374	the	
4-63	1375-1385	operations	
4-64	1386-1388	on	
4-65	1389-1391	it	
4-66	1391-1392	.	
4-67	1393-1398	Spark	
4-68	1399-1402	1.3	
4-69	1403-1413	introduced	
4-70	1414-1415	a	
4-71	1416-1419	new	
4-72	1420-1431	abstraction	
4-73	1432-1433	—	
4-74	1434-1435	a	
4-75	1436-1445	DataFrame	
4-76	1445-1446	,	
4-77	1447-1449	in	
4-78	1450-1455	Spark	
4-79	1456-1459	1.6	
4-80	1460-1463	the	
4-81	1464-1471	Project	
4-82	1472-1480	Tungsten	
4-83	1481-1484	was	
4-84	1485-1495	introduced	
4-85	1495-1496	,	
4-86	1497-1499	an	
4-87	1500-1510	initiative	
4-88	1511-1516	which	
4-89	1517-1522	seeks	
4-90	1523-1525	to	
4-91	1526-1533	improve	
4-92	1534-1537	the	
4-93	1538-1549	performance	
4-94	1550-1553	and	
4-95	1554-1565	scalability	
4-96	1566-1568	of	
4-97	1569-1574	Spark	
4-98	1574-1575	.	
4-99	1576-1585	DataFrame	
4-100	1586-1590	data	
4-101	1591-1593	is	
4-102	1594-1603	organized	
4-103	1604-1608	into	
4-104	1609-1614	named	
4-105	1615-1622	columns	
4-106	1622-1623	.	
4-107	1624-1626	It	
4-108	1627-1629	is	
4-109	1630-1642	conceptually	
4-110	1643-1653	equivalent	
4-111	1654-1656	to	
4-112	1657-1658	a	
4-113	1659-1664	table	
4-114	1665-1667	in	
4-115	1668-1669	a	
4-116	1670-1680	relational	
4-117	1681-1689	database	
4-118	1690-1692	or	
4-119	1693-1694	a	
4-120	1695-1699	data	
4-121	1700-1705	frame	
4-122	1706-1708	in	
4-123	1709-1710	R	
4-124	1710-1711	/	
4-125	1711-1717	Python	
4-126	1717-1718	,	
4-127	1719-1722	but	
4-128	1723-1727	with	
4-129	1728-1734	richer	
4-130	1735-1748	optimizations	
4-131	1749-1754	under	
4-132	1755-1758	the	
4-133	1759-1763	hood	
4-134	1763-1764	.	

#Text=Using DataFrame API you get: Efficient serialization/deserialization and memory usage In RDDs Spark uses Java serialization when it is necessary to distribute data across a cluster. The serialization of individual Scala and Java objects is expensive. In Pyspark, it has become more expensive when all data is double-serialized/deserialized to Java/Scala and then to Python (using cloudpickle) and back.
5-1	1765-1770	Using	
5-2	1771-1780	DataFrame	
5-3	1781-1784	API	
5-4	1785-1788	you	
5-5	1789-1792	get	
5-6	1792-1793	:	
5-7	1794-1803	Efficient	
5-8	1804-1817	serialization	
5-9	1817-1818	/	
5-10	1818-1833	deserialization	
5-11	1834-1837	and	
5-12	1838-1844	memory	
5-13	1845-1850	usage	
5-14	1851-1853	In	
5-15	1854-1858	RDDs	
5-16	1859-1864	Spark	
5-17	1865-1869	uses	
5-18	1870-1874	Java	
5-19	1875-1888	serialization	
5-20	1889-1893	when	
5-21	1894-1896	it	
5-22	1897-1899	is	
5-23	1900-1909	necessary	
5-24	1910-1912	to	
5-25	1913-1923	distribute	
5-26	1924-1928	data	
5-27	1929-1935	across	
5-28	1936-1937	a	
5-29	1938-1945	cluster	
5-30	1945-1946	.	
5-31	1947-1950	The	
5-32	1951-1964	serialization	
5-33	1965-1967	of	
5-34	1968-1978	individual	
5-35	1979-1984	Scala	
5-36	1985-1988	and	
5-37	1989-1993	Java	
5-38	1994-2001	objects	
5-39	2002-2004	is	
5-40	2005-2014	expensive	
5-41	2014-2015	.	
5-42	2016-2018	In	
5-43	2019-2026	Pyspark	
5-44	2026-2027	,	
5-45	2028-2030	it	
5-46	2031-2034	has	
5-47	2035-2041	become	
5-48	2042-2046	more	
5-49	2047-2056	expensive	
5-50	2057-2061	when	
5-51	2062-2065	all	
5-52	2066-2070	data	
5-53	2071-2073	is	
5-54	2074-2091	double-serialized	
5-55	2091-2092	/	
5-56	2092-2104	deserialized	
5-57	2105-2107	to	
5-58	2108-2112	Java	
5-59	2112-2113	/	
5-60	2113-2118	Scala	
5-61	2119-2122	and	
5-62	2123-2127	then	
5-63	2128-2130	to	
5-64	2131-2137	Python	
5-65	2138-2139	(	
5-66	2139-2144	using	
5-67	2145-2156	cloudpickle	
5-68	2156-2157	)	
5-69	2158-2161	and	
5-70	2162-2166	back	
5-71	2166-2167	.	

#Text=The cost of double serialization is the most expensive part when working with Pyspark. For distributed systems such as Spark, most of our time is spent only on serialization of data. Actual computing is often not a big blocking part — most of it is just serializing things and shuffling them around. We want to avoid this double cost of serialization. In RDD we have this double serialization, and we pay for it on every operation — wherever we need to distribute data into a cluster or call the python function. It also requires both data and structure to be transferred between nodes.
6-1	2168-2171	The	
6-2	2172-2176	cost	
6-3	2177-2179	of	
6-4	2180-2186	double	
6-5	2187-2200	serialization	
6-6	2201-2203	is	
6-7	2204-2207	the	
6-8	2208-2212	most	
6-9	2213-2222	expensive	
6-10	2223-2227	part	
6-11	2228-2232	when	
6-12	2233-2240	working	
6-13	2241-2245	with	
6-14	2246-2253	Pyspark	
6-15	2253-2254	.	
6-16	2255-2258	For	
6-17	2259-2270	distributed	
6-18	2271-2278	systems	
6-19	2279-2283	such	
6-20	2284-2286	as	
6-21	2287-2292	Spark	
6-22	2292-2293	,	
6-23	2294-2298	most	
6-24	2299-2301	of	
6-25	2302-2305	our	
6-26	2306-2310	time	
6-27	2311-2313	is	
6-28	2314-2319	spent	
6-29	2320-2324	only	
6-30	2325-2327	on	
6-31	2328-2341	serialization	
6-32	2342-2344	of	
6-33	2345-2349	data	
6-34	2349-2350	.	
6-35	2351-2357	Actual	
6-36	2358-2367	computing	
6-37	2368-2370	is	
6-38	2371-2376	often	
6-39	2377-2380	not	
6-40	2381-2382	a	
6-41	2383-2386	big	
6-42	2387-2395	blocking	
6-43	2396-2400	part	
6-44	2401-2402	—	
6-45	2403-2407	most	
6-46	2408-2410	of	
6-47	2411-2413	it	
6-48	2414-2416	is	
6-49	2417-2421	just	
6-50	2422-2433	serializing	
6-51	2434-2440	things	
6-52	2441-2444	and	
6-53	2445-2454	shuffling	
6-54	2455-2459	them	
6-55	2460-2466	around	
6-56	2466-2467	.	
6-57	2468-2470	We	
6-58	2471-2475	want	
6-59	2476-2478	to	
6-60	2479-2484	avoid	
6-61	2485-2489	this	
6-62	2490-2496	double	
6-63	2497-2501	cost	
6-64	2502-2504	of	
6-65	2505-2518	serialization	
6-66	2518-2519	.	
6-67	2520-2522	In	
6-68	2523-2526	RDD	
6-69	2527-2529	we	
6-70	2530-2534	have	
6-71	2535-2539	this	
6-72	2540-2546	double	
6-73	2547-2560	serialization	
6-74	2560-2561	,	
6-75	2562-2565	and	
6-76	2566-2568	we	
6-77	2569-2572	pay	
6-78	2573-2576	for	
6-79	2577-2579	it	
6-80	2580-2582	on	
6-81	2583-2588	every	
6-82	2589-2598	operation	
6-83	2599-2600	—	
6-84	2601-2609	wherever	
6-85	2610-2612	we	
6-86	2613-2617	need	
6-87	2618-2620	to	
6-88	2621-2631	distribute	
6-89	2632-2636	data	
6-90	2637-2641	into	
6-91	2642-2643	a	
6-92	2644-2651	cluster	
6-93	2652-2654	or	
6-94	2655-2659	call	
6-95	2660-2663	the	
6-96	2664-2670	python	
6-97	2671-2679	function	
6-98	2679-2680	.	
6-99	2681-2683	It	
6-100	2684-2688	also	
6-101	2689-2697	requires	
6-102	2698-2702	both	
6-103	2703-2707	data	
6-104	2708-2711	and	
6-105	2712-2721	structure	
6-106	2722-2724	to	
6-107	2725-2727	be	
6-108	2728-2739	transferred	
6-109	2740-2747	between	
6-110	2748-2753	nodes	
6-111	2753-2754	.	

#Text=With the DataFrame API, everything is a bit different. Since Spark understands data structure and column types if they are represented as DataFrame, he can understand how to store and manage them more efficiently(Java objects have a large inherent memory overhead as well as JVM GC). The DataFrame API does two things that help to do this (through the Tungsten project). First, using off-heap storage for data in binary format. Second, generating encoder code on the fly to work with this binary format for your specific objects. In simple words, Spark says:
7-1	2755-2759	With	
7-2	2760-2763	the	
7-3	2764-2773	DataFrame	
7-4	2774-2777	API	
7-5	2777-2778	,	
7-6	2779-2789	everything	
7-7	2790-2792	is	
7-8	2793-2794	a	
7-9	2795-2798	bit	
7-10	2799-2808	different	
7-11	2808-2809	.	
7-12	2810-2815	Since	
7-13	2816-2821	Spark	
7-14	2822-2833	understands	
7-15	2834-2838	data	
7-16	2839-2848	structure	
7-17	2849-2852	and	
7-18	2853-2859	column	
7-19	2860-2865	types	
7-20	2866-2868	if	
7-21	2869-2873	they	
7-22	2874-2877	are	
7-23	2878-2889	represented	
7-24	2890-2892	as	
7-25	2893-2902	DataFrame	
7-26	2902-2903	,	
7-27	2904-2906	he	
7-28	2907-2910	can	
7-29	2911-2921	understand	
7-30	2922-2925	how	
7-31	2926-2928	to	
7-32	2929-2934	store	
7-33	2935-2938	and	
7-34	2939-2945	manage	
7-35	2946-2950	them	
7-36	2951-2955	more	
7-37	2956-2967	efficiently	
7-38	2967-2968	(	
7-39	2968-2972	Java	
7-40	2973-2980	objects	
7-41	2981-2985	have	
7-42	2986-2987	a	
7-43	2988-2993	large	
7-44	2994-3002	inherent	
7-45	3003-3009	memory	
7-46	3010-3018	overhead	
7-47	3019-3021	as	
7-48	3022-3026	well	
7-49	3027-3029	as	
7-50	3030-3033	JVM	
7-51	3034-3036	GC	
7-52	3036-3037	)	
7-53	3037-3038	.	
7-54	3039-3042	The	
7-55	3043-3052	DataFrame	
7-56	3053-3056	API	
7-57	3057-3061	does	
7-58	3062-3065	two	
7-59	3066-3072	things	
7-60	3073-3077	that	
7-61	3078-3082	help	
7-62	3083-3085	to	
7-63	3086-3088	do	
7-64	3089-3093	this	
7-65	3094-3095	(	
7-66	3095-3102	through	
7-67	3103-3106	the	
7-68	3107-3115	Tungsten	
7-69	3116-3123	project	
7-70	3123-3124	)	
7-71	3124-3125	.	
7-72	3126-3131	First	
7-73	3131-3132	,	
7-74	3133-3138	using	
7-75	3139-3147	off-heap	
7-76	3148-3155	storage	
7-77	3156-3159	for	
7-78	3160-3164	data	
7-79	3165-3167	in	
7-80	3168-3174	binary	
7-81	3175-3181	format	
7-82	3181-3182	.	
7-83	3183-3189	Second	
7-84	3189-3190	,	
7-85	3191-3201	generating	
7-86	3202-3209	encoder	
7-87	3210-3214	code	
7-88	3215-3217	on	
7-89	3218-3221	the	
7-90	3222-3225	fly	
7-91	3226-3228	to	
7-92	3229-3233	work	
7-93	3234-3238	with	
7-94	3239-3243	this	
7-95	3244-3250	binary	
7-96	3251-3257	format	
7-97	3258-3261	for	
7-98	3262-3266	your	
7-99	3267-3275	specific	
7-100	3276-3283	objects	
7-101	3283-3284	.	
7-102	3285-3287	In	
7-103	3288-3294	simple	
7-104	3295-3300	words	
7-105	3300-3301	,	
7-106	3302-3307	Spark	
7-107	3308-3312	says	
7-108	3312-3313	:	

#Text=I'm going to generate these efficient encoders so that I can pull the data right out in this compact format and process it in that format right through the transformation chain. I'm going to minimize the amount of work I'm required to do in JVM (no, I'm not going to do anything in Python) and maximize the amount of data I can process in my compact format stored in off-heap memory. It turns out that you don't pay a penalty for serialization/deserialization — you work directly on this compact format, and using off-heap memory is a great way to reduce GC pauses because it is not in the scope of GC. So it's great that we can store our data inside the JVM, but we can still write code in Python using a very clear API and it will still be very efficient! Schema Projection The RDD API explicitly uses schematic projection. Therefore the user needs to define the schema manually.
8-1	3314-3317	I'm	
8-2	3318-3323	going	
8-3	3324-3326	to	
8-4	3327-3335	generate	
8-5	3336-3341	these	
8-6	3342-3351	efficient	
8-7	3352-3360	encoders	
8-8	3361-3363	so	
8-9	3364-3368	that	
8-10	3369-3370	I	
8-11	3371-3374	can	
8-12	3375-3379	pull	
8-13	3380-3383	the	
8-14	3384-3388	data	
8-15	3389-3394	right	
8-16	3395-3398	out	
8-17	3399-3401	in	
8-18	3402-3406	this	
8-19	3407-3414	compact	
8-20	3415-3421	format	
8-21	3422-3425	and	
8-22	3426-3433	process	
8-23	3434-3436	it	
8-24	3437-3439	in	
8-25	3440-3444	that	
8-26	3445-3451	format	
8-27	3452-3457	right	
8-28	3458-3465	through	
8-29	3466-3469	the	
8-30	3470-3484	transformation	
8-31	3485-3490	chain	
8-32	3490-3491	.	
8-33	3492-3495	I'm	
8-34	3496-3501	going	
8-35	3502-3504	to	
8-36	3505-3513	minimize	
8-37	3514-3517	the	
8-38	3518-3524	amount	
8-39	3525-3527	of	
8-40	3528-3532	work	
8-41	3533-3536	I'm	
8-42	3537-3545	required	
8-43	3546-3548	to	
8-44	3549-3551	do	
8-45	3552-3554	in	
8-46	3555-3558	JVM	
8-47	3559-3560	(	
8-48	3560-3562	no	
8-49	3562-3563	,	
8-50	3564-3567	I'm	
8-51	3568-3571	not	
8-52	3572-3577	going	
8-53	3578-3580	to	
8-54	3581-3583	do	
8-55	3584-3592	anything	
8-56	3593-3595	in	
8-57	3596-3602	Python	
8-58	3602-3603	)	
8-59	3604-3607	and	
8-60	3608-3616	maximize	
8-61	3617-3620	the	
8-62	3621-3627	amount	
8-63	3628-3630	of	
8-64	3631-3635	data	
8-65	3636-3637	I	
8-66	3638-3641	can	
8-67	3642-3649	process	
8-68	3650-3652	in	
8-69	3653-3655	my	
8-70	3656-3663	compact	
8-71	3664-3670	format	
8-72	3671-3677	stored	
8-73	3678-3680	in	
8-74	3681-3689	off-heap	
8-75	3690-3696	memory	
8-76	3696-3697	.	
8-77	3698-3700	It	
8-78	3701-3706	turns	
8-79	3707-3710	out	
8-80	3711-3715	that	
8-81	3716-3719	you	
8-82	3720-3725	don't	
8-83	3726-3729	pay	
8-84	3730-3731	a	
8-85	3732-3739	penalty	
8-86	3740-3743	for	
8-87	3744-3757	serialization	
8-88	3757-3758	/	
8-89	3758-3773	deserialization	
8-90	3774-3775	—	
8-91	3776-3779	you	
8-92	3780-3784	work	
8-93	3785-3793	directly	
8-94	3794-3796	on	
8-95	3797-3801	this	
8-96	3802-3809	compact	
8-97	3810-3816	format	
8-98	3816-3817	,	
8-99	3818-3821	and	
8-100	3822-3827	using	
8-101	3828-3836	off-heap	
8-102	3837-3843	memory	
8-103	3844-3846	is	
8-104	3847-3848	a	
8-105	3849-3854	great	
8-106	3855-3858	way	
8-107	3859-3861	to	
8-108	3862-3868	reduce	
8-109	3869-3871	GC	
8-110	3872-3878	pauses	
8-111	3879-3886	because	
8-112	3887-3889	it	
8-113	3890-3892	is	
8-114	3893-3896	not	
8-115	3897-3899	in	
8-116	3900-3903	the	
8-117	3904-3909	scope	
8-118	3910-3912	of	
8-119	3913-3915	GC	
8-120	3915-3916	.	
8-121	3917-3919	So	
8-122	3920-3924	it's	
8-123	3925-3930	great	
8-124	3931-3935	that	
8-125	3936-3938	we	
8-126	3939-3942	can	
8-127	3943-3948	store	
8-128	3949-3952	our	
8-129	3953-3957	data	
8-130	3958-3964	inside	
8-131	3965-3968	the	
8-132	3969-3972	JVM	
8-133	3972-3973	,	
8-134	3974-3977	but	
8-135	3978-3980	we	
8-136	3981-3984	can	
8-137	3985-3990	still	
8-138	3991-3996	write	
8-139	3997-4001	code	
8-140	4002-4004	in	
8-141	4005-4011	Python	
8-142	4012-4017	using	
8-143	4018-4019	a	
8-144	4020-4024	very	
8-145	4025-4030	clear	
8-146	4031-4034	API	
8-147	4035-4038	and	
8-148	4039-4041	it	
8-149	4042-4046	will	
8-150	4047-4052	still	
8-151	4053-4055	be	
8-152	4056-4060	very	
8-153	4061-4070	efficient	
8-154	4070-4071	!	
8-155	4072-4078	Schema	
8-156	4079-4089	Projection	
8-157	4090-4093	The	
8-158	4094-4097	RDD	
8-159	4098-4101	API	
8-160	4102-4112	explicitly	
8-161	4113-4117	uses	
8-162	4118-4127	schematic	
8-163	4128-4138	projection	
8-164	4138-4139	.	
8-165	4140-4149	Therefore	
8-166	4150-4153	the	
8-167	4154-4158	user	
8-168	4159-4164	needs	
8-169	4165-4167	to	
8-170	4168-4174	define	
8-171	4175-4178	the	
8-172	4179-4185	schema	
8-173	4186-4194	manually	
8-174	4194-4195	.	

#Text=There's no need to specify the schema explicitly in DataFrame. As a rule, Spark can detect the schema automatically(inferSchema option). But the schema's resolution depends mainly on the data sources. If the source should contain structured data (i.e. relational database), the schema is extracted directly without any guesswork. More complex operations apply to semi-structured data, such as JSON files. In these cases the schema is guesswork. Optimizations
9-1	4196-4203	There's	
9-2	4204-4206	no	
9-3	4207-4211	need	
9-4	4212-4214	to	
9-5	4215-4222	specify	
9-6	4223-4226	the	
9-7	4227-4233	schema	
9-8	4234-4244	explicitly	
9-9	4245-4247	in	
9-10	4248-4257	DataFrame	
9-11	4257-4258	.	
9-12	4259-4261	As	
9-13	4262-4263	a	
9-14	4264-4268	rule	
9-15	4268-4269	,	
9-16	4270-4275	Spark	
9-17	4276-4279	can	
9-18	4280-4286	detect	
9-19	4287-4290	the	
9-20	4291-4297	schema	
9-21	4298-4311	automatically	
9-22	4311-4312	(	
9-23	4312-4323	inferSchema	
9-24	4324-4330	option	
9-25	4330-4331	)	
9-26	4331-4332	.	
9-27	4333-4336	But	
9-28	4337-4340	the	
9-29	4341-4349	schema's	
9-30	4350-4360	resolution	
9-31	4361-4368	depends	
9-32	4369-4375	mainly	
9-33	4376-4378	on	
9-34	4379-4382	the	
9-35	4383-4387	data	
9-36	4388-4395	sources	
9-37	4395-4396	.	
9-38	4397-4399	If	
9-39	4400-4403	the	
9-40	4404-4410	source	
9-41	4411-4417	should	
9-42	4418-4425	contain	
9-43	4426-4436	structured	
9-44	4437-4441	data	
9-45	4442-4443	(	
9-46	4443-4446	i.e	
9-47	4446-4447	.	
9-48	4448-4458	relational	
9-49	4459-4467	database	
9-50	4467-4468	)	
9-51	4468-4469	,	
9-52	4470-4473	the	
9-53	4474-4480	schema	
9-54	4481-4483	is	
9-55	4484-4493	extracted	
9-56	4494-4502	directly	
9-57	4503-4510	without	
9-58	4511-4514	any	
9-59	4515-4524	guesswork	
9-60	4524-4525	.	
9-61	4526-4530	More	
9-62	4531-4538	complex	
9-63	4539-4549	operations	
9-64	4550-4555	apply	
9-65	4556-4558	to	
9-66	4559-4574	semi-structured	
9-67	4575-4579	data	
9-68	4579-4580	,	
9-69	4581-4585	such	
9-70	4586-4588	as	
9-71	4589-4593	JSON	
9-72	4594-4599	files	
9-73	4599-4600	.	
9-74	4601-4603	In	
9-75	4604-4609	these	
9-76	4610-4615	cases	
9-77	4616-4619	the	
9-78	4620-4626	schema	
9-79	4627-4629	is	
9-80	4630-4639	guesswork	
9-81	4639-4640	.	
9-82	4641-4654	Optimizations	

#Text=RDD cannot be optimized by Spark — they are fully lambda driven. RDD is rather a "black box" of data, which cannot be optimized because Spark can' t look inside what the data actually consists of. So Spark can't do any optimizations on your behalf. This feels more acute in non-JVM languages such as Python. DataFrame has additional metadata due to its column format, which allows Spark to perform certain optimizations on a completed request. Before your query is run, a logical plan is created using Catalyst Optimizer and then it's executed using the Tungsten execution engine.
10-1	4655-4658	RDD	
10-2	4659-4665	cannot	
10-3	4666-4668	be	
10-4	4669-4678	optimized	
10-5	4679-4681	by	
10-6	4682-4687	Spark	
10-7	4688-4689	—	
10-8	4690-4694	they	
10-9	4695-4698	are	
10-10	4699-4704	fully	
10-11	4705-4711	lambda	
10-12	4712-4718	driven	
10-13	4718-4719	.	
10-14	4720-4723	RDD	
10-15	4724-4726	is	
10-16	4727-4733	rather	
10-17	4734-4735	a	
10-18	4736-4737	"	
10-19	4737-4742	black	
10-20	4743-4746	box	
10-21	4746-4747	"	
10-22	4748-4750	of	
10-23	4751-4755	data	
10-24	4755-4756	,	
10-25	4757-4762	which	
10-26	4763-4769	cannot	
10-27	4770-4772	be	
10-28	4773-4782	optimized	
10-29	4783-4790	because	
10-30	4791-4796	Spark	
10-31	4797-4800	can	
10-32	4800-4801	'	
10-33	4802-4803	t	
10-34	4804-4808	look	
10-35	4809-4815	inside	
10-36	4816-4820	what	
10-37	4821-4824	the	
10-38	4825-4829	data	
10-39	4830-4838	actually	
10-40	4839-4847	consists	
10-41	4848-4850	of	
10-42	4850-4851	.	
10-43	4852-4854	So	
10-44	4855-4860	Spark	
10-45	4861-4866	can't	
10-46	4867-4869	do	
10-47	4870-4873	any	
10-48	4874-4887	optimizations	
10-49	4888-4890	on	
10-50	4891-4895	your	
10-51	4896-4902	behalf	
10-52	4902-4903	.	
10-53	4904-4908	This	
10-54	4909-4914	feels	
10-55	4915-4919	more	
10-56	4920-4925	acute	
10-57	4926-4928	in	
10-58	4929-4936	non-JVM	
10-59	4937-4946	languages	
10-60	4947-4951	such	
10-61	4952-4954	as	
10-62	4955-4961	Python	
10-63	4961-4962	.	
10-64	4963-4972	DataFrame	
10-65	4973-4976	has	
10-66	4977-4987	additional	
10-67	4988-4996	metadata	
10-68	4997-5000	due	
10-69	5001-5003	to	
10-70	5004-5007	its	
10-71	5008-5014	column	
10-72	5015-5021	format	
10-73	5021-5022	,	
10-74	5023-5028	which	
10-75	5029-5035	allows	
10-76	5036-5041	Spark	
10-77	5042-5044	to	
10-78	5045-5052	perform	
10-79	5053-5060	certain	
10-80	5061-5074	optimizations	
10-81	5075-5077	on	
10-82	5078-5079	a	
10-83	5080-5089	completed	
10-84	5090-5097	request	
10-85	5097-5098	.	
10-86	5099-5105	Before	
10-87	5106-5110	your	
10-88	5111-5116	query	
10-89	5117-5119	is	
10-90	5120-5123	run	
10-91	5123-5124	,	
10-92	5125-5126	a	
10-93	5127-5134	logical	
10-94	5135-5139	plan	
10-95	5140-5142	is	
10-96	5143-5150	created	
10-97	5151-5156	using	
10-98	5157-5165	Catalyst	
10-99	5166-5175	Optimizer	
10-100	5176-5179	and	
10-101	5180-5184	then	
10-102	5185-5189	it's	
10-103	5190-5198	executed	
10-104	5199-5204	using	
10-105	5205-5208	the	
10-106	5209-5217	Tungsten	
10-107	5218-5227	execution	
10-108	5228-5234	engine	
10-109	5234-5235	.	

#Text=What is Catalyst? Catalyst Optimizer — the name of the integrated query optimizer and execution scheduler for Spark Datasets/DataFrame.
11-1	5236-5240	What	
11-2	5241-5243	is	
11-3	5244-5252	Catalyst	
11-4	5252-5253	?	
11-5	5254-5262	Catalyst	
11-6	5263-5272	Optimizer	
11-7	5273-5274	—	
11-8	5275-5278	the	
11-9	5279-5283	name	
11-10	5284-5286	of	
11-11	5287-5290	the	
11-12	5291-5301	integrated	
11-13	5302-5307	query	
11-14	5308-5317	optimizer	
11-15	5318-5321	and	
11-16	5322-5331	execution	
11-17	5332-5341	scheduler	
11-18	5342-5345	for	
11-19	5346-5351	Spark	
11-20	5352-5360	Datasets	
11-21	5360-5361	/	
11-22	5361-5370	DataFrame	
11-23	5370-5371	.	

#Text=Catalyst Optimizer is the place where most of the "magic" tends to improve the speed of your code execution by logically improving it. But in any complex system, unfortunately, "magic" is not enough to always guarantee optimal performance. As with relational databases, it is useful to learn a little bit about how the optimizer works to understand how it plans and customizes your applications. In particular, Catalyst Optimizer can perform refactoring of complex queries. However, almost all of its optimizations are qualitative and rule-based rather than quantitative and statistical. For example, Spark knows how and when to do things like combine filters or move filters before joining. Spark 2.0 even allows you to define, add, and test your own additional optimization rules at runtime.
12-1	5372-5380	Catalyst	
12-2	5381-5390	Optimizer	
12-3	5391-5393	is	
12-4	5394-5397	the	
12-5	5398-5403	place	
12-6	5404-5409	where	
12-7	5410-5414	most	
12-8	5415-5417	of	
12-9	5418-5421	the	
12-10	5422-5423	"	
12-11	5423-5428	magic	
12-12	5428-5429	"	
12-13	5430-5435	tends	
12-14	5436-5438	to	
12-15	5439-5446	improve	
12-16	5447-5450	the	
12-17	5451-5456	speed	
12-18	5457-5459	of	
12-19	5460-5464	your	
12-20	5465-5469	code	
12-21	5470-5479	execution	
12-22	5480-5482	by	
12-23	5483-5492	logically	
12-24	5493-5502	improving	
12-25	5503-5505	it	
12-26	5505-5506	.	
12-27	5507-5510	But	
12-28	5511-5513	in	
12-29	5514-5517	any	
12-30	5518-5525	complex	
12-31	5526-5532	system	
12-32	5532-5533	,	
12-33	5534-5547	unfortunately	
12-34	5547-5548	,	
12-35	5549-5550	"	
12-36	5550-5555	magic	
12-37	5555-5556	"	
12-38	5557-5559	is	
12-39	5560-5563	not	
12-40	5564-5570	enough	
12-41	5571-5573	to	
12-42	5574-5580	always	
12-43	5581-5590	guarantee	
12-44	5591-5598	optimal	
12-45	5599-5610	performance	
12-46	5610-5611	.	
12-47	5612-5614	As	
12-48	5615-5619	with	
12-49	5620-5630	relational	
12-50	5631-5640	databases	
12-51	5640-5641	,	
12-52	5642-5644	it	
12-53	5645-5647	is	
12-54	5648-5654	useful	
12-55	5655-5657	to	
12-56	5658-5663	learn	
12-57	5664-5665	a	
12-58	5666-5672	little	
12-59	5673-5676	bit	
12-60	5677-5682	about	
12-61	5683-5686	how	
12-62	5687-5690	the	
12-63	5691-5700	optimizer	
12-64	5701-5706	works	
12-65	5707-5709	to	
12-66	5710-5720	understand	
12-67	5721-5724	how	
12-68	5725-5727	it	
12-69	5728-5733	plans	
12-70	5734-5737	and	
12-71	5738-5748	customizes	
12-72	5749-5753	your	
12-73	5754-5766	applications	
12-74	5766-5767	.	
12-75	5768-5770	In	
12-76	5771-5781	particular	
12-77	5781-5782	,	
12-78	5783-5791	Catalyst	
12-79	5792-5801	Optimizer	
12-80	5802-5805	can	
12-81	5806-5813	perform	
12-82	5814-5825	refactoring	
12-83	5826-5828	of	
12-84	5829-5836	complex	
12-85	5837-5844	queries	
12-86	5844-5845	.	
12-87	5846-5853	However	
12-88	5853-5854	,	
12-89	5855-5861	almost	
12-90	5862-5865	all	
12-91	5866-5868	of	
12-92	5869-5872	its	
12-93	5873-5886	optimizations	
12-94	5887-5890	are	
12-95	5891-5902	qualitative	
12-96	5903-5906	and	
12-97	5907-5917	rule-based	
12-98	5918-5924	rather	
12-99	5925-5929	than	
12-100	5930-5942	quantitative	
12-101	5943-5946	and	
12-102	5947-5958	statistical	
12-103	5958-5959	.	
12-104	5960-5963	For	
12-105	5964-5971	example	
12-106	5971-5972	,	
12-107	5973-5978	Spark	
12-108	5979-5984	knows	
12-109	5985-5988	how	
12-110	5989-5992	and	
12-111	5993-5997	when	
12-112	5998-6000	to	
12-113	6001-6003	do	
12-114	6004-6010	things	
12-115	6011-6015	like	
12-116	6016-6023	combine	
12-117	6024-6031	filters	
12-118	6032-6034	or	
12-119	6035-6039	move	
12-120	6040-6047	filters	
12-121	6048-6054	before	
12-122	6055-6062	joining	
12-123	6062-6063	.	
12-124	6064-6069	Spark	
12-125	6070-6073	2.0	
12-126	6074-6078	even	
12-127	6079-6085	allows	
12-128	6086-6089	you	
12-129	6090-6092	to	
12-130	6093-6099	define	
12-131	6099-6100	,	
12-132	6101-6104	add	
12-133	6104-6105	,	
12-134	6106-6109	and	
12-135	6110-6114	test	
12-136	6115-6119	your	
12-137	6120-6123	own	
12-138	6124-6134	additional	
12-139	6135-6147	optimization	
12-140	6148-6153	rules	
12-141	6154-6156	at	
12-142	6157-6164	runtime	
12-143	6164-6165	.	

#Text=Catalyst Optimizer supports both rule-based and cost-based optimization. Cost-Based Optimizer(CBO): If an SQL query can be executed in two different ways (e.g. #1 and #2 for the same original query), then CBO essentially calculates the cost of each path and analyzes which path is cheaper, and then executes that path to improve the query execution.
13-1	6166-6174	Catalyst	
13-2	6175-6184	Optimizer	
13-3	6185-6193	supports	
13-4	6194-6198	both	
13-5	6199-6209	rule-based	
13-6	6210-6213	and	
13-7	6214-6224	cost-based	
13-8	6225-6237	optimization	
13-9	6237-6238	.	
13-10	6239-6249	Cost-Based	
13-11	6250-6259	Optimizer	
13-12	6259-6260	(	
13-13	6260-6263	CBO	
13-14	6263-6264	)	
13-15	6264-6265	:	
13-16	6266-6268	If	
13-17	6269-6271	an	
13-18	6272-6275	SQL	
13-19	6276-6281	query	
13-20	6282-6285	can	
13-21	6286-6288	be	
13-22	6289-6297	executed	
13-23	6298-6300	in	
13-24	6301-6304	two	
13-25	6305-6314	different	
13-26	6315-6319	ways	
13-27	6320-6321	(	
13-28	6321-6324	e.g	
13-29	6324-6325	.	
13-30	6326-6328	#1	
13-31	6329-6332	and	
13-32	6333-6335	#2	
13-33	6336-6339	for	
13-34	6340-6343	the	
13-35	6344-6348	same	
13-36	6349-6357	original	
13-37	6358-6363	query	
13-38	6363-6364	)	
13-39	6364-6365	,	
13-40	6366-6370	then	
13-41	6371-6374	CBO	
13-42	6375-6386	essentially	
13-43	6387-6397	calculates	
13-44	6398-6401	the	
13-45	6402-6406	cost	
13-46	6407-6409	of	
13-47	6410-6414	each	
13-48	6415-6419	path	
13-49	6420-6423	and	
13-50	6424-6432	analyzes	
13-51	6433-6438	which	
13-52	6439-6443	path	
13-53	6444-6446	is	
13-54	6447-6454	cheaper	
13-55	6454-6455	,	
13-56	6456-6459	and	
13-57	6460-6464	then	
13-58	6465-6473	executes	
13-59	6474-6478	that	
13-60	6479-6483	path	
13-61	6484-6486	to	
13-62	6487-6494	improve	
13-63	6495-6498	the	
13-64	6499-6504	query	
13-65	6505-6514	execution	
13-66	6514-6515	.	

#Text=Rule-Based optimizer(RBO): follows different optimization rules that apply depending on the query. These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean simplification, and other rules. In fact, there is no separation inside the Spark, the two approaches work together, cost-based optimization is performed by generating multiple plans using rules, and then computing their costs. As an example, imagine that we have users (they come from the database) and their transactions (I will generate some random values, but it could be a database as well):
14-1	6516-6526	Rule-Based	
14-2	6527-6536	optimizer	
14-3	6536-6537	(	
14-4	6537-6540	RBO	
14-5	6540-6541	)	
14-6	6541-6542	:	
14-7	6543-6550	follows	
14-8	6551-6560	different	
14-9	6561-6573	optimization	
14-10	6574-6579	rules	
14-11	6580-6584	that	
14-12	6585-6590	apply	
14-13	6591-6600	depending	
14-14	6601-6603	on	
14-15	6604-6607	the	
14-16	6608-6613	query	
14-17	6613-6614	.	
14-18	6615-6620	These	
14-19	6621-6628	include	
14-20	6629-6637	constant	
14-21	6638-6645	folding	
14-22	6645-6646	,	
14-23	6647-6656	predicate	
14-24	6657-6665	pushdown	
14-25	6665-6666	,	
14-26	6667-6677	projection	
14-27	6678-6685	pruning	
14-28	6685-6686	,	
14-29	6687-6691	null	
14-30	6692-6703	propagation	
14-31	6703-6704	,	
14-32	6705-6712	Boolean	
14-33	6713-6727	simplification	
14-34	6727-6728	,	
14-35	6729-6732	and	
14-36	6733-6738	other	
14-37	6739-6744	rules	
14-38	6744-6745	.	
14-39	6746-6748	In	
14-40	6749-6753	fact	
14-41	6753-6754	,	
14-42	6755-6760	there	
14-43	6761-6763	is	
14-44	6764-6766	no	
14-45	6767-6777	separation	
14-46	6778-6784	inside	
14-47	6785-6788	the	
14-48	6789-6794	Spark	
14-49	6794-6795	,	
14-50	6796-6799	the	
14-51	6800-6803	two	
14-52	6804-6814	approaches	
14-53	6815-6819	work	
14-54	6820-6828	together	
14-55	6828-6829	,	
14-56	6830-6840	cost-based	
14-57	6841-6853	optimization	
14-58	6854-6856	is	
14-59	6857-6866	performed	
14-60	6867-6869	by	
14-61	6870-6880	generating	
14-62	6881-6889	multiple	
14-63	6890-6895	plans	
14-64	6896-6901	using	
14-65	6902-6907	rules	
14-66	6907-6908	,	
14-67	6909-6912	and	
14-68	6913-6917	then	
14-69	6918-6927	computing	
14-70	6928-6933	their	
14-71	6934-6939	costs	
14-72	6939-6940	.	
14-73	6941-6943	As	
14-74	6944-6946	an	
14-75	6947-6954	example	
14-76	6954-6955	,	
14-77	6956-6963	imagine	
14-78	6964-6968	that	
14-79	6969-6971	we	
14-80	6972-6976	have	
14-81	6977-6982	users	
14-82	6983-6984	(	
14-83	6984-6988	they	
14-84	6989-6993	come	
14-85	6994-6998	from	
14-86	6999-7002	the	
14-87	7003-7011	database	
14-88	7011-7012	)	
14-89	7013-7016	and	
14-90	7017-7022	their	
14-91	7023-7035	transactions	
14-92	7036-7037	(	
14-93	7037-7038	I	
14-94	7039-7043	will	
14-95	7044-7052	generate	
14-96	7053-7057	some	
14-97	7058-7064	random	
14-98	7065-7071	values	
14-99	7071-7072	,	
14-100	7073-7076	but	
14-101	7077-7079	it	
14-102	7080-7085	could	
14-103	7086-7088	be	
14-104	7089-7090	a	
14-105	7091-7099	database	
14-106	7100-7102	as	
14-107	7103-7107	well	
14-108	7107-7108	)	
14-109	7108-7109	:	

#Text=transactions = spark.range(160000)\\ .select(F.col('id').alias('key'), F.rand(12).alias('value')) users = spark.read.jdbc(
15-1	7110-7122	transactions	
15-2	7123-7124	=	
15-3	7125-7136	spark.range	
15-4	7136-7137	(	
15-5	7137-7143	160000	
15-6	7143-7144	)	
15-7	7144-7145	\	
15-8	7146-7147	.	
15-9	7147-7153	select	
15-10	7153-7154	(	
15-11	7154-7159	F.col	
15-12	7159-7160	(	
15-13	7160-7161	'	
15-14	7161-7163	id	
15-15	7163-7164	'	
15-16	7164-7165	)	
15-17	7165-7166	.	
15-18	7166-7171	alias	
15-19	7171-7172	(	
15-20	7172-7173	'	
15-21	7173-7176	key	
15-22	7176-7177	'	
15-23	7177-7178	)	
15-24	7178-7179	,	
15-25	7180-7186	F.rand	
15-26	7186-7187	(	
15-27	7187-7189	12	
15-28	7189-7190	)	
15-29	7190-7191	.	
15-30	7191-7196	alias	
15-31	7196-7197	(	
15-32	7197-7198	'	
15-33	7198-7203	value	
15-34	7203-7204	'	
15-35	7204-7205	)	
15-36	7205-7206	)	
15-37	7207-7212	users	
15-38	7213-7214	=	
15-39	7215-7230	spark.read.jdbc	
15-40	7230-7231	(	

#Text=table='users', url='jdbc:postgresql://localhost:5432/postgres') users.join(transactions, 'key')\\ .filter(F.col('key') > 100)\\ .explain()
16-1	7232-7237	table	
16-2	7237-7238	=	
16-3	7238-7239	'	
16-4	7239-7244	users	
16-5	7244-7245	'	
16-6	7245-7246	,	
16-7	7247-7250	url	
16-8	7250-7251	=	
16-9	7251-7252	'	
16-10	7252-7256	jdbc	
16-11	7256-7257	:	
16-12	7257-7267	postgresql	
16-13	7267-7268	:	
16-14	7268-7269	/	
16-15	7269-7270	/	
16-16	7270-7279	localhost	
16-17	7279-7280	:	
16-18	7280-7284	5432	
16-19	7284-7285	/	
16-20	7285-7293	postgres	
16-21	7293-7294	'	
16-22	7294-7295	)	
16-23	7296-7306	users.join	
16-24	7306-7307	(	
16-25	7307-7319	transactions	
16-26	7319-7320	,	
16-27	7321-7322	'	
16-28	7322-7325	key	
16-29	7325-7326	'	
16-30	7326-7327	)	
16-31	7327-7328	\	
16-32	7329-7330	.	
16-33	7330-7336	filter	
16-34	7336-7337	(	
16-35	7337-7342	F.col	
16-36	7342-7343	(	
16-37	7343-7344	'	
16-38	7344-7347	key	
16-39	7347-7348	'	
16-40	7348-7349	)	
16-41	7350-7351	>	
16-42	7352-7355	100	
16-43	7355-7356	)	
16-44	7356-7357	\	
16-45	7358-7359	.	
16-46	7359-7366	explain	
16-47	7366-7367	(	
16-48	7367-7368	)	

#Text=We see here that our developer in a programming rush wrote user filtering after join. It would be more reasonable to do the filter first and then join because the shuffle data will be reduced. But if we see the physical plan, we see that Catalyst Optimizer did it for us. What's more, it did a predicate pushdown of our filter to the database (Spark will try to move the filtering data as close to the source as possible to avoid loading unnecessary data into memory, see PushedFilters). == Physical Plan == *(5) SortMergeJoin [cast(key#6 as bigint)], [key#2L], Inner :- *(2) Sort [cast(key#6 as bigint) ASC NULLS FIRST], false, 0
17-1	7369-7371	We	
17-2	7372-7375	see	
17-3	7376-7380	here	
17-4	7381-7385	that	
17-5	7386-7389	our	
17-6	7390-7399	developer	
17-7	7400-7402	in	
17-8	7403-7404	a	
17-9	7405-7416	programming	
17-10	7417-7421	rush	
17-11	7422-7427	wrote	
17-12	7428-7432	user	
17-13	7433-7442	filtering	
17-14	7443-7448	after	
17-15	7449-7453	join	
17-16	7453-7454	.	
17-17	7455-7457	It	
17-18	7458-7463	would	
17-19	7464-7466	be	
17-20	7467-7471	more	
17-21	7472-7482	reasonable	
17-22	7483-7485	to	
17-23	7486-7488	do	
17-24	7489-7492	the	
17-25	7493-7499	filter	
17-26	7500-7505	first	
17-27	7506-7509	and	
17-28	7510-7514	then	
17-29	7515-7519	join	
17-30	7520-7527	because	
17-31	7528-7531	the	
17-32	7532-7539	shuffle	
17-33	7540-7544	data	
17-34	7545-7549	will	
17-35	7550-7552	be	
17-36	7553-7560	reduced	
17-37	7560-7561	.	
17-38	7562-7565	But	
17-39	7566-7568	if	
17-40	7569-7571	we	
17-41	7572-7575	see	
17-42	7576-7579	the	
17-43	7580-7588	physical	
17-44	7589-7593	plan	
17-45	7593-7594	,	
17-46	7595-7597	we	
17-47	7598-7601	see	
17-48	7602-7606	that	
17-49	7607-7615	Catalyst	
17-50	7616-7625	Optimizer	
17-51	7626-7629	did	
17-52	7630-7632	it	
17-53	7633-7636	for	
17-54	7637-7639	us	
17-55	7639-7640	.	
17-56	7641-7647	What's	
17-57	7648-7652	more	
17-58	7652-7653	,	
17-59	7654-7656	it	
17-60	7657-7660	did	
17-61	7661-7662	a	
17-62	7663-7672	predicate	
17-63	7673-7681	pushdown	
17-64	7682-7684	of	
17-65	7685-7688	our	
17-66	7689-7695	filter	
17-67	7696-7698	to	
17-68	7699-7702	the	
17-69	7703-7711	database	
17-70	7712-7713	(	
17-71	7713-7718	Spark	
17-72	7719-7723	will	
17-73	7724-7727	try	
17-74	7728-7730	to	
17-75	7731-7735	move	
17-76	7736-7739	the	
17-77	7740-7749	filtering	
17-78	7750-7754	data	
17-79	7755-7757	as	
17-80	7758-7763	close	
17-81	7764-7766	to	
17-82	7767-7770	the	
17-83	7771-7777	source	
17-84	7778-7780	as	
17-85	7781-7789	possible	
17-86	7790-7792	to	
17-87	7793-7798	avoid	
17-88	7799-7806	loading	
17-89	7807-7818	unnecessary	
17-90	7819-7823	data	
17-91	7824-7828	into	
17-92	7829-7835	memory	
17-93	7835-7836	,	
17-94	7837-7840	see	
17-95	7841-7854	PushedFilters	
17-96	7854-7855	)	
17-97	7855-7856	.	
17-98	7857-7858	=	
17-99	7858-7859	=	
17-100	7860-7868	Physical	
17-101	7869-7873	Plan	
17-102	7874-7875	=	
17-103	7875-7876	=	
17-104	7877-7878	*	
17-105	7878-7879	(	
17-106	7879-7880	5	
17-107	7880-7881	)	
17-108	7882-7895	SortMergeJoin	
17-109	7896-7897	[	
17-110	7897-7901	cast	
17-111	7901-7902	(	
17-112	7902-7905	key	
17-113	7905-7907	#6	
17-114	7908-7910	as	
17-115	7911-7917	bigint	
17-116	7917-7918	)	
17-117	7918-7919	]	
17-118	7919-7920	,	
17-119	7921-7922	[	
17-120	7922-7925	key	
17-121	7925-7928	#2L	
17-122	7928-7929	]	
17-123	7929-7930	,	
17-124	7931-7936	Inner	
17-125	7937-7938	:	
17-126	7938-7939	-	
17-127	7940-7941	*	
17-128	7941-7942	(	
17-129	7942-7943	2	
17-130	7943-7944	)	
17-131	7945-7949	Sort	
17-132	7950-7951	[	
17-133	7951-7955	cast	
17-134	7955-7956	(	
17-135	7956-7959	key	
17-136	7959-7961	#6	
17-137	7962-7964	as	
17-138	7965-7971	bigint	
17-139	7971-7972	)	
17-140	7973-7976	ASC	
17-141	7977-7982	NULLS	
17-142	7983-7988	FIRST	
17-143	7988-7989	]	
17-144	7989-7990	,	
17-145	7991-7996	false	
17-146	7996-7997	,	
17-147	7998-7999	0	

#Text=+- Exchange hashpartitioning(cast(key#6 as bigint), 200) +- *(1) Filter (cast(key#6 as int) > 100) +- *(1) Scan JDBCRelation(users) [numPartitions=1] [key#6,name#7] PushedFilters: [*IsNotNull(key)], ReadSchema: struct<key:string,name:string>
18-1	8000-8001	+	
18-2	8001-8002	-	
18-3	8003-8011	Exchange	
18-4	8012-8028	hashpartitioning	
18-5	8028-8029	(	
18-6	8029-8033	cast	
18-7	8033-8034	(	
18-8	8034-8037	key	
18-9	8037-8039	#6	
18-10	8040-8042	as	
18-11	8043-8049	bigint	
18-12	8049-8050	)	
18-13	8050-8051	,	
18-14	8052-8055	200	
18-15	8055-8056	)	
18-16	8057-8058	+	
18-17	8058-8059	-	
18-18	8060-8061	*	
18-19	8061-8062	(	
18-20	8062-8063	1	
18-21	8063-8064	)	
18-22	8065-8071	Filter	
18-23	8072-8073	(	
18-24	8073-8077	cast	
18-25	8077-8078	(	
18-26	8078-8081	key	
18-27	8081-8083	#6	
18-28	8084-8086	as	
18-29	8087-8090	int	
18-30	8090-8091	)	
18-31	8092-8093	>	
18-32	8094-8097	100	
18-33	8097-8098	)	
18-34	8099-8100	+	
18-35	8100-8101	-	
18-36	8102-8103	*	
18-37	8103-8104	(	
18-38	8104-8105	1	
18-39	8105-8106	)	
18-40	8107-8111	Scan	
18-41	8112-8124	JDBCRelation	
18-42	8124-8125	(	
18-43	8125-8130	users	
18-44	8130-8131	)	
18-45	8132-8133	[	
18-46	8133-8146	numPartitions	
18-47	8146-8147	=	
18-48	8147-8148	1	
18-49	8148-8149	]	
18-50	8150-8151	[	
18-51	8151-8154	key	
18-52	8154-8156	#6	
18-53	8156-8157	,	
18-54	8157-8161	name	
18-55	8161-8163	#7	
18-56	8163-8164	]	
18-57	8165-8178	PushedFilters	
18-58	8178-8179	:	
18-59	8180-8181	[	
18-60	8181-8182	*	
18-61	8182-8191	IsNotNull	
18-62	8191-8192	(	
18-63	8192-8195	key	
18-64	8195-8196	)	
18-65	8196-8197	]	
18-66	8197-8198	,	
18-67	8199-8209	ReadSchema	
18-68	8209-8210	:	
18-69	8211-8217	struct	
18-70	8217-8218	<	
18-71	8218-8221	key	
18-72	8221-8222	:	
18-73	8222-8228	string	
18-74	8228-8229	,	
18-75	8229-8233	name	
18-76	8233-8234	:	
18-77	8234-8240	string	
18-78	8240-8241	>	

#Text=+- *(4) Sort [key#2L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(key#2L, 200) +- *(3) Project [id#0L AS key#2L, rand(12) AS value#3] +- *(3) Range (0, 160000, step=1, splits=1)
19-1	8242-8243	+	
19-2	8243-8244	-	
19-3	8245-8246	*	
19-4	8246-8247	(	
19-5	8247-8248	4	
19-6	8248-8249	)	
19-7	8250-8254	Sort	
19-8	8255-8256	[	
19-9	8256-8259	key	
19-10	8259-8262	#2L	
19-11	8263-8266	ASC	
19-12	8267-8272	NULLS	
19-13	8273-8278	FIRST	
19-14	8278-8279	]	
19-15	8279-8280	,	
19-16	8281-8286	false	
19-17	8286-8287	,	
19-18	8288-8289	0	
19-19	8290-8291	+	
19-20	8291-8292	-	
19-21	8293-8301	Exchange	
19-22	8302-8318	hashpartitioning	
19-23	8318-8319	(	
19-24	8319-8322	key	
19-25	8322-8325	#2L	
19-26	8325-8326	,	
19-27	8327-8330	200	
19-28	8330-8331	)	
19-29	8332-8333	+	
19-30	8333-8334	-	
19-31	8335-8336	*	
19-32	8336-8337	(	
19-33	8337-8338	3	
19-34	8338-8339	)	
19-35	8340-8347	Project	
19-36	8348-8349	[	
19-37	8349-8351	id	
19-38	8351-8354	#0L	
19-39	8355-8357	AS	
19-40	8358-8361	key	
19-41	8361-8364	#2L	
19-42	8364-8365	,	
19-43	8366-8370	rand	
19-44	8370-8371	(	
19-45	8371-8373	12	
19-46	8373-8374	)	
19-47	8375-8377	AS	
19-48	8378-8383	value	
19-49	8383-8385	#3	
19-50	8385-8386	]	
19-51	8387-8388	+	
19-52	8388-8389	-	
19-53	8390-8391	*	
19-54	8391-8392	(	
19-55	8392-8393	3	
19-56	8393-8394	)	
19-57	8395-8400	Range	
19-58	8401-8402	(	
19-59	8402-8403	0	
19-60	8403-8404	,	
19-61	8405-8411	160000	
19-62	8411-8412	,	
19-63	8413-8417	step	
19-64	8417-8418	=	
19-65	8418-8419	1	
19-66	8419-8420	,	
19-67	8421-8427	splits	
19-68	8427-8428	=	
19-69	8428-8429	1	
19-70	8429-8430	)	

#Text=Spark does not "own" any storage, so it does not build indexes, B-trees, etc. on the disk. (although support for parquet files, if used well, may give you some related features). Spark has been optimized for the volume, variety, etc. of big data — so, traditionally, it is not designed to maintain and use statistics. about the dataset. For example, in cases where the DBMS may know that a particular filter will remove most records and apply it early in the query, Spark does not know this fact and will not perform such optimization. transactions = spark.range(160000) \\ .select(F.col('id').alias('key'), F.rand(12).alias('value')) users = spark.read.jdbc(
20-1	8431-8436	Spark	
20-2	8437-8441	does	
20-3	8442-8445	not	
20-4	8446-8447	"	
20-5	8447-8450	own	
20-6	8450-8451	"	
20-7	8452-8455	any	
20-8	8456-8463	storage	
20-9	8463-8464	,	
20-10	8465-8467	so	
20-11	8468-8470	it	
20-12	8471-8475	does	
20-13	8476-8479	not	
20-14	8480-8485	build	
20-15	8486-8493	indexes	
20-16	8493-8494	,	
20-17	8495-8502	B-trees	
20-18	8502-8503	,	
20-19	8504-8507	etc	
20-20	8507-8508	.	
20-21	8509-8511	on	
20-22	8512-8515	the	
20-23	8516-8520	disk	
20-24	8520-8521	.	
20-25	8522-8523	(	
20-26	8523-8531	although	
20-27	8532-8539	support	
20-28	8540-8543	for	
20-29	8544-8551	parquet	
20-30	8552-8557	files	
20-31	8557-8558	,	
20-32	8559-8561	if	
20-33	8562-8566	used	
20-34	8567-8571	well	
20-35	8571-8572	,	
20-36	8573-8576	may	
20-37	8577-8581	give	
20-38	8582-8585	you	
20-39	8586-8590	some	
20-40	8591-8598	related	
20-41	8599-8607	features	
20-42	8607-8608	)	
20-43	8608-8609	.	
20-44	8610-8615	Spark	
20-45	8616-8619	has	
20-46	8620-8624	been	
20-47	8625-8634	optimized	
20-48	8635-8638	for	
20-49	8639-8642	the	
20-50	8643-8649	volume	
20-51	8649-8650	,	
20-52	8651-8658	variety	
20-53	8658-8659	,	
20-54	8660-8663	etc	
20-55	8663-8664	.	
20-56	8665-8667	of	
20-57	8668-8671	big	
20-58	8672-8676	data	
20-59	8677-8678	—	
20-60	8679-8681	so	
20-61	8681-8682	,	
20-62	8683-8696	traditionally	
20-63	8696-8697	,	
20-64	8698-8700	it	
20-65	8701-8703	is	
20-66	8704-8707	not	
20-67	8708-8716	designed	
20-68	8717-8719	to	
20-69	8720-8728	maintain	
20-70	8729-8732	and	
20-71	8733-8736	use	
20-72	8737-8747	statistics	
20-73	8747-8748	.	
20-74	8749-8754	about	
20-75	8755-8758	the	
20-76	8759-8766	dataset	
20-77	8766-8767	.	
20-78	8768-8771	For	
20-79	8772-8779	example	
20-80	8779-8780	,	
20-81	8781-8783	in	
20-82	8784-8789	cases	
20-83	8790-8795	where	
20-84	8796-8799	the	
20-85	8800-8804	DBMS	
20-86	8805-8808	may	
20-87	8809-8813	know	
20-88	8814-8818	that	
20-89	8819-8820	a	
20-90	8821-8831	particular	
20-91	8832-8838	filter	
20-92	8839-8843	will	
20-93	8844-8850	remove	
20-94	8851-8855	most	
20-95	8856-8863	records	
20-96	8864-8867	and	
20-97	8868-8873	apply	
20-98	8874-8876	it	
20-99	8877-8882	early	
20-100	8883-8885	in	
20-101	8886-8889	the	
20-102	8890-8895	query	
20-103	8895-8896	,	
20-104	8897-8902	Spark	
20-105	8903-8907	does	
20-106	8908-8911	not	
20-107	8912-8916	know	
20-108	8917-8921	this	
20-109	8922-8926	fact	
20-110	8927-8930	and	
20-111	8931-8935	will	
20-112	8936-8939	not	
20-113	8940-8947	perform	
20-114	8948-8952	such	
20-115	8953-8965	optimization	
20-116	8965-8966	.	
20-117	8967-8979	transactions	
20-118	8980-8981	=	
20-119	8982-8993	spark.range	
20-120	8993-8994	(	
20-121	8994-9000	160000	
20-122	9000-9001	)	
20-123	9002-9003	\	
20-124	9004-9005	.	
20-125	9005-9011	select	
20-126	9011-9012	(	
20-127	9012-9017	F.col	
20-128	9017-9018	(	
20-129	9018-9019	'	
20-130	9019-9021	id	
20-131	9021-9022	'	
20-132	9022-9023	)	
20-133	9023-9024	.	
20-134	9024-9029	alias	
20-135	9029-9030	(	
20-136	9030-9031	'	
20-137	9031-9034	key	
20-138	9034-9035	'	
20-139	9035-9036	)	
20-140	9036-9037	,	
20-141	9038-9044	F.rand	
20-142	9044-9045	(	
20-143	9045-9047	12	
20-144	9047-9048	)	
20-145	9048-9049	.	
20-146	9049-9054	alias	
20-147	9054-9055	(	
20-148	9055-9056	'	
20-149	9056-9061	value	
20-150	9061-9062	'	
20-151	9062-9063	)	
20-152	9063-9064	)	
20-153	9065-9070	users	
20-154	9071-9072	=	
20-155	9073-9088	spark.read.jdbc	
20-156	9088-9089	(	

#Text=table='users', url='jdbc:postgresql://localhost:5432/postgres') users.join(transactions, 'key') \\ .drop_duplicates(['key']) \\ .explain() This gives us one more Exchange(shuffle) but not a predicate pushdown of distinct operation into the database. We need to rewrite it ourselves to do distinct first.
21-1	9090-9095	table	
21-2	9095-9096	=	
21-3	9096-9097	'	
21-4	9097-9102	users	
21-5	9102-9103	'	
21-6	9103-9104	,	
21-7	9105-9108	url	
21-8	9108-9109	=	
21-9	9109-9110	'	
21-10	9110-9114	jdbc	
21-11	9114-9115	:	
21-12	9115-9125	postgresql	
21-13	9125-9126	:	
21-14	9126-9127	/	
21-15	9127-9128	/	
21-16	9128-9137	localhost	
21-17	9137-9138	:	
21-18	9138-9142	5432	
21-19	9142-9143	/	
21-20	9143-9151	postgres	
21-21	9151-9152	'	
21-22	9152-9153	)	
21-23	9154-9164	users.join	
21-24	9164-9165	(	
21-25	9165-9177	transactions	
21-26	9177-9178	,	
21-27	9179-9180	'	
21-28	9180-9183	key	
21-29	9183-9184	'	
21-30	9184-9185	)	
21-31	9186-9187	\	
21-32	9188-9189	.	
21-33	9189-9204	drop_duplicates	
21-34	9204-9205	(	
21-35	9205-9206	[	
21-36	9206-9207	'	
21-37	9207-9210	key	
21-38	9210-9211	'	
21-39	9211-9212	]	
21-40	9212-9213	)	
21-41	9214-9215	\	
21-42	9216-9217	.	
21-43	9217-9224	explain	
21-44	9224-9225	(	
21-45	9225-9226	)	
21-46	9227-9231	This	
21-47	9232-9237	gives	
21-48	9238-9240	us	
21-49	9241-9244	one	
21-50	9245-9249	more	
21-51	9250-9258	Exchange	
21-52	9258-9259	(	
21-53	9259-9266	shuffle	
21-54	9266-9267	)	
21-55	9268-9271	but	
21-56	9272-9275	not	
21-57	9276-9277	a	
21-58	9278-9287	predicate	
21-59	9288-9296	pushdown	
21-60	9297-9299	of	
21-61	9300-9308	distinct	
21-62	9309-9318	operation	
21-63	9319-9323	into	
21-64	9324-9327	the	
21-65	9328-9336	database	
21-66	9336-9337	.	
21-67	9338-9340	We	
21-68	9341-9345	need	
21-69	9346-9348	to	
21-70	9349-9356	rewrite	
21-71	9357-9359	it	
21-72	9360-9369	ourselves	
21-73	9370-9372	to	
21-74	9373-9375	do	
21-75	9376-9384	distinct	
21-76	9385-9390	first	
21-77	9390-9391	.	

#Text=== Physical Plan == *(6) HashAggregate(keys=[key#6], functions=[first(key#2L, false), first(value#3, false)]) +- Exchange hashpartitioning(key#6, 200)
22-1	9392-9393	=	
22-2	9393-9394	=	
22-3	9395-9403	Physical	
22-4	9404-9408	Plan	
22-5	9409-9410	=	
22-6	9410-9411	=	
22-7	9412-9413	*	
22-8	9413-9414	(	
22-9	9414-9415	6	
22-10	9415-9416	)	
22-11	9417-9430	HashAggregate	
22-12	9430-9431	(	
22-13	9431-9435	keys	
22-14	9435-9436	=	
22-15	9436-9437	[	
22-16	9437-9440	key	
22-17	9440-9442	#6	
22-18	9442-9443	]	
22-19	9443-9444	,	
22-20	9445-9454	functions	
22-21	9454-9455	=	
22-22	9455-9456	[	
22-23	9456-9461	first	
22-24	9461-9462	(	
22-25	9462-9465	key	
22-26	9465-9468	#2L	
22-27	9468-9469	,	
22-28	9470-9475	false	
22-29	9475-9476	)	
22-30	9476-9477	,	
22-31	9478-9483	first	
22-32	9483-9484	(	
22-33	9484-9489	value	
22-34	9489-9491	#3	
22-35	9491-9492	,	
22-36	9493-9498	false	
22-37	9498-9499	)	
22-38	9499-9500	]	
22-39	9500-9501	)	
22-40	9502-9503	+	
22-41	9503-9504	-	
22-42	9505-9513	Exchange	
22-43	9514-9530	hashpartitioning	
22-44	9530-9531	(	
22-45	9531-9534	key	
22-46	9534-9536	#6	
22-47	9536-9537	,	
22-48	9538-9541	200	
22-49	9541-9542	)	

#Text=+- *(5) HashAggregate(keys=[key#6], functions=[partial_first(key#2L, false), partial_first(value#3, false)]) +- *(5) SortMergeJoin [cast(key#6 as bigint)], [key#2L], Inner :- *(2) Sort [cast(key#6 as bigint) ASC NULLS FIRST], false, 0
23-1	9543-9544	+	
23-2	9544-9545	-	
23-3	9546-9547	*	
23-4	9547-9548	(	
23-5	9548-9549	5	
23-6	9549-9550	)	
23-7	9551-9564	HashAggregate	
23-8	9564-9565	(	
23-9	9565-9569	keys	
23-10	9569-9570	=	
23-11	9570-9571	[	
23-12	9571-9574	key	
23-13	9574-9576	#6	
23-14	9576-9577	]	
23-15	9577-9578	,	
23-16	9579-9588	functions	
23-17	9588-9589	=	
23-18	9589-9590	[	
23-19	9590-9603	partial_first	
23-20	9603-9604	(	
23-21	9604-9607	key	
23-22	9607-9610	#2L	
23-23	9610-9611	,	
23-24	9612-9617	false	
23-25	9617-9618	)	
23-26	9618-9619	,	
23-27	9620-9633	partial_first	
23-28	9633-9634	(	
23-29	9634-9639	value	
23-30	9639-9641	#3	
23-31	9641-9642	,	
23-32	9643-9648	false	
23-33	9648-9649	)	
23-34	9649-9650	]	
23-35	9650-9651	)	
23-36	9652-9653	+	
23-37	9653-9654	-	
23-38	9655-9656	*	
23-39	9656-9657	(	
23-40	9657-9658	5	
23-41	9658-9659	)	
23-42	9660-9673	SortMergeJoin	
23-43	9674-9675	[	
23-44	9675-9679	cast	
23-45	9679-9680	(	
23-46	9680-9683	key	
23-47	9683-9685	#6	
23-48	9686-9688	as	
23-49	9689-9695	bigint	
23-50	9695-9696	)	
23-51	9696-9697	]	
23-52	9697-9698	,	
23-53	9699-9700	[	
23-54	9700-9703	key	
23-55	9703-9706	#2L	
23-56	9706-9707	]	
23-57	9707-9708	,	
23-58	9709-9714	Inner	
23-59	9715-9716	:	
23-60	9716-9717	-	
23-61	9718-9719	*	
23-62	9719-9720	(	
23-63	9720-9721	2	
23-64	9721-9722	)	
23-65	9723-9727	Sort	
23-66	9728-9729	[	
23-67	9729-9733	cast	
23-68	9733-9734	(	
23-69	9734-9737	key	
23-70	9737-9739	#6	
23-71	9740-9742	as	
23-72	9743-9749	bigint	
23-73	9749-9750	)	
23-74	9751-9754	ASC	
23-75	9755-9760	NULLS	
23-76	9761-9766	FIRST	
23-77	9766-9767	]	
23-78	9767-9768	,	
23-79	9769-9774	false	
23-80	9774-9775	,	
23-81	9776-9777	0	

#Text=+- Exchange hashpartitioning(cast(key#6 as bigint), 200) +- *(1) Scan JDBCRelation(users) [numPartitions=1] [key#6] PushedFilters: [*IsNotNull(key)], ReadSchema: struct<key:string>
24-1	9778-9779	+	
24-2	9779-9780	-	
24-3	9781-9789	Exchange	
24-4	9790-9806	hashpartitioning	
24-5	9806-9807	(	
24-6	9807-9811	cast	
24-7	9811-9812	(	
24-8	9812-9815	key	
24-9	9815-9817	#6	
24-10	9818-9820	as	
24-11	9821-9827	bigint	
24-12	9827-9828	)	
24-13	9828-9829	,	
24-14	9830-9833	200	
24-15	9833-9834	)	
24-16	9835-9836	+	
24-17	9836-9837	-	
24-18	9838-9839	*	
24-19	9839-9840	(	
24-20	9840-9841	1	
24-21	9841-9842	)	
24-22	9843-9847	Scan	
24-23	9848-9860	JDBCRelation	
24-24	9860-9861	(	
24-25	9861-9866	users	
24-26	9866-9867	)	
24-27	9868-9869	[	
24-28	9869-9882	numPartitions	
24-29	9882-9883	=	
24-30	9883-9884	1	
24-31	9884-9885	]	
24-32	9886-9887	[	
24-33	9887-9890	key	
24-34	9890-9892	#6	
24-35	9892-9893	]	
24-36	9894-9907	PushedFilters	
24-37	9907-9908	:	
24-38	9909-9910	[	
24-39	9910-9911	*	
24-40	9911-9920	IsNotNull	
24-41	9920-9921	(	
24-42	9921-9924	key	
24-43	9924-9925	)	
24-44	9925-9926	]	
24-45	9926-9927	,	
24-46	9928-9938	ReadSchema	
24-47	9938-9939	:	
24-48	9940-9946	struct	
24-49	9946-9947	<	
24-50	9947-9950	key	
24-51	9950-9951	:	
24-52	9951-9957	string	
24-53	9957-9958	>	

#Text=+- *(4) Sort [key#2L ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(key#2L, 200) +- *(3) Project [id#0L AS key#2L, rand(12) AS value#3] +- *(3) Range (0, 160000, step=1, splits=1)
25-1	9959-9960	+	
25-2	9960-9961	-	
25-3	9962-9963	*	
25-4	9963-9964	(	
25-5	9964-9965	4	
25-6	9965-9966	)	
25-7	9967-9971	Sort	
25-8	9972-9973	[	
25-9	9973-9976	key	
25-10	9976-9979	#2L	
25-11	9980-9983	ASC	
25-12	9984-9989	NULLS	
25-13	9990-9995	FIRST	
25-14	9995-9996	]	
25-15	9996-9997	,	
25-16	9998-10003	false	
25-17	10003-10004	,	
25-18	10005-10006	0	
25-19	10007-10008	+	
25-20	10008-10009	-	
25-21	10010-10018	Exchange	
25-22	10019-10035	hashpartitioning	
25-23	10035-10036	(	
25-24	10036-10039	key	
25-25	10039-10042	#2L	
25-26	10042-10043	,	
25-27	10044-10047	200	
25-28	10047-10048	)	
25-29	10049-10050	+	
25-30	10050-10051	-	
25-31	10052-10053	*	
25-32	10053-10054	(	
25-33	10054-10055	3	
25-34	10055-10056	)	
25-35	10057-10064	Project	
25-36	10065-10066	[	
25-37	10066-10068	id	
25-38	10068-10071	#0L	
25-39	10072-10074	AS	
25-40	10075-10078	key	
25-41	10078-10081	#2L	
25-42	10081-10082	,	
25-43	10083-10087	rand	
25-44	10087-10088	(	
25-45	10088-10090	12	
25-46	10090-10091	)	
25-47	10092-10094	AS	
25-48	10095-10100	value	
25-49	10100-10102	#3	
25-50	10102-10103	]	
25-51	10104-10105	+	
25-52	10105-10106	-	
25-53	10107-10108	*	
25-54	10108-10109	(	
25-55	10109-10110	3	
25-56	10110-10111	)	
25-57	10112-10117	Range	
25-58	10118-10119	(	
25-59	10119-10120	0	
25-60	10120-10121	,	
25-61	10122-10128	160000	
25-62	10128-10129	,	
25-63	10130-10134	step	
25-64	10134-10135	=	
25-65	10135-10136	1	
25-66	10136-10137	,	
25-67	10138-10144	splits	
25-68	10144-10145	=	
25-69	10145-10146	1	
25-70	10146-10147	)	

#Text=To sum up, use Spark DataFrames, release the power of [Catalyst Optimizer] (https://databricks.com/glossary/catalyst-optimizer) and [Tungsten execution engine] (https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html). You do not have to waste time tuning your RDDs, you can focus on the business problem. Understandable code The code in RDD expresses more how of a solution better than what. It is a bit difficult to read the code based on RDD and understand what is going on.
26-1	10148-10150	To	
26-2	10151-10154	sum	
26-3	10155-10157	up	
26-4	10157-10158	,	
26-5	10159-10162	use	
26-6	10163-10168	Spark	
26-7	10169-10179	DataFrames	
26-8	10179-10180	,	
26-9	10181-10188	release	
26-10	10189-10192	the	
26-11	10193-10198	power	
26-12	10199-10201	of	
26-13	10202-10203	[	
26-14	10203-10211	Catalyst	
26-15	10212-10221	Optimizer	
26-16	10221-10222	]	
26-17	10223-10224	(	
26-18	10224-10229	https	
26-19	10229-10230	:	
26-20	10230-10231	/	
26-21	10231-10232	/	
26-22	10232-10246	databricks.com	
26-23	10246-10247	/	
26-24	10247-10255	glossary	
26-25	10255-10256	/	
26-26	10256-10274	catalyst-optimizer	
26-27	10274-10275	)	
26-28	10276-10279	and	
26-29	10280-10281	[	
26-30	10281-10289	Tungsten	
26-31	10290-10299	execution	
26-32	10300-10306	engine	
26-33	10306-10307	]	
26-34	10308-10309	(	
26-35	10309-10314	https	
26-36	10314-10315	:	
26-37	10315-10316	/	
26-38	10316-10317	/	
26-39	10317-10331	databricks.com	
26-40	10331-10332	/	
26-41	10332-10336	blog	
26-42	10336-10337	/	
26-43	10337-10341	2015	
26-44	10341-10342	/	
26-45	10342-10344	04	
26-46	10344-10345	/	
26-47	10345-10347	28	
26-48	10347-10348	/	
26-49	10348-10405	project-tungsten-bringing-spark-closer-to-bare-metal.html	
26-50	10405-10406	)	
26-51	10406-10407	.	
26-52	10408-10411	You	
26-53	10412-10414	do	
26-54	10415-10418	not	
26-55	10419-10423	have	
26-56	10424-10426	to	
26-57	10427-10432	waste	
26-58	10433-10437	time	
26-59	10438-10444	tuning	
26-60	10445-10449	your	
26-61	10450-10454	RDDs	
26-62	10454-10455	,	
26-63	10456-10459	you	
26-64	10460-10463	can	
26-65	10464-10469	focus	
26-66	10470-10472	on	
26-67	10473-10476	the	
26-68	10477-10485	business	
26-69	10486-10493	problem	
26-70	10493-10494	.	
26-71	10495-10509	Understandable	
26-72	10510-10514	code	
26-73	10515-10518	The	
26-74	10519-10523	code	
26-75	10524-10526	in	
26-76	10527-10530	RDD	
26-77	10531-10540	expresses	
26-78	10541-10545	more	
26-79	10546-10549	how	
26-80	10550-10552	of	
26-81	10553-10554	a	
26-82	10555-10563	solution	
26-83	10564-10570	better	
26-84	10571-10575	than	
26-85	10576-10580	what	
26-86	10580-10581	.	
26-87	10582-10584	It	
26-88	10585-10587	is	
26-89	10588-10589	a	
26-90	10590-10593	bit	
26-91	10594-10603	difficult	
26-92	10604-10606	to	
26-93	10607-10611	read	
26-94	10612-10615	the	
26-95	10616-10620	code	
26-96	10621-10626	based	
26-97	10627-10629	on	
26-98	10630-10633	RDD	
26-99	10634-10637	and	
26-100	10638-10648	understand	
26-101	10649-10653	what	
26-102	10654-10656	is	
26-103	10657-10662	going	
26-104	10663-10665	on	
26-105	10665-10666	.	

#Text=Using DataFrame API it's easy to go one level up — to business logic. Since DataFrame API looks like SQL and there is a schema, it's easy to see the real work with data. So, using DataFrame API, you are closer to what you are trying to achieve and not to how you are doing it. You don't have to worry about how the DBMS find out if you need to perform a table scan or which indexes to use — all you need is a result. You express what you want, and you let Spark under the cover find the most effective way to do it.
27-1	10667-10672	Using	
27-2	10673-10682	DataFrame	
27-3	10683-10686	API	
27-4	10687-10691	it's	
27-5	10692-10696	easy	
27-6	10697-10699	to	
27-7	10700-10702	go	
27-8	10703-10706	one	
27-9	10707-10712	level	
27-10	10713-10715	up	
27-11	10716-10717	—	
27-12	10718-10720	to	
27-13	10721-10729	business	
27-14	10730-10735	logic	
27-15	10735-10736	.	
27-16	10737-10742	Since	
27-17	10743-10752	DataFrame	
27-18	10753-10756	API	
27-19	10757-10762	looks	
27-20	10763-10767	like	
27-21	10768-10771	SQL	
27-22	10772-10775	and	
27-23	10776-10781	there	
27-24	10782-10784	is	
27-25	10785-10786	a	
27-26	10787-10793	schema	
27-27	10793-10794	,	
27-28	10795-10799	it's	
27-29	10800-10804	easy	
27-30	10805-10807	to	
27-31	10808-10811	see	
27-32	10812-10815	the	
27-33	10816-10820	real	
27-34	10821-10825	work	
27-35	10826-10830	with	
27-36	10831-10835	data	
27-37	10835-10836	.	
27-38	10837-10839	So	
27-39	10839-10840	,	
27-40	10841-10846	using	
27-41	10847-10856	DataFrame	
27-42	10857-10860	API	
27-43	10860-10861	,	
27-44	10862-10865	you	
27-45	10866-10869	are	
27-46	10870-10876	closer	
27-47	10877-10879	to	
27-48	10880-10884	what	
27-49	10885-10888	you	
27-50	10889-10892	are	
27-51	10893-10899	trying	
27-52	10900-10902	to	
27-53	10903-10910	achieve	
27-54	10911-10914	and	
27-55	10915-10918	not	
27-56	10919-10921	to	
27-57	10922-10925	how	
27-58	10926-10929	you	
27-59	10930-10933	are	
27-60	10934-10939	doing	
27-61	10940-10942	it	
27-62	10942-10943	.	
27-63	10944-10947	You	
27-64	10948-10953	don't	
27-65	10954-10958	have	
27-66	10959-10961	to	
27-67	10962-10967	worry	
27-68	10968-10973	about	
27-69	10974-10977	how	
27-70	10978-10981	the	
27-71	10982-10986	DBMS	
27-72	10987-10991	find	
27-73	10992-10995	out	
27-74	10996-10998	if	
27-75	10999-11002	you	
27-76	11003-11007	need	
27-77	11008-11010	to	
27-78	11011-11018	perform	
27-79	11019-11020	a	
27-80	11021-11026	table	
27-81	11027-11031	scan	
27-82	11032-11034	or	
27-83	11035-11040	which	
27-84	11041-11048	indexes	
27-85	11049-11051	to	
27-86	11052-11055	use	
27-87	11056-11057	—	
27-88	11058-11061	all	
27-89	11062-11065	you	
27-90	11066-11070	need	
27-91	11071-11073	is	
27-92	11074-11075	a	
27-93	11076-11082	result	
27-94	11082-11083	.	
27-95	11084-11087	You	
27-96	11088-11095	express	
27-97	11096-11100	what	
27-98	11101-11104	you	
27-99	11105-11109	want	
27-100	11109-11110	,	
27-101	11111-11114	and	
27-102	11115-11118	you	
27-103	11119-11122	let	
27-104	11123-11128	Spark	
27-105	11129-11134	under	
27-106	11135-11138	the	
27-107	11139-11144	cover	
27-108	11145-11149	find	
27-109	11150-11153	the	
27-110	11154-11158	most	
27-111	11159-11168	effective	
27-112	11169-11172	way	
27-113	11173-11175	to	
27-114	11176-11178	do	
27-115	11179-11181	it	
27-116	11181-11182	.	

#Text=DataFrame-based API is the primary API for MLlib As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package. Recommended books
28-1	11183-11198	DataFrame-based	
28-2	11199-11202	API	
28-3	11203-11205	is	
28-4	11206-11209	the	
28-5	11210-11217	primary	
28-6	11218-11221	API	
28-7	11222-11225	for	
28-8	11226-11231	MLlib	
28-9	11232-11234	As	
28-10	11235-11237	of	
28-11	11238-11243	Spark	
28-12	11244-11247	2.0	
28-13	11247-11248	,	
28-14	11249-11252	the	
28-15	11253-11262	RDD-based	
28-16	11263-11267	APIs	
28-17	11268-11270	in	
28-18	11271-11274	the	
28-19	11275-11286	spark.mllib	
28-20	11287-11294	package	
28-21	11295-11299	have	
28-22	11300-11307	entered	
28-23	11308-11319	maintenance	
28-24	11320-11324	mode	
28-25	11324-11325	.	
28-26	11326-11329	The	
28-27	11330-11337	primary	
28-28	11338-11345	Machine	
28-29	11346-11354	Learning	
28-30	11355-11358	API	
28-31	11359-11362	for	
28-32	11363-11368	Spark	
28-33	11369-11371	is	
28-34	11372-11375	now	
28-35	11376-11379	the	
28-36	11380-11395	DataFrame-based	
28-37	11396-11399	API	
28-38	11400-11402	in	
28-39	11403-11406	the	
28-40	11407-11415	spark.ml	
28-41	11416-11423	package	
28-42	11423-11424	.	
28-43	11425-11436	Recommended	
28-44	11437-11442	books	

#Text=Spark: The Definitive Guide Learning Spark: Lightning-Fast Data Analytics #big_data #spark #python Buy me a coffee More? Well, there you go: Azure Blob Storage with Pyspark Data Challenges in Big Data
29-1	11443-11448	Spark	
29-2	11448-11449	:	
29-3	11450-11453	The	
29-4	11454-11464	Definitive	
29-5	11465-11470	Guide	
29-6	11471-11479	Learning	
29-7	11480-11485	Spark	
29-8	11485-11486	:	
29-9	11487-11501	Lightning-Fast	
29-10	11502-11506	Data	
29-11	11507-11516	Analytics	
29-12	11517-11518	#	
29-13	11518-11526	big_data	
29-14	11527-11528	#	
29-15	11528-11533	spark	
29-16	11534-11535	#	
29-17	11535-11541	python	
29-18	11542-11545	Buy	
29-19	11546-11548	me	
29-20	11549-11550	a	
29-21	11551-11557	coffee	
29-22	11558-11562	More	
29-23	11562-11563	?	
29-24	11564-11568	Well	
29-25	11568-11569	,	
29-26	11570-11575	there	
29-27	11576-11579	you	
29-28	11580-11582	go	
29-29	11582-11583	:	
29-30	11584-11589	Azure	
29-31	11590-11594	Blob	
29-32	11595-11602	Storage	
29-33	11603-11607	with	
29-34	11608-11615	Pyspark	
29-35	11616-11620	Data	
29-36	11621-11631	Challenges	
29-37	11632-11634	in	
29-38	11635-11638	Big	
29-39	11639-11643	Data	
