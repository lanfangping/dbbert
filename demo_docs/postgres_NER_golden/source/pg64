Apache Flink 1.12 Documentation: JDBC SQL Connector v1.12 Home Try Flink Local Installation Fraud Detection with the DataStream API Real Time Reporting with the Table API
Flink Operations Playground Learn Flink Overview Intro to the DataStream API Data Pipelines & ETL Streaming Analytics Event-driven Applications Fault Tolerance Concepts Overview Stateful Stream Processing
Timely Stream Processing Flink Architecture Glossary Application Development DataStream API Overview Execution Mode (Batch/Streaming) Event Time Overview Generating Watermarks Builtin Watermark Generators
State & Fault Tolerance Overview Working with State The Broadcast State Pattern Checkpointing Queryable State State Backends State Schema Evolution Custom State Serialization User-Defined Functions Operators Overview
Windows Joining Process Function Async I/O Data Sources Side Outputs Handling Application Parameters Testing Experimental Features Scala API Extensions Java Lambda Expressions Project Configuration
DataSet API Overview Transformations Iterations Zipping Elements Hadoop Compatibility Local Execution Cluster Execution Batch Examples Table API & SQL Overview Concepts & Common API
Streaming Concepts Overview Dynamic Tables Time Attributes Versioned Tables Joins in Continuous Queries Detecting Patterns Query Configuration Legacy Features Data Types Table API SQL
Overview Queries CREATE Statements DROP Statements ALTER Statements INSERT Statement SQL Hints DESCRIBE Statements EXPLAIN Statements USE Statements SHOW Statements
Functions Overview System (Built-in) Functions User-defined Functions Modules Catalogs SQL Client Configuration Performance Tuning Streaming Aggregation User-defined Sources & Sinks
Python API Overview Installation Table API Tutorial DataStream API Tutorial Table API User's Guide Intro to the Python Table API TableEnvironment Operations Data Types
System (Built-in) Functions User Defined Functions General User-defined Functions Vectorized User-defined Functions Conversions between PyFlink Table and Pandas DataFrame Dependency Management SQL Catalogs
Metrics Connectors DataStream API User's Guide Data Types Operators Dependency Management Configuration Environment Variables FAQ Data Types & Serialization Overview
Custom Serializers Managing Execution Execution Configuration Program Packaging Parallel Execution Execution Plans Task Failure Recovery API Migration Guides Libraries Event Processing (CEP) State Processor API
Graphs: Gelly Overview Graph API Iterative Graph Processing Library Methods Graph Algorithms Graph Generators Bipartite Graph Connectors DataStream Connectors
Overview Fault Tolerance Guarantees Kafka Cassandra Kinesis Elasticsearch File Sink Streaming File Sink RabbitMQ NiFi Google Cloud PubSub Twitter
JDBC Table & SQL Connectors Overview Formats Overview CSV JSON Confluent Avro Avro Debezium Canal Maxwell Parquet Orc
Raw Kafka Upsert Kafka Kinesis JDBC Elasticsearch FileSystem HBase DataGen Print BlackHole Hive Overview Hive Catalog
Hive Dialect Hive Read & Write Hive Functions Download DataSet Connectors Deployment Overview Resource Providers Standalone Overview Docker Kubernetes
Native Kubernetes YARN Mesos Configuration Memory Configuration Set up Flink's Process Memory Set up TaskManager Memory Set up JobManager Memory Memory tuning guide Troubleshooting
Migration Guide Command-Line Interface File Systems Overview Common Configurations Amazon S3 Aliyun OSS Azure Blob Storage Plugins High Availability (HA) Overview
ZooKeeper HA Services Kubernetes HA Services Metric Reporters Security SSL Setup Kerberos REPLs Python REPL Scala REPL Advanced External Resources History Server Logging
Operations State & Fault Tolerance Checkpoints Savepoints State Backends Tuning Checkpoints and Large State Metrics REST API Debugging Debugging Windows & Event Time Debugging Classloading
Application Profiling & Debugging Monitoring Monitoring Checkpointing Monitoring Back Pressure Upgrading Applications and Flink Versions Production Readiness Checklist Flink Development Importing Flink into an IDE Building Flink from Source Internals
Jobs and Scheduling Task Lifecycle File Systems Javadocs Scaladocs Pythondocs Project Page Pick Docs Version v1.11 v1.10 v1.9 v1.8 v1.7
v1.6 v1.5 v1.4 v1.3 v1.2 v1.1 v1.0 中文版 Connectors Table & SQL Connectors JDBC
JDBC SQL Connector Scan Source: Bounded Lookup Source: Sync Mode Sink: Batch Sink: Streaming Append & Upsert Mode Dependencies How to create a JDBC table Connector Options
Features Key handling Partitioned Scan Lookup Cache Idempotent Writes Postgres Database as a Catalog Data Type Mapping
The JDBC connector allows for reading data from and writing data into any relational databases with a JDBC driver. This document describes how to setup the JDBC connector to run SQL queries against relational databases. The JDBC sink operate in upsert mode for exchange UPDATE/DELETE messages with the external system if a primary key is defined on the DDL, otherwise, it operates in append mode and doesn’t support to consume UPDATE/DELETE messages. Dependencies
In order to use the JDBC connector the following dependencies are required for both projects using a build automation tool (such as Maven or SBT) and SQL Client with SQL JAR bundles. Maven dependency SQL Client JAR <dependency>
<groupId>org.apache.flink</groupId> <artifactId>flink-connector-jdbc_2.11</artifactId> <version>1.12.0</version> </dependency> Download A driver dependency is also required to connect to a specified database. Here are drivers currently supported:
Driver Group Id Artifact Id JAR MySQL mysql mysql-connector-java Download PostgreSQL org.postgresql postgresql
Download Derby org.apache.derby derby Download JDBC connector and drivers are not currently part of Flink’s binary distribution. See how to link with them for cluster execution here. How to create a JDBC table
The JDBC table can be defined as following: -- register a MySQL table 'users' in Flink SQL CREATE TABLE MyUserTable ( id BIGINT, name STRING, age INT, status BOOLEAN,
PRIMARY KEY (id) NOT ENFORCED ) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://localhost:3306/mydatabase', 'table-name' = 'users'
-- write data into the JDBC table from the other table "T" INSERT INTO MyUserTable SELECT id, name, age, status FROM T; -- scan data from the JDBC table SELECT id, name, age, status FROM MyUserTable;
-- temporal join the JDBC table as a dimension table SELECT * FROM myTopic LEFT JOIN MyUserTable FOR SYSTEM_TIME AS OF myTopic.proctime ON myTopic.key = MyUserTable.id;
Connector Options Option Required Default Type Description connector required (none) String Specify what connector to use, here should be 'jdbc'. url required
(none) String The JDBC database url. table-name required (none) String The name of JDBC table to connect. driver optional (none) String
The class name of the JDBC driver to use to connect to this URL, if not set, it will automatically be derived from the URL. username optional (none) String The JDBC user name. 'username' and 'password' must both be specified if any of them is specified. password optional (none) String The JDBC password.
scan.partition.column optional (none) String The column name used for partitioning the input. See the following Partitioned Scan section for more details. scan.partition.num optional (none) Integer The number of partitions.
scan.partition.lower-bound optional (none) Integer The smallest value of the first partition. scan.partition.upper-bound optional (none) Integer The largest value of the last partition. scan.fetch-size optional
Integer The number of rows that should be fetched from the database when reading per round trip. If the value specified is zero, then the hint is ignored. scan.auto-commit optional true Boolean Sets the auto-commit flag on the JDBC driver,
which determines whether each statement is committed in a transaction automatically. Some JDBC drivers, specifically Postgres, may require this to be set to false in order to stream results. lookup.cache.max-rows optional (none) Integer The max number of rows of lookup cache, over this value, the oldest rows will be expired.
Lookup cache is disabled by default. See the following Lookup Cache section for more details. lookup.cache.ttl optional (none) Duration The max time to live for each rows in lookup cache, over this time, the oldest rows will be expired. Lookup cache is disabled by default. See the following Lookup Cache section for more details.
lookup.max-retries optional Integer The max retry times if lookup database failed. sink.buffer-flush.max-rows optional 100 Integer The max size of buffered records before flush. Can be set to zero to disable it.
sink.buffer-flush.interval optional Duration The flush interval mills, over this time, asynchronous threads will flush data. Can be set to '0' to disable it. Note, 'sink.buffer-flush.max-rows' can be set to '0' with the flush interval set allowing for complete async processing of buffered actions.
sink.max-retries optional Integer The max retry times if writing records to database failed. Features Key handling Flink uses the primary key that defined in DDL when writing data to external databases. The connector operate in upsert mode if the primary key was defined, otherwise, the connector operate in append mode.
In upsert mode, Flink will insert a new row or update the existing row according to the primary key, Flink can ensure the idempotence in this way. To guarantee the output result is as expected, it’s recommended to define primary key for the table and make sure the primary key is one of the unique key sets or primary key of the underlying database table. In append mode, Flink will interpret all records as INSERT messages, the INSERT operation may fail if a primary key or unique constraint violation happens in the underlying database. See CREATE TABLE DDL for more details about PRIMARY KEY syntax. Partitioned Scan To accelerate reading data in parallel Source task instances, Flink provides partitioned scan feature for JDBC table.
All the following scan partition options must all be specified if any of them is specified. They describe how to partition the table when reading in parallel from multiple tasks. The scan.partition.column must be a numeric, date, or timestamp column from the table in question. Notice that scan.partition.lower-bound and scan.partition.upper-bound are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned.
scan.partition.column: The column name used for partitioning the input. scan.partition.num: The number of partitions. scan.partition.lower-bound: The smallest value of the first partition. scan.partition.upper-bound: The largest value of the last partition. Lookup Cache
JDBC connector can be used in temporal join as a lookup source (aka. dimension table). Currently, only sync lookup mode is supported. By default, lookup cache is not enabled. You can enable it by setting both lookup.cache.max-rows and lookup.cache.ttl. The lookup cache is used to improve performance of temporal join the JDBC connector. By default, lookup cache is not enabled, so all the requests are sent to external database.
When lookup cache is enabled, each process (i.e. TaskManager) will hold a cache. Flink will lookup the cache first, and only send requests to external database when cache missing, and update cache with the rows returned. The oldest rows in cache will be expired when the cache hit to the max cached rows lookup.cache.max-rows or when the row exceeds the max time to live lookup.cache.ttl.
The cached rows might not be the latest, users can tune lookup.cache.ttl to a smaller value to have a better fresh data, but this may increase the number of requests send to database. So this is a balance between throughput and correctness. Idempotent Writes JDBC sink will use upsert semantics rather than plain INSERT statements if primary key is defined in DDL. Upsert semantics refer to atomically adding a new row or updating the existing row if there is a unique constraint violation in the underlying database, which provides idempotence.
If there are failures, the Flink job will recover and re-process from last successful checkpoint, which can lead to re-processing messages during recovery. The upsert mode is highly recommended as it helps avoid constraint violations or duplicate data if records need to be re-processed. Aside from failure recovery, the source topic may also naturally contain multiple records over time with the same primary key, making upserts desirable. As there is no standard syntax for upsert, the following table describes the database-specific DML that is used. Database Upsert Grammar
MySQL INSERT .. ON DUPLICATE KEY UPDATE .. PostgreSQL INSERT .. ON CONFLICT .. DO UPDATE SET .. Postgres Database as a Catalog
The JdbcCatalog enables users to connect Flink to relational databases over JDBC protocol. Currently, PostgresCatalog is the only implementation of JDBC Catalog at the moment, PostgresCatalog only supports limited Catalog methods include: // The supported methods by Postgres Catalog. PostgresCatalog.databaseExists(String databaseName)
PostgresCatalog.listDatabases() PostgresCatalog.getDatabase(String databaseName) PostgresCatalog.listTables(String databaseName) PostgresCatalog.getTable(ObjectPath tablePath)
PostgresCatalog.tableExists(ObjectPath tablePath) Other Catalog methods is unsupported now. Usage of PostgresCatalog Please refer to Dependencies section for how to setup a JDBC connector and Postgres driver. Postgres catalog supports the following options:
name: required, name of the catalog. default-database: required, default database to connect to. username: required, username of Postgres account. password: required, password of the account. base-url: required, should be of format "jdbc:postgresql://<ip>:<port>", and should not contain database name here.
CREATE CATALOG mypg WITH( 'type' = 'jdbc', 'default-database' = '...', 'username' = '...', 'password' = '...', 'base-url' = '...'
USE CATALOG mypg; EnvironmentSettings settings = EnvironmentSettings.newInstance().inStreamingMode().build(); TableEnvironment tableEnv = TableEnvironment.create(settings); String name
= "mypg"; String defaultDatabase = "mydb"; String username = "..."; String password = "..."; String baseUrl = "..."
JdbcCatalog catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl); tableEnv.registerCatalog("mypg", catalog); // set the JdbcCatalog as the current catalog of the session tableEnv.useCatalog("mypg");
val settings = EnvironmentSettings.newInstance().inStreamingMode().build() val tableEnv = TableEnvironment.create(settings) val name = "mypg" val defaultDatabase = "mydb" val username = "..." val password
= "..." val baseUrl = "..." val catalog = new JdbcCatalog(name, defaultDatabase, username, password, baseUrl) tableEnv.registerCatalog("mypg", catalog)
// set the JdbcCatalog as the current catalog of the session tableEnv.useCatalog("mypg") from pyflink.table.catalog import JdbcCatalog environment_settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
t_env = StreamTableEnvironment.create(environment_settings=environment_settings) name = "mypg" default_database = "mydb" username = "..." password = "..." base_url = "..."
catalog = JdbcCatalog(name, default_database, username, password, base_url) t_env.register_catalog("mypg", catalog) # set the JdbcCatalog as the current catalog of the session t_env.use_catalog("mypg") execution: planner: blink
... current-catalog: mypg # set the JdbcCatalog as the current catalog of the session current-database: mydb catalogs: - name: mypg type: jdbc default-database: mydb
username: ... password: ... base-url: ... PostgresSQL Metaspace Mapping PostgresSQL has an additional namespace as schema besides database. A Postgres instance can have multiple databases, each database can have multiple schemas with a default one named “public”, each schema can have multiple tables.
In Flink, when querying tables registered by Postgres catalog, users can use either schema_name.table_name or just table_name. The schema_name is optional and defaults to “public”. Therefor the metaspace mapping between Flink Catalog and Postgres is as following: Flink Catalog Metaspace Structure Postgres Metaspace Structure catalog name (defined in Flink only)  database name database name table name
[schema_name.]table_name The full path of Postgres table in Flink should be "<catalog>.<db>.`<schema.table>`" if schema is specified, note the <schema.table> should be escaped. Here are some examples to access Postgres tables:
-- scan table 'test_table' of 'public' schema (i.e. the default schema), the schema name can be omitted SELECT * FROM mypg.mydb.test_table; SELECT * FROM mydb.test_table; SELECT * FROM test_table; -- scan table 'test_table2' of 'custom_schema' schema,
-- the custom schema can not be omitted and must be escaped with table. SELECT * FROM mypg.mydb.`custom_schema.test_table2` SELECT * FROM mydb.`custom_schema.test_table2`;
SELECT * FROM `custom_schema.test_table2`; Data Type Mapping Flink supports connect to several databases which uses dialect like MySQL, PostgresSQL, Derby. The Derby dialect usually used for testing purpose. The field data type mappings from relational databases data types to Flink SQL data types are listed in the following table, the mapping table can help define JDBC table in Flink easily.
MySQL type PostgreSQL type Flink SQL type TINYINT TINYINT SMALLINT TINYINT UNSIGNED SMALLINT INT2 SMALLSERIAL
SERIAL2 SMALLINT INT MEDIUMINT SMALLINT UNSIGNED INTEGER SERIAL INT BIGINT INT UNSIGNED BIGINT
BIGSERIAL BIGINT BIGINT UNSIGNED DECIMAL(20, 0) BIGINT BIGINT BIGINT FLOAT REAL FLOAT4
FLOAT DOUBLE DOUBLE PRECISION FLOAT8 DOUBLE PRECISION DOUBLE NUMERIC(p, s) DECIMAL(p, s)
NUMERIC(p, s) DECIMAL(p, s) DECIMAL(p, s) BOOLEAN TINYINT(1) BOOLEAN BOOLEAN DATE DATE
DATE TIME [(p)] TIME [(p)] [WITHOUT TIMEZONE] TIME [(p)] [WITHOUT TIMEZONE] DATETIME [(p)]
TIMESTAMP [(p)] [WITHOUT TIMEZONE] TIMESTAMP [(p)] [WITHOUT TIMEZONE] CHAR(n) VARCHAR(n) TEXT CHAR(n) CHARACTER(n)
VARCHAR(n) CHARACTER VARYING(n) TEXT STRING BINARY VARBINARY BLOB BYTEA BYTES ARRAY ARRAY
