Performance Guidelines | GitLab
GitLab Docs
GitLab.com (13.11-pre)
GitLab.com (13.11-pre)
13.10
13.9
13.8
12.10
11.11
Archives
Get free trial
GitLab Docs
Subscriptions
GitLab.com subscriptions
Storage usage quota
Self-managed subscriptions
Activate Enterprise Edition
Features available to Starter and Bronze subscribers
Installation
Requirements
Omnibus packages
Kubernetes
Docker
From source
Azure
Google Cloud Platform (GCP)
Amazon Web Services (AWS)
Administration
Reference Architectures
Up to 1,000 users
Up to 2,000 users
Up to 3,000 users
Up to 5,000 users
Up to 10,000 users
Up to 25,000 users
Up to 50,000 users
Troubleshooting
Authentication and authorization
Atlassian Crowd
Atlassian
Auth0
Authentiq
AWS Cognito
Azure
Bitbucket Cloud
CAS
Facebook
Generic OAuth2
GitHub
GitLab.com
Google
JWT
Kerberos
LDAP
LDAP (Google Secure)
LDAP Troubleshooting
OAuth service provider
Okta
OmniAuth
OpenID Connect OmniAuth
OpenID Connect identity
Salesforce
SAML
Smartcard
Twitter
Vault
Configuration and Admin Area
Admin Area settings
Appearance
Authentication
CI/CD
Custom instance-level project templates
Diff limits
Email
External pipeline validation
Feature flags
Geo nodes
Git LFS administration
GitLab Pages
Health Check
Incoming email
Job artifacts
Job logs
Labels
Log system
Maintenance Mode
Rate limits on issue creation
Rate limits on note creation
Reply by email
Repository checks
Repository size
Sign-in restrictions
Sign-up restrictions
System Hooks
Timezone
Uploads
User Cohorts
Security
Password storage
Custom password length limits
Generated passwords and integrated authentication
Credentials inventory
Limits on SSH keys
Rate limits
Webhooks
Information exclusivity
Reset user password
Unlock a user
Unknown sign-ins, email notification
Users pending approval
User file uploads
Manage the CRIME vulnerability
Enforce two-factor authentication (2FA)
User email confirmation
Security of running jobs
Proxying assets
CI/CD variables
Token overview
Abuse reports
Activate and deactivate users
Create users
Analytics
Audit events
Audit reports
Block and unblock users
Broadcast messages
Consul
Compliance features
Email from GitLab
Global user settings
Instance Review
Invalidate Markdown cache
Issue closing pattern
PostgreSQL
PgBouncer
Replication and failover
External database service
Load balancer
NFS
Redis
Sidekiq
Kubernetes Agent Server
Repository storage
Repository storage types
Gitaly and Gitaly Cluster
Configure Gitaly
Configure Gitaly Cluster
Gitaly reference
Gitaly timeouts
Metrics
Configure GitLab
Configure Grafana
GitHub imports
GitLab exporter
GitLab Prometheus metrics
GitLab self monitoring project
IP allowlist endpoints
Node exporter
PGBouncer exporter
PostgreSQL server exporter
Prometheus
Performance bar
Performance monitoring
Redis exporter
Registry exporter
Request profiling
Usage statistics
Object storage
Operations
Clean up Redis sessions
Fast SSH key lookup
Filesystem benchmarking
Move repositories
Multiple Sidekiq processes
Rails console
Sidekiq MemoryKiller
Switch to Puma
Unicorn
Use SSH certificates
Packages
Container Registry
Dependency Proxy
Geo
Setting up Geo
Database replication
External PostgreSQL instances
Configuration
Using a Geo site
Updating Geo nodes
Using object storage
Using Docker Registry
Geo for multiple servers
Geo security review
Location-aware Git remote URLs
Tuning Geo
Disable Geo
Removing a Geo site
Supported data types
Frequently asked questions
Troubleshooting
Validation tests
Geo Glossary
Disaster recovery (Geo)
Planned failover
Bring primary back
Automatic background verification
Rake tasks
Backup and restore
Clean up
Enable namespaces
General maintenance
Geo tasks
GitHub import
Import repositories
Integrity check
LDAP maintenance
List repositories
Praefect tasks
Project import and export
Repository storage
Sample Prometheus data
Uploads migration
Uploads sanitization
User management
Webhooks administration
X509 signatures
Server hooks
Static objects external storage
Terraform state
Update
Releases and maintenance
Analytics
Instance-level
DevOps Report
Usage Trends
Group-level
Contribution
DevOps Adoption
Insights
Issue
Productivity
Repository
Value Stream
Project-level
CI/CD
Code Review
Insights
Issue
Merge Request
Repository
Value Stream
Projects
Working with projects
Badges
Bulk editing (project)
Code intelligence
Code owners
Compliance
License Compliance
Compliance Dashboard
Description templates
Deploy keys
Deploy tokens
File finder
GitLab Pages
Getting started
Default domains, URLs, and baseurls
CI/CD for GitLab Pages
Custom domains and SSL/TLS certificates
Let's Encrypt integration
Access control
Redirects
Exploring GitLab Pages
Insights
Members
Migrating projects
Bitbucket Cloud
Bitbucket Server
ClearCase
CVS
FogBugz
Gemnasium
GitHub
GitLab.com
Gitea
Jira
Perforce Helix
Phabricator
Repo by manifest file
Repo by URL
SVN
TFVC
Push options
Releases
Repositories
Branches
Default branch
Git attributes
Git LFS
Locked files
File Blame
File History
Mirroring
Protected branches
Protected tags
Push rules
Reduce repository size
Signed Commits
Syntax highlighting
Web Editor
Web IDE
Requirements
Settings
Project Import/Export
Project/Group Import/Export rate limits
Project access tokens
Share projects
Snippets
Static Site Editor
Wikis
Groups
Bulk editing (group)
Contribution analytics
Custom group-level project templates
Epics
Epic boards
Manage epics
Group Import/Export
Insights
Issue analytics
Iterations
Migrating groups
Public access
Roadmaps
Repositories Analytics
SAML SSO for GitLab.com groups
Group Managed Accounts (Closed Beta)
SCIM provisioning
Troubleshooting Group SAML and SCIM
Subgroups
User
Abuse reports
Account
Active sessions
Delete account
Permissions
Personal access tokens
Profile preferences
Two-factor authentication
Discussions
Git
Partial clone
Troubleshooting Git
Useful commands
GitLab.com settings
Keyboard shortcuts
Markdown
AsciiDoc
Notification emails
Quick actions
Autocomplete characters
Reserved project and group names
Search
Advanced Search
SSH
Time tracking
To-Do lists
Issues
Award emoji
Confidential issues
Crosslinking issues
CSV export
CSV import
Design management
Due dates
Issue Boards
Issue data and actions
Labels
Managing issues
Milestones
Burndown and burnup charts
Multiple assignees
Linked issues
Service Desk
Sorting and ordering issue lists
Weight
Zoom meetings in issues
Merge requests
Allow collaboration
Approvals
Creating merge requests
Cherry pick changes
Drafts
Export Merge Requests to CSV
Getting started
Merge request dependencies
Fast forward
Merge when pipeline succeeds
Resolve conflicts
Reverting changes
Reviewing and managing merge requests
Squash and merge
Versions
Workflows
Draft merge requests
Operations
Metrics dashboard
Set up alerts for metrics
Embedding metrics in Markdown
Embedding metrics in Grafana
Custom dashboards
GitLab-defined metrics dashboards
Dashboard YAML properties
Dashboard settings
Panel types for dashboards
Using variables
Templates for custom dashboards
Templating variables for dashboards
Metrics library
Monitoring AWS resources
HAProxy
Kubernetes
NGINX
NGINX Ingress
NGINX Ingress VTS
Error Tracking
Tracing
Incident management
Alerts
Paging and notifications
Incidents
Integrations
Status page
On-call schedules
Feature Flags
Product analytics
CI/CD
Get started
CI/CD concepts
Migrate from CircleCI
Migrate from Jenkins
Enable or disable CI/CD
Pipelines
Schedule a pipeline
Trigger a pipeline
Pipeline settings
Pipeline architectures
Pipeline efficiency
Directed acyclic graph (DAG)
Multi-project pipelines
Parent-child pipelines
Pipelines for merge requests
Pipelines for merged results
Merge trains
Jobs
Access a terminal for a running job
Format scripts and job logs
Git submodules
Variables
Predefined variables
Where variables can be used
Environments and deployments
Environments Dashboard
Protected environments
Deployment safety
Roll out an application incrementally
Deploy to AWS
Review Apps
Runners
Best practices for large repositories
Cache and artifacts
Job artifacts
Pipeline artifacts
.gitlab-ci.yml
.gitlab-ci.yml reference
Validate syntax
Pipeline Editor
Include examples
Docker
Run CI/CD jobs in Docker containers
Use Docker to build Docker images
Use kaniko to build Docker images
Services
MySQL Service
PostgreSQL Service
Redis Service
Auto DevOps
Get started
Requirements
Stages
Customize
Upgrade PostgreSQL
Upgrade Auto Deploy dependencies
Troubleshooting
Testing
Accessibility testing
Browser performance testing
Code quality
Load performance testing
Metrics reports
Test cases
External integrations
SSH keys
Bitbucket Cloud
GitHub
Slack
CI/CD examples
Deployment with Dpl
End-to-end testing
NPM with semantic-release
PHP with PHPunit and atoum
PHP with NPM and SCP
PHP with Laravel and Envoy
Troubleshooting CI/CD
Application security
Security Configuration
Container Scanning
Threat Monitoring
Dependency Scanning
Dependency Scanning Analyzers
Dependency List
Static Application Security Testing
SAST Analyzers
Secret Detection
Dynamic Application Security Testing (DAST)
DAST Troubleshooting
API Fuzzing
Coverage Fuzzing
Security Dashboard
Offline Environments
Vulnerability Reports
Vulnerability Pages
Scan Policies
Security scanner integration
Secure and Protect Terminology
Packages & Registries
Package Registry
Composer
Conan
Go Proxy
Maven
npm
NuGet
PyPI
Ruby gems
Generic
Store all packages in one project
Container Registry
Dependency Proxy
Infrastructure
Adding and removing clusters
Add EKS clusters
Add GKE clusters
Group-level clusters
Instance-level clusters
Canary deployments
Cluster environments
Cluster cost management
Deploy boards
GitLab Managed Apps
Configuring Crossplane
Infrastructure as code
GitLab managed Terraform state
Terraform integration in merge requests
Kubernetes Agent
Agent configuration repository
Management project
Pod logs
Runbooks
Serverless
Deploying AWS Lambda functions
Securing your deployed applications
Web Application Firewall
Container Network Security
Container Host Security
Integrations
Akismet
Elasticsearch
Gitpod
Jira integrations
Jira
GitLab for Jira app
Jira DVCS connector
Jira Development Panel
Create Jira Server user
Create Jira Cloud API token
Kroki diagrams
PlantUML
Project integration management
Project integrations
Overview
Asana
Bamboo CI
Discord
Emails on push
GitHub
Hangouts Chat
Irker
Jenkins
Mattermost notifications
Mattermost slash commands
Microsoft Teams
Mock CI
Prometheus
Service templates
Slack notifications
Slack slash commands
Slack application
Unify Circuit
Webex Teams
Webhooks
External issue tracker
Bugzilla
Custom issue tracker
IBM Engineering Workflow Management
Redmine
YouTrack
Gmail actions buttons
reCAPTCHA
Security partners
Sourcegraph
Trello
API
Resources
.gitignore (templates)
.gitlab-ci.yml (templates)
Access requests
Appearance (application)
Applications
Audit events
Avatar
Award emoji
Badges (project)
Badges (group)
Branches
Broadcast messages
Clusters (project)
Clusters (group)
Clusters (instance)
Commits
Composer
Conan
Container Registry
Custom attributes
Dashboard annotations
Dependencies
Dependency Proxy
Deploy keys
Deploy tokens
Deployments
Discussions
Dockerfile (templates)
DORA4 metrics
DORA4 project analytics
DORA4 group analytics
Environments
Epics
Error tracking
Events
Experiments
Features flags
Feature flag user lists
Freeze periods
Geo nodes
GitLab Pages
Group activity analytics
Group Import/Export
Group repository storage moves
Group wikis
Groups
Import
Instance-level CI/CD variables
Invitations
Issue boards (project)
Issue boards (group)
Issues
Issues (epic)
Issues statistics
Iterations (project)
Iterations (group)
Jobs
Job artifacts
Keys
Labels (project)
Labels (group)
License
Licenses (templates)
Links (issue)
Links (epic)
Managed licenses
Markdown
Maven
Members
Merge request approvals
Merge request context commits
Merge requests
Merge trains
Milestones (project)
Milestones (group)
Namespaces
Notes (comments)
Notification settings
NuGet
Packages
Pages domains
Personal access tokens
Pipelines schedules
Pipeline triggers
Pipelines
Plan limits
Project access tokens
Project aliases
Project import/export
Project remote mirrors
Project repository storage moves
Project statistics
Project templates
Project vulnerabilities
Projects
Protected branches
Protected environments
Protected tags
PyPI
Releases
Release links
Repositories
Repository files
Repository submodules
Resource iteration events
Resource label events
Resource milestone events
Resource state events
Resource weight events
Ruby gems
Runners
SCIM
Search
Services
Settings (application)
Sidekiq metrics
Sidekiq queues
Snippet repository storage moves
Snippets
Snippets (project)
Statistics (application)
Suggestions
System hooks
Tags
To-Do lists
Users
User-starred metrics dashboards
Variables (project)
Variables (group)
Version
Visual Review discussions
Vulnerabilities
Vulnerability export
Vulnerability Findings
Wikis
GraphQL
Getting started
GraphQL reference
Create audit report (example)
Identify issue boards (example)
Removed items
v3 to v4
Lint .gitlab-ci.yml
GitLab as an OAuth2 provider
Omnibus GitLab
Architecture
Omnibus packages and images
Package information
Package defaults
Package licensing
Package signatures
Installation
Container Registry
Deprecation policy
Deprecated OSes
Docker images
Manual installation
Install JiHu Edition
Configure
Custom environment variables
Backups
Database
GitLab Mattermost
Grafana
High availability roles
LDAP
Logs
NGINX
Gitaly Cluster
Prometheus
Puma
Raspberry Pi
Redis
SMTP
SSL
DNS
Unicorn
Image scaling
Memory-constrained environments
Release process
OpenShift release process
Update
Convert to Omnibus
Package signatures
GitLab 13 changes
GitLab 12 changes
GitLab 11 changes
GitLab 10 changes
Maintain
Troubleshoot
GitLab Runner
Install
Docker
FreeBSD
Kubernetes (Helm Chart)
Kubernetes (Agent)
Linux
macOS
OpenShift
Windows
Bleeding edge releases
Official Linux packages
Old GitLab Runner URLs
Configure
Advanced config
Autoscale config
Autoscale on AWS EC2
Autoscale on AWS Fargate
Commands
Feature flags
OpenShift
Running behind a proxy
Rate limited requests
Self-signed certificates
System services
Speed up job execution
Register
Examples
Executors
Custom
libvirt
LXD
Docker
Docker Machine
Kubernetes
Shell
SSH
Parallels
Virtual Box
Monitor
Security
Shells
Troubleshoot
Best Practices
GitLab Helm Charts
Install
Required tools
Cloud cluster preparation
AKS
EKS
GKE
OpenShift
Deploy
Upgrade
Backup and Restore
Backup
Restore
Migrate from Omnibus
Version mappings
Configure
Globals
GitLab sub-charts
Gitaly chart
GitLab Exporter chart
GitLab Grafana chart
GitLab Pages chart
GitLab Runner chart
GitLab Shell chart
KAS chart
Migrations chart
Praefect chart
Sidekiq chart
Task Runner chart
Webservice chart
Minio chart
Nginx chart
Redis chart
Redis HA chart
Registry chart
Advanced
External database
External Gitaly
External Mattermost
External Nginx
External object storage
External Redis
Persistent volumes
Troubleshoot
Contribute to GitLab
Get started
Changelog entries
Community roles
Design and UI
GitLab Development Kit
Issues workflow
Merge request workflow
Code review guidelines
Style guides
Architecture
CI/CD development
CI/CD template development
Auto DevOps development
Code intelligence
Danger bot
Database development
Case study - filtering by label
Case study - namespaces storage statistics
Database review guidelines
Migrations style guide
SQL guidelines
Understanding EXPLAIN plans
Avoiding downtime in migrations
Developer guide to logging
Development Rake tasks
Mass insert Rails models
Documentation
Style guide
GraphQL style guide
RESTful API style guide
Topic types
Process
Testing
Site architecture
Global navigation
Deployment process
Release process
Distributed tracing
Experiments
Experimentation module
GLEX
Feature flags for GitLab development
Controlling feature flags
Documenting feature flags
Framework - DeclarativePolicy
Frontend development
Accessibility
Architecture
Axios
Dark mode
Design patterns
Development process
Droplab
Emojis
Filter
Frontend FAQ
GraphQL
Icons and SVG illustrations
InputSetter
Performance
Principles
Security
Tooling
Vuex
Vue
Widgets
GitLab Pages development
Geo development
Geo framework
Gitaly development
GitLab Design System
GitLab development style guides
API style guide
Go standards and style guidelines
GraphQL API style guide
Guidelines for shell commands
HTML style guide
JavaScript style guide
Newlines style guide
Python development guidelines
SCSS style guide
Shell scripting standards and style guidelines
Sidekiq debugging
Sidekiq style guide
Vue style guide
GitLab group migration
GitLab project pipelines
GitLab Runner
Review GitLab Runner
Add new Windows version support for Docker executor
GraphQL development
GraphQL BatchLoader
GraphQL pagination
GraphQL Pro
Helm Charts
Architecture of Cloud native GitLab Helm charts
Backup and Restore
Goals
Architecture
Design Decisions
Resource Usage
Environment setup
Style guide
Versioning and release
Import/Export
Instrumenting Ruby code
Issuable-like Rails models utilities
Issue types
Kubernetes Agent
Routing kas requests
Repository overview
Identity and authentication
User stories
GitOps with the Kubernetes Agent
Running locally
Kubernetes integration
Omnibus GitLab
Build locally
Build Omnibus GitLab package
Build all-in-one Docker image
Information for GitLab team members
Set up a development environment
Config options
Changing YAML config options
Adding deprecation messages
Adding new gitlab-ctl commands
Adding new services
Adding new software definitions
Creating patches
Installing OpenShift
Managing PostgreSQL versions
Working with public_attributes.json
Package development
Permissions guide
Testing standards and styles
Flaky tests
Frontend testing standards and style guidelines
GitLab tests in CI context
Review apps
Smoke tests
Testing best practices
Testing levels
Testing Rails migrations
Testing Rake tasks
Testing (end to end)
Beginner's guide to writing end-to-end tests
Best practices when writing end-to-end tests
Dynamic element validation
Flows in GitLab QA
Page objects in GitLab QA
Resource class in GitLab QA
Style guide for writing end-to-end tests
Testing with feature flags
Translate GitLab
Externalization
Translation
Proofreading
Merging
Snowplow guide
Usage Ping guide
Metrics dictionary guide
Metrics dictionary
Value Stream Analytics
Working with Prometheus metrics
GitLab Docs
Contributor and Development Docs
Performance Guidelines
Workflow
Tooling
Benchmarks
Profiling
Development
Production
RSpec profiling
Memory optimization
Memory allocations
Checking memory pressure of own code
Different types of allocations
Using Memory Profiler
Rbtrace
Importance of Changes
Slow Operations & Sidekiq
Git Operations
Caching
String Freezing
Banzai pipelines and filters
Reading from files and other data sources
Recommendations
Anti-Patterns
Moving Allocations to Constants
How to seed a database with millions of rows
Examples
Performance Guidelines
This document describes various guidelines to follow to ensure good and
consistent performance of GitLab.
Workflow
The process of solving performance problems is roughly as follows:
Make sure there’s an issue open somewhere (for example, on the GitLab CE issue
tracker), and create one if there is not. See #15607 for an example.
Measure the performance of the code in a production environment such as
GitLab.com (see the Tooling section below). Performance should be
measured over a period of at least 24 hours.
Add your findings based on the measurement period (screenshots of graphs,
timings, etc) to the issue mentioned in step 1.
Solve the problem.
Create a merge request, assign the “Performance” label and follow the performance review process.
Once a change has been deployed make sure to again measure for at least 24
hours to see if your changes have any impact on the production environment.
Repeat until you’re done.
When providing timings make sure to provide:
The 95th percentile
The 99th percentile
The mean
When providing screenshots of graphs, make sure that both the X and Y axes and
the legend are clearly visible. If you happen to have access to GitLab.com’s own
monitoring tools you should also provide a link to any relevant
graphs/dashboards.
Tooling
GitLab provides built-in tools to help improve performance and availability:
Profiling.
Distributed Tracing
GitLab Performance Monitoring.
Request Profiling.
QueryRecoder for preventing N+1 regressions.
Chaos endpoints for testing failure scenarios. Intended mainly for testing availability.
Service measurement for measuring and logging service execution.
GitLab team members can use GitLab.com’s performance monitoring systems located at
dashboards.gitlab.net, this requires you to log in using your
@gitlab.com email address. Non-GitLab team-members are advised to set up their
own Prometheus and Grafana stack.
Benchmarks
Benchmarks are almost always useless. Benchmarks usually only test small bits of
code in isolation and often only measure the best case scenario. On top of that,
benchmarks for libraries (such as a Gem) tend to be biased in favour of the
library. After all there’s little benefit to an author publishing a benchmark
that shows they perform worse than their competitors.
Benchmarks are only really useful when you need a rough (emphasis on “rough”)
understanding of the impact of your changes. For example, if a certain method is
slow a benchmark can be used to see if the changes you’re making have any impact
on the method’s performance. However, even when a benchmark shows your changes
improve performance there’s no guarantee the performance also improves in a
production environment.
When writing benchmarks you should almost always use
benchmark-ips. Ruby’s Benchmark
module that comes with the standard library is rarely useful as it runs either a
single iteration (when using Benchmark.bm) or two iterations (when using
Benchmark.bmbm). Running this few iterations means external factors, such as a
video streaming in the background, can very easily skew the benchmark
statistics.
Another problem with the Benchmark module is that it displays timings, not
iterations. This means that if a piece of code completes in a very short period
of time it can be very difficult to compare the timings before and after a
certain change. This in turn leads to patterns such as the following:
Benchmark.bmbm(10) do |bench|
bench.report 'do something' do
100.times do
... work here ...
end
end
end
This however leads to the question: how many iterations should we run to get
meaningful statistics?
The benchmark-ips Gem basically takes care of all this and much more, and as a
result of this should be used instead of the Benchmark module.
In short:
Don’t trust benchmarks you find on the internet.
Never make claims based on just benchmarks, always measure in production to
confirm your findings.
X being N times faster than Y is meaningless if you don’t know what impact it
has on your production environment.
A production environment is the only benchmark that always tells the truth
(unless your performance monitoring systems are not set up correctly).
If you must write a benchmark use the benchmark-ips Gem instead of Ruby’s
Benchmark module.
Profiling
By collecting snapshots of process state at regular intervals, profiling allows
you to see where time is spent in a process. The
Stackprof gem is included in GitLab,
allowing you to profile which code is running on CPU in detail.
It’s important to note that profiling an application alters its performance.
Different profiling strategies have different overheads. Stackprof is a sampling
profiler. It samples stack traces from running threads at a configurable
frequency (e.g. 100hz, that is 100 stacks per second). This type of profiling
has quite a low (albeit non-zero) overhead and is generally considered to be
safe for production.
Development
A profiler can be a very useful tool during development, even if it does run in
an unrepresentative environment. In particular, a method is not necessarily
troublesome just because it’s executed many times, or takes a long time to
execute. Profiles are tools you can use to better understand what is happening
in an application - using that information wisely is up to you!
Keeping that in mind, to create a profile, identify (or create) a spec that
exercises the troublesome code path, then run it using the bin/rspec-stackprof
helper, for example:
$ LIMIT=10 bin/rspec-stackprof spec/policies/project_policy_spec.rb
8/8 |====== 100 ======>| Time: 00:00:18
Finished in 18.19 seconds (files took 4.8 seconds to load)
8 examples, 0 failures
==================================
Mode: wall(1000)
Samples: 17033 (5.59% miss rate)
GC: 1901 (11.16%)
==================================
TOTAL
(pct)
SAMPLES
(pct)
FRAME
6000
(35.2%)
2566
(15.1%)
Sprockets::Cache::FileStore#get
2018
(11.8%)
888
(5.2%)
ActiveRecord::ConnectionAdapters::PostgreSQLAdapter#exec_no_cache
1338
(7.9%)
640
(3.8%)
ActiveRecord::ConnectionAdapters::PostgreSQL::DatabaseStatements#execute
3125
(18.3%)
394
(2.3%)
Sprockets::Cache::FileStore#safe_open
913
(5.4%)
301
(1.8%)
ActiveRecord::ConnectionAdapters::PostgreSQLAdapter#exec_cache
288
(1.7%)
288
(1.7%)
ActiveRecord::Attribute#initialize
246
(1.4%)
246
(1.4%)
Sprockets::Cache::FileStore#safe_stat
295
(1.7%)
193
(1.1%)
block (2 levels) in class_attribute
187
(1.1%)
187
(1.1%)
block (4 levels) in class_attribute
You can limit the specs that are run by passing any arguments rspec would
normally take.
The output is sorted by the Samples column by default. This is the number of
samples taken where the method is the one currently being executed. The Total
column shows the number of samples taken where the method, or any of the methods
it calls, were being executed.
To create a graphical view of the call stack:
stackprof tmp/project_policy_spec.rb.dump --graphviz > project_policy_spec.dot
dot -Tsvg project_policy_spec.dot > project_policy_spec.svg
To load the profile in KCachegrind:
stackprof tmp/project_policy_spec.rb.dump --callgrind > project_policy_spec.callgrind
kcachegrind project_policy_spec.callgrind # Linux
qcachegrind project_policy_spec.callgrind # Mac
For flame graphs, enable raw collection first. Note that raw
collection can generate a very large file, so increase the INTERVAL, or
run on a smaller number of specs for smaller file size:
RAW=true bin/rspec-stackprof spec/policies/group_member_policy_spec.rb
You can then generate, and view the resultant flame graph. It might take a
while to generate based on the output file size:
# Generate
stackprof --flamegraph tmp/group_member_policy_spec.rb.dump > group_member_policy_spec.flame
# View
stackprof --flamegraph-viewer=group_member_policy_spec.flame
It may be useful to zoom in on a specific method, for example:
$ stackprof tmp/project_policy_spec.rb.dump --method warm_asset_cache
TestEnv#warm_asset_cache (/Users/lupine/dev/gitlab.com/gitlab-org/gitlab-development-kit/gitlab/spec/support/test_env.rb:164)
samples:
0 self (0.0%)
6288 total (36.9%)
callers:
6288
100.0%)
block (2 levels) in <top (required)>
callees (6288 total):
6288
100.0%)
Capybara::RackTest::Driver#visit
code:
164
def warm_asset_cache
165
return if warm_asset_cache?
166
return unless defined?(Capybara)
167
6288
(36.9%)
168
Capybara.current_session.driver.visit '/'
169
end
$ stackprof tmp/project_policy_spec.rb.dump --method BasePolicy#abilities
BasePolicy#abilities (/Users/lupine/dev/gitlab.com/gitlab-org/gitlab-development-kit/gitlab/app/policies/base_policy.rb:79)
samples:
0 self (0.0%)
50 total (0.3%)
callers:
50.0%)
BasePolicy.abilities
50.0%)
BasePolicy#collect_rules
callees (50 total):
50.0%)
ProjectPolicy#rules
50.0%)
BasePolicy#collect_rules
code:
def abilities
return RuleSet.empty if @user && @user.blocked?
return anonymous_abilities if @user.nil?
(0.3%)
collect_rules { rules }
end
Since the profile includes the work done by the test suite as well as the
application code, these profiles can be used to investigate slow tests as well.
However, for smaller runs (like this example), this means that the cost of
setting up the test suite tends to dominate.
Production
Stackprof can also be used to profile production workloads.
In order to enable production profiling for Ruby processes, you can set the STACKPROF_ENABLED environment variable to true.
The following configuration options can be configured:
STACKPROF_ENABLED: Enables Stackprof signal handler on SIGUSR2 signal.
Defaults to false.
STACKPROF_MODE: See sampling modes.
Defaults to cpu.
STACKPROF_INTERVAL: Sampling interval. Unit semantics depend on STACKPROF_MODE.
For object mode this is a per-event interval (every nth event is sampled)
and defaults to 1000.
For other modes such as cpu this is a frequency and defaults to 10000 μs (100hz).
STACKPROF_FILE_PREFIX: File path prefix where profiles are stored. Defaults
to $TMPDIR (often corresponds to /tmp).
STACKPROF_TIMEOUT_S: Profiling timeout in seconds. Profiling will
automatically stop after this time has elapsed. Defaults to 30.
STACKPROF_RAW: Whether to collect raw samples or only aggregates. Raw
samples are needed to generate flame graphs, but they do have a higher memory
and disk overhead. Defaults to true.
Once enabled, profiling can be triggered by sending a SIGUSR2 signal to the
Ruby process. The process begins sampling stacks. Profiling can be stopped
by sending another SIGUSR2. Alternatively, it stops automatically after
the timeout.
Once profiling stops, the profile is written out to disk at
$STACKPROF_FILE_PREFIX/stackprof.$PID.$RAND.profile. It can then be inspected
further via the stackprof command line tool, as described in the previous
section.
Currently supported profiling targets are:
Puma worker
Sidekiq
noteThe Puma master process is not supported. Neither is Unicorn.
Sending SIGUSR2 to either of those triggers restarts. In the case of Puma,
take care to only send the signal to Puma workers.
This can be done via pkill -USR2 puma:. The : distinguishes between puma
4.3.3.gitlab.2 ... (the master process) from puma: cluster worker 0: ... (the
worker processes), selecting the latter.
For Sidekiq, the signal can be sent to the sidekiq-cluster process via pkill
-USR2 bin/sidekiq-cluster, which forwards the signal to all Sidekiq
children. Alternatively, you can also select a specific PID of interest.
Production profiles can be especially noisy. It can be helpful to visualize them
as a flame graph. This can be done
via:
bundle exec stackprof --stackcollapse /tmp/stackprof.55769.c6c3906452.profile | flamegraph.pl > flamegraph.svg
RSpec profiling
The GitLab development environment also includes the
rspec_profiling gem, which is used
to collect data on spec execution times. This is useful for analyzing the
performance of the test suite itself, or seeing how the performance of a spec
may have changed over time.
To activate profiling in your local environment, run the following:
export RSPEC_PROFILING=yes
rake rspec_profiling:install
This creates an SQLite3 database in tmp/rspec_profiling, into which statistics
are saved every time you run specs with the RSPEC_PROFILING environment
variable set.
Ad-hoc investigation of the collected results can be performed in an interactive
shell:
$ rake rspec_profiling:console
irb(main):001:0> results.count
=> 231
irb(main):002:0> results.last.attributes.keys
=> ["id", "commit", "date", "file", "line_number", "description", "time", "status", "exception", "query_count", "query_time", "request_count", "request_time", "created_at", "updated_at"]
irb(main):003:0> results.where(status: "passed").average(:time).to_s
=> "0.211340155844156"
These results can also be placed into a PostgreSQL database by setting the
RSPEC_PROFILING_POSTGRES_URL variable. This is used to profile the test suite
when running in the CI environment.
We store these results also when running nightly scheduled CI jobs on the
default branch on gitlab.com. Statistics of these profiling data are
available online. For
example, you can find which tests take longest to run or which execute the most
queries. This can be handy for optimizing our tests or identifying performance
issues in our code.
Memory optimization
We can use a set of different techniques, often in combination, to track down memory issues:
Leaving the code intact and wrapping a profiler around it.
Use memory allocation counters for requests and services.
Monitor memory usage of the process while disabling/enabling different parts of the code we suspect could be problematic.
Memory allocations
Ruby shipped with GitLab includes a special patch to allow tracing memory allocations.
This patch is available by default for
Omnibus,
CNG,
GitLab CI,
GCK
and can additionally be enabled for GDK.
This patch provides a set of 3 metrics that makes it easier to understand efficiency of memory usage for a given codepath:
mem_objects: the number of objects allocated.
mem_bytes: the number of bytes allocated by malloc.
mem_mallocs: the number of malloc allocations.
The number of objects and bytes allocated impact how often GC cycles happen.
Fewer objects allocations result in a significantly more responsive application.
It is advised that web server requests do not allocate more than 100k mem_objects
and 100M mem_bytes. You can view the current usage on GitLab.com.
Checking memory pressure of own code
There are two ways of measuring your own code:
Review api_json.log, development_json.log, sidekiq.log that includes memory allocation counters.
Use Gitlab::Memory::Instrumentation.with_memory_allocations for a given codeblock and log it.
Use Measuring module
{"time":"2021-02-15T11:20:40.821Z","severity":"INFO","duration_s":0.27412,"db_duration_s":0.05755,"view_duration_s":0.21657,"status":201,"method":"POST","path":"/api/v4/projects/user/1","mem_objects":86705,"mem_bytes":4277179,"mem_mallocs":22693,"correlation_id":"...}
Different types of allocations
The mem_* values represent different aspects of how objects and memory are allocated in Ruby:
The following example will create around of 1000 of mem_objects since strings
can be frozen, and while the underlying string object remains the same, we still need to allocate 1000 references to this string:
Gitlab::Memory::Instrumentation.with_memory_allocations do
1_000.times { '0123456789' }
end
=> {:mem_objects=>1001, :mem_bytes=>0, :mem_mallocs=>0}
The following example will create around of 1000 of mem_objects, as strings are created dynamically.
Each of them will not allocate additional memory, as they fit into Ruby slot of 40 bytes:
Gitlab::Memory::Instrumentation.with_memory_allocations do
s = '0'
1_000.times { s * 23 }
end
=> {:mem_objects=>1002, :mem_bytes=>0, :mem_mallocs=>0}
The following example will create around of 1000 of mem_objects, as strings are created dynamically.
Each of them will allocate additional memory as strings are larger than Ruby slot of 40 bytes:
Gitlab::Memory::Instrumentation.with_memory_allocations do
s = '0'
1_000.times { s * 24 }
end
=> {:mem_objects=>1002, :mem_bytes=>32000, :mem_mallocs=>1000}
The following example will allocate over 40kB of data, and perform only a single memory allocation.
The existing object will be reallocated/resized on subsequent iterations:
Gitlab::Memory::Instrumentation.with_memory_allocations do
str = ''
append = '0123456789012345678901234567890123456789' # 40 bytes
1_000.times { str.concat(append) }
end
=> {:mem_objects=>3, :mem_bytes=>49152, :mem_mallocs=>1}
The following example will create over 1k of objects, perform over 1k of allocations, each time mutating the object.
This does result in copying a lot of data and perform a lot of memory allocations
(as represented by mem_bytes counter) indicating very inefficient method of appending string:
Gitlab::Memory::Instrumentation.with_memory_allocations do
str = ''
append = '0123456789012345678901234567890123456789' # 40 bytes
1_000.times { str += append }
end
=> {:mem_objects=>1003, :mem_bytes=>21968752, :mem_mallocs=>1000}
Using Memory Profiler
We can use memory_profiler for profiling.
The memory_profiler gem is already present in the GitLab Gemfile,
you just need to require it:
require 'sidekiq/testing'
report = MemoryProfiler.report do
# Code you want to profile
end
output = File.open('/tmp/profile.txt','w')
report.pretty_print(output)
The report breaks down 2 key concepts:
Retained: long lived memory use and object count retained due to the execution of the code block.
Allocated: all object allocation and memory allocation during code block.
As a general rule, retained is always smaller than or equal to allocated.
The actual RSS cost is always slightly higher as MRI heaps are not squashed to size and memory fragments.
Rbtrace
One of the reasons of the increased memory footprint could be Ruby memory fragmentation.
To diagnose it, you can visualize Ruby heap as described in this post by Aaron Patterson.
To start, you want to dump the heap of the process you’re investigating to a JSON file.
You need to run the command inside the process you’re exploring, you may do that with rbtrace.
rbtrace is already present in GitLab Gemfile, you just need to require it.
It could be achieved running webserver or Sidekiq with the environment variable set to ENABLE_RBTRACE=1.
To get the heap dump:
bundle exec rbtrace -p <PID> -e 'File.open("heap.json", "wb") { |t| ObjectSpace.dump_all(output: t) }'
Having the JSON, you finally could render a picture using the script provided by Aaron or similar:
ruby heapviz.rb heap.json
Fragmented Ruby heap snapshot could look like this:
Memory fragmentation could be reduced by tuning GC parameters as described in this post. This should be considered as a tradeoff, as it may affect overall performance of memory allocation and GC cycles.
Importance of Changes
When working on performance improvements, it’s important to always ask yourself
the question “How important is it to improve the performance of this piece of
code?”. Not every piece of code is equally important and it would be a waste to
spend a week trying to improve something that only impacts a tiny fraction of
our users. For example, spending a week trying to squeeze 10 milliseconds out of
a method is a waste of time when you could have spent a week squeezing out 10
seconds elsewhere.
There is no clear set of steps that you can follow to determine if a certain
piece of code is worth optimizing. The only two things you can do are:
Think about what the code does, how it’s used, how many times it’s called and
how much time is spent in it relative to the total execution time (for example, the
total time spent in a web request).
Ask others (preferably in the form of an issue).
Some examples of changes that are not really important/worth the effort:
Replacing double quotes with single quotes.
Replacing usage of Array with Set when the list of values is very small.
Replacing library A with library B when both only take up 0.1% of the total
execution time.
Calling freeze on every string (see String Freezing).
Slow Operations & Sidekiq
Slow operations, like merging branches, or operations that are prone to errors
(using external APIs) should be performed in a Sidekiq worker instead of
directly in a web request as much as possible. This has numerous benefits such
as:
An error doesn’t prevent the request from completing.
The process being slow doesn’t affect the loading time of a page.
In case of a failure you can retry the process (Sidekiq takes care of
this automatically).
By isolating the code from a web request it should be easier to test
and maintain.
It’s especially important to use Sidekiq as much as possible when dealing with
Git operations as these operations can take quite some time to complete
depending on the performance of the underlying storage system.
Git Operations
Care should be taken to not run unnecessary Git operations. For example,
retrieving the list of branch names using Repository#branch_names can be done
without an explicit check if a repository exists or not. In other words, instead
of this:
if repository.exists?
repository.branch_names.each do |name|
...
end
end
You can just write:
repository.branch_names.each do |name|
...
end
Caching
Operations that often return the same result should be cached using Redis,
in particular Git operations. When caching data in Redis, make sure the cache is
flushed whenever needed. For example, a cache for the list of tags should be
flushed whenever a new tag is pushed or a tag is removed.
When adding cache expiration code for repositories, this code should be placed
in one of the before/after hooks residing in the Repository class. For example,
if a cache should be flushed after importing a repository this code should be
added to Repository#after_import. This ensures the cache logic stays within
the Repository class instead of leaking into other classes.
When caching data, make sure to also memoize the result in an instance variable.
While retrieving data from Redis is much faster than raw Git operations, it still
has overhead. By caching the result in an instance variable, repeated calls to
the same method don’t retrieve data from Redis upon every call. When
memoizing cached data in an instance variable, make sure to also reset the
instance variable when flushing the cache. An example:
def first_branch
@first_branch ||= cache.fetch(:first_branch) { branches.first }
end
def expire_first_branch_cache
cache.expire(:first_branch)
@first_branch = nil
end
String Freezing
In recent Ruby versions calling freeze on a String leads to it being allocated
only once and re-used. For example, on Ruby 2.3 or later this only allocates the
“foo” String once:
10.times do
'foo'.freeze
end
Depending on the size of the String and how frequently it would be allocated
(before the .freeze call was added), this may make things faster, but
this isn’t guaranteed.
Strings are frozen by default in Ruby 3.0. To prepare our codebase for
this eventuality, we are adding the following header to all Ruby files:
# frozen_string_literal: true
This may cause test failures in the code that expects to be able to manipulate
strings. Instead of using dup, use the unary plus to get an unfrozen string:
test = +"hello"
test += " world"
When adding new Ruby files, please check that you can add the above header,
as omitting it may lead to style check failures.
Banzai pipelines and filters
When writing or updating Banzai filters and pipelines,
it can be difficult to understand what the performance of the filter is, and what effect it might
have on the overall pipeline performance.
To perform benchmarks run:
bin/rake benchmark:banzai
This command generates output like this:
--> Benchmarking Full, Wiki, and Plain pipelines
Calculating -------------------------------------
Full pipeline
1.000
i/100ms
Wiki pipeline
1.000
i/100ms
Plain pipeline
1.000
i/100ms
-------------------------------------------------
Full pipeline
3.357
(±29.8%) i/s -
31.000
Wiki pipeline
2.893
(±34.6%) i/s -
25.000
10.677014s
Plain pipeline
15.447
(±32.4%) i/s -
119.000
Comparison:
Plain pipeline:
15.4 i/s
Full pipeline:
3.4 i/s - 4.60x slower
Wiki pipeline:
2.9 i/s - 5.34x slower
--> Benchmarking FullPipeline filters
Calculating -------------------------------------
Markdown
24.000
i/100ms
Plantuml
8.000
i/100ms
SpacedLink
22.000
i/100ms
...
TaskList
49.000
i/100ms
InlineDiff
9.000
i/100ms
SetDirection
369.000
i/100ms
-------------------------------------------------
Markdown
237.796
(±16.4%) i/s -
2.304k
Plantuml
80.415
(±36.1%) i/s -
520.000
SpacedLink
168.188
(±10.1%) i/s -
1.672k
...
TaskList
101.145
(± 6.9%) i/s -
1.029k
InlineDiff
52.925
(±15.1%) i/s -
522.000
SetDirection
3.728k (±17.2%) i/s -
34.317k in
10.617882s
Comparison:
Suggestion:
739616.9 i/s
Kroki:
306449.0 i/s - 2.41x slower
InlineGrafanaMetrics:
156535.6 i/s - 4.72x slower
SetDirection:
3728.3 i/s - 198.38x slower
...
UserReference:
2.1 i/s - 360365.80x slower
ExternalLink:
1.6 i/s - 470400.67x slower
ProjectReference:
0.7 i/s - 1128756.09x slower
--> Benchmarking PlainMarkdownPipeline filters
Calculating -------------------------------------
Markdown
19.000
i/100ms
-------------------------------------------------
Markdown
241.476
(±15.3%) i/s -
2.356k
This can give you an idea how various filters perform, and which ones might be performing the slowest.
The test data has a lot to do with how well a filter performs. If there is nothing in the test data
that specifically triggers the filter, it might look like it’s running incredibly fast.
Make sure that you have relevant test data for your filter in the
spec/fixtures/markdown.md.erb
file.
Reading from files and other data sources
Ruby offers several convenience functions that deal with file contents specifically
or I/O streams in general. Functions such as IO.read and IO.readlines make
it easy to read data into memory, but they can be inefficient when the
data grows large. Because these functions read the entire contents of a data
source into memory, memory use grows by at least the size of the data source.
In the case of readlines, it grows even further, due to extra bookkeeping
the Ruby VM has to perform to represent each line.
Consider the following program, which reads a text file that is 750MB on disk:
File.readlines('large_file.txt').each do |line|
puts line
end
Here is a process memory reading from while the program was running, showing
how we indeed kept the entire file in memory (RSS reported in kilobytes):
$ ps -o rss -p <pid>
RSS
783436
And here is an excerpt of what the garbage collector was doing:
pp GC.stat
:heap_live_slots=>2346848,
:malloc_increase_bytes=>30895288,
...
We can see that heap_live_slots (the number of reachable objects) jumped to ~2.3M,
which is roughly two orders of magnitude more compared to reading the file line by
line instead. It was not just the raw memory usage that increased, but also how the garbage collector (GC)
responded to this change in anticipation of future memory use. We can see that malloc_increase_bytes jumped
to ~30MB, which compares to just ~4kB for a “fresh” Ruby program. This figure specifies how
much additional heap space the Ruby GC claims from the operating system next time it runs out of memory.
Not only did we occupy more memory, we also changed the behavior of the application
to increase memory use at a faster rate.
The IO.read function exhibits similar behavior, with the difference that no extra memory is
allocated for each line object.
Recommendations
Instead of reading data sources into memory in full, it is better to read them line by line
instead. This is not always an option, for instance when you need to convert a YAML file
into a Ruby Hash, but whenever you have data where each row represents some entity that
can be processed and then discarded, you can use the following approaches.
First, replace calls to readlines.each with either each or each_line.
The each_line and each functions read the data source line by line without keeping
already visited lines in memory:
File.new('file').each { |line| puts line }
Alternatively, you can read individual lines explicitly using IO.readline or IO.gets functions:
while line = file.readline
# process line
end
This might be preferable if there is a condition that allows exiting the loop early, saving not
just memory but also unnecessary time spent in CPU and I/O for processing lines you’re not interested in.
Anti-Patterns
This is a collection of anti-patterns that should be avoided
unless these changes have a measurable, significant, and positive impact on
production environments.
Moving Allocations to Constants
Storing an object as a constant so you only allocate it once may improve
performance, but this is not guaranteed. Looking up constants has an
impact on runtime performance, and as such, using a constant instead of
referencing an object directly may even slow code down. For example:
SOME_CONSTANT = 'foo'.freeze
9000.times do
SOME_CONSTANT
end
The only reason you should be doing this is to prevent somebody from mutating
the global String. However, since you can just re-assign constants in Ruby
there’s nothing stopping somebody from doing this elsewhere in the code:
SOME_CONSTANT = 'bar'
How to seed a database with millions of rows
You might want millions of project rows in your local database, for example,
in order to compare relative query performance, or to reproduce a bug. You could
do this by hand with SQL commands or using Mass Inserting Rails
Models functionality.
Assuming you are working with ActiveRecord models, you might also find these links helpful:
Insert records in batches
BulkInsert gem
ActiveRecord::PgGenerateSeries gem
Examples
You may find some useful examples in this snippet.
Help & feedback
Docs
Edit this page
to fix an error or add an improvement in a merge request.
Create an issue
to suggest an improvement to this page.
Show and post comments
to review and give feedback about this page.
Product
Create an issue
if there's something you don't like about this feature.
Propose functionality
by submitting a feature request.
Join First Look
to help shape new features.
Feature availability and product trials
View pricing
to see all GitLab tiers and features, or to upgrade.
Try GitLab for free
with access to all features for 30 days.
Get Help
If you didn't find what you were looking for,
search the docs.
If you want help with something specific and could use community support,
post on the GitLab forum.
For problems setting up or using this feature (depending on your GitLab
subscription).
Request support
Please enable JavaScript to view the
comments powered by Disqus.
Twitter
Facebook
YouTube
LinkedIn
Docs Repo
About GitLab
Terms
Privacy Policy
Cookies Policy
Contact
View page source -
Edit in Web IDE
Twitter
Facebook
YouTube
LinkedIn
